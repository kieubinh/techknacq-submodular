{
  "info": {
    "authors": [
      "Smruthi Mukund",
      "Rohini Srihari"
    ],
    "book": "Proceedings of the Second Workshop on Language in Social Media",
    "id": "acl-W12-2101",
    "title": "Analyzing Urdu Social Media for Sentiments using Transfer Learning with Controlled Translations",
    "url": "https://aclweb.org/anthology/W12-2101",
    "year": 2012
  },
  "references": [
    "acl-C10-2099",
    "acl-D07-1129",
    "acl-N03-1027",
    "acl-P07-1033",
    "acl-P11-2103"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The main aim of this work is to perform sentiment analysis on Urdu blog data.",
        "We use the method of structural correspondence learning (SCL) to transfer sentiment analysis learning from Urdu newswire data to Urdu blog data.",
        "The pivots needed to transfer learning from newswire domain to blog domain is not trivial as Urdu blog data, unlike newswire data is written in Latin script and exhibits code-mixing and code-switching behavior.",
        "We consider two oracles to generate the pivots.",
        "1.",
        "Transliteration oracle, to accommodate script variation and spelling variation and 2.",
        "Translation oracle, to accommodate code-switching and code-mixing behavior.",
        "In order to identify strong candidates for translation, we propose a novel part-of-speech tagging method that helps select words based on POS categories that strongly reflect code-mixing behavior.",
        "We validate our approach against a supervised learning method and show that the performance of our proposed approach is comparable."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The ability to break language barriers and understand people's feelings and emotions towards societal issues can assist in bridging the gulf that exists today.",
        "Often emotions are captured in blogs or discussion forums where writers are common people empathizing with the situations they describe.",
        "As an example, the incident where a cricket team visiting Pakistan was attacked caused widespread anguish among the youth in that country who thought that they will no longer be able to host international tournaments.",
        "The angry emotion was towards the failure of the government to provide adequate protection for citizens and visitors.",
        "Discussion forums and blogs on cricket, mainly written by Pakistani cricket fans, around the time, verbalized this emotion.",
        "Clearly analyzing blog data helps to estimate emotion responses to domestic situations that are common to many societies.",
        "Traditional approaches to sentiment analysis require access to annotated data.",
        "But facilitating such data is laborious, time consuming and most importantly fail to scale to new domains and capture peculiarities that blog data exhibits; 1. spelling variations and 2. code mixing and code switching.",
        "3. script difference (Nastaliq vs Latin script).",
        "In this work, we present a new approach to polarity classification of code-mixed data that builds on a theory called structural correspondence learning (SCL) for domain adaptation.",
        "This approach uses labeled polarity data from the base language (in this case, Urdu newswire data - source) along with two simple oracles that provide one-one mapping between the source and the target data set (Urdu blog data).",
        "Subsequent sections are organized as follows.",
        "Section 2 describes the issues seen in Urdu blog data followed by section 3 that explains the concept of structural correspondence learning.",
        "Section 4 details the code mixing and code switching behavior seen in blog data.",
        "Section 5 describes the statistical part of speech (POS) tagger developed for blog data required to identify mixing patterns followed by the sentiment analysis model in section 6.",
        "We conclude with section 7 and briefly outline analysis and future work in section 8."
      ]
    },
    {
      "heading": "2 Urdu Blog Data",
      "text": [
        "Though non-topical text analysis like emotion detection and sentiment analysis, have been explored mostly in the English language, they have also gained some exposure in non-English languages like Urdu (Mukund and Srihari, 2010), Arabic (Mageed et al., 2011) and Hindi (Joshi and Bhattacharya, 2012).",
        "Urdu newswire data is written using Nastaliq script and follows a relatively strict grammatical guideline.",
        "Many of the techniques proposed either depend heavily on NLP features or annotated data.",
        "But, data in blogs and discussion forums especially written in a language like Urdu cannot be analyzed by using modules developed for Nastaliq script for the following reasons; (1) the tone of the text in blogs and discussion forums is informal and hence differs in the grammatical structure (2) the text is written using Latin script (3) the text exhibits code mixing and code switching behavior (with English) (4) there exists spelling errors which occur mostly due to the lack of predefined standards to represent Urdu data in Latin script.",
        "Urdish (Urdu blog data) is the term used for Urdu, which is (1) written either in Nastaliq or Latin script, and (2) contains several English words/phrases/sentences.",
        "In other words, Urdish is a name given to a language that has Urdu as the base language and English as the seasoning language.",
        "With the wide spread use of English keyboards these days, using Latin script to encode Urdu is very common.",
        "Data in Urdish is never in pure Urdu.",
        "English words and phrases are commonly used in the flow integrating tightly with the base language.",
        "Table 1 shows examples of different flavors in which Urdu appears in the internet.",
        "variations that need to be normalized Afsoos key baat hai .",
        "kal tak jo batain Non Muslim bhi kartay hoay dartay thay abhi this man has brought it out in the open.",
        "[It is sad to see that those words that even a non muslim would fear to utter till yesterday, this man had brought it out in the",
        "guage on the internet Blog data follows the order shown in example 4 of table 1.",
        "Such a code-switching phenomenon is very common in multilingual societies that have significant exposure to English.",
        "Other languages exhibiting similar behaviors are Hinglish (Hindi and English), Arabic with English and Spanglish (Spanish with English)."
      ]
    },
    {
      "heading": "3 Structural Correspondence Learning",
      "text": [
        "For a problem where domain and data changes requires new training and learning, resorting to classical approaches that need annotated data becomes expensive.",
        "The need for domain adaptation arises in many NLP tasks ?",
        "part of speech tagging, semantic role labeling, dependency parsing, and sentiment analysis and has gained high visibility in the recent years (Daumé III and Marcu, 2006; Daumé III et al., 2007; Blitzer et al., 2006, Pret-tenhofer and Stein et al., 2010).",
        "There exists two main approaches; supervised and semi-supervised.",
        "In the supervised domain adaptation approach along with labeled source data, there is also access to a small amount of labeled target data.",
        "Techniques proposed by Gildea (2001), Roark and Bacchiani (2003), Daumé III (2007) are based on the supervised approach.",
        "Studies have shown that baseline approaches (based on source only, target only or union of data) for supervised domain adaption work reasonably well and beating this is surprisingly difficult (Daumé III, 2007).",
        "In contract, the semi supervised domain adaptation approach has access to labeled data only in the source domain (Blitzer et al., 2006; Dredze et al., 2007; Prettenhofer and Stein et al., 2010).",
        "Since there is no access to labeled target data, achieving baseline performance exhibited in the supervised approach requires innovative thinking.",
        "The method of structural correspondence learning (SCL) is related to the structural learning paradigm introduced by Ando and Zhang (2005).",
        "The basic idea of structural learning is to constrain the hypothesis space of a learning task by considering multiple different but related tasks on the same input space.",
        "SCL was first proposed by Blitzer et al., (2006) for the semi supervised domain adaptation problem and works as follows (Shimizu and Nakagawa, 2007).",
        "1.",
        "A set of pivot features are defined on unla-beled data from both the source domain and the target domain 2.",
        "These pivot features are used to learn a mapping from the original feature spaces of both domains to a shared, low-dimensional real?",
        "valued feature space.",
        "A high inner product in this new space indicates a high degree of correspondence along that feature dimension 3.",
        "Both the transformed and the original features in the source domain are used to train a learning model 4.",
        "The effectiveness of the classifier in the source",
        "domain transfers to the target domain based on the mapping learnt This approach of SCL was applied in the field of cross language sentiment classification scenario by Prettenhofer and Stein (2010) where English was used as the source language and German, French and Japanese as target languages.",
        "Their approach induces correspondence among the words from both languages by means of a small number of pivot pairs that are words that process similar semantics in both the source and the target languages.",
        "The correlation between the pivots is mod-eled by a linear classifier and used as a language independent predictor for the two equivalent classes.",
        "This approach solves the classification problem directly, instead of resorting to a more general and potentially much harder problem such as machine translation.",
        "The problem of sentiment classification in blog data can be considered as falling in the realm of domain adaptation.",
        "In this work, we approach this problem using SCL tailored to accommodate the challenges that code-mixed data exhibits.",
        "Similar to the work done by Prettenhofer and Stein (2010), we look at generating pivot pairs that capture code-mixing and code-switching behavior and language change."
      ]
    },
    {
      "heading": "4 Code Switching and Code Mixing",
      "text": [
        "Code switching refers to the switch that exists from one language to another and typically involves the use of longer phrases or clauses of another language while conversing in a totally different base language.",
        "Code mixing, on the other hand, is a phenomenon of mixing words and other smaller units of one language into the structure of another language.",
        "This is mostly inter-sentential.",
        "In a society that is bilingual such as that in Pakistan and India, the use of English in the native language suggests power, social prestige and the status.",
        "The younger crowd that is technologically well equipped tends to use the switching phenomenon in their language, be it spoken or written.",
        "Several blogs, discussion forums, chat rooms etc.",
        "hold information that is expressed is intensely code mixed.",
        "Urdu blog data exhibits mix of Urdu language with English.",
        "There are several challenges associated with developing NLP systems for code-switched languages.",
        "Work done by Kumar (1986) and Sinha & Thakur, (2005) address issues and challenges associated with Hinglish (Hindi ?",
        "English) data.",
        "Dussias (2003) and Celia (1997) give an overview of the behavior of code switching occurring in Spanish - Spanglish.",
        "This phenomenon can be seen in other languages like Kannada and English, German and English.",
        "Rasul (2006) analyzes the linguistic patterns occurring in Urdish (Urdu and English) language.",
        "He tries to quantize the extent to which code-mixing occurs in media data, in particular television.",
        "Most of his rules are based on",
        "what is proposed by Kachru (1978) for Hinglish and has a pure linguistic approach with manual intervention for both qualitative and quantitative analysis.",
        "Several automated techniques proposed for Hinglish and Spanglish are in the context of machine translation and may not be relevant for a task like information retrieval since converting the data to one standardized form is not required.",
        "A more recent work was by Goyal et al., (2003) where they developed a bilingual parser for Hindi and English by treating the code mixed language as a completely different variety.",
        "However, the credibility of the system depends on the availability of WordNet1."
      ]
    },
    {
      "heading": "4.1 Understanding Mixing Patterns",
      "text": [
        "Performing analysis on data that exhibit code-switching has been attempted by many across various languages.",
        "Since the Urdu language is very similar to Hindi, in this section we discuss the code-mixing behavior based on a whole battery of work done by researchers in the Hindi language.",
        "Researchers have studied the behavior of the mixed patterns and generated rules and constraints on code-mixing.",
        "The study of code mixing with Hindi as the base language is attempted by Sinha and Thakur (2005) in the context of machine translation.",
        "They categorize the phenomenon into two types based on the extent to which mixing happens in text in the context of the main verb.",
        "Linguists such as Kachru (1996) and Poplack (1980) have tried to formalize the terminologies used in this kind of behavior.",
        "Kumar (1986) says that the motivation for assuming that the switching occurs based on certain set of rules and constraints are based on the fact that users who use this can effectively communicate with each other despite the mixed language.",
        "In his paper he proposes a set of rules and constraints for Hindi-English code switching.",
        "However, these rules and constraints have been countered by examples proposed in the literature (Agnihotri, 1998).",
        "This does not mean that researchers earlier had not considered all the possibilities.",
        "It only means that like any other language, the language of code-mixing is evolving over time but at a very fast pace.",
        "One way to address this problem of code-mixing and code switching for our task of sentiment analy",
        "sis in blog data is rely on predefined rules to identify mixed words.",
        "But this can get laborious and the rules may be insufficient to capture the latest behavior.",
        "Our approach is to use a statistical POS model to determine part of speech categories of words that typically undergo such switches."
      ]
    },
    {
      "heading": "5 Statistical Part of Speech Tagger",
      "text": [
        "Example 5.1 showcases a typical sentence seen in blog data.",
        "Example 5.2 shows the issue with spelling variations sometimes that occur in the same sentence Example 5.1: Otherwise humara bhi wohi haal hoga jo is time Palestine, Iraq, Afghanistan wagera ka hai ~ Otherwise our state will also be like what is in Palestine, Iraq, Afghanistan etc.",
        "are experiencing at this time Example 5.2: Shariyat ke aitebaar se bhi ghaur kia jaey tu aap ko ilm ho jaega key joh haraam khata hai uska dil kis tarhan ka hota hey ~ If you look at it from morals point of you too you will understand the heart of people who cheat A statistical POS tagger for blog data has to take into consideration spelling variations, mixing patterns and script change.",
        "The goal here is not to generate a perfect POS tagger for blog data (though the idea explained here can be extended for further improvisation) but to be able to identify POS categories that are candidates for switch and mix.",
        "The basic idea of our approach is as follows",
        "1.",
        "Train Latin script POS tagger (LS tagger) on pure Urdu Latin script data (Example 2 in table 1 ?",
        "using Urdu POS tag set, Muaz et al., 2009) 2.",
        "Train English POS tagger on English data (based on English tag sets, Santorini, 1990) 3.",
        "Apply LS tagger and English tagger on Urdish data and note the confidence measures of the applied tags on each word 4.",
        "Use confidence measures, LS tags, phoneme codes (to accommodate spelling variations) as features to train a new learning model on Urdish data 5.",
        "Those words that get tagged with the English tagset are potential place holders for mixing",
        "format tries to capture the pronunciation information by assigning unique English characters to Hindi characters.",
        "Since this data is already in Latin script with each character capturing a unique pronunciation, changing this data to a form that replicates chat data using heuristic rules is trivial.",
        "However, this data is highly sanskritized and hence need to be changed by replacing Sanskrit words with equivalent Urdu words.",
        "This replacement is done by using online English to Urdu dictionaries (www.urduword.com and www.hamariweb.com).",
        "We have succeeded in replacing 20,000 pure Sanskrit words to Urdu by performing a manual lookup.",
        "The advantage with this method is that",
        "1.",
        "The whole process of setting up annotation guidelines and standards is eliminated.",
        "2.",
        "The replacement of pure Hindi words with Ur",
        "du words in most cases is one-one and the POS assignment is retained without disturbing the entire structure of the sentence.",
        "Our training data now consists of Urdu words written in Latin script.",
        "We also generate phonemes for each word by running the phonetic model.",
        "A POS model is trained using CRF (Lafferty, 2001) learning method with current word, previous word and the phonemes as features.",
        "This model called the Latin Script (LS) POS model has an F-score of 83%.",
        "English POS tagger is the Stanford tagger that has a tagging accuracy of about 98.7%2 ."
      ]
    },
    {
      "heading": "5.1 Approach",
      "text": [
        "Urdish blog data consists of Urdu code-mixed with English.",
        "Running simple Latin script based Urdu POS tagger results in 81.2% accuracy when POS tags on the entire corpus is considered and 52.3% 2 http://nlp.stanford.edu/software/tagger.shtml accuracy on only the English words.",
        "Running English tagger on the entire corpus improves the POS tagging accuracy of English words to 79.2% accuracy.",
        "However, the tagging accuracy on the entire corpus reduces considerably ?",
        "55.4%.",
        "This indicates that identifying the language of the words will definitely improve tagging.",
        "Identifying the language of the words can be done simply by a lexicon lookup.",
        "Since English words are easily accessible and more enriched, English Wordnet3 makes a good source to perform this lookup.",
        "Running Latin script POS tagger and English tagger on the language specific words resulted in 79.82% accuracy for the entire corpus and 59.2% accuracy for English words.",
        "Clearly there is no significant gain in the performance.",
        "This is on account of English equivalent Urdu representation of words (e.g. key ~ their, more ~ peacock, bat ~ speak).",
        "Since identifying the language explicitly yields less benefit, we showcase a new approach that is based on the confidence measures of the taggers.",
        "We first run the English POS tagger on the entire corpus.",
        "This tagger is trained using a CRF model.",
        "Scores that indicate the confidence with which this tagger has applied tags to each word in the corpus is also estimated (table 2).",
        "Next, the Latin script tagger is applied on the entire corpus and the confidence scores for the selected tags are estimated.",
        "So, for each word, there exist two tags, one from the English tagger and the other from the Latin script Urdish tagger along with their confidence scores.",
        "This becomes our training corpus.",
        "The CRF learning model trained on the above corpus using features shown in table 3 generates a cross validation accuracy is 90.34%.",
        "The accuracy on the test set is 88.2%, clearly indicating the advantages of the statistical approach.",
        "Features used to train Urdish POS tagger Urdish word POS tag generated by LS tagger POS tag generated by English tagger Confidence measure by LS tagger Confidence measure by English tagger",
        "hai, hum definitely dekhtai ~ I would definitely watch any movie of Babra"
      ]
    },
    {
      "heading": "6 Sentiment Polarity Detection",
      "text": [
        "The main goal of this work is to perform sentiment analysis in Urdu blog data.",
        "However, this task is not trivial owing to all the peculiarities that blog data exhibits.",
        "The work done on Urdu sentiment analysis (Mukund and Srihari, 2010) provided annotated data for sentiments in newswire domain.",
        ".",
        "Newspaper data make a good corpus to analyze different kinds of emotions and emotional traits of the people.",
        "They reflect the collective sentiments and emotions of the people and in turn the society to which they cater.",
        "When specific frames are considered (such as semantic verb frames) in the context of the triggering entities ?",
        "opinion holders (entities who express these emotions) and opinion targets (entities towards whom the emotion is directed) - performing sentiment analysis becomes more meaningful and newspapers make an excellent source to analyze such phenomena (Mukund et al., 2011).",
        "We use SCL to transfer sentiment analysis learning from this newswire data to blog data.",
        "Inspired by the work done by (Prettenhofer and Stein, 2010), we rely on oracles to generate pivot pairs.",
        "A pivot pair {wS, wT} where wS ?",
        "9S (the source language ?",
        "Urdu newswire data) and wT ?",
        "VT (the target language ?",
        "Urdish data) should satisfy two conditions 1. high support and 2. high confidence, making sure that the pairs are predictive of the task.",
        "Prettenhofer and Stein (2010) used a simple translation oracle in their experiments.",
        "However there exist several challenges with Urdish data that inhibits the use of a simple translation oracle.",
        "1.",
        "Script difference in the source and target languages.",
        "Source corpus (Urdu) is written in Nastaleeq and the target corpus (Urdish) is written in ASCII 2.",
        "Spelling variations in roman Urdu 3.",
        "Frequent use of English words to express",
        "strong emotions We use two oracles to generate pivot pairs.",
        "The first oracle accommodates the issue with spelling variations.",
        "Each Urdu word is converted to roman Urdu using IPA (1999) guidelines.",
        "Using the double metaphone algorithm4 phoneme code for the Urdu word is determined.",
        "This is also applied to Urdish data at the target end.",
        "Words that have the same metaphone code across the source and target languages are considered pivot pairs.",
        "The second oracle is a simple translation oracle between Urdu and English.",
        "Our first experiment (experiment 1) is using words that belong to the adjective part of speech category as candidates for pivots.",
        "We augment this set to include words that belong to other POS categories shown in table 4 that exhibit pattern mixing (experiment 2)."
      ]
    },
    {
      "heading": "6.1 Implementation",
      "text": [
        "The feature used to train the learning algorithm is limited to unigrams.",
        "For linear classification, we use libSVM (Chang and Lin, 2011).",
        "The computational bottleneck of this method is in the SVD decomposition of the dense parameter matrix W. We set the negative values of W to zero to get a sparse representation of the matrix.",
        "For SVD computation the Lanczos algorithm provided by SVDLIBC5 is employed.",
        "Each feature matrix used in libSVM is scaled between 1 and 1 and the final matrix for SVD is standardized to zero mean and unit variance estimated on DS U Du (source subset and target subset)."
      ]
    },
    {
      "heading": "6.2 Results",
      "text": [
        "The domain of the source data set is limited to cricket and movies in order to ensure domain over",
        "lap between newswire data that we have and blog data.",
        "In order to benchmark the proposed technique, our baseline technique is based on the conventional method of supervised learning approach on annotated data.",
        "Urdish data set used for polarity classification contains 705 sentences written in ASCII format (example 6.1).",
        "This corpus is manually annotated by one annotator (purely based on intuition and does not follow any predefined annotation guidelines) to get 440 negative sentences and 265 positive sentences.",
        "The annotated corpus is purely used for testing and in this work considered as unlabeled data.",
        "A suitable linear kernel based support vector machine is modeled on the annotated data and a fivefold cross validation on this set gives an F-Measure of 64.3%.",
        "Example 6.1: General zia-ul-haq ke zamane mai qabayli elaqe Russia ke khilaf jang ka merkaz thea aur general Pervez Musharraf ke zamane mai ye qabayli elaqe Pakistan ke khilaf jang ka markaz ban gye .",
        "~ negative Our first experiment is based on using the second oracle for translations on only adjectives (most obvious choice for emotion words).",
        "We use 438 pivot pairs.",
        "The average F-measure for the performance is at 55.78% which is still much below the baseline performance of 64.3% if we had access to annotated data.",
        "However, the results show the ability of this method.",
        "Our second experiment expands the power of the second oracle to provide translations to other POS categories that exhibit pattern switching.",
        "This increased the number of pivot pairs to 640.",
        "Increase in pivots improved the precision.",
        "Also we see significant improvement in the recall.",
        "The newly added pivots brought more sentences under the radar of the transfer model.",
        "The average F-Measure increased to 59.71%.",
        "The approach can be further enhanced by improving the oracle used to select pivot features.",
        "One way is add more pivot pairs based on the correlation in the topic space across language domains (future work)."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "In this work we show a way to perform sentiment analysis in blog data by using the method of structural correspondence learning.",
        "This method accommodates the various issues with blog data such as spelling variations, script difference, pattern switching.",
        "Table 5.",
        "SCL based polarity classification for Urdish data We rely on two oracles, one that takes care of spelling variations and the other that provides translations.",
        "The words that are selected to be translated by the second oracle are carefully chosen based on POS categories that exhibit emotions and pattern switching.",
        "We show that the performance of this approach is comparable to what is achieved by training a supervised learning model.",
        "In order to identify the POS categories that exhibit pattern switching, we developed a statistical POS tagger for Urdish blog data using a method that does not require annotated data in the target language.",
        "Through these two modules (sentiment analysis and POS tagger for Urdish data) we successfully show that the efforts in performing non-topical analysis in Urdu newswire data can easily be extended to work on Urdish data."
      ]
    },
    {
      "heading": "8 Future work",
      "text": [
        "Analyzing the test data set for missing and false positives, here are some of the examples of where the model did not work Example 7.1: ?tring tring tring tring..",
        "Phone to bar bar bajta hai.",
        "Annoying.?",
        "~ tring tring tring tring tring.. the phone rings repeatedly.",
        "Annoying.",
        "Example 7.2: ?bookon ko padna tho ab na mumkin hai.",
        "Yaha thak mere friends mujhe blindee pukarthey hai?",
        "~ cannot read books any more.",
        "Infact, my friends call me blindee.",
        "verbs as verbs but tags them as nouns.",
        "Hence,"
      ]
    },
    {
      "heading": "GRHVQ?W RFFXU LQ WKH SLYRW VHW",
      "text": [
        "Our second issue is with Morpho syntactic switching, a behavior seen in example 7.2.",
        "Nadhkarni (1975) and Pandaripande (1983) have shown that when two or more languages come into contact, there is mutual feature transfer from one language to another.",
        "The languages influence each other considerably and constraints associated with free morphemes fail in most cases.",
        "The direction and frequency of influence depends on the social status associated with the languages used in mixing.",
        "The language that has a high social status tends to use the morphemes of the lower language.",
        "Example 7.4: Bookon ?",
        "in books, Fileon ?",
        "in files, Companiyaa ?",
        "many companies Clearly we can see that English words due to their frequent contact with Urdu grammatical system tend to adopt the morphology associated with the base language and used mostly as native Urdu words.",
        "These are some issues, if addressed, will definitely improve the performance of the sentiment analysis model in Urdish data."
      ]
    }
  ]
}
