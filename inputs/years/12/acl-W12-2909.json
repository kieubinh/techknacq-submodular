{
  "info": {
    "authors": [
      "Pengfei Lu",
      "Matt Huenerfauth"
    ],
    "book": "Proceedings of the Third Workshop on Speech and Language Processing for Assistive Technologies",
    "id": "acl-W12-2909",
    "title": "Learning a Vector-Based Model of American Sign Language Inflecting Verbs from Motion-Capture Data",
    "url": "https://aclweb.org/anthology/W12-2909",
    "year": 2012
  },
  "references": [],
  "sections": [
    {
      "heading": "3 Related Work on Sign Animation Given how the association of entities with locations in space affects how signs are performed, it is not possible to pre-store all possible combinations of all the signs the system may need. For pointing signs, inflecting verbs, and other space-affected signs, successful ASL systems must synthesize a specific instance of the sign as needed. Few sign language animation researchers have studied spatial inflection of verbs. There are two major types of ASL animation research: scripting software (El-liott et al., 2008; Traxler, 2000) or generation software (e.g., Fotinea et al., 2008; Huenerfauth, 2006; Marshall and Safar, 2005; VCom3D, 2012) as surveyed previously by (Huenerfauth and Han-son, 2009). Unfortunately, current generation and scripting systems for sign language animations typically do not make extensive use of spatial locations to represent entities under discussion, the output of these systems looks much like the animations without space use and without verb inflection that we evaluated in (Huenerfauth and Lu, 2012). For instance, Sign Smith Studio (VCom3D, 2012), a commercially available scripting system for ASL, contains a single uninflected version of most ASL verbs in its dictionary. To produce an inflected form of a verb, a user must use an accompanying piece of software to precisely pose a character's hands to produce a verb sign; this significantly slows down the process of scripting an ASL animation. One British Sign Language animation generator (Marshall and Safar, 2005) can associate entities under discussion with a finite number of locations in space (approximately 6). Its repertoire also includes a few verbs whose sub-ject/object are positioned at these locations. How-ever, most of the verbs handled by their system involved relatively simple motion paths for the hands from subject to object locations, and the system did not allow for the arrangement of pronominal reference points at arbitrary locations in space. Toro (Toro, 2004; 2005) focused on ASL inflected verbs; they analyzed the videos of human signers to note the 2D hand locations in the image for different verbs. Next, they wrote animation code for planning motion paths for the hands based on their observations. A limitation of this work is that asking humans to look for hand locations in a video and write down angles and coordinates is",
      "text": [
        "Using these results as intuition, we present a new model of ASL inflecting verbs in this paper, based on this ?vector?",
        "approach to modeling the movement of the signer's hands through space.",
        "We assume that what is essential to a human's performance of an inflected ASL verb is the direction that the hands travel through space, not the specific starting and ending locations in space.",
        "Thus, we model each verb example as a tuple of values: the difference between the x-, the y-, and the z-axis values for the starting and ending location of the hand.",
        "(The model has three parameters for a one-handed sign and six parameters for a two-handed sign.)",
        "Using this model, we followed a similar polynomial fitting technique summarized in section 4 ?",
        "except that we are now modeling a smaller number of parameters ?",
        "our new ?vector?",
        "model uses only three values per hand (deltax, deltay, deltaz), instead of six per hand in our prior ?point?",
        "model, which represented start and end location of the hand as (xstart, ystart, zstart, xend, yend, zend).",
        "This new model can then be used to synthesize animations of ASL verb signs for given subject and object arc positions around the signer ?",
        "the difference from our prior work is that these new models only represent the movement vector for the hands, not their specific starting and ending locations.",
        "The purpose of building a model of a verb is that we wish to use it as a parameterized lexical entry in a sign language animation synthesis sys-tem; thus, we must explain how the model can be used to synthesize a novel verb example, given its input parameters (the arc position of the subject and the object of the verb).",
        "While our new vector model predicts the motion vector for the hands, this is not enough; we need starting and ending locations for the hands (an infinite number of which are possible for a given vector).",
        "Thus, we need a way to select a starting location for the hands for a specific verb instance (and then based on the vector, we would know the ending location).",
        "We observe that, for a given verb, there are some locations in the signing space that are likely for the signer's hands to occupy and some regions that are less likely.",
        "Some motion paths through the signing space travel through high-likelihood ?pop-ular?",
        "regions of the signing space, and some, through less likely regions.",
        "Thus, we can build a Gaussian mixture model of the likelihood that a hand might occupy a specific location in the signing space during a particular ASL verb.",
        "For a given motion vector, one possible starting point in the signing space will lead to a path that travels through a maximally likely region of the signing space.",
        "Thus, we can search possible starting points for the hands for a given vector and identify an optimal path for the hands given a Gaussian mixture model of hand location likelihood.",
        "Fig.",
        "8 shows a (two-dimensional) illustration of our approach for selecting a starting location for the hand when synthesizing a verb.",
        "The concentric groups of ovals in the image represent the component Gaussians in the mixture model, which was fit on the data from the locations that one hand occupied during a signer's performances of a verb.",
        "Given the vector (direction and magnitude) for the hand's motion path for a verb (predicted by our model), we can systematically search the signing space for all possible starting locations for the hand ?",
        "to identify the starting location that yields a path through the signing space with maximum probability (as predicted by the Gaussian model).",
        "The arrows shown in Fig. 8 represent a few possible paths for the hand given several possible starting locations, and one of these arrows travels a path through the model with maximum probability.",
        "Gaussian model for the locations of the hand during this verb (TELL is a one-handed verb).",
        "When we need to synthesize a verb, then we use our vector model to predict a movement vector for the hands, and then we perform a grid search through the signing space (in the x, y, and z dimensions) to identify an optimal starting location for the hand.",
        "If runtime efficiency is a concern, optimization or estimation methods could be applied to this search.",
        "In summary, the vector direction and magnitude of the hands are based on a model that is parame-terized on: the verb, the location of the subject on an arc around the signer, and the location of the object on this arc.",
        "When a specific instance of a verb must be synthesized, a starting point for the hand is selected that maximizes the probability of the entire trajectory of the hands through space, based on a Gaussian mixture model specific to that verb (but not parameterized on any specific sub-ject/object locations in space).",
        "All instances of the verb in the training data were used to train the mixture model, due to data sparseness considerations.",
        "7 Distance Metric Evaluation Because the premise of this paper is that models of ASL verbs based on a motion vector representation would do a better job of capturing the essential aspects of a verb's motion path across signers, we conducted an inter-signer cross-validation of our new model.",
        "We built separate models on the data from each of our three signers, and then we compared the resulting model's predictions for all 42 verb instances collected from the other two signers.",
        "For comparison purposes, we also trained three models (one per signer) using the ?point?-based model from our prior work (Lu and Huenerfauth, 2011).",
        "Fig.",
        "9 presents the results; the values of each bar are the average ?error?",
        "for each synthesized verb example for all five ASL verbs in Table 1.",
        "The error score for a verb example is the average of four values: (1) Euclidean distance between the start location of the right hand as predicted by the model and the start location of the right hand of the human signer data being used for evaluation, (2) same for the end location for the right hand, (3) same for the start location for the left hand, and (4) same for end location for the left hand.",
        "Fig.",
        "9 shows that the new ?vector?",
        "model has lower error scores than our older ?point?",
        "model presented in prior work.",
        "To interpret the Euclidean distance value, it is useful to know that the scale of the coordinate space used for the verb model is set such that shoulder width of a signer would be 1.0.",
        "As a baseline for comparison, the average inter-signer variation (based on the values shown in Fig. 7) is also plotted in Fig. 9.",
        "ers).",
        "For comparison purposes, we also trained three models (one based on each of the three two-signer data sets) using the ?point?-based model from our prior work (Lu and Huenerfauth, 2011).",
        "Fig.",
        "10 shows the results for two of the verbs in Table 1 (ASK and GIVE); the ?vector?",
        "model has lower error scores than our older ?point?",
        "model.",
        "Fig.",
        "10.",
        "Evaluation of the ?Point?",
        "and ?Vector?",
        "models trained on a small ?mixed?",
        "data set from two signers.",
        "Examples of animations of the ASL verbs synthesized using each of these models are on our lab website: http://latlab.cs.qc.cuny.edu/slpat2012/ 8 Conclusion And Future Work This paper presented and evaluated a new method of constructing a lexicon of ASL verb signs whose motion path depends on the location in the signing space associated with the verb's subject and object.",
        "We used motion capture data from multiple signers to evaluate whether our new models do a better job of capturing the signer-invariant and occasion-invariant aspect of an ASL inflected verb's move-ment, compared to our prior modeling approach.",
        "The parameterized models of ASL verb movements produced in this paper could be used to synthesize a desired verb instance for a potentially infinite number of arrangements of the subject and object of the verb in the signing space ?",
        "based on the collection of a finite number of examples of a verb performance from a human signer.",
        "Using this technique, generation software could include flexible lexicons that can be used to synthesize an infinite variety of inflecting verb in-stances, and scripting software could more easily enable users to include inflecting verbs in a sentence (without requiring the user to create a custom animations of a body movement for a particular inflected verb sign).",
        "While this paper demonstrates our method on five ASL verbs, this technique should be applicable to more ASL verbs, more ASL signs parameterized on spatial locations, and signs in other sign languages used internationally.",
        "In this paper, we studied a set of ASL verbs with relatively simple motion-paths (consisting of straight line movements, which therefore only required two keyframes per verb); in future work, we may analyze verbs with more complex movements of the hands.",
        "Further, our vector models represent the magnitude (length) of the hands?",
        "motion path through space; in future work, we may explore techniques for rescaling these vector lengths.",
        "In future work, we will also use hand orientation data from our motion capture sessions to synthesize hand orientation for sign animations.",
        "We also plan to experiment with modeling how the timing of keyframes varies with subject/object positions.",
        "Finally, we also plan on conducting a user-based evaluation study using animations synthesized by the models presented in this paper ?",
        "to determine if native ASL signers who view animations containing such verbs find them to be more grammatical, understandable, and natural.",
        "Acknowledgments This material is based upon work supported in part by the US.",
        "National Science Foundation under award number 0746556 and award number 1065009, by The City University of New York PSC-CUNY Research Award Program, by Siemens A&D UGS PLM Software through a Go PLM Academic Grant, and by Visage Technologies AB through a free academic license for character animation software.",
        "Jonathan Lamberton assisted with the recruitment of participants and the conduct of experimental sessions.",
        "Kenya Bry-ant, Wesley Clarke, Kelsey Gallagher, Amanda Krieger, Giovanni Moriarty, Aaron Pagan, Jaime Penzellna, Raymond Ramirez, and Meredith Turtletaub have also assisted with data collection and contributed their ASL expertise to the project."
      ]
    }
  ]
}
