{
  "info": {
    "authors": [
      "Desislava Zhekova",
      "Sandra Kübler",
      "Joshua Bonner",
      "Marwa Ragheb",
      "Yu-Yin Hsu"
    ],
    "book": "Joint Conference on EMNLP and CoNLL – Shared Task",
    "id": "acl-W12-4509",
    "title": "UBIU for Multilingual Coreference Resolution in OntoNotes",
    "url": "https://aclweb.org/anthology/W12-4509",
    "year": 2012
  },
  "references": [
    "acl-A00-1020",
    "acl-C02-1139",
    "acl-D09-1101",
    "acl-H05-1083",
    "acl-J01-4004",
    "acl-P04-1020",
    "acl-S10-1001",
    "acl-S10-1019",
    "acl-S10-1021",
    "acl-W05-0406",
    "acl-W11-1901",
    "acl-W11-1902",
    "acl-W12-4501"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The current work presents the participation of UBIU (Zhekova and Ku?bler, 2010) in the CoNLL-2012 Shared Task: Model-ing Multilingual Unrestricted Coreference in OntoNotes (Pradhan et al., 2012).",
        "Our system deals with all three languages: Arabic, Chinese and English.",
        "The system results show that UBIU works reliably across all three languages, reaching an average score of 40.57 for Arabic, 46.12 for Chinese, and 48.70 for English.",
        "For Arabic and Chinese, the system produces high precision, while for English, precision and recall are balanced, which leads to the highest results across languages."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Multilingual coreference resolution has been gaining considerable interest among researchers in recent years.",
        "Yet, only a very small number of systems target coreference resolution (CR) for more than one language (Mitkov, 1999; Harabagiu and Maiorano, 2000; Luo and Zitouni, 2005).",
        "A first attempt at gaining insight into the comparability of systems on different languages was accomplished in the SemEval-2010 Task 1: Coreference Resolution in Multiple Languages (Recasens et al., 2010).",
        "Six systems participated in that task, UBIU (Zhekova and Ku?bler, 2010) among them.",
        "However, since systems participated across the various languages rather irregularly, Recasens et al. (2010) reported that the data points were too few to allow for a proper comparison between different approaches.",
        "Further significant issues concerned system portability across the various languages and the respective language tuning, the influence of the quantity and quality of diverse linguistic annotations as well as the performance and behavior of various evaluation metrics.",
        "The CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes (Pradhan et al., 2011) targeted unrestricted CR, which aims at identifying nominal coreference but also event coreference, within an English data set from the OntoNotes corpus.",
        "Not surprisingly, attempting to include such event mentions had a detrimental effect on overall accuracy, and the best performing systems (e.g., (Lee et al., 2011)) did not attempt event anaphora.",
        "The current shared task extends the task definition to three different languages (Arabic, Chinese and English), which can prove challenging for rule-based approaches such as the best performing system from 2011 (Lee et al., 2011).",
        "In the current paper, we present UBIU, a memory-based coreference resolution system, and its results in the CoNLL-2012 Shared Task.",
        "We give an overview of UBIU in Section 2.",
        "In Section 3, we present the system results, after which Section 4 lays out some conclusive remarks."
      ]
    },
    {
      "heading": "2 UBIU UBIU (Zhekova and Ku?bler, 2010) is a corefer",
      "text": [
        "ence resolution system designed specifically for a multilingual setting.",
        "As shown by Recasens et al. (2010), multilingual coreference resolution can be approached by various machine learning methods since machine learning provides a possibility for robust abstraction over the variation of language phenomena and specificity.",
        "Therefore, UBIU employs88 a machine learning approach, memory-based learning (MBL) since it has proven to be a good solution to various natural language processing tasks (Daelemans and van den Bosch, 2005).",
        "We employ TiMBL (Daelemans et al., 2010), which uses k nearest neighbour classification to assign class labels to the targeted instances.",
        "The classifier settings we used were determined by a non-exhaustive search over the development data and are as follows: the IB1 algorithm, similarity is computed based on weighted overlap, gain ratio is used for the relevance weights and the number of nearest neighbors is set to k=3 (cf. (Daelemans et al., 2010) for an explanation of the system parameters).",
        "In UBIU, we use a pairwise mention model (Soon et al., 2001; Broscheit et al., 2010) since this model has proven more robust towards multiple languages (Wunsch, 2009) than more elaborate ones.",
        "We concentrate on nominal coreference resolution, i.e. we ignore the more unrestricted cases of event coreference.",
        "Below, we describe the modules used in UBIU in more detail."
      ]
    },
    {
      "heading": "2.1 Preprocessing",
      "text": [
        "The preprocessing module oversees the proper formatting of the data for all modules applied in later stages during coreference resolution.",
        "During preprocessing, we use the speaker information, if provided, and replace all 1st person singular pronouns from the token position with the information provided in the speaker column and adjust the POS tag correspondingly."
      ]
    },
    {
      "heading": "2.2 Mention Detection",
      "text": [
        "Mention detection is the process of detecting the phrases that are potentially coreferent and are thus considered candidates for the coreference process.",
        "Mention detection in UBIU is based on the parse and named entity information provided by the shared task.",
        "This step is crucial for the overall system performance, and we aim for high recall at this stage.",
        "Singleton mentions that are added in this step can be filtered out in later stages.",
        "However, if we fail to detect a mention in this stage, it cannot be added later.",
        "We predict a mention for each noun phrase and named entity provided in the data.",
        "Additionally, we extract mentions for possessive pronouns in English as only those did not correspond to a noun phrase in",
        "the syntactic structure provided by the task.",
        "In Arabic and Chinese, possessives are already marked as noun phrases.",
        "The system results on mention detection on the development set are listed in Table 1.",
        "The results show that we reach very high recall but low precision, as intended.",
        "The majority of the errors are due to discrepancies between noun phrases and named entities on the one hand and mentions on the other.",
        "Furthermore, since we do not target event coreference, we do not add mentions for the verbs in the data, which leads to a reduction of recall.",
        "In all further system modules, we represent a mention by its head, which is extracted via heuristic methods.",
        "For Arabic, we select the first noun or pronoun while for Chinese and English, we extract the the pronoun or the last noun of a mention unless it is a common title.",
        "Additionally, we filter out mentions that correspond to types of named entities that in a majority of the cases in the training data are not coreferent (i.e. cardinals, ordinals, etc.).",
        "One problem with representing mentions mostly by their head is that it is difficult to decide between the different mention spans of a head.",
        "Since automatic mentions are considered correct only if they match the exact span of a gold mention, we include all identified mention spans for every extracted head for classification, which can lead to losses in evaluation.",
        "For example, consider the instance from the development set in (1): the noun phrase the Avenue of Stars is coreferent and thus marked as a gold mention (key 7).",
        "UBIU extracts two different spans for the same head Avenue: the Avenue (MD 3) and the",
        "tion length, head modification, etc.)",
        "that will allow the resolver to distinguish between the spans.",
        "The classifier decides that the shorter mention is coreferent and that the longer mention is a singleton.",
        "In order to show the effect of this decision, we assume that there is one coreferent mention to key 7.",
        "We consider the two possible spans and show the respective scores in Table 2.",
        "The evaluation in Table 2 shows that providing the correct coreference link but the wrong, short mention span, the Avenue, has considerable effects to the overall performance.",
        "First, as defined by the task, the mention is ignored by all evaluation metrics leading to a decrease in mention detection and coreference performance.",
        "Moreover, the fact that this mention is ignored means that the second mention becomes a singleton and is not considered by MUC either, leading to an F1 score of 0.",
        "This example shows the importance of selecting the correct mention span."
      ]
    },
    {
      "heading": "2.3 Singleton Classification",
      "text": [
        "A singleton is a mention which corefers with no other mention, either because it does not refer to any entity or because it refers to an entity with no other mentions in the discourse.",
        "Because singletons comprise the majority of mentions in a discourse, their presence can have a substantial effect on the performance of machine learning approaches to CR, both because they complicate the learning task and because they heavily skew the proportion in the training data towards negative instances, which can bias the learner towards assuming no coreference relation between pairs of mentions.",
        "For this reason, information concerning singletons needs to be incorporated into the CR process so that such mentions can be eliminated from consideration.",
        "Boyd et al. (2005), Ng and Cardie (2002), and Evans (2001) experimented with machine learning approaches to detect and/or eliminate singletons, finding that such a module provides an improve",
        "ment in CR performance provided that the classifier # Feature Description 1 the depth of the mention in the syntax tree 2 the length of the mention 3 the head token of the mention 4 the POS tag of the head 5 the NE of the head 6 the NE of the mention 7 PR if the head is premodified, PO if it is not; UN otherwise 8 D if the head is in a definite mention; I otherwise 9 the predicate argument corresponding to the mention 10 left context token on position token 3 11 left context token on position token 2 12 left context token on position token 1 13 left context POS tag of token on position token 3 14 left context POS tag of token on position token 2 15 left context POS tag of token on position token 1 10 right context token on position token +1 11 right context token on position token +2 12 right context token on position token +3 13 right context POS tag of token on position token +1 14 right context POS tag of token on position token +2 15 right context POS tag of token on position token +3 16 the syntactic label of the mother node 17 the syntactic label of the grandmother node 18 a concatenation of the labels of the preceding nodes 19 C if the mention is in a PP; else I",
        "does not eliminate non-singletons too frequently.",
        "Ng (2004) additionally compared various feature-and constraint-based approaches to incorporating singleton information into the CR pipeline.",
        "Feature-based approaches integrate information from the singleton classifier as features while constraint-based approaches filter singletons from the mention set.",
        "Following these works, we include a k nearest neigh-bor classifier for singleton mentions in UBIU with 19 commonly-used features described below.",
        "However, unlike Ng (2004), we use a combination of the feature-and constraint-based approaches to incorporate the classifier's results.",
        "Each training/testing instance represents a noun phrase or a named entity from the data together with features describing this phrase in its discourse.",
        "The list of features is shown in Table 3.",
        "The instances that are classified by the learner as singletons with a distance to their nearest neighbor below a threshold (i.e., half the average distance observed in the training data) are filtered from the mention set, and are thus not considered in the pairwise coreference classification.",
        "For the remainder of the mentions, the class that the singletons classifier has assigned to the instance is used as a feature in the coreference classifier.",
        "Experiments on the development set showed90",
        "gleton classifier in UBIU on the development set.",
        "that the most important features across all languages are the POS tag of the head word, definiteness, and the mother node in the syntactic representation.",
        "Information about head modification is helpful for English and Arabic, but not for Chinese.",
        "The results of using the singleton classifier in UBIU on the development set are shown in Table 4.",
        "They show a moderate improvement for all evaluation metrics and all languages, with the exception of MUC and B3 for Arabic.",
        "The most noticeable improvement can be observed in mention detection, which gains approx.",
        "2% in all languages.",
        "A manual inspection of the development data shows that the version using the singleton classifier extracts a slightly higher number of coreferent mentions than the version without.",
        "However, the reduction of mentions that are never coreferent, which was the main goal of the singleton classifier, is also present in the version without the classifier, so that the results of the classifier only have a minimal influence on the final results."
      ]
    },
    {
      "heading": "2.4 Coreference Classification",
      "text": [
        "Coreference classification is the process in which all identified mentions are paired up and features are extracted to build feature vectors that represent the mention pairs in their context.",
        "Each mention is represented in the feature vector by its syntactic head.",
        "The vectors for the pairs are then used by the memory-based learner TiMBL.",
        "As anaphoric mentions, we consider all definite phrases; we then create a pair for each anaphor with each mention preceding it within a window of 10 (English, Chinese) or 7 (Arabic) sentences.",
        "We consider a shorter window of sentences for Arabic because of its NP-rich syntactic structure and its longer sentences, which leads to an increased number of possible mention pairs.",
        "The set of features that we use, listed in Table 5, is an extension of the set by Rahman and Ng (2009).",
        "Before classification, we apply a morphological filter, which excludes vectors that disagree in number or gender (applied only if the respective information is provided or can be deduced from the data).",
        "Both the anaphor and the antecedent carry a label assigned to them by the singletons classifier.",
        "Yet, we consider as anaphoric only the heads of definite mentions.",
        "Including a feature representing the class assigned by the singletons classifier for each anaphor triggers a conservative learner behav-ior, i.e., fewer positive classes are assigned.",
        "Thus, to account for this behavior, we ignore those labels for the anaphor and include only one feature (no.",
        "25 in Table 5) in the vector for the antecedent."
      ]
    },
    {
      "heading": "2.5 Postprocessing",
      "text": [
        "In postprocessing, we create the equivalence classes of mentions that were classified as coreferent and",
        "30 the predicate argument label for mj 31 the predicate argument label for mk 32 C if both m. agree in number; else I 33 C if both m. agree in gender; else I",
        "insert the appropriate class/entity IDs in the data, removing mentions that constitute a class on their own ?",
        "singletons.",
        "We bind all pronouns (except the ones that were labeled as singletons by the singleton classifier) that were not assigned an antecedent to the last seen subject and if such is not present to the last seen mention.",
        "We consider all positively classified instances in the clustering process."
      ]
    },
    {
      "heading": "3 Evaluation",
      "text": [
        "The results of the final system evaluation are presented in Table 6.",
        "Comparing the results for mention detection (MD) on the development set (see Table 1, which shows MD before the resolution step) and the final test set (Table 6, showing MD after resolution and the deletion of singletons), we encounter a reversal of precision and recall tendencies (even though the results are not fully comparable since they are based on different data sets).",
        "This is due to the fact that during mention detection, we aim for high recall, and after coreference resolution, all mentions identified as singletons by the system are excluded from the answer set.",
        "Thus mentions that are coreferent in the key set but wrongly classified in the answer set are removed, leading to a decrease in recall.",
        "With regard to MD precision, a considerable increase is recorded, showing that the majority of the mentions that the system indicates as coreferent have the correct mention spans.",
        "Additionally, the problem of selecting the correct span (as described in Section 2) is another factor that has a considerable effect on precision at that stage ?",
        "mentions that were accurately attached to the correct coreference chain are not considered if their span is not identical to the span of their counterparts in the key set.",
        "Automatic Mention Detection In the first part in Table 6, we show the system scores for UBIU's performance when no mention information is provided in the data.",
        "We report both gold (using gold linguistic annotations) and auto (using automatically annotated data) settings.",
        "A comparison of the results shows that there are only minor differences between them with gold outperforming auto apart from Arabic for which there is a drop of 3.75 points in the gold setting.",
        "However, the small difference between all results shows that the quality of the automatic annotation is good enough for a CR system and that further improvements in the quality of the linguistic information will not necessarily improve CR.",
        "If we compare results across languages, we see that Arabic has the lowest results.",
        "One of the reasons for this decreased performance can be found in the NP-rich syntactic structure of Arabic.",
        "This leads to a high number of identified mentions and in combination with the longer sentence length to a higher92 number of training/test instances.",
        "Another reason for the drop in performance for Arabic can be found in the lack of annotations expected by our system (named entities and predicted arguments) that were not provided by the task due to time constraints and the accuracy of the annotations.",
        "Further, Arabic is a morphologically rich language for which only the simplified standard POS tags were provided and not the gold standard ones that contain much richer and thus more helpful morphology information.",
        "The results for Chinese and English are relatively close.",
        "We can also see that the CEAFE results are extremely close, with a difference of less than 1%.",
        "MUC, in contrast, shows the largest differences with more than 30% between Arabic and English in the gold setting.",
        "It is also noteworthy that the results for English show a balance between precision and recall while both Arabic and Chinese favor precision over recall in terms of mention detection, MUC, and B3.",
        "The reasons for this difference between languages need to be investigated further.",
        "Gold Mention Boundaries The results for this set of experiments is based on a version of the test set that contains the gold boundaries of all mentions, including singletons.",
        "Thus, we use these gold mention boundaries instead of the ones generated by our system.",
        "These experiments give us an insight on how well UBIU performs on selecting the correct boundaries.",
        "Since we do not expect the system's selection to be perfect, we would expect to see improved system performance given the correct boundaries.",
        "The results are shown in the second part of Table 6.",
        "As for using automatically generated mentions the tendencies in scores between gold and auto linguistic annotations are kept.",
        "A further comparison of the overall results between the two settings also shows only minor changes.",
        "The only exception is the auto setting for Arabic, for which we see drop in MD precision of approximately 5%.",
        "This also results in lower MUC and B3 precision and CEAFE recall.",
        "The reasons for this drop in performance need to be investigated further.",
        "The fact that most results for both auto and gold settings change only sightly shows that having information about the correct mention boundaries is not very helpful.",
        "Thus, the system seems to have reached its optimal performance on selecting mention boundaries given the information that it has.",
        "Gold Mentions The last set of experiments is based on a version of the test set that contains the gold mentions, i.e., all mentions that are coreferent, but without any information about the identity of the coreference chains.",
        "The results of this set of experiments gives us information about the quality of the coreference classifier.",
        "The results are shown in the third part of Table 6.",
        "Using gold parses leads to only minor improvement of the overall system performance, yet, in that case all languages, including Arabic, show consistent increase of results.",
        "Altogether, there is a major improvement of the scores in MD, MUC, and CEAFE .",
        "The B 3 scores only show minor improvements, resulting from a slight drop in precision across languages.",
        "The results also show considerably higher precision than recall for MUC and B3, and higher recall for CEAFE .",
        "This means that the coreference decisions that the system makes are highly reliable but that it still has a preference for treating coreferent mentions as singletons.",
        "A comparison across languages shows that providing gold mentions has a considerable positive effect on the system performance for Arabic since for that setting Chinese leads to lower overall scores.",
        "We assume that this is again due to the NP-rich syntactic structure of Arabic and the fact that providing the mentions decreases drastically the number of mentions the system works with and has to choose from during the resolution process."
      ]
    },
    {
      "heading": "4 Conclusion and Future Work",
      "text": [
        "We presented the UBIU system for coreference resolution in a multilingual setting.",
        "The system performed reliably across all three languages of the CoNLL 2012 shared task.",
        "For the future, we are planning an in-depth investigation of the performance of the mention detection module and the singleton classifier, as well as in investigation into more complex models for coreference classification than the mention pair model."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work is based on research supported by the US Office of Naval Research (ONR) Grant #N0001410-1-0140.",
        "We would also like to thank Kiran Kumar for his help with tuning the system.93"
      ]
    }
  ]
}
