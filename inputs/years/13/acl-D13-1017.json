{
  "info": {
    "authors": [
      "Om P. Damani",
      "Shweta Ghonge"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1017",
    "title": "Appropriately Incorporating Statistical Significance in PMI",
    "url": "https://aclweb.org/anthology/D13-1017",
    "year": 2013
  },
  "references": [
    "acl-D07-1061",
    "acl-D09-1066",
    "acl-D11-1129",
    "acl-J93-1003",
    "acl-N09-1003",
    "acl-P06-1127",
    "acl-P06-2084",
    "acl-P89-1010",
    "acl-S13-2008",
    "acl-W09-3206",
    "acl-W13-3206",
    "acl-W13-3503",
    "acl-W13-3513"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Two recent measures incorporate the notion of statistical significance in basic PMI formulation.",
        "In some tasks, we find that the new measures perform worse than the PMI.",
        "Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately.",
        "By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well.",
        "In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The notion of word association is used in many language processing and information retrieval applications and it is important to have low-cost, high-quality association measures.",
        "Lexical co-occurrence based word association measures are popular because they are computationally efficient and they can be applied to any language easily.",
        "One of the most popular co-occurrence measure is Pointwise Mutual Information (PMI) (Church and Hanks, 1989).",
        "One of the limitations of PMI is that it only works with relative probabilities and ignores the absolute amount of evidence.",
        "To overcome this, recently two new measures have been proposed that incorporate the notion of statistical significance in basic PMI formulation.",
        "In (Washtell and Markert, 2009), statistical significance is introduced in PMIsig by multiplying PMI value with the square root of the evidence.",
        "In contrast, in (Damani, 2013), cPMId is introduced by bounding the probability of observing a given deviation between a given word pair's co-occurrence count and its expected value under a null model where with each word a global unigram generation probability is associated.",
        "In Table 1, we give the definitions of PMI, PMIsig, and cPMId.",
        "While these new measures perform better than PMI on some of the tasks, on many other tasks, we find that the new measures perform worse than the PMI.",
        "In Table 3, we show how these measures perform compared to PMI on four different tasks.",
        "We find that PMIsig degrades performance in three out of these four tasks while cPMId degrades performance in two out of these four tasks.",
        "The experimental details and discussion are given in Section 4.2.",
        "Our analysis shows that while the basic ideas in incorporating statistical significance are reasonable, they have been applied slightly inappropriately.",
        "By fixing this, we get new measures that improve performance over not just PMI, but also on other popular co-occurrence measures on most of these tasks.",
        "In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also."
      ]
    },
    {
      "heading": "2 Adapting PMI for Statistical Significance",
      "text": [
        "In (Washtell and Markert, 2009), it is assumed that the statistical significance of a word pair association is proportional to the square root of the evidence.",
        "The question of what constitutes the evidence is answered by taking the lesser of the frequencies of the two words in the word pair, since at most that many pairings are possible.",
        "Hence the PMI value is multi",
        "f(x), f(y) unigram frequencies of x, y respectively in the corpus d(x), d(y) Total number of documents in the corpus containing at least one occurrence of x and y respectively f(x, y) Span-constrained (x, y) word pair frequency in the corpus d(x, y) Total number of documents in the corpus having at-least one span-constrained occurrence of the word pair (x, y) ?",
        "a parameter varying between 0 and 1",
        "between the original formulas and the revised formulas.",
        "The product max(d(x), d(y)) ?min(d(x), d(y)) in sPMId formula can be simplified to f(x) ?",
        "f(y), however, we left it this way to emphasize the transformation from cPMId.",
        "plied by ?",
        "min(f(x), f(y)) to get PMIsig.",
        "In (Damani, 2013), statistical significance is introduced by bounding the probability of observing a given number of word-pair occurrences in the corpus, just by chance, under a null model of independent unigram occurrences.",
        "For this computation, one needs to decide what constitutes a random trial when looking for a word-pair occurrence.",
        "Is it the occurrence of the first word (say x) in the pair, or the second (say y).",
        "In (Damani, 2013), occurrences of x are arbitrarily chosen to represent the sites of the random trial.",
        "Using Hoeffdings Inequality:",
        "ln ?/(?2 ?",
        "f(x)), we get ?",
        "as an upper bound on probability of observing more than f(x)?f(y)/W +f(x)?",
        "t bigram occurrences in the corpus, just by chance.",
        "Based on this Corpus Level",
        "In (Damani, 2013), several variants of cPMI are introduced that incorporate different notions of statistical significance.",
        "Of these Corpus Level Significant PMI based on Document count(cPMId - defined in Table 1) is found to be the best performing, and hence we consider this variant only in this work."
      ]
    },
    {
      "heading": "2.1 Choice of Random Trial",
      "text": [
        "While considering statistical significance, one has to decide what constitutes a random trial.",
        "When looking for a word-pair (x, y)'s occurrences, y can potentially occur near each occurrence of x, or x can potentially occur near each occurrence of y.",
        "Which of these two set of occurrences should be considered the sites of random trial.",
        "We believe that the occurrences of the more frequent of x and y should be considered, since near each of these occurrences the other word could have occurred.",
        "Hence f(x) and f(y) in cPMI definition should be replaced with max(f(x), f(y)) and min(f(x), f(y)) respectively.",
        "Similarly, d(x) and d(y) in cPMId formula should be replaced with max(d(x), d(y)) and min(d(x), d(y)) respectively to give a new measure Significant PMI based on Document count(sPMId).",
        "Using the same logic, ?",
        "min(f(x), f(y)) in PMIsig formula should be replaced with?",
        "max(f(x), f(y)) to give the formula for a new measure PMI-significant(PMIs).",
        "The definitions of sPMId and PMIs are also given in Table 1."
      ]
    },
    {
      "heading": "3 Related Work",
      "text": [
        "There are three main types of word association measures: Knowledge based, Distributional Similarity based, and Lexical Co-occurrence based.",
        "Based on Firth's You shall know a word by the company it keeps (Firth, 1957), distributional similarity based measures characterize a word by the distribution of other words around it and compare",
        "co-occurrence measures for each dataset is shown in bold and underline respectively.",
        "Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05.",
        "For each task, the best known result for different non co-occurrence based methods is also shown.",
        "two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006).",
        "They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013).",
        "Knowledge-based measures use knowledge-sources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009).",
        "Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair.",
        "In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages."
      ]
    },
    {
      "heading": "3.1 Co-occurrence Measures being Compared",
      "text": [
        "Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc.",
        "To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (?2), Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dunning, 1993), Simpson (Simpson, 1943), and T-test from these domains.",
        "In addition to these popular measures, we also experiment with other known variations of PMI like nPMI (Bouma, 2009), PMI2 (Daille, 1994), Ochiai (Janson and Vegelius, 1981), and SCI (Washtell and Markert, 2009).",
        "Since PMI2 is a monotonic transformation of Ochiai, we present their results together.",
        "In Table 2, we present the definitions of these measures.",
        "While the motivation given for SCI in (Washtell and Markert, 2009) is slightly different, in light of the discussion in Section 2.1, we can assume that SCI is PMI adapted for statistical significance (multiplied by ?",
        "f(y)), where the site of random trial is taken to be the occurrences of the second word y, instead of the less frequent word, as in the case of PMIsig.",
        "When counting co-occurrences, we only consider the non-overlapping span-constrained occurrences.",
        "The span of a word-pair's occurrence is the direction-independent distance between the occurrences of the members of the pair.",
        "We consider only those co-occurrences where span is less than a given threshold.",
        "Therefore, span threshold is a parameter for all the co-occurrence measures being considered."
      ]
    },
    {
      "heading": "4 Performance Evaluation",
      "text": [
        "Having introduced the revised measures PMIs and sPMId, we need to evaluate the performance of these measures compared to PMI and the original measures introducing significance.",
        "In addition, we also wish to compare the performance of these measures with other co-occurrence measures.",
        "To compare the performance of these measures with more resource heavy non co-occurrence based measures, we have chosen those tasks and datasets on which published results exist for distributional similarity and knowledge based word association measures."
      ]
    },
    {
      "heading": "4.1 Task Details",
      "text": [
        "We evaluate these measures on three tasks: Sentence Similarity(65 sentence-pairs from (Li et al., 2006)), Synonym Selection(50 questions ESL (Turney, 2001) and 80 questions TOEFL (Landauer and Dutnais, 1997) datasets), and, Semantic Relatedness(353 words Wordsim (Finkelstein et al., 2002) dataset).",
        "For each of these tasks, gold standard human judgment results exist.",
        "For sentence similarity, following (Li et al., 2006), we evaluate a measure by the Pearsons correlation between the ranking produced by the measure and the human ranking.",
        "For synonym selection, we compute the percentage of correct answers, since there is a unique answer for each challenge word in the datasets.",
        "Semantic relatedness has been evaluated by Spearman's rank correlation with human judgment instead of Pearsons correlation in literature and we follow the same practice to make results comparable.",
        "For sentence similarity detection, the algorithm used by us (Li et al., 2006) assumes that the association scores are between 0 and 1.",
        "Hence we normalize the value produced by each measure using",
        "brass metal plastic 15923 125088 24985 228 75 14 24 40 30 twist intertwine curl 11407 153 2047 1 9 7 17 61 41 saucer dish frisbee 2091 12453 1186 5 1 9 14 21 18 mass lump element 90398 1595 43321 14 189 4 10 29 15 applause approval friends 1998 19673 11689 8 6 9 11 29 28 confession statement plea 7687 47299 5232 76 12 18 22 45 26 swing sway bounce 33580 2994 4462 13 17 7 8 24 21 sheet leaf book 20470 20979 586581 20 194 7 2 7 12",
        "gray-row, for all other questions, incorrect answers becomes correct on using PMIs instead of PMIsig , and vice-versa for the gray-row.",
        "The association values have been suitably scaled for readability.",
        "To save space, of the four choices, options not selected by either of the methods have been omitted.",
        "These results are for a 10 word span.",
        "max-min normalization: v?",
        "= v ?min max?min where max and min are computed over all association scores for the entire task for a given measure."
      ]
    },
    {
      "heading": "4.2 Experimental Results",
      "text": [
        "We use a 1.24 Gigawords Wikipedia corpus for getting co-occurrence statistics.",
        "Since co-occurrence methods have span-threshold as a parameter, we follow the standard methodology of fivefold cross validation.",
        "Note that, in addition to span-threshold, cPMId and sPMId have an additional parameter ?.",
        "In Table 3, we present the performance of all the co-occurrence measures considered on all the tasks.",
        "Note that, except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05.",
        "For completeness of comparison, we also include the best known results from literature for different non co-occurrence based word association measures on these tasks."
      ]
    },
    {
      "heading": "4.3 Performance Analysis and Conclusions",
      "text": [
        "We find that on average, PMIsig and cPMId, the recently introduced measures that incorporate significance in PMI, do not perform better than PMI on the given datasets.",
        "Both of them perform worse than PMI on three out of four datasets.",
        "By appropriately incorporating significance, we get new measures PMIs and sPMId that perform better than PMI(also PMIsig and cPMId respectively) on most datasets.",
        "PMIs improves performance over PMI on three out of four datasets, while sPMId improves performance on all four datasets.",
        "The performance improvement of PMIs over PMIsig and of sPMId over cPMId, is not random.",
        "For example, on the ESL dataset, while the percentage of correct answers increases from 58 to 66 from PMIsig to PMIs, it is not the case that on moving from PMIsig to PMIs, several correct answers become incorrect and an even larger number of incorrect answers become correct.",
        "As shown in Table 4, only one correct answers become incorrect while seven incorrect answers get corrected.",
        "The same trend holds for most parameters values, and for moving from cPMId to sPMId.",
        "This substantiates the claim that the improvement is not random, but due to the appropriate incorporation of significance, as discussed in Section 2.1.",
        "PMIs and sPMId perform better than not just PMI, but they perform better than all popular co-occurrence measures on most of these tasks.",
        "When compared with any other co-occurrence measure, on three out of four datasets each, both PMIs and sPMId perform better than that measure.",
        "In fact, PMIs and sPMId perform reasonably well compared with more resource intensive non co-occurrence based methods as well.",
        "Note that different non co-occurrence based measures perform well on different tasks.",
        "We are comparing the performance of a single measure (say sPMId or PMIs) against the best measure for each task."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We thank Dipak Chaudhari for his help with the implementation."
      ]
    }
  ]
}
