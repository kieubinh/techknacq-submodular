{
  "info": {
    "authors": [
      "Johannes Daxenberger",
      "Iryna Gurevych"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1055",
    "title": "Automatically Classifying Edit Categories in Wikipedia Revisions",
    "url": "https://aclweb.org/anthology/D13-1055",
    "year": 2013
  },
  "references": [
    "acl-D11-1038",
    "acl-E12-1036",
    "acl-E12-1054",
    "acl-N10-1056",
    "acl-N12-1019",
    "acl-N13-1055",
    "acl-P08-2035",
    "acl-P11-4017",
    "acl-P13-1162",
    "acl-S12-1059",
    "acl-W10-3504",
    "acl-W12-4006"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we analyze a novel set of features for the task of automatic edit category classification.",
        "Edit category classification assigns categories such as spelling error correction, paraphrase or vandalism to edits in a document.",
        "Our features are based on differences between two versions of a document including meta data, textual and language properties and markup.",
        "In a supervised machine learning experiment, we achieve a micro-averaged F1 score of .62 on a corpus of edits from the English Wikipedia.",
        "In this corpus, each edit has been multi-labeled according to a 21-category taxonomy.",
        "A model trained on the same data achieves state-of-the-art performance on the related task of fluency edit classification.",
        "We apply pattern mining to automatically labeled edits in the revision histories of different Wikipedia articles.",
        "Our results suggest that high-quality articles show a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Due to its ever-evolving and collaboratively built content, Wikipedia has been the subject of many NLP studies.",
        "While the number of newly created articles in the online encyclopedia declined in the last few years (Suh et al., 2009), the number of edits in existing articles is rather stable.1 It is reasonable to assume that the latter will not change in the near"
      ]
    },
    {
      "heading": "TablesDatabaseEdits.htm",
      "text": [
        "future.",
        "One of the major reasons for the popularity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activity.",
        "Wikipedia's revision history stores all changes made to any page in the encyclopedia in separate revisions.",
        "Previous studies have exploited revision history data in tasks such as preposition error correction (Cahill et al., 2013), spelling error correction (Zesch, 2012) or paraphrasing (Max and Wisniewski, 2010).",
        "However, they all use different approaches to extract the information needed for their task.",
        "Ferschke et al. (2013) outline several applications benefiting from revision history data.",
        "They argue for a unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy.",
        "In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach.",
        "Therefore, we use the 21-category edit classification taxonomy developed in previous work (Daxenberger and Gurevych, 2012).",
        "This taxonomy enables a fine-grained analysis of edit activity in revision histories.",
        "We present the results from an automatic classification experiment, based on an annotated corpus of edits in the English Wikipedia.",
        "Additional information necessary to reproduce our results, including word lists and training, development and test data, is released online.2 To the best of our knowledge, this is the first approach allowing to classify each single edit in Wikipedia into one or more of 21 different edit categories using a supervised machine learning",
        "approach.",
        "We define our task as edit category classification.",
        "An edit is a coherent, local change which modifies a document and which can be related to certain meta data (e.g. its author, time stamp etc.).",
        "In edit category classification, we aim to detect all n edits ekv?1,v with 0 ?",
        "k < n in adjacent versions rv?1, rv of a document (we refer to the older revision as rv?1 and to the newer as rv) and assign each of them to one or more edit categories.",
        "There exist at least two main applications of edit category classification: First, a fine-grained classification of edits in collaboratively created documents such as Wikipedia articles, scientific papers or research proposals, would help us to better understand the collaborative writing process.",
        "This includes answers to questions about the kind of contribution of individual authors (Who has added substantial contents?, Who has improved stylistic issues?)",
        "and about the kind of collaboration which characterizes different articles (Liu and Ram, 2011).",
        "Second, automatic classification of edits generates huge amounts of training data for the above mentioned NLP systems.",
        "Edit category classification is related to the better known task of document pair classification.",
        "In document pair classification, a pair of documents has to be assigned to one or more categories (e.g. paraphrase/non-paraphrase, plagiarism/nonplagiarism).",
        "Here, the document may be a very short text, such as a sentence or a single word.",
        "Applications of document pair classification include plagiarism detection (Potthast et al., 2012), paraphrase detection (Madnani et al., 2012) or text similarity detection (Ba?r et al., 2012).",
        "In edit category classification, we also have two documents.",
        "However, these documents are different versions of the same text.",
        "This scenario implies certain characteristics for a well-designed feature set as we will demonstrate in this study.",
        "The main contributions of this paper are: First, we introduce a novel feature set for edit category classification.",
        "Second, we evaluate the performance of this feature set on different tasks within a corpus of Wikipedia edits.",
        "We propose the new task of edit category classification and show that our model is able to classify edits from a 21-category taxonomy.",
        "Furthermore, our model achieves state-of-the-art performance in a fluency edit classification task (Bronner and Monz, 2012).",
        "Third, we analyze collaboration patterns based on edit categories on two subsets of Wikipedia articles, namely featured and non-featured articles.",
        "We detect correlations between collaboration patterns and high-quality articles.",
        "This is demonstrated by the fact that featured articles have a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles.",
        "The rest of this paper is structured as follows.",
        "In Section 2, we motivate our experiments based on previous work.",
        "Section 3 explains our training data and the features we use for the machine learning experiments.",
        "In Section 4, we present and discuss the results of our experiments.",
        "We also demonstrate an application of our classifier model in Section 5 by mining frequent collaboration patterns in the revision histories of different articles.",
        "Finally, we draw a conclusion in Section 6."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Wikipedia is a huge data source for generating training data for edit category classification, as all previous versions of each page in the encyclopedia are stored in its revision history.",
        "Unsurprisingly, the number of studies extracting certain kinds of Wikipedia edits keeps growing.",
        "Most of these use manually defined rules or filters find the right kind of edits.",
        "Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Re-casens et al., 2013).",
        "Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits.",
        "Their features are calculated both on character-and word-level.",
        "Furthermore, they use features based on POS tags, named entities, acronyms, and a lan",
        "character-level features and named entity features show the highest improvement over the baseline.",
        "Vandalism detection in Wikipedia has mostly been defined as a binary machine learning task, where the goal is to classify a pair of adjacent revisions as vandalized or not-vandalized based on edit category features.",
        "In Adler et al. (2011), the authors group these features into meta data (author, comment and time stamp of a revision), reputation (author and article reputation), textual (language independent, i.e. token-and character-based) and language features (language dependent, mostly dictionary-based).",
        "They carry out cross-validation experiments on the PAN-WVC-10 corpus (Potthast and Holfeld, 2011).",
        "Classifiers based on reputation and text performed best.",
        "Adler et al. (2011) use Random Forests as classifier (Breiman, 2001) in their experiments.",
        "This classifier was also used in the vandalism detection study of Javanmardi et al. (2011) where it outperformed the classifiers based on Logistic Regression and Naive Bayes.",
        "Different to the approach of Bronner and Monz (2012) and previous vandalism classification studies, we built a model which accounts for multi-labeling and a fine-grained edit category system.",
        "Our feature set builds upon existing work while adding a substantial number of new features."
      ]
    },
    {
      "heading": "3 Experiments",
      "text": []
    },
    {
      "heading": "3.1 Wikipedia Edit Category Corpus",
      "text": [
        "For our experiments, we used the freely available Wikipedia Edit Category Corpus (WPEC) compiled in previous work (Daxenberger and Gurevych, 2012).",
        "In this corpus, each pair of adjacent revisions is segmented into one or more edits.",
        "This enables an accurate picture of the editing process, as an author may perform several independent edits in the same revision.",
        "Furthermore, edits are multi-labeled, i.e. each edit is assigned one or more categories.",
        "This is important for a precise description of major edits, e.g. when an entire new paragraph including text, references and markup is added.",
        "There are four basic types of edits, namely Insertions, Deletions, Modifications and Relocations.",
        "These are calculated via a line-based diff comparison on the source text (including wiki markup).",
        "As previously suggested (Daxenberger and Gurevych, 2012), inside modified lines, only the span of text which has actually been changed is marked as edit (either Insertion, Deletion or Modification), not the entire line.",
        "We extracted the data which is not contained in WPEC (meta data and plain text of rv?1 and rv) using the Java Wikipedia Library (JWPL) with the Revision Toolkit (Ferschke et al., 2011).",
        "In Daxenberger and Gurevych (2012), we divide the 21-category taxonomy into text-base (meaning-changing edits), surface (non meaning-changing edits) and Wikipedia policy (VANDALISM and REVERT) edits.",
        "Among the text-base edits, we include categories for templates, references (internal and external links), files and information, each of which is further divided into an insertion (I), deletion (D) and modification (M) category.",
        "Surface edits consist of paraphrases, spelling and grammar corrections, relocations and markup edits.",
        "The latter category contains all edits which affect markup elements that are not covered by any of the other categories and is divided into insertions, deletions and modifications.",
        "This includes, for example, apostrophes in '''bold text'''.",
        "We also suggested an OTHER category, which is intended for edits which cannot be labeled due to segmentation errors.",
        "Figure 1 shows an example edit from WPEC, labeled with the REFERENCE",
        "from Figure 1. m may refer to internal link, external link, image, template or markup element.",
        "Features marked with * have previously been mentioned in Adler et al. (2011), Javanmardi et al. (2011) or Bronner and Monz (2012).",
        "M category.",
        "WPEC was created in a manual annotation study with three annotators.",
        "The overall inter-annotator agreement measured as Krippendorf's ?",
        "is .67 (Daxenberger and Gurevych, 2012).",
        "The experiments in this study are based on the gold standard annotations in WPEC, which have been derived by means of a majority vote for each edit.",
        "WPEC consists of 981 revision pairs, segmented into 1,995 edits.",
        "We define edit category classification as a multi-label classification task.",
        "For the sake of readability, in the following we will refer to an edit ekv?1,v as ei, with ei ?",
        "E, where 0 ?",
        "i < 1995 and E is the set of all edits.",
        "An edit ei is the basic classification unit in our task.",
        "Each ei has to be labeled with a set of categories y ?",
        "C, where C is the set of all edit categories, |C |= 21."
      ]
    },
    {
      "heading": "3.2 Features for Edit Category Classification",
      "text": [
        "We grouped our features into meta data, textual, markup and language features.",
        "An overview and explanation of all features can be found in Table 1.",
        "The scheme we apply to group edit category classification features is similar to the system used by Adler et al. (2011).",
        "We reuse some of the features suggested by Adler et al. (2011), Javanmardi et al. (2011) and Bronner and Monz (2012), as marked in Table 1.",
        "Features are calculated on edited text spans.",
        "We label the edited text span corresponding to ei in rv?1 as tv?1 and the edited text span in rv as tv.",
        "In edits which are insertions, we consider tv?1 to be empty, while tv is considered empty for deletions.",
        "For Relocations, tv?1 = tv.",
        "Table 1 includes the value of each feature for the example edit from Figure 1.",
        "This edit modifies the link [[Dactyl|Dactylic]] by adding a specification to the target of that link.",
        "For spell-checking, we use British and US-American English Jazzy dictionaries.3 Markup elements are detected by the Sweble Wikitext parser (Dohrn and Riehle, 2011).",
        "Meta data features We consider the comment, author, time stamp or any other flag (?minor change?)",
        "of rv as meta data.",
        "The Wikimedia user group4 of an author specifies the edit permissions",
        "of this user (e.g. bot, administrator, blocked user).",
        "We indicate whether the revision comments or parts of it have been auto-generated.",
        "This happens when a page is blanked, i.e. all of its content has been deleted or replaced or when a new page or redirect is created (denoted by the Comment is auto-generated feature).",
        "Furthermore, edits within a specific section of an article are automatically marked by adding a prefix with the name of this section to the comment of the revision (denoted by the Auto-generated comment ratio feature).",
        "Meta data features have the same value for all edits in a (rv?1, rv)-pair.",
        "Textual features Textual features are calculated based on a certain property of the changed text.",
        "In a preprocessing step, any wiki markup inside tv?1 and tv is deleted.",
        "As for the example edit from Figure 1, tv?1 would correspond to an empty string and tv would be represented as ?",
        "(poetry)?.",
        "The n-gram feature spaces are composed of n-grams that are present either in tv?1 but not tv, or vice verse.",
        "Character n-grams only contain English alphabet characters, token n-grams consist of words excluding special characters.",
        "Markup features As opposed to textual features, wiki markup features account for the Wikimedia specific markup elements.",
        "Markup features are calculated based on the number and type of a markup element m and the surrounding context of an edit.",
        "Here, m can be a template, an external or internal link, an image or any other element used to describe markup including HTML tags.",
        "The type of m is defined by the link target for internal and external links and images, by the name of the template for templates and by the wiki markup element name for markup elements.",
        "Markup features are calculated on text spans tv?1 and tv.",
        "Naturally, wiki markup is not deleted beforehand.",
        "The edited text spans tv?1 and tv may be located inside a markup elementm (e.g. a link or a template).",
        "In such cases, our diff algorithm will not label the entire element m, but rather the actually modified text.",
        "However, such an edit may change the name of a template or the target of a link (as in the example edit from Figure 1).",
        "We therefore include the immediate context sv?1 and sv of each edit and compare the type of potential markup elements m in sv?1 and sv.",
        "Here, sv (sv?1) is defined as tv (tv?1) including all preceding and follow",
        "set.",
        "Cardinality is the average number of edit categories assigned to an edit.",
        "ing characters in rv (rv?1) which are not separated from tv (tv?1) by a boundary character (whitespace or line break).",
        "The above described features model what is actually edited in the text.",
        "A number of features are calculated on tv?1 only.",
        "These features are more likely to inform about where an edit is conducted.",
        "They specify whether tv?1 covers (i.e. contains) a certain wiki markup element and vice versa, i.e. whether tv?1 is located inside a text span that belongs to a markup element.",
        "Language Language features are calculated on the context sv?1 and sv of edits, any wiki markup is deleted.",
        "For the Explicit Semantic Analysis, we use Wiktionary (Zesch et al., 2008) and not Wikipedia assuming that the former has a better coverage with respect to different lexical classes.",
        "POS tagging was carried out using the OpenNLP POS tagger.",
        "The vandalism word list contains a hand-crafted set of around 100 vandalism and spam words from various places in the web."
      ]
    },
    {
      "heading": "3.3 Experimental Setup",
      "text": [
        "We extract features with the help of ClearTK (Ogren et al., 2008).",
        "For the machine learning part, we use Weka (Hall et al., 2009) with the Meka6 and Mu-lan (Tsoumakas et al., 2010) extensions for multi-label classification.",
        "We use DKPro Lab (Eckart de Castilho et al., 2011) to test different parameter combinations.",
        "We randomly split the gold standard data from WPEC into 80% training, 10% test and 10% development set, as shown in Table 2.",
        "Multi-label Classification We report the performance of various machine learning algorithms.",
        "A comprehensive overview of multi-label classification algorithms and evaluation measures can be",
        "classifiers and a C4.5 decision tree base classifier, as compared to random and majority category baselines.",
        "found in Madjarov et al. (2012).",
        "Multi-label classification problems are solved by either transforming the multi-label classification task into one or more single-label classification tasks (problem transformation method) or by adapting single-label classification algorithms (algorithm adaption method).",
        "Several algorithms have been developed on top of the former methods and use ensembles of such classifiers (ensemble methods).",
        "We applied the Binary Relevance approach (BR), a simple transformation method which converts the multi-label problem into |C |binary single-label problems, where |C| is the number of categories.",
        "Hence, this method trains a classifier for each category in the corpus (one-against-all).",
        "It is the most straightforward approach when dealing with multi-labeled data.",
        "However, it does not consider possible relationships or dependencies between categories.",
        "Therefore, we tested two more sophisticated methods.",
        "Hierarchy of multi-label classifiers HOMER (Tsoumakas et al., 2008) is a problem transformation method.",
        "It accounts for possibly hierarchical relationships among categories by dividing the overall category set into a treelike structure with nodes of small category sets of size k and leaves of single categories.",
        "Subsequently, a multi-label classifier is applied to each node in the tree.",
        "Random k-labelsets RAKEL (Tsoumakas et al., 2011) is an ensemble method, which randomly chooses l typically small subsets with k categories from the overall set of categories.",
        "Subsequently, all k-labelsets which are found in the multi-labeled data set are converted into new categories in a single-labeled data set using the la",
        "bel powerset transformation (Trohidis et al., 2008).",
        "HOMER and BR are among the multi-label classifiers, which Madjarov et al. (2012) recommend as benchmark methods.",
        "As underlying single-label classification algorithm, we used a C4.5 decision tree classifier (Quinlan, 1993), as decision tree classifiers yield state-of-the-art performance in the related work.",
        "Multi-label Evaluation We denote the set of relevant categories for each edit ei ?",
        "E as yi ?",
        "C and the set of predicted categories as h(ei).",
        "Evaluation measures for multi-label classification systems are based on either bipartitions or rankings.",
        "Among the former, we report example-based (weighting each edit equally) and label-based (weighting each edit category equally) measures.",
        "The accuracy of a multi-label classifier is defined as 1|E|",
        ", which corresponds to the Jac-card similarity of h(ei) and yi averaged over all edits.",
        "We report subset accuracy (exact match), calculated as 1|E|",
        "For the label-based measures, we report macro-and micro-averaged F1 scores.",
        "As a ranking-based measure, we report one error, which is defined as",
        "f(ei, c)] /?",
        "yiK, JexprK = 1 if expr is true and JexprK = 0 otherwise.",
        "f(ei, c) denotes the rank of category c ?",
        "C as predicted by the classifier.",
        "The one error measure evaluates the number of edits where the highest ranked category in the predictions is not in the set of relevant categories.",
        "It becomes smaller when the performance of the classifier increases.",
        "Table 3 shows the overall classification scores.",
        "We calculated a random baseline, which multi-labels edits at random considering the label powerset frequencies it has learned from the training data.",
        "Furthermore, we calculated a majority category baseline, which labels all edits with the most frequent edit category in the training data.",
        "In Figure 2, we list the results for each category, together with the average pairwise inter-rater agreement (F1 scores).",
        "The F1 scores are calculated based on the study we carried out in Daxenberger and Gurevych (2012).",
        "Parameters and Feature selection All parameters have been adjusted on the development set using the RAKEL classifier, aiming to optimize accuracy.",
        "With respect to the n-gram features, we tested values for n = 1, 2 and 3.",
        "For comment n-grams, unigrams turned out to yield the best overall performance, and bigrams for character and token n-grams.",
        "The word and character n-gram spaces are limited to the 500 most frequent items, the comment n-gram space is limited to the 1,500 most frequent items.",
        "To transform ranked output into bipartitions, it is necessary to set a threshold.",
        "This threshold is reported in Table 3 and has been optimized for each classifier with respect to label cardinality (average number of labels assigned to edits) on the development set.",
        "Since most of the traditional feature selection methods cannot be applied directly to multi-labeled data, we used the label powerset approach to transform the multi-labeled data into single-labeled data and subsequently applied ?2.",
        "Feature reduction to the highest-ranked features clearly improved the classifier performance on the development set.",
        "We therefore limited the feature space to the 150 highest-ranked features in our experiments.",
        "For the RAKEL classifier, we set l = 42 (twice the size of the category set) and k = 3.",
        "In HOMER, we used BR as transformation method, random distribution of categories to the children nodes and k = 3.",
        "For all other classifier parameters, we used the default settings as configured in Meka respective Mulan."
      ]
    },
    {
      "heading": "4 Discussion",
      "text": [
        "The classifiers significantly outperformed both baselines.",
        "RAKEL shows best performance for almost all measures in Table 3.",
        "The simpler BR approach, which assumes no dependencies between categories, still outperforms HOMER.",
        "We trained and tested the classifier with different feature groups (see Table 1), to analyze the importance of single types of features.",
        "As shown in Figure 2, textual features had the highest impact on classification performance.",
        "On the opposite, language features played a minor role in our experiments.",
        "Among the highest ranked individual features for the entire set of categories, we find textual (Levenshtein distance, Simple edit type), markup (Diff number",
        "agreement as average pairwise F1 scores as well as F1 scores for classifiers trained and tested on single feature groups, cf. Table 1.",
        "The number of edits labeled with each category in the test set is given in brackets.",
        "The FILE-M and TEMPLATE-M categories are omitted in this Figure, as they had no examples in the development or test set.",
        "markup elements) and meta data (Number of edits) features.",
        "Bronner and Monz (2012) report an accuracy of .88 for their best performing system on the binary classification task of distinguishing fluency and factual edits.",
        "The best performing classifier in their study was Random Forests (Breiman, 2001).",
        "To compare our features with their approach, we mapped the 21 edit categories from Daxenberger and Gurevych (2012) to the binary category set (factual vs. fluency) of Bronner and Monz (2012).",
        "Edits labeled as SPELLING/GRAMMAR, MARKUP, RELOCATION and PARAPHRASE are considered fluency edits, the remaining categories factual edits.",
        "We removed all edits labeled as OTHER, REVERT or VANDALISM from WPEC.",
        "After applying the category mapping, we deleted all edits which were labeled with both the fluency and factual category.",
        "The latter may happen due to multi-labeling.",
        "This resulted in 1,262 edits labeled as either fluency or factual.",
        "On the 80% training split from Table 2, we trained a Random Forests classifier with the optimized feature set and feature reduction as described in Section 3.3.",
        "The number of trees was set to 100, with unlimited depth.",
        "On the remaining data (test and development split), we achieved an accuracy of .90.",
        "Although we did not use the same data set as Bronner and Monz (2012), this result suggests that our feature set is suited for related tasks such as fluency detection.",
        "With respect to vandalism detection in Wikipedia, state-of-the-art systems have a performance of around .82 to .85 AUC-PR on the English Wikipedia (Adler et al., 2011).",
        "We suspect that the low performance of our system for Vandalism edits is mostly due to a lower amount of training data, a higher skew in the training and test data and the fact that we did not include features which inform about future actions (e.g. whether a revision is reverted).",
        "Error Analysis Sparseness is a major problem for some of the 21 categories, as shown in Figure 2 by categories such as FILE-D, TEMPLATE-D, MARKUP-M or PARAPHRASE which have only very few examples in training, development and test set.",
        "Categories with low inter-annotator agreement in WPEC such as MARKUP-M, PARAPHRASE or OTHER also yielded low classification accuracy.",
        "We analyzed frequent errors of the classifier with the help of a confusion matrix.",
        "PARAPHRASE edits have been confused with INFORMATION-M by the classifier.",
        "Furthermore, the classifier had problems to distinguish between VANDALISM and REVERT as well as INFORMATION-I.",
        "Generally, modifications as compared to insertions or deletions perform worse.",
        "All of the classifiers we tested, build",
        "the classifier is not able to make a prediction, if it does not have enough confidence for any of the categories.",
        "The imbalance of the data, because of the high skew in the category distribution, is another reason for classification errors.",
        "In ambiguous cases, the classifier will be biased toward the category with more examples in the training data.",
        "5 A closer look at edit sequences: Mining collaboration patterns An edit category classifier allows us to label entire article revision histories.",
        "We applied the best-performing model from Section 3.3 trained on the entire WPEC to automatically classify all edits in the Wikipedia Quality Assessment Corpus (WPQAC) as presented in previous work (Daxenberger and Gurevych, 2012).",
        "WPQAC consists of 10 featured and 10 non-featured articles7, with an overall number of 21,578 revisions (9,986 revisions from featured articles and 11,592 from non-featured articles), extracted from the April 2011 English Wikipedia dump.",
        "The articles in WPQAC are carefully chosen to form comparable pairs of featured and non-featured articles, which should reduce the noise of external influences on edit activity such as popularity or visibility.",
        "In Daxenberger and Gurevych (2012), we have shown significant differences in the edit category distribution of articles with featured status before and after the articles were featured.",
        "We concluded that articles become more stable after being featured, as shown by the higher number of surface edits and lower number of meaning-changing edits.",
        "Different to our previous approach which is based on the mere distribution of edit categories, in the present study we include the chronological order of edits and use a 10 times larger amount of data for our experiments.",
        "We segmented all adjacent revisions in WPQAC into edits, following the approach explained in Daxenberger and Gurevych (2012).",
        "During the classification process, we discarded revisions where the classifier could not assign any of the 21 edit categories with a confidence higher than the"
      ]
    },
    {
      "heading": "FA",
      "text": [
        "threshold, cf. Table 3.",
        "This resulted in 17,640 remaining revisions.",
        "We applied a sequential pattern mining algorithm with time constraints (Hirate and Yamana, 2006; Fournier-Viger et al., 2008) to the data.",
        "The latter is based on the PrefixSpan algorithm (Pei et al., 2004).",
        "Calculations have been carried out within the open-source SPMF Java data mining platform.8 We created one time-extended sequence database for the 10 featured articles and one for the 10 non-featured articles.",
        "The sequence databases consist of one row per article.",
        "Each row is a chronologically ordered list of revisions.",
        "Each revision is represented by the itemset of all edit categories for all edits in that revision (in alphabetical order).",
        "The output of the algorithm are sequential patterns with time constraints.",
        "To obtain meaningful results, we constrained the output with the following parameters: ?",
        "Minimum support: 1 (the patterns have to be present in each article) ?",
        "Time interval allowed between two successive itemsets in the patterns: 1 (patterns are extracted only from adjacent revisions) ?",
        "Minimum time interval between the first item-set and the last itemset in the patterns: 1 (the length of the patterns is 2 or higher) As this output reflects recurring sequences of adjacent revisions labeled with edit categories, we refer to it as collaboration patterns.",
        "With these parameters, the algorithm discovered 1,358 sequential patterns for featured articles and 968 for non-featured articles.",
        "The number of shared patterns in featured and non-featured articles is 427, this corresponds to the number of frequent patterns in a sequence database which contains all 20 featured and non-featured articles.",
        "The maximum length of patterns which were found was 6 for featured articles, and 5 for non-featured articles.",
        "These numbers show that the defined collaboration patterns seem to have discriminative power for different kinds of articles.",
        "Featured articles can be characterized by a higher",
        "WPQAC.",
        "degree of homogeneity with respect to their collaborative patterns due to a higher number and length of frequent sequential patterns in featured articles as compared to non-featured articles.",
        "In Table 4, we list some examples of collaboration patterns with a minimum support of 1 which we found in featured, but not non-featured articles, or vice verse.",
        "Unsurprisingly, patterns which contain combinations of the most frequent categories (INFORMATION-I, REFERENCE-I), have a high overall frequency.",
        "The diversity inside collaboration patterns measured by the number of different edit categories was higher in non-featured articles.",
        "For example, the VANDALISM - REVERT pattern was only found in non-featured articles.",
        "Patterns in featured articles tended to be more homogeneous, as shown by the first pattern in Table 4, a repetition of additions of information.",
        "We conclude that distinguished, high-quality articles, show a higher degree of homogeneity as compared to a subset of non-featured articles and the overall corpus."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "In this study, we evaluated a novel feature set for building a model to automatically classify Wikipedia edits.",
        "Using a freely available corpus (Daxenberger and Gurevych, 2012), our model achieved a micro-averaged F1 score of .62 classifying edits within a range of 21 categories.",
        "Textual features had the highest impact on classifier performance, whereas language features play a minor role.",
        "The same classifier model obtained state-of-the-art performance on the related task of fluency edit classification.",
        "Applications which potentially benefit from our work include the analysis of the writing process in collaboratively created documents, such as wikis or research papers.",
        "We have demonstrated how our model can be used to detect collaboration patterns in article revision histories.",
        "On a subset of articles from the English Wikipedia, we found that high-quality articles show a higher degree of homogeneity in their collaborative patterns as compared to random articles.",
        "Furthermore, automatic edit category classification allows to generate huge amounts of category-filtered training data for NLP tasks, e.g. spelling and grammar correction or vandalism detection.",
        "With respect to future work, we plan to include more resources, e.g. the PAN-WVC10 (Potthast and Holfeld, 2011) or WiCoPaCo (Max and Wisniewski, 2010) to increase the size of training data.",
        "A larger amount of labeled data would certainly help to improve the classifier performance for weak categories (e.g. VANDALISM and PARAPHRASE) and sparse categories (e.g. TEMPLATE-D, MARKUP-M).",
        "Based on our trained classifier, annotating more examples can be alleviated with the help of active learning."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No.",
        "I/82806, and by the Hessian research excellence program ?Landes-Offensive zur Entwicklung Wissenschaftlicho?konomischer Exzellenz?",
        "(LOEWE) as part of the research center ?Digital Humanities?.",
        "We thank the anonymous reviewers for their valuable feedback."
      ]
    }
  ]
}
