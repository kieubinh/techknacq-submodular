{
  "info": {
    "authors": [
      "Joseph Le Roux",
      "Antoine Rozenknop",
      "Jennifer Foster"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1116",
    "title": "Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization",
    "url": "https://aclweb.org/anthology/D13-1116",
    "year": 2013
  },
  "references": [
    "acl-A00-2031",
    "acl-C10-1094",
    "acl-D07-1111",
    "acl-D09-1161",
    "acl-D10-1001",
    "acl-D10-1069",
    "acl-D10-1125",
    "acl-D11-1022",
    "acl-D12-1105",
    "acl-D12-1133",
    "acl-E03-1008",
    "acl-H05-1078",
    "acl-H94-1020",
    "acl-N06-1024",
    "acl-N07-1051",
    "acl-N10-1003",
    "acl-N10-1091",
    "acl-P05-1010",
    "acl-P07-1031",
    "acl-P08-1067",
    "acl-P08-1108",
    "acl-P12-1024",
    "acl-P12-1046",
    "acl-W08-1301",
    "acl-W10-1408",
    "acl-W99-0623"
  ],
  "sections": [
    {
      "text": [
        "?",
        "Universite?",
        "Paris 13, Sorbonne Paris Cite?, LIPN, F-93430, Villetaneuse, France ?",
        "NCLT/CNGL, School of Computing, Dublin City University, Dublin 9, Ireland"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "It has recently been shown that different NLP models can be effectively combined using dual decomposition.",
        "In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way.",
        "We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23 ?",
        "this represents an absolute improvement of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model baseline.",
        "Although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to other grammar extraction strategies."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Because of the large amount of possibly contradictory information contained in a treebank, learning a phrase-structure-based parser implies making several choices regarding the prevalent annotations which have to be kept ?",
        "or discarded ?",
        "in order to guide the learning algorithm.",
        "These choices, which include whether to keep function labels and empty nodes, how to binarize the trees and whether to alter the granularity of the tagset, are often motivated empirically by parsing performance rather than by the different aspects of the language they may be able to capture.",
        "Recently Rush et al. (2010), Martins et al. (2011) and Koo et al. (2010) have shown that Dual Decomposition or Lagrangian Relaxation is an elegant",
        "framework for combining different types of NLP tasks or for building parsers from simple slave processes that only check partial well-formedness.",
        "Here we propose to follow this idea, but with a different objective.",
        "We want to mix different parsers trained on different versions of a treebank each of which makes some annotation choices in order to learn more specific or richer information.",
        "We will use state-of-the-art unlexicalized probabilistic context-free grammars with latent annotations (PCFG-LA) in order to compare our approach with a strong baseline of high-quality parses.",
        "Dual Decomposition is used to mix several systems (between two and four) that may in turn be combinations of grammars, here products of PCFG-LAs (Petrov, 2010).",
        "The systems being combined make different choices with regard to i) function labels and ii) grammar binarization.",
        "Common sense would suggest that information in the form of function labels ?",
        "syntactic labels such as SBJ and PRD and semantic labels such as TMP and LOC ?",
        "might help in obtaining a fine-grained analysis.",
        "On the other hand, the independence hypothe",
        "sis on which CFGs rely and on which most popular parsers are based may be too strong to learn the dependencies between functions across the parse trees.",
        "Also, the number of parameters increases with the use of function labels and this can affect the learning process.",
        "At first glance, binarization need not be an issue, as CFGs admit a binarized form recognizing exactly the same language.",
        "But binarization can be associated with horizontal markovization and in this case the recognized language will differ.",
        "Furthermore this can impose an unwanted emphasis on what frontier information is more relevant to learning (beginning or end of constituents).",
        "In the toy example of Figure 1, the original grammar consisting of a unique rule extracted from one tree only recognizes the string bcdef, while the grammar learned from the left binarized and markovized tree recognizes (among others) bcdef and bdcef and the grammar learned from the right binarized and markovized tree recognizes (among others) bcdef and bcedf.",
        "We find that i) retaining the function labels in non-terminal categories loses its negative impact on parsing as the number of grammars increases in PCFG-LA product models, ii) the function labels themselves can be recovered with near state-of-the-art-accuracy, iii) combining grammars with and without function labels using dual decomposition is beneficial, iv) combining left and right-binarized grammars using dual decomposition also leads to better trees and, v) our best results (a Parseval labeled F-score of 92.4, a Stanford labeled attachment score (LAS) of 93.0 and a penn2malt unlabeled attachment score (UAS) of 94.3 on Section 23 of the Wall Street Journal) are obtained by combining three grammars which encode different function label/binarization decisions.",
        "The paper is organized as follows.",
        "?",
        "2 reviews related work.",
        "?",
        "3 presents approximate PCFG-LA parsers as linear models, while ?",
        "4 shows how we can use dual decomposition to derive an algorithm for combining these models.",
        "Experimental results are presented and discussed in ?",
        "5."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Parser Model Combination It is well known that improved parsing performance can be achieved by leveraging the alternative perspectives provided by several parsing models rather than relying on just one.",
        "Examples are parser co-training (Steedman et al., 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several non-projective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012).",
        "In this last example, the models must factor in the same way: in other words, the grammars must use the same binarization scheme.",
        "In our study, we employ PCFG-LA product models with dual decomposition, and we relax the constraints on factorization, as we require only a loose coupling of the models.",
        "Function Label Parsing Although function labels have been available in the Penn Treebank (PTB) for almost twenty years (Marcus et al., 1994), they have been to a large extent overlooked in English parsing research ?",
        "most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trained on a version of the WSJ trees where the function labels have been removed.",
        "Notable exceptions are Merlo and Musillo (2005) and Gabbard et al. (2006) who each trained a parsing model on a version of the PTB with function labels intact.",
        "Gabbard et al. (2006) found that parsing accuracy was not affected by keeping the function labels.",
        "There have also been attempts to use machine learning to recover the function labels post-parsing (Blaheta and Charniak, 2000; Chrupala et al., 2007).",
        "We recover function labels as part of the parsing process, and use dual decomposition to combine parsing models with and without function labels.",
        "We are not aware of any other work that leverages the benefits of both types of models.",
        "Grammar Binarization Matsuzaki et al. (2005) compare binarization strategies for PCFG-LA parsing, and conclude that the differences between them have a minor effect on parsing accuracy as the num",
        "ber of latent annotations increases beyond two.",
        "Hall and Klein (2012) are forced to use head binarization when combining their lexicalized and unlexicalized parsers.",
        "Dual decomposition allows us to combine models with different binarization schemes."
      ]
    },
    {
      "heading": "3 Approximation of PCFG-LAs as Linear",
      "text": []
    },
    {
      "heading": "Models",
      "text": [
        "In this section, we explain how we can use PCFG-LAs to devise linear models suitable for the dual decomposition framework."
      ]
    },
    {
      "heading": "3.1 PCFG-LA",
      "text": [
        "Let us recall that PCFG-LAs are defined as tuples G = (N , T ,H,RH, S, p) where: ?",
        "N is a set of observed non-terminals, among which S is the distinguished initial symbol, ?",
        "T is a set of terminals (words), ?",
        "H is a set of latent annotations or hidden states, ?",
        "RH is a set of annotated rules, of the form a[h1] ?",
        "b[h2] c[h3] for internal rules1 and a[h1] ?",
        "w for lexical rules.",
        "Here a, b, c ?",
        "N are non-terminals, w ?",
        "T is a terminal and h1, h2, h3 ?",
        "H are latent annotations.",
        "Following Cohen et al. (2012) we also define the set of skeletal rules R, in other words, rules without hidden states, of the form a?",
        "b c or a?",
        "w. ?",
        "p : RH ?",
        "R?0 defines the probabilities associated with rules conditioned on their left-hand side.",
        "Like Petrov and Klein (2007), we impose that the initial symbol S has only one latent annotation.",
        "In other words, among rules with S on the left-hand side, only those of the form S[0]?",
        "?",
        "are inRH.",
        "With such a grammar G we can define probabilities over trees in the following way.",
        "We will consider two types of trees, annotated trees and skeletal trees.",
        "An annotated tree is a sequence of rules from RH, while a skeletal tree is a sequence of skeletal rules from R. An annotated tree TH is obtained by leftmost derivation from S[0].",
        "Its probability is:",
        "We define a projection ?",
        "from annotated trees to skeletal trees.",
        "?",
        "(TH) is a tree T isomorphic to TH with the same terminal and non-terminal symbols la-beling nodes, without hidden states.",
        "The probability of a skeletal tree T is a sum of the probabilities of all annotated trees that admit T as their projection.",
        "PCFG-LA parsing amounts to, given a sequence of words, finding the most probable skeletal tree with this sequence as its yield according to a gram",
        "Because of this alternation of sum and products, the parsing problem is intractable.",
        "Moreover, the PCFG-LAs do not belong to the family of linear models that are assumed in the Lagrangian framework of (Rush and Collins, 2012).",
        "We now turn to approximations for the parsing problem in order to address both issues."
      ]
    },
    {
      "heading": "3.2 Variational Inference and MaxRule",
      "text": [
        "Variational inference is a common technique to approximate a probability distribution p with a cruder one q, as close as possible to the original one, by minimizing the Kullback-Liebler divergence between the two ?",
        "see for instance (Smith, 2011), chapter 5 for an introduction.",
        "Matsuzaki et al. (2005) showed that one can easily find such a cruder distribution for PCFG-LAs and demonstrated experimentally that this approximation gives good results.",
        "More precisely, they find a PCFG that only recognizes the input sentence where the probabilities q(rs) of the rules are set according to their marginal probabilities in the original PCFG-LA parse forest.",
        "The parameters rs are skeletal rules with span information.",
        "Distribution q is defined in Figure 2.",
        "Other approximations are possible.",
        "In particular, Petrov and Klein (2007) found that normalizing by the forest probability (in other words the inside probability of the root node) give better exper",
        "score(a?",
        "b c, i, j, k) =",
        "imental results although its interpretation as variational inference is still unclear.",
        "This approximation is called MaxRule-Product and amounts to replacing the norm function (see Figure 2).",
        "In both cases, the probability of a skeletal tree now becomes a simple product of parameters associated with anchored skeletal rules.",
        "For our purpose, the consequence is twofold: 1.",
        "The parsing problem becomes tractable by applying standard PCFG algorithms relying on dynamic programming (CKY for example).",
        "2.",
        "Equivalent to probability, a score ?",
        "can be defined as the logarithm of the probability.",
        "The parsing problem becomes2:",
        "Thus, from a PCFG-LA we are able to define a linear model whose parameters are the log-probabilities of the rules in distribution q.",
        "2We denote the parse forest of a sentence by F and the characteristic function of a set by 1."
      ]
    },
    {
      "heading": "3.3 Products of PCFG-LAs",
      "text": [
        "Although PCFG-LA training is beyond the scope of this paper, it is worthwhile mentioning that the most common way to learn their parameters relies on Expectation-Maximization which is not guaranteed to find the optimal estimation.",
        "Fortunately, this can be partly overcome by combining grammars that only differ on the initial parameterization of the EM algorithm.",
        "The probability of a skeletal tree is the product of the probabilities assigned by each single grammar Gi.",
        "Since grammars only differ by their numerical parameters (i.e. skeletal rules are the same), inference can be efficiently implemented using dynamic programming (Petrov, 2010).",
        "Scoring with n such grammars now becomes:",
        "The distributions qGi still have to be computed independently ?",
        "and possibly in parallel ?",
        "but the final decoding can be performed jointly.",
        "This is still a linear model for PCFG-LA parsing, but restricted to grammars that share the same skeletal rules."
      ]
    },
    {
      "heading": "4 Dual Decomposition",
      "text": [
        "In this section, we show how we derive an algorithm to work out the best parse according to a set of n grammars that do not share the exact same skeletal rules.",
        "As such, the grammars?",
        "product cannot be easily conducted inside the parser to produce and score a same and unique best tree, and we now consider a c(ompound)-parse as a tuple (T1 .",
        ".",
        ".",
        "Tn) of n compatible trees.",
        "Each grammar Gi is responsible for scoring tree Ti, and we seek to obtain the c-parse that maximizes the sum of the scores of its different trees.",
        "For a c-parse to be consistent, we have to precisely define the parts on which the trees must agree to be compatible with each other, so that we can model these as agreement constraints."
      ]
    },
    {
      "heading": "4.1 Compound Parse Consistency",
      "text": [
        "Let us suppose we have a set of phrase-structure parsers trained on different versions of the same treebank.",
        "Hence, some elements in the charts will either be the same or can be mapped to each other provided an equivalence relation and we define consensus between parsers on these elements.",
        "When the grammar is not functionally annotated, phrase-structure trees can be decomposed into a set of anchored (syntactical) categories Xs, asserting that a category X is in the tree at position3 s. Thus, such a tree T can be described by means of a boolean vector z(T ) indexed by anchored labels Xs, where z(T )Xs = 1 if Xs is in T and 0 otherwise.",
        "We will differentiate the set of natural non-terminals that occur in the treebanks from the set of artificial non-terminals that do not occur in the treebank and are the results of a binarization with markovization.",
        "As these artificial non-terminals disappear after reversing binarization in solution trees, they do not play any role in the consensus between parsers, and we only consider natural non-terminals in the set of anchored labels.",
        "When the grammar is functionally annotated, each label X?",
        "in a tree is a pair (X,F ), where X is a syntactical category and F is a function label.",
        "In this case, in order to manage the consensus with",
        "noting that the label covers terminals of the input sentence from index i to index j.",
        "In case the grammar contains unary non-lexical rules, the anchor also discriminates the different positions in a sequence of unary rules.",
        "non-functional grammars, we decompose such a tree into two sets: a set of anchored categories Xs and a set of anchored function labels Fs.",
        "Thus, a tree T can be described by means of two boolean vectors: ?",
        "z(T ) indexed by anchored categories Xs, z(T )Xs = 1 if there exists a function label F so that (X,F )s is in T , and 0 otherwise; ?",
        "?",
        "(T ) indexed by anchored function labels Fs, ?",
        "(T )Fs = 1 if there exists a category X so that (X,F )s is in T , and 0 otherwise.",
        "In the present work, a compound parse (T1 .",
        ".",
        ".",
        "Tn) is said to be consistent iff every tree shares the same set of anchored categories, i.e. iff: ?",
        "(i, j) ?",
        "J1, nK2, z(Ti) = z(Tj)"
      ]
    },
    {
      "heading": "4.2 Combining Parsers through Dual Decomposition",
      "text": [
        "Like previous applications, we base our reasoning on the assumption that computing the optimal score with each grammar Gi can be efficiently calculated, which is the case for approximate PCFG-LA parsing.",
        "We follow the presentation of the decomposition from (Martins et al., 2011) to explain how we can combine several PCFG-LA parsers together.",
        "For a sentence s, we want to obtain the best consistent compound parse from a set of n parsers:",
        "where C = F1(s) ?",
        "... ?",
        "Fn(s) is the product of parse forests F i(s), and F i(s) is the set of trees in grammar Gi whose yields are the input sentence s. Solving this problem with an exact algorithm is intractable.",
        "While artificial nodes could be inferred using a traditional parsing algorithm based on dynamic programming (i.e. CKY), the natural nodes require a coupling of the parsers?",
        "items to enforce the fact that natural daughter nodes must be identical (or equivalent) with the same spans for all parsers.",
        "Since the debinarization of markovized rules enables the creation of arbitrarily long nary rules, in the worst case the number of natural daughters to check is exponential in the size of the span to infer.",
        "Even if",
        "we bound the length of debinarized rules, the problem is hardly tractable.",
        "As this problem is intractable, even for approximate PCFG-LA parsing, we apply the iterate method presented in (Komodakis et al., 2007) for MRFs, also applied for joint tasks in NLP such as combined parsing and POS tagging in (Rush et al., 2010).",
        "First, we introduce a witness vector u in order to simplify constraints in (8).",
        "Problem (P ) can then be written in an equivalent form :",
        "Next, we proceed to a Lagrangian decomposition.",
        "This decomposition is a two-step process: Step 1 (Relaxation): the coupling constraints (10) are removed by introducing a vector of Lagrange multipliers ?i = (?i,Xs)Xs for each parser i, indexed by anchored categories Xs, and writing the equivalent problem:",
        "Intuitively, we can see the equivalence of (RP ) and (P ) with the following reasoning: ?",
        "whenever all constraints (10) are met, the second sum in f is nullified and f(u, T1...n,?)",
        "=?",
        "i ?i(Ti), which is a finite value and precisely the objective function maximized in (P ); ?",
        "if there is at least one (i,X, s) such that z(Ti)Xs 6= uXs , then the value of",
        "u) ?",
        "?i can be made arbitrarily small by an appropriate choice of ?i,Xs ; in this case, min?",
        "f(u, T1...n,?)",
        "= ??.",
        "Thus, (RP ) can not reach its maximum at a point where constraints (10) are not satisfied.",
        "Step 2 (dualization): the dual problem (LP ) is obtained by permuting max and min in (RP ):",
        "Finally, u can be removed from (LP1) by adding the constraint:",
        "one can see that if this constraint is not matched, maxu,T1...n f(u, T1...n,?)",
        "= +?",
        "and (LP1) can not reach its minimum on such a point.",
        "We can now find the maximum of f by maxing each Ti independently of each other.",
        "The dual problem becomes:",
        "using the projected subgradient method.",
        "Finding a subgradient amounts to computing the optimal solution (Rush and Collins, 2012) for each of the n subproblems (the slave problems in the terminology of (Martins et al., 2011) and (Komodakis et al., 2007)) which can be done efficiently, by incorporating the calculation of the penalties in the parsing algorithm, and in parallel.",
        "Until the agreement constraints are met (or a maximal number of iterations ?",
        "), the Lagrangian multipliers are updated according to the deviations from the average solutions (i.e. updates are zeros for a natural span if the parsers agree on it).",
        "This leads to Algorithm 1.",
        "It should be noted that the DP charts are built and pruned during the first iteration only (t = 0); further iterations do not require recreating the DP chart, which is memory intensive and time consuming, nor recomputing the approximate distribution for variational inference.",
        "As DP on the pruned charts is a fast process, the bottleneck of the algorithm still is in the first calculation of slave solutions.",
        "The stepsize sequence (?t)0?t must be diminishing and non-summable, that is to say: ?t, ?t ?",
        "0,",
        "objective function oP has increased since iterations began.",
        "Solving (P): it is easy to see that oLP is an upper bound of oP , but we do not necessarily have",
        "Algorithm 1 Find best compound parse with constraints on natural spans Require: n parsers {pi}1?i?n for all i, syntactical category X , anchor s do",
        "for all parsers pi do",
        "for all parsers pi do",
        "strong duality (i.e. oLP = oP ) due to the facts that parse forests are discrete sets.",
        "Furthermore, they get pruned independently of each other.",
        "Thus, the algorithm is not guaranteed to find a t such that z(T (t)i ) is the same for every parser i.",
        "However ?",
        "see (Koo et al., 2010) ?",
        "if it does reach such a state, then we have the guarantee of having found an exact solution of the primal problem (P ).",
        "We show in the experiments that this occurs very frequently."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": []
    },
    {
      "heading": "5.1 Experimental Setup",
      "text": [
        "We perform our experiments on the WSJ sections of the PTB with the usual split: sections 2 to 21 for training, section 23 for testing, and we run benchmarks on section 22. evalb is used for evaluation.",
        "We use the LORG parser modified with Algorithm 1.",
        "4 All grammars are trained using 6 split/merge EM cycles.",
        "For the handling of unknown words, we removed all words occurring once in the training set and replaced them by their morphological signature (Attia et al., 2010).",
        "Grammars for products are obtained by training with 16 random seeds for each setting.",
        "We use the approximate al",
        "The basic settings are a combination of the two following parameters: left or right binarization: we conjecture that this affects the quality of the parsers by impacting the recognition of left and right constituent frontiers.",
        "We set vertical markovization to 1 (no parent annotation) and horizontal markovization to 0 (we drop all left/right annotations).",
        "with or without functional annotations: in particular when non-terminals are annotated with multiple functions, all are kept."
      ]
    },
    {
      "heading": "5.2 Products of Grammars",
      "text": [
        "We first evaluate each setting on its own before combining them.",
        "We test the 4 different settings on the development set, using a single grammar or a product of n grammars.",
        "Results are reported on Figure 3.",
        "We can see that right binarization performs better than left binarization.",
        "Contrary to the results of Gabbard et al. (2006), function labels are detrimental for parsing performance for one grammar only.",
        "However, they do not penalize performance when using the product model with 8 grammars or more.",
        "EM is not guaranteed to find the optimal model and the problem is made harder by the increased number of parameters.",
        "Product models effectively alleviate this curse of dimensionality by letting some models compensate for the errors made by others.",
        "On the other hand, as differences between left and right binarization settings remain over all product sizes, right binarization seems more useful on its own.",
        "The first part of Table 1 gives F-score and",
        "Exact Match results of the product models with 16 grammars on the development set."
      ]
    },
    {
      "heading": "5.3 Combinations with Dual Decomposition",
      "text": [
        "We now turn to a series of experiments combining product models of 16 grammars.",
        "In all these experiments, we set the maximum number of iterations in Algorithm 1 to 1000.",
        "The system then returns the first element of the c-parse.",
        "We first try to combine two settings in four different combinations: DD Right Bin the two right-binarized systems ?",
        "with and without functions ?",
        "the system returns the function-labeled parse; DD Left Bin the two left-binarized systems ?",
        "with and without functions ?",
        "the system returns the function-labeled parse; DD Func the two systems with functions ?",
        "left and right binarization ?",
        "the system returns the right-binarized parse; DD No Func the two systems without functions ?",
        "left and right binarization ?",
        "the system returns the right-binarized parse; Results are in the second part of Table 1.",
        "Unsurprisingly, the best configuration is the one combining the two best product systems (with right binarization) but all combined systems perform better than their single components.",
        "We also combine 3 and 4 parsers to see if combining the above DD Right Bin setting with information that could improve the recognition of beginning of constituents can be helpful.",
        "We have 2 settings: DD3 The 2 right-binarized parsers combined with the left binarized parser without functions, DD4 The 4 parsers together.",
        "In both cases the system returns the right-binarized function annotated parse.",
        "The results are shown in the last part of Table 1.",
        "These 2 new configurations give similar F-scores, better than all 2 parser configurations.",
        "We conclude from these results that left-binarization and right-binarization capture different linguistic aspects, even in the case of heavy horizontal markovization, and that the method we propose enables a practical integration of these models.",
        "Table 2 shows for each setting how often the systems agree before 1000 iterations of Algorithm 1.",
        "As one might expect, the more diverse the systems are, the lower the rate of agreement."
      ]
    },
    {
      "heading": "5.4 Evaluation of Function Labeling",
      "text": [
        "We also evaluate the quality of the function labels.",
        "We compare the results obtained directly from the parser output with results obtained with Funtag, a state-of-the-art functional tagger that is applied on parser output, using a gold model trained on sections 02 to 21 of the WSJ (Chrupala et al., 2007).",
        "The results are shown in Table 3.",
        "First, we can see that the parser output is always outperformed by Funtag.",
        "This is expected from a context-free parser",
        "that has a limited domain of locality with strong independence constraints, compared to a voted-SVM classifier that can rely on arbitrarily rich features.",
        "Second, the quality of the Funtag prediction seems to be influenced by the fact that parser already handle functions and by the accuracy of the parser (Parseval F-score).",
        "This is because we use a model trained on the gold reference and so the closer the parser output is from the reference, the better the prediction.",
        "On the other hand, this is not the case with parser predicted functions, where the best system is the right-binarized product model with functions, with very similar performance obtained by the combinations consisting of 2 function parsers, settings DD Func and DD4.",
        "This tends to indicate that the constraints we have set to define consistencies in c-parses, focusing on syntactical categories, do not help in retrieving better function labels.",
        "This suggests some possible further improvements where parsers with functional annotations should be forced to agree on these too."
      ]
    },
    {
      "heading": "5.5 Evaluation of Dependencies",
      "text": [
        "Dependency-based evaluation of phrase structure parser output has been used in recent years to provide a more rounded view on parser performance and to compare with direct dependency parsers (Cer et al., 2010; Petrov et al., 2010; Nivre et al., 2010; Foster et al., 2011; Petrov and McDonald, 2012).",
        "We evaluate our various parsing models on their ability to recover three types of dependencies: basic Stanford dependencies (de Marneffe and Manning, 2008)5, LTH dependencies (Johansson and Nugues, 5We used the latest version at the time of writing, i.e. 3.20.",
        "2007)6 and penn2malt dependencies.",
        "The latter are a simpler version of the LTH dependencies but are still used when reporting unlabeled attachment scores for dependency parsing.",
        "The results, shown in Table 4, mirror the constituency evaluation results in that the dual decomposition results tend to outperform the basic product model results, and combining three or four grammars using dual decomposition yields the highest scores.",
        "The differences between the Func and No Func results highlight an important difference between the Stanford and LTH dependency schemes.",
        "The tool used to produce Stanford dependencies has been designed to work with phrase structure trees that do not contain function labels.",
        "In contrast, the LTH tool makes use of function label information in phrase structure trees.",
        "Thus, their availability results in only a moderate improvement in LAS for the Stanford dependencies and a very striking improvement for the LTH dependencies.",
        "By retaining function labels during parsing, we have shown that LTH dependencies can be recovered with a high level of accuracy without having to resort to a post-parsing function labeling step."
      ]
    },
    {
      "heading": "5.6 Test Set Results",
      "text": [
        "We now evaluate our various systems on the test set (the first half of Table 5) and compare these results with state-of-the-art systems (the second half of Table 5).",
        "We present parser accuracy results, measured using Parseval F-score and penn2malt UAS, and, for our systems, function label accuracy for labels produced during parsing and after parsing using Funtag.",
        "We also carried out statistical significance testing8 on the F-score differences between our various systems on the development and test sets.",
        "The results 6nlp.cs.lth.se/software/treebank_converter.",
        "It is recommended that LTH is used with the version of the Penn Treebank which contains the more detailed NP bracketing provided by Vadas and Curran (2007).",
        "However, to facilitate comparison with other parsers and dependency schemes, we did not use it in our experiments.",
        "We ran the converter with the right-Branching=false option to indicate that we are using the version without extra noun phrase bracketing.",
        "7stp.lingfil.uu.se/?nivre/research/Penn2Malt.",
        "The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed.",
        "stratified shuffling to compute significance.",
        "We consider a p value < 0.05 to indicate a statistically significant difference.",
        "We measured the performance of DD4 on the test set.",
        "It is approximately 3 times slower than the slowest product model (left binarization with function labels) and 7 slower than the fastest one (right binarization without function labels).",
        "This system performs on average 85.5 iterations of the DD algorithm.",
        "If we exclude the non-converging cases (5.1% of the cases), this drops to 39.4.",
        "Finally we compare our results with systems trained and evaluated on the PTB, see the lower half of Table 5.",
        "Our product models are not different from those presented in (Petrov, 2010) and it is not surprising to see that the F-scores are similar.",
        "More interestingly our DD4 setting improves on these results and compares favorably with systems relying on richer syntactic information, such as the discriminative parser of (Huang, 2008) that makes use of non-local features to score trees and the TSG parser of (Shindo et al., 2012) that can take into account larger tree fragments: this would indicate that by combining our parsers we extend the domain of locality, horizontally with binarization schemes and vertically with function labels.",
        "Our system also performs better than the combination system presented in (Zhang et al., 2009) that only relies on material from the PTB9 but a more detailed comparison is difficult: this system does not use products of latent models and more generally their method is orthogonal to ours.",
        "We also include for comparison state-of-the-art dependency parsing results (Bohnet and Nivre, 2012)."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We presented an algorithm and a set of experiments showing that grammar extraction strategies can be combined in an elegant way and give state-of-the-art results when applied to high-quality phrase-based parsers.",
        "As well as repeating these experiments for languages which rely more on function annotation, we also plan to apply our method to other types of annotations, e.g. more linguistically motivated binarization strategies or ?",
        "of particular interest to us ?",
        "annotation of empty elements."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We are grateful to the reviewers for their helpful comments.",
        "We also thank Joachim Wagner for providing feedback on an early version of the paper.",
        "This work has been partially funded by the Labex"
      ]
    },
    {
      "heading": "EFL (ANR/CGI).",
      "text": []
    }
  ]
}
