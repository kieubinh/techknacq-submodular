{
  "info": {
    "authors": [
      "Wenliang Chen",
      "Min Zhang",
      "Yue Zhang"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1129",
    "title": "Semi-Supervised Feature Transformation for Dependency Parsing",
    "url": "https://aclweb.org/anthology/D13-1129",
    "year": 2013
  },
  "references": [
    "acl-C04-1010",
    "acl-C10-1011",
    "acl-C96-1058",
    "acl-D07-1013",
    "acl-D07-1096",
    "acl-D07-1101",
    "acl-D07-1111",
    "acl-D08-1059",
    "acl-D09-1058",
    "acl-D09-1060",
    "acl-D11-1109",
    "acl-E06-1011",
    "acl-J93-2004",
    "acl-P05-1001",
    "acl-P05-1012",
    "acl-P06-1043",
    "acl-P08-1068",
    "acl-P08-1076",
    "acl-P09-1058",
    "acl-P10-1001",
    "acl-P11-1156",
    "acl-P11-2033",
    "acl-P11-2112",
    "acl-P12-1023",
    "acl-W06-2920",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In current dependency parsing models, conventional features (i.e. base features) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus exhibit less discriminative power on unseen data.",
        "In this paper, we propose a novel semi-supervised approach to addressing the problem by transforming the base features into high-level features (i.e. meta features) with the help of a large amount of automatically parsed data.",
        "The meta features are used together with base features in our final parser.",
        "Our studies indicate that our proposed approach is very effective in processing unseen data and features.",
        "Experiments on Chinese and English data sets show that the final parser achieves the best-reported accuracy on the Chinese data and comparable accuracy with the best known parsers on the English data."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In recent years, supervised learning models have achieved lots of progress in the dependency parsing task, as can be found in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007).",
        "The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights.",
        "?Corresponding author In the supervised learning scenarios, many previous studies explore rich feature representation that leads to significant improvements.",
        "McDonald and Pereira (2006) and Carreras (2007) define second-order features over two adjacent arcs in second-order graph-based models.",
        "Koo and Collins (2010) use third-order features in a third-order graph-based model.",
        "Bohnet (2010) considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models.",
        "All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004).",
        "However, the richer feature representations result in a high-dimensional feature space.",
        "Features in such a space may suffer from the data sparseness problem and thus have less discriminative power on unseen data.",
        "If input sentences contain unknown features that are not included in training data, the parsers can usually give lower accuracy.",
        "Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (Mc-Closky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008; Chen et al., 2009).",
        "In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation (Ando and Zhang, 2005).",
        "More",
        "specifically, we transform base features to a higher-level space.",
        "The base features defined over surface words, part-of-speech tags, and dependency trees are high dimensional and have been explored in the above previous studies.",
        "The higher-level features, which we call meta features, are low dimensional, and newly defined in this paper.",
        "The key idea behind is that we build connections between known and unknown base features via the meta features.",
        "From another viewpoint, we can also interpret the meta features as a way of doing feature smoothing.",
        "Our feature transfer method is simpler than that of Ando and Zhang (2005), which is based on splitting the original problem into multiple auxiliary problems.",
        "In our approach, the base features are grouped and each group relates to a meta feature.",
        "In the first step, we use a baseline parser to parse a large amount of unannotated sentences.",
        "Then we collect the base features from the parse trees.",
        "The collected features are transformed into predefined discrete values via a transformation function.",
        "Based on the transformed values, we define a set of meta features.",
        "Finally, the meta features are incorporated directly into parsing models.",
        "To demonstrate the effectiveness of the proposed approach, we apply it to the graph-based parsing models (McDonald and Nivre, 2007).",
        "We conduct experiments on the standard data split of the Penn English Treebank (Marcus et al., 1993) and the Chinese Treebank Version 5.1 (Xue et al., 2005).",
        "The results indicate that the approach significantly improves the accuracy.",
        "In summary, we make the following contributions: ?",
        "We define a simple yet useful transformation function to transform base features to meta features automatically.",
        "The meta features build connections between known and unknown base features, and relieve the data sparseness problem.",
        "?",
        "Compared to the base features, the number of meta features is remarkably small.",
        "?",
        "We build semi-supervised dependency parsers that achieve the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.",
        "The rest of this paper is organized as follows.",
        "Section 2 introduces the graph-based parsing model.",
        "Section 3 describes the meta features and meta parser.",
        "Section 4 describes the experiment settings and reports the experimental results on English and Chinese data sets.",
        "Section 5 discusses related work.",
        "Finally, in Section 6 we summarize the proposed approach."
      ]
    },
    {
      "heading": "2 Baseline parser",
      "text": [
        "In this section, we introduce a graph-based parsing model proposed by McDonald et al. (2005) and build a baseline parser."
      ]
    },
    {
      "heading": "2.1 Graph-based parsing model",
      "text": [
        "Given an input sentence, dependency parsing is to build a dependency tree.",
        "We define X as the set of possible input sentences, Y as the set of possible dependency trees, and D = (x1, y1), ..., (xi, yi), ..., (xn, yn) as a training set of n pairs of xi ?",
        "X and yi ?",
        "Y .",
        "A sentence is denoted by x = (w0, w1, ..., wi, ..., wm), where w0 is ROOT and does not depend on any other word and wi refers to a word.",
        "In the graph-based model, we define ordered pair (wi, wj) ?",
        "y as a dependency relation in tree y from word wi to word wj (wi is the head and wj is the dependent), Gx as a graph that consists of a set of nodes Vx = {w0, w1, ..., wi, ..., wm} and a set of arcs (edges) Ex = {(wi, wj)|i ?= j, wi ?",
        "Vx, wj ?",
        "(Vx?{w0})}.",
        "The parsing model of McDonald et al. (2005) is to search for the maximum spanning tree (MST) in graph Gx.",
        "We denote Y (Gx) as the set of all the subgraphs of Gx that are valid dependency trees (McDonald and Nivre, 2007) for sentence x.",
        "We define the score of a dependency tree y ?",
        "Y (Gx) to be the sum of the subgraph scores,",
        "where g is a spanning subgraph of y, which can be a single arc or adjacent arcs.",
        "In this paper we assume the dependency tree to be a spanning projective tree.",
        "The model scores each subgraph using a linear representation.",
        "Then scoring function score(x, g) is, score(x, g) = f(x, g) ?",
        "w (2) where f(x, g) is a high-dimensional feature vector based on features defined over g and x and w refers to the weights for the features.",
        "The maximum spanning tree is the highest scoring tree in Y (Gx).",
        "The task of decoding algorithms in the parsing model for an input sentence x is to find y?, where",
        "In our system, we use the decoding algorithm proposed by Carreras (2007), which is a second-order CKY-style algorithm (Eisner, 1996) and feature weights w are learned during training using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; McDonald et al., 2005)."
      ]
    },
    {
      "heading": "2.2 Base features",
      "text": [
        "Previous studies have defined different sets of features for the graph-based parsing models, such as the first-order features defined in McDonald et al. (2005), the second-order parent-siblings features defined in McDonald and Pereira (2006), and the second-order parent-child-grandchild features defined in Carreras (2007).",
        "Bohnet (2010) explorers a richer set of features than the above sets.",
        "We further extend the features defined by Bohnet (2010) by introducing more lexical features as the base features.",
        "The base feature templates are listed in Table 1, where h, d refer to the head, the dependent respectively, c refers to d's sibling or child, b refers to the word between h and d, +1 (?1) refers to the next (previous) word, w and p refer to the surface word and part-of-speech tag respectively, [wp] refers to the surface word or part-of-speech tag, d(h, d) is the direction of the dependency relation between h and d, and d(h, d, c) is the directions of the relation among h, d, and c. We generate the base features based on the above templates."
      ]
    },
    {
      "heading": "2.3 Baseline parser",
      "text": [
        "We train a parser with the base features as the Baseline parser.",
        "We define fb(x, g) as the base features and wb as the corresponding weights.",
        "The scoring function becomes, score(x, g) = fb(x, g) ?",
        "wb (4)"
      ]
    },
    {
      "heading": "3 Meta features",
      "text": [
        "In this section, we propose a semi-supervised approach to transform the features in the base feature space (FB) to features in a higher-level space (FM ) with the following properties: ?",
        "The features in FM are able to build connections between known and unknown features in FB and therefore should be highly informative.",
        "?",
        "The transformation should be learnable based on a labeled training set and an automatically parsed data set, and automatically computable for the test sentences.",
        "The features in FM are referred to as meta features.",
        "In order to perform the feature transformation, we choose to define a simple yet effective mapping function.",
        "Based on the mapped values, we define feature templates for generating the meta features.",
        "Finally, we build a new parser with the base and meta features."
      ]
    },
    {
      "heading": "3.1 Template-based mapping function",
      "text": [
        "We define a template-based function for mapping the base features to predefined discrete values.",
        "We first put the base features into several groups and then perform mapping.",
        "We have a set of base feature templates TB .",
        "For each template Ti ?",
        "TB , we can generate a set of base features Fi from dependency trees in the parsed data, which is automatically parsed by the Baseline parser.",
        "We collect the features and count their frequencies.",
        "The collected features are sorted in decreasing order of frequencies.",
        "The mapping function for a base feature fb of Fi is defined as follows,",
        "where R(fb) is the position number of fb in the sorted list, ?Others?",
        "is defined for the base features that are not included in the list, and TOP10 and TOP 30 refer to the position numbers of top 10% and top 30% respectively.",
        "The numbers, 10% and 30%, are tuned on the development sets in the experiments.",
        "For a base feature generated from template Ti, we have four possible values: Hi, Mi, Li, and Oi.",
        "In",
        "total, we have 4?N(TB) possible values for all the base features, where N(TB) refers to the number of the base feature templates, which is usually small.",
        "We can obtain the mapped values of all the collected features via the mapping function."
      ]
    },
    {
      "heading": "3.2 Meta feature templates",
      "text": [
        "Based on the mapped values, we define meta feature templates in FM for dependency parsing.",
        "The meta feature templates are listed in Table 2, where fb is a base feature of FB , hp refers to the part-of-speech tag of the head and hw refers to the surface word of the head.",
        "Of the table, the first template uses the mapped value only, the second and third templates combine the value with the head information.",
        "The number of the meta features is relatively small.",
        "It has 4 ?",
        "N(TB) for the first type,"
      ]
    },
    {
      "heading": "4 ? N(TB) ? N(POS) for the second type, and 4 ?N(TB) ?N(WORD) for the third one, where",
      "text": [
        "N(POS) refers to the number of part-of-speech tags, N(WORD) refers to the number of words.",
        "We remove any feature related to the surface form if the word is not one of the Top-N most frequent words in the training data.",
        "We used N=1000 for the experiments for this paper.",
        "This method can reduce the size of the feature sets.",
        "The empirical statistics of the feature sizes at Section 4.2.2 shows that the size of meta features is only 1.2% of base features.",
        "[?",
        "(fb)] [?",
        "(fb)], hp [?",
        "(fb)], hw"
      ]
    },
    {
      "heading": "3.3 Generating meta features",
      "text": [
        "We use an example to demonstrate how to generate the meta features based on the meta feature templates in practice.",
        "Suppose that we have sentence ?I ate the meat with a fork.?",
        "and want to generate the meta features for the relation among ?ate?, ?meat?, and ?with?, where ?ate?",
        "is the head, ?meat?",
        "is the dependent, and ?with?",
        "is the closest left sibling of ?meat?.",
        "Figure 1 shows the example.",
        "We demonstrate the generating procedure using template Tk = ?hw, dw, cw, d(h, d, c)?",
        "(the second template of Table 1-(c) ), which contains the surface forms of the head, the dependent, its sibling, and the directions of the dependencies among h, d, and c. We can have a base feature ?ate, meat, with, RIGHTSIB?, where ?RIGHTSIB?",
        "refers to the parent-siblings structure with the right direction.",
        "In the auto-parsed data, this feature occurs 200 times and ranks between TOP10 and TOP30.",
        "Accord",
        "ing to the mapping function, we obtain the mapped value Mk.",
        "Finally, we have the three meta features ?",
        "[Mk]?, ?",
        "[Mk], V V ?, and ?",
        "[Mk], ate?, where V V is the part-of-speech tag of word ?ate?.",
        "In this way, we can generate all the meta features for the graph-based model."
      ]
    },
    {
      "heading": "3.4 Meta parser",
      "text": [
        "We combine the base features with the meta features by a new scoring function, score(x, g) = fb(x, g) ?",
        "wb + fm(x, g) ?",
        "wm (5) where fb(x, g) refers to the base features, fm(x, g) refers to the meta features, and wb and wm are their corresponding weights respectively.",
        "The feature weights are learned during training using MIRA (Crammer and Singer, 2003; McDonald et al., 2005).",
        "Note that wb is also retrained here.",
        "We use the same decoding algorithm in the new parser as in the Baseline parser.",
        "The new parser is referred to as the meta parser."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "We evaluated the effect of the meta features for the graph-based parsers on English and Chinese data."
      ]
    },
    {
      "heading": "4.1 Experimental settings",
      "text": [
        "In our experiments, we used the Penn Treebank (PTB) (Marcus et al., 1993) for English and the Chinese Treebank version 5.1 (CTB5) (Xue et al., 2005) for Chinese.",
        "The tool ?Penn2Malt?1 was used",
        "to convert the data into dependency structures with the English head rules of Yamada and Matsumoto (2003) and the Chinese head rules of Zhang and Clark (2008).",
        "We followed the standard data splits as shown in Table 3.",
        "Following the work of Koo et al.",
        "(2008), we used a tagger trained on training data to provide part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate part-of-speech tags for the training set.",
        "We used the MXPOST (Ratnaparkhi, 1996) tagger for English and the CRF-based tagger for Chinese.",
        "We used gold standard segmentation in the CTB5.",
        "The data partition of Chinese were chosen to match previous work (Duan et al., 2007; Li et al., 2011; Hatori et al., 2011).",
        "For the unannotated data in English, we used the BLLIP WSJ corpus (Charniak et al., 2000) containing about 43 million words.2 We used the MXPOST tagger trained on the training data to assign part-of-speech tags and used the Baseline parser to process the sentences of the Brown corpus.",
        "For the unannotated data in Chinese, we used the Xinhua portion of Chinese Gigaword3 Version 2.0 (LDC2009T14) (Huang, 2009), which has approximately 311 million words.",
        "We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse the sentences in the Giga-word data.",
        "In collecting the base features, we removed the features which occur only once in the English data and less than four times in the Chinese data.",
        "The feature occurrences of one time and four times are based on the development data performance.",
        "We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of to",
        "We evaluated the parsers with different settings on the development sets to select the meta features.",
        "In this section, we investigated the effect of different types of meta features for the models trained on different sizes of training data on English.",
        "There are too many base feature templates to test one by one.",
        "We divided the templates into several categories.",
        "Of Table 1, some templates are only related to part-of-speech tags (P), some are only related to surface words (W), and the others contain both part-of-speech tags and surfaces (M).",
        "Table 4 shows the categories, where numbers [1?4] refer to the numbers of words involved in templates.",
        "For example, the templates of N3WM are related to three words and contain the templates of W and M. Based on different categories of base templates, we have different sets of meta features.4",
        "We randomly selected 1% and 10% of the sentences respectively from the training data.",
        "We trained the POS taggers and Baseline parsers on these small training data and used them to process the unannotated data.",
        "Then, we generated the meta features based on the newly auto-parsed data.",
        "The 4We also tested the settings of dividing WM into two subtypes: W and M. The results showed that both two subtypes provided positive results.",
        "To simplify, we merged W and M into one category WM.",
        "meta parsers were trained on the different subsets of the training data with different sets of meta features.",
        "Finally, we have three meta parsers: MP1, MP10, MPFULL, which were trained on 1%, 10% and 100% of the training data.",
        "Table 5 shows the results, where we add each category of Table 4 individually.",
        "From the table, we found that the meta features that are only related to part-of-speech tags did not always help, while the ones related to the surface words were very helpful.",
        "We also found that MP1 provided the largest relative improvement among the three settings.",
        "These suggested that the more sparse the base features were, the more effective the corresponding meta features were.",
        "Thus, we built the final parsers by adding the meta features of N1WM, N2WM, N3WM, and N4WM.",
        "The results showed that OURS achieved better performance than the systems with individual sets of meta features.",
        "In Table 2, there are three types of meta feature templates.",
        "Here, the results of the parsers with different settings are shown in Table 6, where CORE refers to the first type, WithPOS refers to the second one, and WithWORD refers to the third one.",
        "The results showed that with all the types the parser (OURS) achieved the best.",
        "We also counted the numbers of the meta features.",
        "Only 327,864 (or 1.2%) features were added into OURS.",
        "Thus, we used all the three types of meta features in our final meta parsers."
      ]
    },
    {
      "heading": "4.3 Main results on test sets",
      "text": [
        "We then evaluated the meta parsers on the English and Chinese test sets.",
        "The results are shown in Table 7, where Meta-Parser refers to the meta parser.",
        "We found that the meta parser outperformed the baseline with an absolute improvement of 1.01 points (UAS).",
        "The improvement was significant in McNemar's Test (p",
        "The results are shown in Table 8.",
        "As in the experiment on English, the meta parser outperformed the baseline.",
        "We obtained an absolute improvement of 2.07 points (UAS).",
        "The improvement was significant in McNemar's Test (p < 10?8 ).",
        "In summary, Tables 7 and 8 convincingly show the effectiveness of our proposed approach."
      ]
    },
    {
      "heading": "4.4 Different sizes of unannotated data",
      "text": [
        "Here, we considered the improvement relative to the sizes of the unannotated data used to generate the meta features.",
        "We randomly selected the 0.1%, 1%, and 10% of the sentences from the full data.",
        "Table",
        "9 shows the results, where P0.1, P1, and P10 correspond to 0.1%, 1%, and 10% respectively.",
        "From the table, we found that the parsers obtained more benefits as we used more raw sentences.",
        "We also tried generating the meta features from the training data only, shown as TrainData in Table 9.",
        "However, the results shows that the parsers performed worse than the baselines.",
        "This is not surprising because only the known base features are included in the training data."
      ]
    },
    {
      "heading": "4.5 Comparison with previous work 4.5.1 English",
      "text": [
        "Table 10 shows the performance of the previous systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo10 refers to the third-order parser with model1 of Koo and Collins (2010), Zhang11 refers to the parser of Zhang and Nivre (2011), Li12 refers to the unlabeled parser of Li et al.",
        "(2012), Koo08 refers to the parser of Koo et al. (2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09 refers to the parser of Chen et al. (2009), Zhou11 refers to the parser of Zhou et al. (2011), Suzuki11 refers to the parser of Suzuki et al. (2011), and Chen12 refers to the parser of Chen et al.",
        "(2012).",
        "The results showed that our meta parser outperformed most of the previous systems and obtained the comparable accuracy with the best result of Suzuki11 (Suzuki et al., 2011) which combined the clustering-based word representations of Koo et al.",
        "(2008) and a condensed feature representation.",
        "However, our approach is much simpler than theirs and we believe that our meta parser can be further improved by combining their methods.",
        "Table 11 shows the comparative results, where Li11 refers to the parser of Li et al. (2011), Hatori11 refers to the parser of Hatori et al. (2011), and Li12 refers to the unlabeled parser of Li et al. (2012).",
        "The reported scores on this data were produced by the supervised learning methods and our Baseline (supervised) parser provided the comparable accuracy.",
        "We found that the score of our meta parser for this data was the best reported so far and significantly higher than the previous scores.",
        "Note that we used the auto-assigned POS tags in the test set to match the above previous studies."
      ]
    },
    {
      "heading": "4.6 Analysis",
      "text": [
        "Here, we analyzed the effect of the meta features on the data sparseness problem.",
        "We first checked the effect of unknown features on the parsing accuracy.",
        "We calculated the number of unknown features in each sentence and computed the average number per word.",
        "The average numbers were used to eliminate the influence of varied sentence sizes.",
        "We sorted the test sentences in increasing orders of these average numbers, and divided equally into five bins.",
        "BIN 1 is assigned the sentences with the smallest numbers and BIN 5 is with the largest ones.",
        "Figure 2 shows the average accuracy scores of the Baseline parsers against to the bins.",
        "From the figure, we found that for both two languages the Baseline parsers performed worse while the sentences contained more unknown features.",
        "tures (average per word) by Baseline parsers Then, we investigated the effect of the meta features.",
        "We calculated the average number of active meta features per word that were transformed from the unknown features for each sentence.",
        "We sorted the sentences in increasing order of the average numbers of active meta features and divided them into five bins.",
        "BIN 1 is assigned the sentences with the smallest numbers and BIN 5 is with the largest ones.",
        "Figures 3 and 4 show the results, where ?Better?",
        "is for the sentences where the meta parsers provided better results than the baselines and ?Worse?",
        "is for those where the meta parsers provided worse results.",
        "We found that the gap between ?Better?",
        "and ?Worse?",
        "became larger while the sentences contain more active meta features for the unknown features.",
        "The gap means performance improvement.",
        "This indicates that the meta features are very effective in processing the unknown features."
      ]
    },
    {
      "heading": "5 Related work",
      "text": [
        "Our approach is to use unannotated data to generate the meta features to improve dependency parsing.",
        "features on Chinese (average per word) Several previous studies relevant to our approach have been conducted.",
        "Koo et al. (2008) used a word clusters trained on a large amount of unannotated data and designed a set of new features based on the clusters for dependency parsing models.",
        "Chen et al. (2009) extracted sub-tree structures from a large amount of data and represented them as the additional features to improve dependency parsing.",
        "Suzuki et al. (2009) extended a"
      ]
    },
    {
      "heading": "Semi-supervised Structured Conditional Model (SS",
      "text": [
        "SCM) of Suzuki and Isozaki (2008) to the dependency parsing problem and combined their method with the word clustering feature representation of Koo et al. (2008).",
        "Chen et al. (2012) proposed an approach to representing high-order features for graph-based dependency parsing models using a dependency language model and beam search.",
        "In future work, we may consider to combine their methods with ours to improve performance.",
        "Several previous studies used co-training/selftraining methods.",
        "McClosky et al. (2006) presented a self-training method combined with a reranking algorithm for constituency parsing.",
        "Sagae and Tsujii (2007) applied the standard co-training method for dependency parsing.",
        "In their approaches, some automatically parsed sentences were selected as new training data, which was used together with the original labeled data to retrain a new parser.",
        "We are able to use their approaches on top of the output of our parsers.",
        "With regard to feature transformation, the work of Ando and Zhang (2005) is similar in spirit to our work.",
        "They studied semi-supervised text chunking by using a large projection matrix to map sparse base features into a small number of high level features.",
        "Their project matrix was trained by transforming the original problem into a large number of auxiliary problems, obtaining training data for the auxiliary problems by automatically labeling raw data and using alternating structure optimization to estimate the matrix across all auxiliary tasks.",
        "In comparison with their approach, our method is simpler in the sense that we do not request any intermediate step of splitting the prediction problem, and obtain meta features directly from self-annotated data.",
        "The training of our meta feature values is highly efficient, requiring the collection of simple statistics over base features from huge amount of data.",
        "Hence our method can potentially be useful to other tasks also."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "In this paper, we have presented a simple but effective semi-supervised approach to learning the meta features from the auto-parsed data for dependency parsing.",
        "We build a meta parser by combining the meta features with the base features in a graph-based model.",
        "The experimental results show that the proposed approach significantly improves the accuracy.",
        "Our meta parser achieves comparable accuracy with the best known parsers on the English data (Penn English Treebank) and the best accuracy on the Chinese data (Chinese Treebank Version 5.1) so far.",
        "Further analysis indicate that the meta features are very effective in processing the unknown features.",
        "The idea described in this paper is general and can be applied to other NLP applications, such as part",
        "of-speech tagging and Chinese word segmentation, in future work."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This study was started when Wenliang Chen and Min Zhang were members of the Department of Human Language Technology, Institute for Info-comm Research, Singapore.",
        "Wenliang Chen was funded partially by the National Science Foundation of China (61203314) and Yue Zhang was supported by MOE grant 2012-T2-2-163.",
        "We would also thank the anonymous reviewers for their detailed comments, which have helped us to improve the quality of this work."
      ]
    }
  ]
}
