{
  "info": {
    "authors": [
      "William Yang Wang",
      "Edward Lin",
      "John Kominek"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1131",
    "title": "This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics",
    "url": "https://aclweb.org/anthology/D13-1131",
    "year": 2013
  },
  "references": [
    "acl-D11-1139",
    "acl-P11-1137",
    "acl-P12-1078",
    "acl-W11-2018",
    "acl-W12-1603"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1325?1336, Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics This Text Has the Scent of Starbucks:"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "We propose a Laplacian structured sparsity model to study computational branding ana-lytics.",
        "To do this, we collected customer reviews from Starbucks, Dunkin?",
        "Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA.",
        "We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors.",
        "In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brand-gender-satisfaction prediction.",
        "This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model.",
        "Our quantitative evaluation shows that our approach which combines the advantages of graphical model-ing and sparsity modeling techniques significantly outperforms various standard and state-of-the-art text classification algorithms.",
        "In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In marketing science, branding is a modern marketing strategy of creating a unique image for a product in the customers?",
        "mind.",
        "Establishing the brand in the broad social context is just as important as building a good product (Makens, 1965; Lederer and Hill, 2001; Kim et al., 2013).",
        "In fact, blind taste test experiments have frequently shown how branding directly leads to the success of products and companies.",
        "Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge1, where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola's branding has made it more popular.",
        "Even now, Microsoft uses similar blind taste tests2 to compare search engines, Bing and Google, showing that although participants prefer Bing's results, Google's brand might have strengthened over the years.",
        "These studies all suggest that brand and its associations play important roles in the customers?",
        "perceptions and decisions.",
        "To accommodate the market change, companies frequently adjust branding strategies by analyzing how their customers receive and respond to branding messages.",
        "So far, such analysis is often done by using surveys and focus groups (Moon and Quelch, 2006), which is expensive and not time-efficient.",
        "Recently, with the advance of machine learning techniques, researchers from the chemistry and vision communities started to pay attention to the problem of automatic brand identification from smell (Luo et al., 2004) and images (Pelisson et al., 2003).",
        "In contrast, even though textual data that contains hidden branding information is abundantly available in many forms over the Web, automatic discovery and computational analysis on such data are not well studied in the past.",
        "Computational branding analytics (CBA) seeks to extract information, trends, and demographics about a brand on the basis of free-form text, e.g. from blogs, Twitter comments, reviews, or forum posts.",
        "As described in Section 3, in this study we use a sub",
        "set of online Yelp reviews that discuss coffee shops.",
        "The main reason is that this source has the advantage of providing ground truth of multi-labeled data: each review has meta-information defining a 5-star rating, the object of the review, and the reviewer's name (from which we infer gender).",
        "For the purpose of this paper we decompose CBA into three sub-problems.",
        "?",
        "How well can the brand being discussed be identified by the raw text?",
        "?",
        "How well can the joint value of brand and rating be predicted?",
        "?",
        "How well can the joint value of brand, rating, and gender be predicted?",
        "There are two reasons why one may want to construct text-based classifiers of brand, rating, and gender, when such information is present in the review header.",
        "The first is that trained classifiers can then be applied to other data sources, such as blogs, where what is available is only the review itself.",
        "The second is that by ?opening the hood?",
        "to the classifier one can examine which words exhibit high affiliation with the predicted variables.",
        "This can be done, for example, to contrast the preferences of males and females with respect to evaluating the qualities of a coffee shop.",
        "Examples of such insights are provided in Section 5.5.",
        "In this paper, we propose a Laplacian structured sparsity model for computational branding analytics.",
        "Our main contributions are two-fold: first, in the novel task of automatic brand identification from text, we show that by incorporating the dependency structure and graphical interactions among local features, our model significantly outperforms various text classification algorithms such as the standard logistic regression, principle component analysis (PCA), linear kernel support vector machine (SVM), sparse, non-sparse, and mixed-penalty log-linear models.",
        "These improvements could also be seen from a joint brand-satisfaction prediction task and a gender-specific joint brand-satisfaction prediction task.",
        "In addition, our Laplacian augmented L1-ball projection experiment shows that the advantage of Laplacian structured sparsity is robust across different parameter settings in a L1-constrained problem.",
        "Secondly, the qualitative analysis of our machine learning model shows the interesting features and language use that relate to brand and its associated pragmatics.",
        "In the next section, we outline related work in CBA, sparsity, and spectral graph learning.",
        "In Section 3, we describe the corpus in this study.",
        "The Laplacian structured sparsity model is introduced in Section 4.",
        "The experimental setup and results are presented in Section 5.",
        "A short discussion is followed in Section 6 and we conclude in Section 7."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Early work on statistical brand analysis in the marketing community dates back to the work of Kuehn (1962), where he first hypothesizes that brand choice could be described as a learning process.",
        "Guadagni and Little (1983) further empirically tested the hypothesis by building a calibrated multinomial logistic regression model to predict the purchase of ground coffee, using the data from the optical scanning of product code in supermarkets.",
        "Outside the marketing community, statistical brand analysis is rarely seen.",
        "More recently, a study (Luo et al., 2004) applies neural networks to identify cigarette brands, with the hope of detecting illegal cigarettes from smell features.",
        "In image processing, researchers have studied the problem of brand identification from image using histogram comparison (Pelisson et al., 2003).",
        "However, to the best of our knowledge, even though textual data is vastly available, the problems of automatic brand identification from raw text and computational branding analytics, are new.",
        "Although the domain of our data is on branding, our work also aligns with previous work in text and language classification.",
        "Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Bi-adsy et al., 2011).",
        "Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results.",
        "For example, Eisenstein et al. (2011b) use the L1,?",
        "sparsity model to discover sociolinguistic patterns.",
        "Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations.",
        "Martins et al. (2011) investigate the tree-structured overlapping group lasso for",
        "structured prediction problems.",
        "Chen et al. (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation.",
        "Sparse inducing priors are also investigated and shown to be effective in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Paul and Dredze, 2012).",
        "Besides lacking sparsity, since the traditional discriminative methods in NLP often use interdependent features such as n-grams tokens, and part-of-speech tags, they also suffer from the problem of not explicitly modeling the complex dependency structure and interaction of local features from a global perspective.",
        "To solve this problem, graph methods seem to be a good solution, because they are simple, generalizable, and are often used to model such complex dependency structures (Cohen, 2012).",
        "However, combining the sparse modeling and spectral graphical modeling approaches in a principled way is challenging.",
        "Belkin et al. (2006) and Wein-berger et al. (2007) are among the first to investigate graph Laplacians as a manifold regularization method for statistical learning.",
        "Recently, Gao et al.",
        "(2012) propose a histogram intersection based kNN method to construct a Laplacian matrix for a least-square sparse coding problem in image processing.",
        "Unfortunately, this method might be too specific to the SIFT-based image coding tasks, thus might not be applicable to the text classification problem that utilizes n-gram lexical features."
      ]
    },
    {
      "heading": "3 Datasets",
      "text": [
        "We collected Yelp reviews from 1,860 Starbucks, Dunkin?",
        "Donuts3, and other coffee shops all over the Midwest and Northeast regions in the period of 2009.",
        "A detail statistics of our data can be found in Table 1.",
        "The Midwest region includes 12 states4 and 19 major cities, and the Northeast region includes 9 states5 and 19 major cities.",
        "For each region, we divide the coffee shops into 60% training, 20% development, and 20% test, and there are no overlaps of coffee shops among these subsets.",
        "There are three values for the brand label: Starbucks, Dunkin?",
        "Donuts, and all other coffee shop brands.",
        "The ma",
        "east region.",
        "T.: total.",
        "jority class is ?all other coffee shop brands?, and the majority baseline is shown in Table 2.",
        "In the task of joint brand-satisfaction prediction, we utilize the review scores to approximate user satisfaction: scores 1-2 as the unsatisfactory label, 3 as moderate, and 4-5 as satisfactory.",
        "Since the Yelp reviews do not reveal the reviewer's gender, we use a similar method that U.S. Census Bureau used (OConnell and Gooding, 2006): we first automatically match the first name of the reviewer with the prior name-gender distributions in the census records, then manually examine the no-match cases and a subsample of the matched cases.",
        "For those who we cannot determine the gender, the review will be dropped from the gender-specific brand-satisfaction prediction task.",
        "After filtering, there are 8,528 documents for training, 2,928 for development, and, 3,046 for testing.",
        "Since the focus of this paper is not on feature engineering, we use unigram features to represent each review.",
        "Below is an example of positive review from a male Starbucks customer from Midwest.",
        "My favorite place for my iced vanilla lattes.",
        "They have screwed up my order before: instead of a grande, I got a venti.",
        "Not a fan of their pastries though.",
        "I got a donut once, and ended up feeding it to a pigeon in city garden.",
        "Friendly and fast service.",
        "Not open Sundays.",
        "The coffee shop dataset is freely available6 for research purposes."
      ]
    },
    {
      "heading": "4 Our Approach",
      "text": []
    },
    {
      "heading": "4.1 Problem Formulation and Predictive Tasks",
      "text": [
        "The automatic brand identification problem could be considered as a traditional multiclass classifica",
        "tion problem where the estimated label Y?",
        "could be drawn from Mult(?",
        "), where ?",
        "is the parameter for the multinomial distribution.",
        "To solve this, a simple but accurate solution is to decompose the multiclass problem into multiple binary classification problems (Rifkin and Klautau, 2004) by training k one-vs-all binary classifiers, and then use the argmax criteria to select the best hypothesis from the k posteriors.",
        "As for a binary classifier, we need to infer the posterior from a Bernoulli distribution that is parametrized by ??y.",
        "Similarly, we can derive k binary classifiers:",
        "So, instead of drawing Y?",
        "from a multinomial distribution Mult(?",
        "), we can draw the final label Y?",
        "that has the largest posterior across all k classifiers:",
        "the posterior probability given the learned classifiers and the testing vector.",
        "In this paper, we investigate three multiclass classification tasks: first, we perform a 3-way classification task for automatic brand identification.",
        "In the task of brand-satisfaction prediction, we model the brand and the satisfaction label at the same time (Chahuneau et al., 2012): we perform the task of jointly predicting aggregate brand-satisfaction score for a review using 9-way classification.",
        "Similarly, we perform 18-way classification for the gender-specific joint brand-satisfaction prediction task."
      ]
    },
    {
      "heading": "4.2 The Log-Linear Framework and Its Regularized Variants",
      "text": [
        "If we consider the standard logistic regression model as the binary classifier in this log-linear framework7, then each classifier can be written as:",
        "here, ~Xj is the j-th observed feature vector, label y ?",
        "{0, 1}, and ~W is a vector of the coefficients.",
        "To 7We thank Jacob Eisenstein for the initial derivation of the logistic regression model.",
        "estimate the model parameters in equation (3), we only need to set the weights ~W .",
        "We can obtain the following log likelihood, and its gradient function by taking the first-order partial derivative of ~W :",
        "since the log likelihood objective function (4) is concave, using standard gradient ascent with maximum likelihood estimation can solve the problem.",
        "However, this model does not penalize the noisy features and unreliable features that might overfit to the training data.",
        "To address this issue, we introduce the L1 norm from lasso technique (Tibshirani, 1996) to regularize the above likelihood function.",
        "Thus, instead of maximizing the likelihood, we can minimize the loss function of the negative log-likelihood with a",
        "where ?1 is the regularization coefficient.",
        "The benefit of L1 penalty in a discriminative model is similar to the double exponential distribution of the sparse priors in generative models (Eisenstein et al., 2011a): they both push the weights of many noisy features into zeros, revealing only the important features.",
        "However, since the L1 penalty can introduce discontinuities to the original convex function, we can also consider an alternative non-sparse ridge estimator (Le Cessie and Van Houwelingen, 1992) with log loss and L2 norm, and has the convex property:",
        "Another option that balances the sparsity and smoothness would be the elastic net model (Zou and Hastie, 2005) that uses the composite penalty:"
      ]
    },
    {
      "heading": "4.3 The Laplacian Structured Sparsity Model",
      "text": [
        "So far, none of the above element-wise penalty models in the previous subsection takes into account the",
        "dependency structure of the local features.",
        "Inspired by Gao et al(2012), we group the local features that have similar distributions together.",
        "The intuition is that, for features that have very similar empirical distributions in the training set, their weights should not be drastically different after the learning process in the same task.",
        "In our new objective function, it is desirable to introduce a new component that structurally penalize these cases where features that are very similar to each other, but have learned completely different weights, probably due to the noise or the data sparsity issue in the training data.",
        "The Objective Function: To do this, we first define an inter-feature affinity matrix A, where A(p,q) measures the similarity between a pair of features p and q.",
        "In the spectral graph theory, this affinity matrix can be viewed as a weighted undirected graph G = (V,E), where each node Vp denotes a feature p, and each edge E(p,q) indicates the closeness among the features p and q.",
        "We also introduce a weighted diagonal degree matrix D, of which each element in the diagonal D(p,p) is the sum of all weighted connections of node Vp: D(p,p) =",
        "propose the following objective function:",
        "We then denote a graph Laplacian matrix L = D ?",
        "A (Belkin and Niyogi, 2001), and rewrite the objective function as:",
        "where ?",
        "is the regularization parameter for the Laplacian structured sparsity term.",
        "Intuitively, the objective function can be interpreted as the sum of a negative log loss function, the sparsity-inducing penalty, the quadratic penalty, and the Laplacian structured penalty.",
        "Or, another view of this new model could be seen as a Laplacian augmented elastic net model where structured sparsity and feature interaction are considered.",
        "The Laplacian Matrix: In this model, a key aspect is to derive the Laplacian matrix L. We propose the following three steps to learn the Laplacian matrix:",
        "L. 1.",
        "Construct the distance matrix Dist.",
        "To con",
        "struct the distance matrix between each feature, we first transpose the instance-feature ma",
        "~Xj , and assume that each feature (e.g. unigram in our task) is a random variable that has a multinomial distribution over the instances in the training set.",
        "Then, we compare each pair of features, and calculate the inter-feature distance matrix Dist with Euclidean distance as a measure, and use the k-nearest neighbors (kNN) method (Beyer et al., 1999) to select the top neighbors of each feature.",
        "2.",
        "Derive the affinity matrix A.",
        "To assign the weight on the edge E(p,q) for each connected nodes (the kNN of V in Dist), we use the cosine similarity cosine(Vp, Vq) metric (Wang and Hirschberg, 2011).",
        "3.",
        "Generate the degree matrix D and Lapla",
        "cian matrix L. As discussed earlier, we sum up the symmetric affinity matrix by row, and obtain a diagonal degree matrix D, and we further define a Laplacian matrix L = D ?A.",
        "To calculate the above matrices in an efficient manner, we partition the covariate into blocks, and process each block in parallel (Chen et al., 2011).",
        "An intuitive example of the graphG, its associated affinity matrix A, and Laplacian matrix L, is shown in",
        "tion of objective function in (12-13), a notable problem is that the sparsity inducing L1 term is non-differentiable, whereas this is not the case for the L2 norm and the Laplacian structured sparsity term.",
        "If we first take the derivative of the latter two terms,",
        "and we can derive the following gradient components:",
        "Then, we combine the gradient of the log loss function in (5) with (17), and apply a bound-constrained reformulation (Schmidt et al., 2007) and the limited memory BFGS (L-BFGS) method (Liu and No-cedal, 1989) to solve the L1 regularized problem.",
        "The L-BFGS method has relatively low space complexity, and does not require the calculation of full Hessian matrix, thus it is often used for L1 optimization problems.",
        "Augmented Laplacian for an L1-Constrained Problem: Instead of formulating the L1-regularized problem by adding the L1 norm, an alternative solution is to formulate a L1-constrained problem by fixing the sum of all weights ?",
        "in the weight vector ~W .",
        "The reason is because adding the L1 norm will make the objective function not continuously differentiable, where as the L1 constraint could be just a simple linear constraint (Lee et al., 2006).",
        "Thus, the alternative L1-constrained problem could be defined as: min(?`), s.t.",
        "To test the robustness of Laplacian structured sparsity term in the setup of a L1-constrained problem, we can incorporate the Laplacian penalty term into the above formula, and derive:",
        "Note that the Laplacian matrix is positive",
        "because this graph Laplacian penalty can be viewed as a quadratic term, and the objective function in equation 19 is now convex differentiable and will produce sparse estimates, so that we are able to use a limited-memory projected quasi-Newton method (Schmidt et al., 2009) to solve the dual form of this problem.",
        "The Lagrangian dual form of the problem in equation 19 can be written as:",
        "where ?",
        "?",
        "R is a Lagrange multiplier, and ?",
        "?",
        "Rp+ is a p-dimensional vector of non-negative Lagrange multipliers.",
        "And then we can take first-order partial derivative with regard to ~W , and set it to zero to derive the optimality:",
        "To speed up the training, we use the linear-time L1-ball projection method from Duchi et al. (2008) in our implementation."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "We first compare our model to various baselines in the 3-way automatic brand identification task.",
        "Besides the logistic regression, lasso, ridge and elastic net model that we introduced in Section 4.2, we also compare with a PCA-based logistic regression model where the dimensions of the feature space is reduced in half before the classification.",
        "A state-of-the-art linear kernel SVM model (Chang and Lin, 2011) is also taken into the comparison.",
        "In the second part, we perform 9-way joint classification of the brand-satisfaction labels.",
        "Similarly, we also perform a 18-way joint classification of the brand-gender-satisfaction labels.",
        "To test the robustness of our model, we vary the levels of sparsity of our Laplacian augmented method in a L1-constrained problem.",
        "Finally, we analyze the identified features for CBA.",
        "Throughout this section, we use classification accuracy to report the results.",
        "We tune the regularization parameters of log-linear models and",
        "formances.",
        "The best result is highlighted in bold.",
        "* indicates p < .001 comparing to the second best result.",
        "the cost parameter of the SVM on the development set, and report results on both the development set and the held-out test set.",
        "The parameter for kNN was set to 5 according to previous literature (Gao et al., 2012).",
        "A paired two-tailed t-test is used to test the statistical differences among various models."
      ]
    },
    {
      "heading": "5.1 Automatic Brand Identification from Text",
      "text": [
        "Given any piece of raw text from the Web (e.g. blogs, tweets, news, or forum posts), the first task for CBA is to identify which brand this text is related to.",
        "Our customer review data set is useful for this task, because the ground truth of the brand label is attached to each review.",
        "Table 2 shows the result of our model in this automatic brand identification task.",
        "In this 3-way classification task, the overall results indicate that it is relatively easy to identify the related brand from customer reviews.",
        "When evaluating our Laplacian structured sparsity model, our proposed model obtains the best performances of 93.17% and 92.44%, which are statistically better than the second best results (p < .001) in both datasets."
      ]
    },
    {
      "heading": "5.2 Joint Brand-Satisfaction Prediction",
      "text": [
        "In our training data set, we observe a subtle correlation between the brand and satisfaction labels (r = 0.09, p < .001), which suggests us that it might be interesting to perform a joint prediction task for the brand-satisfaction labels.",
        "This task is also attractive from the business perspective, because it would be very useful for the companies to directly identify user's level of satisfaction about their brands.",
        "Table 3 shows that we achieve 69.56% accuracy on the",
        "performances.",
        "The best result is highlighted in bold.",
        "* indicates p < .001 comparing to the second best result.",
        "bold.",
        "* indicates p < .001 comparing to the second best result.",
        "development set, and 67.32% accuracy on the test set using our proposed Laplacian structured model (p < .001 comparing to the second best results)."
      ]
    },
    {
      "heading": "5.3 Joint Brand-Gender-Satisfaction Prediction",
      "text": [
        "Another big interest in the marketing community is to predict subgroup preferences of specific brands.",
        "In this direction, we perform a 18-way joint brand-gender-satisfaction prediction using the gender labels that we described in Section 3.",
        "Table 4 shows that our proposed Laplacian structured sparsity model obtains a test accuracy of 40.22%, significantly better than the second best result (p < .001).",
        "mance varying the level of sparsity ?",
        "in a L1 constrained problem."
      ]
    },
    {
      "heading": "5.4 Varying the Level of Sparsity in a L1-Constrained Problem",
      "text": [
        "To test the robustness of the Laplacian structured sparsity component, we exponentially increase the sum of weights ?",
        "to vary the level of sparsity in a L1-constrained setup.",
        "When ?",
        "increases, the non-zero weights in the model also increases.",
        "Figures 2 and 3 show that the Laplacian augmented L1-ball projection statistically outperform the L1-ball projection baseline in all levels of sparsity (p < .001).",
        "In Figure 4, Laplacian augmented L1-ball projection is also statistically better than the L1-ball projection (p < .001), except when ?",
        "= 32 and ?",
        "= 64."
      ]
    },
    {
      "heading": "5.5 Exploratory Data Analysis",
      "text": [
        "We outline the top 15 keywords from the Laplacian structured sparsity model that are associated with the Starbucks and Dunkin?",
        "Donuts brands in the automatic brand identification task in the Table 5.",
        "First of all, it is observed that our model has discovered synonyms for both brands: ?sbux?, ?dd?, ?dds?.",
        "performance varying the level of sparsity ?",
        "in a L1 constrained problem.",
        "Also, the results imply that Starbucks?",
        "unique cup size branding strategy, ?venti?, ?grande?, ?tall?, has resonated with their customers as the words prominently show up as top features in reviews.",
        "Aligned with previous study in marketing science (Moon and Quelch, 2006), an informative set of features related to Starbucks store decorations showed up in our model: ?store?, ?restroom?, ?public?, ?bathroom?, and ?spacious?.",
        "In contrast, these features stopped to show up on the list of Dunkin?",
        "Donuts.",
        "Instead, TV and game (sports), which are indeed important features of dining at Dunkin?",
        "Donut, appeared.",
        "Note that Baskin-Robbins, which is a sub-brand of Dunkin?",
        "Brands Group, Inc., also appeared as informative features to predict Dunkin?",
        "Donuts.",
        "To understand the preferences of different gender subgroups towards the two brands, we contrast in Table 6 and Table 7 the top features that identify the satisfied female and male customers in the joint brand-gender-satisfaction prediction task.",
        "Interestingly, it seems that the female customers identify Starbucks as a place for ?studying?, with ?fireplace?",
        "as the top preference of the spots in the store, and ?winter?",
        "is also a high-ranked feature.",
        "Also, the adjective ?super?",
        "was frequently mentioned by the female Starbucks customers (but not the males).",
        "As for Dunkin?",
        "Donuts, the top-ranked keywords are still mainly associated with its names, but it seems the snack ?Munchkins?",
        "is highly preferred by the female customers.",
        "Not surprisingly, the cue words that the male customers identify the Starbucks brand do not always agree with those of the females.",
        "For example, instead of ?fireplace?, they prefer staying at the ?patio?, and drink the coffee from the ?clover?",
        "brewing system.",
        "Interestingly,",
        "Dunkin?",
        "Donuts brands from the best model.",
        ".",
        "on the Dunkin's side, ?munchkins?",
        "also disappeared and replaced by ?glazed?",
        "(donuts).",
        "However, both males and females agreed that ?fast?",
        "or ?quick?",
        "service was an important feature of creating satisfaction, which echoes with the result from self-reported customer surveys (Moon and Quelch, 2006).",
        "The word ?name?",
        "is a prominent indicator for the female customers of Starbucks: at first we were puzzled, but after we digged into the database, we found reviews such as: ?",
        "?...",
        "and the baristas are one of the nicest they always ask for your name, so you never end up with coffee meant for the guy behind you.?",
        "?",
        "?...",
        "she asked me my name and i told her and she excidetly proclaimed melissa and wrote my name on the cup.",
        "This place was probably one of the better starbucks ive been to.?",
        "?",
        "?...",
        "all of their employees are really friendly, and embarrassingly enough most know me by name and know my typical drink order grande nonfat misto with a flavor shot of white mocha.",
        "This is actually very helpful.?",
        "The above examples show how our system effectively serves as a salient keyword spotter.",
        "And that as a keyword spotter one can use it to extract surrounding context and feed that through to the next",
        "male customers and the Starbucks and Dunkin?",
        "Donuts brands from the best model.",
        "stage of analysis, including examination by humans.",
        "This is extremely practical and useful, because it provides actionable items.",
        "For example, analysts can advise managers to revise their training manual and tell store employees to remember the names of your frequent female customers."
      ]
    },
    {
      "heading": "6 Discussions",
      "text": [
        "In our preliminary experiments, we have also experimented with the setup where the two keywords ?starbucks?",
        "and ?dunkin?",
        "were removed from the list of features.",
        "This change resulted in a uniformed 2% decrease in performances across all the models in Table 2, which did not affect the comparisons.",
        "However, we kept these two keywords in our final experiments, because the reviewers sometimes mention ?Dunkin?",
        "in Starbucks reviews, and vice versa.",
        "Removing the two keywords could be problematic, since it changes the natural distribution of the data.",
        "Regarding the alternative problem setups, our preliminary experiments showed that instead of using one-vs-all binary classifiers, a direct 9-way multiclass classification of joint brand-satisfaction labels using logistic regression only resulted an accuracy of 62%.",
        "We also did not adopt the hierarchical classification pipeline, where instead of performing joint classification, multiple layers of classifiers could be trained to classify brand, gender, and satisfaction labels incrementally.",
        "This is because the hierarchical classifiers suffered from the error propagation problem, and the second/third layer classifier could not correct the errors from the previous layers (Bennett and Nguyen, 2009).",
        "Our proposed method to generate inter-feature affinity matrix captures interesting dependency of features in this dataset.",
        "For example, although the words ?frappuccino?",
        "and ?slurping?, ?furniture?",
        "and ?mismatched?",
        "are semantically very different, our method actually group them together due to the subtle interactions of these word pairs in our tasks.",
        "The example in Figure 1 is also very specific to our dataset.",
        "This is very useful, because the word semantic similarity might be context-dependent, and our method learns and adapts the semantic similarity on the fly, hinges on the particular training set.",
        "On the other end of the spectrum, even though our method is desirable in our task, one might need to be cautious when working on very small data sets with only a handful of samples.",
        "This is because small samples typically have large variances in feature distributions, and that the generated Laplacian matrix might not be as reliable as in our study.",
        "To alleviate this potential problem, one might consider building the Laplacian matrix using external resources such as WordNet or FrameNet, even though this approach could also introduce biases due to the mismatched task domains.",
        "We also observed that the accuracy of the automatic brand identification task was high, indicating the promising future of CBA for hidden brand information from other genres of text over the Web.",
        "Although the performances of joint brand-satisfaction and joint brand-gender-satisfaction predictions are relatively lower, there is still much room for improvements: for example, using the syntactic, semantic, and meta-data features could potentially enrich the proposed model.",
        "Also, it is possible to consider the higher order n-gram features for better exploratory data analysis.",
        "However, since the focus of this paper is a proof of concept for Laplacian structured sparsity models and computational branding analytics, we have not yet explored various multi-view representations to augment our model.",
        "Why does Laplacian structured sparsity model work better in these classication tasks?",
        "Similar to the application in image classifcation (Gao et al., 2010), one advantage of Laplacian regularization in text classification is that our model can explicitly model the dependency of local features.",
        "Another reason is the expressiveness of our model: our model allows one to express the feature interactions in a structured manner.",
        "Thirdly, by embedding the structure in the regularization term, our model is more flexible: one can now control the structured penalty by tuning the regularization parameter on the development set."
      ]
    },
    {
      "heading": "7 Conclusions",
      "text": [
        "We introduce a Laplacian structured sparsity model for computational branding analytics (CBA).",
        "In the automatic brand identification, our model achieves the best result, dominating many competitive baselines.",
        "We also introduce the tasks of joint brand-satisfaction and brand-gender-satisfaction predictions, and show that the Laplacian structured sparsity do well in these tasks.",
        "A closer evaluation that varying the levels of sparsity in a L1 constrained problem also indicates that the Laplacian augmented L1-ball projection model can provide state-of-the-art results.",
        "By examining the weights of the derived Laplacian structured sparsity model, interesting indicators of brands and theirs gender-specific customer satisfaction associations are also discovered.",
        "In the future, we would like to investigate other methods for generating robust inter-feature Lapla-cians that include deep syntactic and semantic features."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": [
        "The authors would like to thank the anonymous reviewers for valuable comments."
      ]
    }
  ]
}
