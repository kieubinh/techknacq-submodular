{
  "info": {
    "authors": [
      "Moshe Koppel",
      "Shachar Seidman"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1151",
    "title": "Automatically Identifying Pseudepigraphic Texts",
    "url": "https://aclweb.org/anthology/D13-1151",
    "year": 2013
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The identification of pseudepigraphic texts ?",
        "texts not written by the authors to which they are attributed ?",
        "has important historical, forensic and commercial applications.",
        "We introduce an unsupervised technique for identifying pseudepigrapha.",
        "The idea is to identify textual outliers in a corpus based on the pairwise similarities of all documents in the corpus.",
        "The crucial point is that document similarity not be measured in any of the standard ways but rather be based on the output of a recently introduced algorithm for authorship verification.",
        "The proposed method strongly outperforms existing techniques in systematic experiments on a blog corpus."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The Shakespeare attribution problem is centuries old and shows no signs of abating.",
        "Some scholars argue that some, or even all, of Shakespeare's works were not actually written by him.",
        "The most mainstream theory ?",
        "and the one that interests us here ?",
        "is that most of the works were written by Shakespeare, but that several of them were not.",
        "Could modern methods of computational authorship attribution be used to detect which, if any, of the works attributed to Shakespeare were not written by him?",
        "More generally, this paper deals with the unsupervised problem of detecting pseudepigrapha: documents in a supposedly single-author corpus that were not actually written by the corpus's presumed author.",
        "Studies as early as Mendenhall (1887), have observed that texts by a single author tend to be somewhat homogeneous in style.",
        "If this is indeed the case, we would expect that pseudepi-grapha would be detectable as outliers.",
        "Identifying such outlier texts is, of course, a special case of general outlier identification, one of the central tasks of statistics.",
        "We will thus consider the pseudepigrapha problem in the context of the more general outlier detection problem.",
        "Typically, research on textual outliers assumes that we have a corpus of known authentic documents and are asked to decide if a specified other document is authentic or not (Juola and Stamata-tos, 2013).",
        "One crucial aspect of our problem is that we do not assume that any specific text in a corpus is known a priori to be authentic or pseudepigraphic; we can assume only that most of the documents in the corpus are authentic.",
        "The method we introduce in this paper builds on the approach of Koppel and Winter (2013) for determining if two documents are by the same author.",
        "We apply that method to every pair of documents in a corpus and use properties of the resulting adjacency graph to identify outliers.",
        "In the following section, we briefly outline previous work.",
        "In Section 3 we provide a framework for outlier detection and in Section 4 we describe our method.",
        "In Section 5 we describe the experimental setting and give results and in Section 6 we present results for the plays of Shakespeare."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Identifying outlier texts consists of two main stages: first, representing each text as a numerical vector representing relevant linguistic features of the text and second, using generic methods to identify outlier vectors.",
        "There is a vast literature on generic methods for outlier detection, summarized in Hodge & Austin (2004) and Chandola et al. (2009).",
        "Since our prob",
        "lem setup does not entail obtaining any labeled examples of authentic or outlier documents, supervised and semi-supervised methods are inapplicable.",
        "The available methods are unsupervised, principally probabilistic or proximity-based methods.",
        "A classical variant of such methods for univariate normally distributed data uses the the z-score (Grubbs, 1969).",
        "Such simple univariate outlier detectors are, however, inappropriate for identifying outliers in a high-dimensional textual corpus.",
        "Subsequent work, such as the Stahel-Donoho Estimator (Stahel, 1981; Donoho, 1982), PCout (Filzmoser et al., 2008), LOF (Breunig and Kriegel, 2000) and ABOD (Kriegel et al., 2008) have generalized univariate methods to high-dimensional data points.",
        "In his comprehensive review of outlier detection methods in textual data, Guthrie (2008) compares a variety of vectorization methods along with a variety of generic outlier methods.",
        "The vectorization methods employ a variety of lexical and syntactic stylistic features, while the outlier detection methods use a variety of similarity/distance measures such as cosine and Euclidean distance.",
        "Similar methods have also been used in the field of intrinsic plagiarism detection, which involves segmenting a text and then identifying outlier segments (Stama-tatos, 2009; Stein et al., 2010)."
      ]
    },
    {
      "heading": "3 Proximity Methods",
      "text": [
        "Formally, the problem we wish to solve is defined as follows: Given a set of documents D = {d1,?,dn}, all or most of which were written by author A, which, if any, documents in D were not written by A?",
        "We begin by considering the kinds of proximity methods for textual outlier detection considered by Guthrie (2008) and in the work on intrinsic plagiarism detection; these will serve as baseline methods for our approach.",
        "The idea is simple: mark as an outlier any document that is too far from the rest of the documents in the corpus.",
        "We briefly sketch the key steps:",
        "1.",
        "Represent a document as a numerical vector.",
        "The kinds of measurable features that can be used to represent a document include frequencies of word unigrams, function words, parts-of-speech and character n-grams, as well as complexity measures such as type/token ratio, sentence and word length and so on.",
        "2.",
        "Measure the similarity of two document vectors.",
        "We can use either inverses of distance measures such as Euclidean distance or Manhattan distance, or else direct similarity measures such as cosine or min-max.",
        "3.",
        "Use an aggregation method to measure the similarity of a document to a set of documents.",
        "One approach is to simply measure the distance from a document to the centroid of all the other documents (centroid method).",
        "Another approach is to first measure the similarity of a document to each other document and then to aggregate the results by averaging all the obtained values (mean method): Alternatively, we can average the values only for the k nearest neighbors (k-NN method): (where Dk = k nearest neighbors of di).",
        "Yet another method is to use median distance (median method).",
        "We note that the centroid method and mean method suffer from the masking effect (Bendre and Kale, 1987; Rousseeuw and Leroy, 2003): the presence of some outliers in the data can greatly distort the estimator's results regarding the presence of other outliers.",
        "The k-NN method and the median method are both much more robust.",
        "4.",
        "Choose some threshold beyond which a document is marked as an outlier.",
        "Choosing the threshold is one of the central issues in statistical approaches.",
        "For our purposes, however, the choice of threshold is simply a parameter trading off recall and precision."
      ]
    },
    {
      "heading": "4 Second-Order Similarity",
      "text": [
        "Our approach is to use an entirely different kind of similarity measure in Step 2.",
        "Rather than use a first-order similarity measure, as is customary, we employ a second-order similarity measure that is the output of an algorithm used for the authorship verification problem (Koppel et al. 2011), in which we need to determine if two, possibly short, documents were written by the same author.",
        "That algorithm, known as the ?impostors method?",
        "(IM), works as follows.",
        "Given two docu",
        "ments, d1 and d2, generate an appropriate set of impostor documents, p1,?,pm and represent each of the documents in terms of some large feature set (for example, the frequencies of various words or character n-grams in the document).",
        "For some random subset of the feature set, measure the similarity of d1 to d2 as well as to each of the documents p1,?,pm and note if d1 is closer to d2 than to any of the impostors.",
        "Repeat this k times, choosing a different random subset of the features in each iteration.",
        "If d1 is closer to d2 than to any of the impostors (and likewise switching the roles of d1 and d2) for at least ?% of iterations, then output that d2 and d1 are the same author.",
        "(The parameter ?",
        "is used to trade-off recall and precision.)",
        "Adapting that method for our purposes, we use the proportion of iterations for which d1 is closer to d2 than to any of the impostors as our similarity measure (adding a small twist to make the measure symmetric over d1 and d2, as can be seen in line 2.2.2 of the algorithm).",
        "More precisely, we do the following:",
        "Given: Corpus D={d1,?,dn} 1.",
        "Choose a feature set FS for representing documents, a first-order similarity measure sim, and an impostor set {p1,?,pm}.",
        "2.",
        "For each pair of documents <di, dj> in set D: 2.1.",
        "Let sim2(di, dj) := 0 2.2.",
        "Iterate K times: 2.2.1.",
        "Randomly choose 40% of features in FS 2.2.2.",
        "If sim(di, dj) 2 > maxu?",
        "{1,..,m}sim(di, pu)*maxu?",
        "{1,..,m}sim(dj, pu), then sim2(di, dj) ?",
        "sim2(di, dj) + 1/K 3.",
        "For each document di in set D: 3.1.",
        "Compute sim2(di, D) = agg w?",
        "{1,..,n}[sim2(di, dw)] where agg is some aggregation function 3.2.",
        "If sim2(di, D) < ?",
        "(where ?",
        "is a parameter),",
        "then mark di as outlier.",
        "The method for choosing the impostor set is corpus-dependent, but quite straightforward: we simply choose random impostors from the same genre and language as the documents in question.",
        "The choice of feature set FS, first-order similarity measure sim, and aggregation function agg can be varied.",
        "For FS, we simply use bag-of-words (BOW).",
        "As for sim and agg, we show below results of experiments comparing the effectiveness of various choices for these parameters.",
        "Using second-order similarity has several surface advantages over standard first-order measures.",
        "First, it is decisive: for most pairs, second-order similarity will be close to 0 or close to 1.",
        "Second, it is self-normalizing: scaling doesn't depend on the size of the underlying feature sets or the lengths of the documents.",
        "As we will see, it is also simply much more effective for identifying outliers."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "We begin by assembling a corpus consisting of 3540 blog posts written by 156 different bloggers.",
        "The blogs are taken from the blog corpus assembled by Schler et al. (2006) for use in authorship attribution tasks.",
        "Each of the blogs was written in English by a single author in 2004 and each post consists of 1000 words (excess is truncated).",
        "For our initial experiments, each trial consists of 10 blog posts, all but p of which are by a single blogger.",
        "The number of pseudepigraphic documents, p, is chosen from a uniform distribution over the set {0,1,2,3}.",
        "Our task is to identify which, if any, documents in the set are not by the main author of the set.",
        "The pseudepigraphic documents might be written by a single author or by multiple authors.",
        "To measure the performance of a given similarity measure sim, we do the following in each trial:",
        "1.",
        "Represent each document in the trial set D in terms of BOW.",
        "2.",
        "Measure the similarity of each pair of documents in the trial set using the similarity measure sim.",
        "3.",
        "Using some aggregation function agg, compute for each document di: sim(di, D) = agg w?",
        "{1,..,n}[sim(di, dw)].",
        "4.",
        "If sim (di, D) < ?, mark di as an outlier",
        "(where ?",
        "is a parameter ).",
        "Our objective is to show that results using second-order similarity are stronger than those using first-order similarity.",
        "Before we do this, we need to determine the best aggregation function to use in our experiments.",
        "In Figure 1, we show recall-precision breakeven values (for the outlier class) over 250 independent trials, for each of our four first-order similarity measures (inverse Euclidean, inverse Manhattan, cosine, min-max) used in conjunction with each of four aggregation functions (centroid, mean, k-NN mean, median).",
        "As is evident, k-NN is the best aggregation function in each case.",
        "We will give these baseline methods an advantage by using k-NN as our aggregation function in all our subsequent experiments.",
        "We are now ready to perform our main experiment.",
        "We use BOW as our feature set and k-NN as our aggregation function.",
        "We use 500 random blog posts as our impostor set.",
        "In Figure 2, we show recall-precision curves for outlier documents over 250 independent trials, as just described, using four first-order similarity measures as well our second-order similarity measure using each of the four as a base measure.",
        "As can be seen, even the worst second-order similarity measure significantly outperforms all the standard first-order measures.",
        "In Figure 3, we show the breakeven values for each measure, pairing each first-order measure with the second-order measure that uses it as a base.",
        "Clearly, the mere use of a second-order method improves results, regardless of the base measure.",
        "Thus far we have considered authorial corpora consisting of only ten documents.",
        "In Figures 4 and 5, we repeat the experiment described in Figures 2 and 3 above, but with each trial consisting of 50 documents including any number of pseudepigraphic documents in the range 0 to 15.",
        "The same phenomenon is apparent: second-order similarity strongly improves results over the corresponding first-order base similarity measure."
      ]
    },
    {
      "heading": "6 Results on Shakespeare",
      "text": [
        "We applied our methods to the texts of 42 plays by Shakespeare (taken from Project Gutenberg).",
        "We included two plays by Thomas Kyd as sanity checks.",
        "In addition, we included three plays occasionally attributed to Shakespeare, but generally regarded by authorities as pseudepigrapha (A Yorkshire Tragedy, The Life of Sir John Oldcastle and Pericles Prince of Tyre).",
        "We also included King Edward III and King Henry VI (Part 1), both of which are subjects of dispute among Shakespeare scholars.",
        "As impostors we used 39 works by contemporaries of Shakespeare, including Christopher Marlowe, Ben Jonson and John Fletcher.",
        "We found that the two plays by Thomas Kyd and the three pseudepigraphic plays were all among the seven furthest outliers, as one would expect.",
        "In addition, King Edward III was 9th furthest.",
        "King Henry VI (Part 1) was not found to be an outlier at all.",
        "Curiously, however, three undisputed plays by Shakespeare were found to be greater outliers than King Edward III.",
        "These are The Merry Wives of Windsor, The Comedy of Errors and The Tragedy of Julius Caesar.",
        "The Merry Wives of Windsor is a particularly distant outlier, even further out than Oldcastle and Pericles.",
        "We leave it to Shakespeare scholars to explain the reasons for these anomalies."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "In this paper we defined the problem of unsupervised outlier detection in the authorship verification domain.",
        "Our method combines standard outlier detection methods with a novel inter-document similarity measure.",
        "This similarity measure is the output of the impostors method recently developed for solving the authorship verification problem.",
        "We have found that use of the kNN method for outlier detection in conjunction with this second-order similarity measure strongly outperforms methods based on any outlier detection method used in conjunction with any standard first-order similarity measures.",
        "This improvement proves to be robust, holding for various corpus sizes and various underlying base similarity measures used in the second-order similarity measure.",
        "The method can be used to resolve historical conundrums regarding the authenticity of works in questioned corpora, such as the Shakespeare corpus briefly considered here.",
        "This is currently the subject of our ongoing research."
      ]
    }
  ]
}
