{
  "info": {
    "authors": [
      "Ivan VuliÄ‡",
      "Marie-Francine Moens"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1168",
    "title": "A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data (and Nothing Else)",
    "url": "https://aclweb.org/anthology/D13-1168",
    "year": 2013
  },
  "references": [
    "acl-C10-1070",
    "acl-C10-2055",
    "acl-C94-1049",
    "acl-D07-1070",
    "acl-D09-1092",
    "acl-D12-1001",
    "acl-D12-1003",
    "acl-D12-1121",
    "acl-E12-1029",
    "acl-J00-2004",
    "acl-J03-1002",
    "acl-J10-4006",
    "acl-N01-1026",
    "acl-N10-1087",
    "acl-N10-1135",
    "acl-N13-1011",
    "acl-P04-1067",
    "acl-P08-1088",
    "acl-P09-1007",
    "acl-P09-1045",
    "acl-P10-1011",
    "acl-P10-1026",
    "acl-P10-1115",
    "acl-P11-1061",
    "acl-P11-1133",
    "acl-P11-2052",
    "acl-P11-2071",
    "acl-P11-2084",
    "acl-P98-1069",
    "acl-P99-1004",
    "acl-P99-1067",
    "acl-Q13-1001",
    "acl-W02-0902",
    "acl-W02-1028",
    "acl-W04-3208"
  ],
  "sections": [
    {
      "heading": "KU Leuven Celestijnenlaan 200A",
      "text": []
    },
    {
      "heading": "Abstract",
      "text": [
        "We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion.",
        "The paper systematically introduces and describes all key elements of the bootstrapping procedure: (1) starting point or seed lexicon, (2) the confidence estimation and selection of new dimensions of the space, and (3) convergence.",
        "We test the quality of the induced bilingual vector spaces, and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction (BLE) for two language pairs.",
        "Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons.",
        "We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Bilingual lexicons serve as an indispensable source of knowledge for various cross-lingual tasks such as cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) or statistical machine translation (Och and Ney, 2003).",
        "Additionally, they are a crucial component in cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another.",
        "The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pado?",
        "and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; Ta?ckstro?m et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Ta?ckstro?m et al., 2013a), etc.",
        "Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003).",
        "However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)).",
        "The models that rely on non-parallel data typically represent each word by a high-dimensional vector in a feature vector space, where the dimensions of the vector are its context features.",
        "The context features are typically words co-occurring with the word in a predefined context.",
        "The similarity of two words, wS1 given in the source language LS with vocabulary V S and wT2 in the target language LT with vocabulary V T is then computed as sim(wS1 , wT2 ) = SF (cv(wS1 ), cv(wT2 )).",
        "cv(wS1 ) = [scS1 (c1), .",
        ".",
        ".",
        ", scS1 (cN )] is a context vector for wS1 with N context features ck, where scS1 (ck) denotes the score for wS1 associated with context feature ck (similar for wT2 ).",
        "SF is a similarity function (e.g., cosine, the Kullback-Leibler divergence, the Jaccard index) operating on the context vectors (Lee, 1999).",
        "When operating with 2 languages, the context features cannot be compared directly.",
        "Therefore, in order to compare the feature vectors cv(wS1 ) and cv(wT2 ), the context features need to span a shared 1The context may be a document, a paragraph, a window of predefined size around each occurrence of wSi in CS , etc.",
        "For an overview, see, e.g., (Tamura et al., 2012).",
        "bilingual vector space.",
        "The standard way of building a bilingual vector space is to use bilingual lexicon entries (Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004) as dimensions of the space.",
        "However, there seems to be an apparent flaw in logic, since the methods assume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons!",
        "Therefore, the focus of the researchers has turned to designing BLE methods that do not rely on any external translation resources such as machine-readable bilingual lexicons and parallel corpora (Haghighi et al., 2008; Vulic?",
        "et al, 2011).",
        "In order to circumvent this issue, one line of recent work aims to bootstrap high-quality bilingual vector spaces from a small initial seed lexicon.",
        "The seed lexicon is constructed by harvesting identical or similarly spelled words across languages (Koehn and Knight, 2002; Peirsman and Pado?, 2010), and it spans the initial bilingual vector space.",
        "The space is then gradually enriched with new dimensions/axes during the bootstrapping procedure.",
        "The bootstrapping process has already proven its validity in inducing bilingual lexicons for closely similar languages such as Spanish-Portuguese or Croatian-Slovene (Fis?er and Ljubes?ic?, 2011), but it still lacks further generalization to more distant language pairs.",
        "The main goal of this paper is to shed new light on the bootstrapping approaches to bilingual lexicon extraction, and to construct a language pair agnostic bootstrapping method that is able to build high-quality bilingual vector spaces that consequently lead to high-quality bilingual lexicons for more distant language pairs where orthographic similarity is not sufficient to seed bilingual vector spaces.",
        "We aim to answer the following key questions: ?",
        "How to seed bilingual vector spaces besides using only orthographically similar words?",
        "?",
        "Is it better to seed bilingual spaces with translation pairs/dimensions that are frequent in the corpus, and does the frequency matter at all?",
        "Does the size of the initial seed lexicon matter?",
        "?",
        "How to enrich bilingual vector spaces with only highly reliable dimensions in order to prevent semantic drift?",
        "With respect to these questions, the main contributions of this article are: ?",
        "We present a complete overview of the framework of bootstrapping bilingual vector spaces from non-parallel data without any additional resources.",
        "We dissect the bootstrapping process and describe all its key components.",
        "?",
        "We introduce a new way of seeding the bootstrapping procedure that does not rely on any orthographic clues and that yields bilingual vector spaces of higher quality.",
        "We analyze the impact of different seed lexicons on the quality of induced bilingual vector spaces.",
        "?",
        "We show that in the setting without any external translation resources, our bootstrapping approach yields lexicons that outperform the best performing corpus-based BLE methods on standard test datasets for 2 language pairs."
      ]
    },
    {
      "heading": "2 Boostrapping Bilingual Vector Spaces: A General Overview",
      "text": [
        "This section presents the complete bootstrapping procedure that starts with an initial seed lexicon which spans the initial bilingual vector space, while as the output in each iteration of the procedure it produces an updated bilingual vector space that can be used to extract a bilingual lexicon."
      ]
    },
    {
      "heading": "2.1 General Framework",
      "text": [
        "We assume that we are solely in possession of a (non-parallel) bilingual corpus C that is composed of a sub-corpus CS given in the source language LS , and a sub-corpus CT in the target language LT .",
        "All word types that occur in CS constitute a set V S .",
        "All word types in CT constitute a set V T .",
        "The goal is to build a bilingual vector space using only corpus C. Assumption 1.",
        "Dimensions of the bilingual vector space are one-to-one word translation pairs.",
        "For instance, dimensions of a Spanish-English space are pairs like (perro, dog), (ciencia, science), etc.",
        "The one-to-one constraint (Melamed, 2000), although not valid in general, simplifies the construction of the bootstrapping procedure.",
        "Z denotes the set of translation pairs that are the dimensions of the space.",
        "Computing cross-lingual word similarity in a bilingual vector space.",
        "Now, assume that our bilingual vector space consists of N one-to-one word translation pairs ck = (cSk , cTk ), k = 1, .",
        ".",
        ".",
        ", N .",
        "For each word wSi ?",
        "V S , we compute the similarity of",
        "that word with each word wTj ?",
        "V T by computing the similarity between their context vectors cv(wSi ) and cv(wTj ), which are actually their representations in the N dimensional bilingual vector space.",
        "The cross-lingual similarity is computed following the standard procedure (Gaussier et al., 2004): (1) For each source word wSi ?",
        "V S , build its N - dimensional context vector cv(wSi ) that consists of association scores scSk (cSk ), that is, we compute the strength of association with the ?source?",
        "part of each dimension ck that constitutes the N dimensional bilingual space.",
        "The association is dependent on the co-occurrence of wSi and cSk in a predefined context.",
        "Various functions such as the log-likelihood ratio (LLR) (Rapp, 1999; Ismail and Manandhar, 2010), TF-IDF (Fung and Yee, 1998), or pointwise mutual information (PMI) (Bullinaria and Levy, 2007; Shezaf and Rappoport, 2010) are typically used as weighting functions to quantify the strength of the",
        "association.",
        "(2) Repeat step (1) for each target word wTj ?",
        "V T and build context vectors cv(wTj ) that consist of scores scTk (cTk ).",
        "(3) Since cSk and cTk address the same dimension ck in the bilingual vector space for each k = 1, .",
        ".",
        ".",
        ", N , we are able to compute the similarity between cv(wSi ) and cv(wTj ) using any similarity mea",
        "sure such as the Jaccard index, the Kullback-Leibler or the Jensen-Shannon divergence, the cosine measure, or others (Lee, 1999; Cha, 2007).",
        "The similarity score for two words wSi and wTj is sim(wSi , wTj ).",
        "For each source word wSi , we can build a ranked listRL(wSi ) that consists of all words wTj ?",
        "V T ranked according to their respective similarity scores sim(wSi , wTj ).",
        "In the similar fashion, we can build a ranked list RL(wTj ), for each target word wTj .",
        "We call the top scoring target word wTj for some source word wSi its translation candidate, and write TC(wSi ) = wTj .",
        "Additionally, we label the ranked list RL(wSi ) that is pruned at position M as RLM (wSi ).",
        "Bootstrapping.",
        "The key idea of the bootstrapping approach relies on an insight that highly reliable translation pairs (wS1 , wT2 ) that are encountered using the N dimensional bilingual vector space might be added as new dimensions of the space.",
        "By adding these new dimensions, it might be possible to extract more highly reliable translation pairs that were previously not used as dimensions of the space, and the iterative procedure repeats until no new dimensions are found.",
        "The induced bilingual vector space may then be observed as a bilingual lexicon per se, but it may also be used to find translation equivalents for other words which are not used to span the space.",
        "Algorithm 1: Bootstrapping a bilingual vector space Input : Bilingual corpus C = CS ?",
        "CT Initialize: (1) Obtain a one-to-one seed lexicon.",
        "The entries from the lexicon are initial dimensions of the space: Z0; (2) s = 0;",
        "and add them to a pool of candidate dimensions ;",
        "4: Choose the best candidates from the pool and add them as new dimensions: Zs+1 ?",
        "Zs ?",
        "{best} ; 5: Resolve collisions in Zs+1; 6: s?",
        "s + 1 ;",
        "until no new dimensions are found (convergence) ; Output: One-to-one translation pairs?",
        "Dimensions of a bilingual vector space Zfinal The overview of the procedure as given by alg.",
        "1 reveals these crucial points in the procedure: (Q1) how to provide initial dimensions of the space?",
        "(the initialization step), (Q2) how to score each translation pair, estimate their confidence, and how to choose the best candidates from the pool of candidates?",
        "(steps 3 and 4), and (Q3) how to resolve potential collisions that violate the one-to-one constraint?",
        "(step 5).",
        "We will discuss (Q1) and (Q2) in more detail later, while we resolve (Q3) following a simple heuristic as follows: Assumption 2.",
        "In case of collision, dimensions/pairs that are found at later stages of the bootstrapping process overwrite previous dimensions.",
        "The intuition here is that we expect for the quality of the space to increase at each stage of the bootstrapping process, and newer translation pairs should be more confident than the older ones.",
        "For instance, if 2 out of N dimensions of a Spanish-English bilingual space are pairs (piedra,wall) and (tapia,stone), but then if during the bootstrapping process we extract a new candidate pair (piedra,stone), we will delete the former two dimensions and add the latter."
      ]
    },
    {
      "heading": "2.2 Initializing Bilingual Vector Spaces",
      "text": [
        "Seeding or initializing a bootstrapping procedure is often a critical step regardless of the actual task (McIntosh and Curran, 2009; Kozareva and Hovy, 2010), and it decides whether the complete process will end as a success or a failure.",
        "However, Peirsman and Pado?",
        "(2011) argue that the initialization step is not crucial when dealing with bootstrapping bilingual vector spaces.",
        "Here, we present two different strategies of initializing the bilingual vector space.",
        "Identical words and cognates.",
        "Previous work relies exclusively on identical and similarly spelled words to build the initial set of dimensions Z0 (Koehn and Knight, 2002; Peirsman and Pado?, 2010; Fis?er and Ljubes?ic?, 2011).",
        "This strategy yields promising results for closely similar language pairs, but is of limited use for other language pairs.",
        "High-frequency seeds.",
        "Another problem with using only identical words and cognates as seeds lies in the fact that many of them might be infrequent in the corpus, and as a consequence the expressiveness of a bilingual vector space might be limited.",
        "On the other hand, high-frequency words offer a lot of evidence in the corpus that could be exploited in the bootstrapping approach.",
        "In order to induce initial translation pairs, we rely on the framework of multilingual probabilistic topic modeling (MuPTM) (Boyd-Graber and Blei, 2009; De Smet and Moens, 2009; Mimno et al., 2009; Zhang et al., 2010), that does not require a bilingual lexicon, it operates with non-parallel data, and is able to produce highly confident translation pairs for high-frequency words (Mimno et al., 2009; Vulic?",
        "and Moens, 2013).2 Therefore,",
        "we can construct the initial seed lexicon as follows: (1) Train a multilingual topic model on the corpus.",
        "(2) Obtain one-to-one translation pairs using any of the MuPTM-based models of cross-lingual similarity, e.g., (Vulic?",
        "et al, 2011; Vulic?",
        "and Moens, 2013).",
        "(3) Retain only symmetric translation pairs.",
        "This step ensures that only highly confident pairs are used as seed translation pairs.",
        "(4) Rank translation pairs according to their fre",
        "such as (Haghighi et al., 2008; Daume?",
        "III and Jagarlamudi, 2011) to produce the initial seed lexicon, but that analysis is beyond the scope of this work.",
        "frequent symmetric pairs as seeds."
      ]
    },
    {
      "heading": "2.3 Estimating Confidence of New Dimensions",
      "text": [
        "Another crucial step in the bootstrapping procedure is the estimation of confidence in a translation pair/candidate dimension.",
        "Errors in the early stages of the procedure may negatively affect the learning process and even cause semantic drift (Riloff and Shepherd, 1999; McIntosh and Curran, 2009).",
        "We therefore impose the constraint which requires translation pairs to be symmetric in order to qualify as potential new dimensions of the space.",
        "In other words, given the current set of dimensions Zs, a translation pair (wSi , wTj ) has a possibility to be chosen as a new dimension from the pool of candidate dimensions if and only if it holds: TC(wSi ) = wTj and TC(wTj ) = wSi .",
        "This symmetry constraint should ensure a relative reliability of translation pairs.",
        "In each iteration of the bootstrapping process, we may add all symmetric pairs from the pool of candidates as new dimensions, or we could impose additional selection criteria that quantify the degree of confidence in translation pairs.",
        "We are then able to rank the symmetric candidate translation pairs in the pool of candidates according to their confidence scores (step 3 of alg.",
        "1), and choose only the best B candidates from the pool in each iteration (step 4) as done in (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Huang and Riloff, 2012).",
        "By picking only a subset of the B most confident candidates in each iteration, we hope to further prevent a possibility of semantic drift, i.e., ?poisoning?",
        "the bootstrapping process that might happen if we include incorrect translation pairs as dimensions of the space.",
        "In this paper, we investigate 3 different confidence estimation functions:3",
        "(1) Absolute similarity score.",
        "Confidence of a translation pair CF (wSi , TC(wSi )) is simply the absolute similarity value sim(wSi , TC(wSi )) (2) M-Best confidence function.",
        "It contrasts the score of the translation candidate with the average score over the first M most similar words in the ranked list.",
        "The larger the difference, the more con",
        "fidence we have in the translation candidate.",
        "Given a word wSi ?",
        "V S and a ranked list RLM (wSi ), the 3A symmetrized version of the confidence functions is computed as the geometric mean of source-to-target and target-to-source confidence scores.",
        "average score of the best M words is computed as:",
        "The final confidence score is then:",
        "CF (wSi , TC(wSi )) = sim(wSi , TC(wSi ))?",
        "simM (wSi ) (3) Entropy-based confidence function.",
        "We adapt the well-known entropy-based confidence (Smith and Eisner, 2007; Tu and Honavar, 2012) to this particular task.",
        "First, we need to define a distribution:",
        "The confidence function is then minus the entropy of the probability distribution p:"
      ]
    },
    {
      "heading": "3 Experimental Setup",
      "text": [
        "Data collections.",
        "We investigate our bootstrapping approach on the BLE task for 2 language pairs: Spanish-English (ES-EN) and Italian-English (IT-EN), and work with the following corpora previously used by Vulic?",
        "and Moens (2013): (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (Wiki-ES-EN), (ii) 18, 898 Italian-English Wikipedia article pairs (Wiki-IT-EN).4 Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011; Vulic?",
        "and Moens, 2013), we use TreeTagger (Schmid, 1994) for POS-tagging and lemmatization of the corpora, and then retain only nouns that occur at least 5 times in the corpus.",
        "We record the lemmatized form when available, and the original form otherwise.",
        "Our final vocabularies consist of 9, 439 Spanish nouns and 4Vulic?",
        "and Moens (2013) also worked with Dutch-English (NL-EN), but we have decided to leave out the results obtained for that language pair due to space constraints, high similarity between the two languages, and the fact that the results obtained for that language pair are qualitatively similar to the results we report for ES-EN and IT-EN.",
        "Hence including the results for NL-EN would not contribute to the paper with any new important insight and conclusion.",
        "12, 945 nouns for ES-EN, and 7, 160 Italian nouns and 9, 116 English nouns for IT-EN.",
        "Ground truth.",
        "The goal of the BLE task is to extract a bilingual lexicon of one-to-one translations.",
        "In order to test the quality of bilingual vector spaces induced by our bootstrapping approach, we evaluate it on standard 1000 ground truth one-to-one translation pairs built for the Wiki-ES-EN and Wiki-ITEN datasets (Vulic?",
        "and Moens, 2013).",
        "Note that we do not explicitly test the bilingual vector space as a bilingual lexicon, but rather its ability to find semantically similar words and translations also for words that are not used as dimensions of the space (see sect.",
        "2.1).",
        "Evaluation metrics.",
        "We measure the performance on the BLE task using a standard Top M accuracy (AccM ) metric.",
        "It denotes the number of source words wSi from ground truth translation pairs whose list RLM (wSi ) contains the correct translation according to our ground truth over the total number of ground truth translation pairs (=1000) (Gaussier et al., 2004; Tamura et al., 2012).5 Additionally, we report the mean reciprocal rank (MRR) scores (Voorhees, 1999) for some experimental runs.",
        "Multilingual topic model.",
        "We utilize a straightforward multilingual extension of the standard Blei et al.",
        "'s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009).",
        "BiLDA training follows the procedure from (Vulic?",
        "and Moens, 2013), that is, the training method is Gibbs sampling with the number of topics set to K = 2000.",
        "Hyperparameters of the model are set to standard values (Steyvers and Grif-fiths, 2007): ?",
        "= 50/K and ?",
        "= 0.01.",
        "Building initial seed lexicons.",
        "To produce the lists of one-to-one translation pairs that are used as seeds for the bootstrapping approach (see sect.",
        "2.2), we experiment with the TopicBC and the ResponseBC methods from (Vulic?",
        "and Moens, 2013), which are the MuPTM-based models of cross-lingual semantic similarity that obtain the best results in the BLE task on these datasets.",
        "In short, the TopicBC method computes the similarity of two words according to the similarity of their conditional topic distributions (Griffiths et al., 2007; Vulic?",
        "et al, 2011) using",
        "the Bhattacharyya coefficient (BC) (Kazama et al., 2010) as the similarity function.",
        "ResponseBC is a second-order similarity method.",
        "It first computes initial similarity scores between all words cross-lingually and monolingually using the cross-lingual topical space and, in the second step, it computes the similarity between 2 words as the similarity between their word vectors that now contain the initial word-to-word similarity scores with all source and target words.",
        "The similarity function is again BC.",
        "We use these models of similarity as a black box to acquire seeds for the bootstrapping approach, but we encourage the interested reader to find more details about the methods in the relevant literature.",
        "These two models also serve as our baseline models, and our goal is to test whether we are able to obtain bilingual lexicons of higher quality using bootstrapping that starts from the output of these models.",
        "Weighting and similarity functions.",
        "We have experimented with different families of weighting (e.g., PMI, LLR, TF-IDF, chi-square) and similarity functions (e.g., cosine, Dice, Kullback-Leibler, Jensen-Shannon) (Lee, 1999; Turney and Pantel, 2010).",
        "In this paper, we present results obtained by positive pointwise mutual information (PPMI) (Niwa and Nitta, 1994) as a weighting function, which is a standard choice in vector space semantics (Turney and Pantel, 2010), and (combined with cosine) yields the best results over a group of semantic tasks according to (Bullinaria and Levy, 2007).",
        "We use a smoothed version of PPMI as presented in (Pantel and Lin, 2002; Turney and Pantel, 2010).",
        "Again, based on the results reported in the relevant literature (Bullinaria and Levy, 2007; Laroche and Langlais, 2010; Turney and Pantel, 2010), we opt for the cosine similarity as a standard choice for SF .",
        "We have also experimented with different window sizes ranging from 3 to 15 in both directions around the pivot word, but we have not detected any major qualitative difference in the results and their interpretation.",
        "Therefore, all results reported in the paper are obtained by setting the window size to 6."
      ]
    },
    {
      "heading": "4 Results and Discussion",
      "text": []
    },
    {
      "heading": "4.1 Are Seeds Important?",
      "text": [
        "In recent work, Peirsman and Pado?",
        "(2010; 2011) report that ?the size and quality of the (seed) lexicon are not of primary importance given that the bootstrapping procedure effectively helped filter out incorrect translation pairs and added more newly identified mutual nearest neighbors.?",
        "According to their findings, (1) noisy translation pairs are corrected in later stages of the bootstrapping process, since the quality of bilingual vector spaces gradually increases, (2) the size of the seed lexicon does not matter since the bootstrapping approach is able to learn translation pairs that were previously not present in the seed lexicon.",
        "Additionally, they do not provide any insight whether the frequency of seeds in the corpus influences the quality of induced bilingual vector spaces.",
        "In this paper, we question these claims with a series of BLE experiments.",
        "All experiments conducted in this section do not rely on any extra confidence estimation except for the symmetry constraint, that is, in each step we enrich the bilingual vector space with all new symmetric translation pairs (see alg.",
        "1 and sect.",
        "2.3).",
        "Exp.",
        "I: Same size, different seedings?",
        "The goal of this experiment is to test whether the quality of seeds plays an important role in the bootstrapping approach.",
        "We experiment with 3 different seed lexicons: (1) Following (Peirsman and Pado?, 2010; Fis?er and Ljubes?ic?, 2011), we harvest identically spelled words across 2 languages and treat them as one-to-one translations.",
        "This procedure results in 459 seed translation pairs for ES-EN, and 431 pairs for IT-EN (SEED-ID), (2) We obtain symmetric translation pairs using the TopicBC method (see sect.",
        "3) and use 459 pairs that have the highest frequency in the Wiki-ES-EN corpus as seeds for ES-EN (similarly 431 pairs for IT-EN) (SEED-TB), (3) As in (2), but we now use the ResponseBC method to acquire seeds (SEED-RB).",
        "The frequency of a one-to-one translation pair is simply computed as the geometric mean of the frequencies of words that constitute the translation pair.",
        "Fig.",
        "1(a) and 1(b) display the progress of the same bootstrapping procedure using the 3 different seed lexicons.",
        "We derive several interesting conclusions: (i) Regardless of the actual choice of the seeding method, the bootstrapping process proves its validity and utility since we observe that the quality of induced bilingual vector spaces increases over time for all 3 seeding methods.",
        "The bootstrapping procedure converges quickly.",
        "The increase is especially",
        "ping approach that starts with a better seed lexicon is able to extract bilingual lexicons of higher quality as reflected in Acc1 scores.",
        "Although the bootstrapping approach seems more beneficial when dealing with noisier seed lexicons (226% increase in terms of Acc1 for ES-EN and 177% increase for IT-EN when starting with SEED-ID, compared to 35% increase for ES-EN, and 15% for IT-EN with SEED-RB), when starting from a noisy seed lexicon such as SEED-ID the method is unable to reach the same level of performance.",
        "Starting with SEED-ID, the approach is able to recover noisy dimensions from an initial bilingual vector space, but it is still unable to match the results that are obtained when starting from a better initial space (e.g., SEED-RB).",
        "(iii) SEED-RB produces slightly better results than SEED-TB (e.g., the final Acc1 of 0.649 for SEED-RB compared to 0.626 for SEED-TB for IT-EN, and 0.572 compared to 0.553 for ES-EN).",
        "This finding is in line with the results reported in (Vulic?",
        "and Moens, 2013) where ResponseBC proved to be a more robust and a more effective method when applied to the BLE task directly.",
        "In all further experiments we use ResponseBC to acquire seed pairs, i.e., the seeding method is SEED-RB.",
        "Exp.",
        "II: Does the frequency of seeds matter?",
        "In the next experiment, we test whether the frequency of seeds in the corpus plays an important role in the bootstrapping process.",
        "The intuition is that by using highly frequent and highly confident translation pairs the bootstrapping method has more reliable clues that help extract new dimensions in subsequent iterations.",
        "On the other hand, low-frequency",
        "pairs (although potentially correct one-to-one translations) do not occur in the corpus and in the contexts of other words frequently enough, and are therefore not sufficient to extract reliable new dimensions of the space.",
        "To test the hypothesis, we again obtain all symmetric translation pairs using ResponseBC and then sort them in descending order based on their frequency in the corpus.",
        "In total, we retrieve a sorted list of 2031 symmetric translation pairs for ES-EN, and 1689 pairs for IT-EN.",
        "Following that, we split the list in 3 parts of equal size: (i) the top third comprises translation pairs with the highest frequency in the corpus (HF-SEED), (ii) the middle third comprises pairs of ?medium?",
        "frequency (MF-SEED), (iii) the bottom third are low-frequency pairs (LF-SEED).",
        "We then use these 3 different seed lexicons of equal size to seed the bootstrapping approach.",
        "Fig.",
        "2(a) and 2(b) show the progress of the bootstrapping process using these 3 seed lexicons.",
        "We again observe several interesting phenomena: (i) High-frequency seed translation pairs are better seeds, and that finding is in line with our hypothesis.",
        "Although the bootstrapping approach again displays a positive trend regardless of the actual choice of seeds (we observe an increase even when using LF-SEED), high-frequency seeds lead to better overall results in the BLE task.",
        "Besides its high presence in contexts of other words, another advantage of high-frequency seed pairs is the fact that an initial similarity method will typically acquire more reliable translation candidates for such words (Pekar et al., 2006).",
        "For instance, 89.5% of ES-EN pairs in HF-SEED are correct one-to-one translations, compared to 65.1% in MF-SEED, and 44.3% in LF-SEED.",
        "(ii) The difference in results between HF-SEED and MF-SEED is more visible in Acc1 scores.",
        "Although both seed lexicons for all test words provide ranked lists which contain words that exhibit some semantic relation to the given word, the reliability and the frequency of translation pairs are especially important for detecting the relation of cross-lingual word synonymy, that is, the translational equivalence that is exploited in building one-to-one bilingual lexicons.",
        "Exp.",
        "III: Does size matter?",
        "The following experiment investigates whether bilingual vector spaces may be effectively bootstrapped from small high-quality seed lexicons, and if larger seed lexicons necessarily lead to bilingual vector spaces of higher quality as reflected in BLE results.",
        "We again retrieve a sorted list of symmetric translation pairs as in Exp.",
        "II.",
        "Following that, we build seed lexicons of various sizes by retaining only the first N pairs from the list, where we vary N from 200 to 1400 in steps of 200.",
        "We also use the entire sorted list as a seed lexicon (All), and compare the results on the BLE task with the results obtained by applying the ResponseBC and TopicBC methods directly (Vulic?",
        "and Moens, 2013).",
        "The results are summarized in tables 1 and 2.",
        "We observe the following: (i) If we provide a seed lexicon with sufficient entries, the bootstrapping procedure provides comparable results regardless of the seed lexicon size, although results tend to be higher for larger seed lexicons (e.g., compare results when starting with 600 and 1200 lexicon entries).",
        "When starting with the size of 600, the bootstrapping approach is able to find dimensions that were already in the seed lexicon of size 1200.",
        "The consequence is that, although bootstrapping with a smaller seed lexicon displays a slower start (see the difference in results at iteration 0), the performances level after convergence.",
        "(ii) Regardless of the seed lexicon size, the bootstrapping approach is valuable.",
        "It consistently improves the quality of the induced bilingual vector space, and consequently, the quality of bilingual lexicons extracted using that vector space.",
        "The positive impact is more prominent for smaller seed lexicons, i.e., we observe an increase of 78% for ES-EN when starting with only 200 seed pairs, compared to an increase of 15% when starting with 800 seed pairs, and 10% when starting with 1400 seed pairs.",
        "(iii) The bootstrapping approach outperforms ResponseBC and TopicBC in terms of Acc1 and MRR scores for both language pairs when the seed lexicon provides a sufficient number of entries.",
        "However, in terms of Acc10, TopicBC and ResponseBC still exhibit comparable (for IT-EN) or even better (ES-EN) results.",
        "Both TopicBC and ResponseBC are MuPTM-based methods that, due to MuPTM properties, model the similarity of two words at the level of documents as contexts, while the bootstrapping approach is a window-based approach that narrows down the context to a local neighborhood of a word.",
        "The MuPTM-based models are better suited to detect a general topical similarity of words, and",
        "dimensions in the bilingual space after the bootstrapping procedure converges.",
        "The seeding method is SEED-RB.",
        "are therefore not always able to push the real cross-lingual synonyms higher in the ranked list of semantically similar words, while the window-based bootstrapping approach is better tailored to model the relation of cross-lingual synonymy, i.e., to extract one-to-one translation pairs (as reflected in Acc1 scores).",
        "A similar conclusion for monolingual settings is drawn by Baroni and Lenci (2010).",
        "(iv) Since our bootstrapping approach utilizes ResponseBC or TopicBC as a preprocessing step, it is obvious that the approach leads to an increased complexity.",
        "On top of the initial complexity of ResponseBC and TopicBC, the bootstrapping method requires |V S ||V T |comparisons at each iteration, but given the fact that each wSi ?",
        "V S may be processed independently of any other wSj ?",
        "V S in each iteration, the bootstrapping method is trivially parallelizable.",
        "That makes the method computationally feasible even for vocabularies larger than the ones reported in the paper."
      ]
    },
    {
      "heading": "4.2 Is Confidence Estimation Important?",
      "text": [
        "According to the results from tables 1 and 2, regardless of the seed lexicon size, the bootstrapping approach does not suffer from semantic drift, i.e., if we seed the process with high-quality symmetric translation pairs, it is able to recover more pairs and add them as new dimensions of the bilingual vector space.",
        "However, we also study the influence of applying different confidence estimation functions on top of the symmetry constraint (see sect 2.3), but we do not observe any improvement in the BLE results, regardless of the actual choice of a confidence estimation function.",
        "The only observed phenomenon, as illustrated by fig. 3, is the slower convergence rate when setting the parameter B to lower values.",
        "The symmetry constraint alone seems to be sufficient to prevent semantic drift, but it might also be a too strong and a too conservative assumption, since only a small portion of all possible translation pairs is used to span the bilingual vector space (for instance,",
        "when starting with 600 entries for ES-EN, the final bilingual vector space consists of only 1554 pairs, while the total number of ES nouns is 9439).",
        "One line of future work will address the construction of bootstrapping algorithms that also enable the usage of highly reliable asymmetric pairs as dimensions, and the confidence estimation functions might have a more important role in that setting."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We have presented a new bootstrapping approach to inducing bilingual vector spaces from non-parallel data, and have shown the utility of the induced space in the BLE task.",
        "The approach is fully corpus-based and, unlike previous work, it does not rely on the availability of machine-readable translation dictionaries or predefined concept categories.",
        "We have systematically described, analyzed and evaluated all key components of the bootstrapping approach.",
        "Results reveal that, contrary to conclusions from prior work, the initialization of the bilingual vector space is especially important.",
        "We have presented a novel approach to initializing the bootstrapping procedure, and have shown that better results in the BLE task are obtained by starting from seed lexicons that comprise highly reliable high-frequent translation pairs.",
        "The bootstrapping framework presented in the paper is completely language pair independent, which makes it effectively applicable to any language pair.",
        "In future work, we will investigate other models of similarity besides TopicBC and ResponseBC (e.g, the method from (Haghighi et al., 2008)) that could be used as preliminary models for constructing an initial bilingual vector space.",
        "Furthermore, we plan to study other confidence functions and explore if asymmetric translation candidates could also contribute to the bootstrapping method.",
        "Finally, we also plan to test the robustness of our fully corpus-based bootstrapping approach by porting it to more language pairs."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank the anonymous reviewers for their useful suggestions.",
        "This research has been carried out in the framework of the TermWise Knowledge Platform (IOF-KP/09/001) funded by the Industrial Research Fund, KU Leuven, Belgium."
      ]
    }
  ]
}
