{
  "info": {
    "authors": [
      "Tiberiu Boros",
      "Radu Ion",
      "Dan Tufis"
    ],
    "book": "ACL",
    "id": "acl-P13-1068",
    "title": "Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language",
    "url": "https://aclweb.org/anthology/P13-1068",
    "year": 2013
  },
  "references": [
    "acl-A00-1031",
    "acl-C94-1027",
    "acl-J96-1002",
    "acl-P99-1065",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 692?700, Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics Large tagset labeling using Feed Forward Neural Networks.",
        "Case study on Romanian Language"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories).",
        "For this reason, a number of alternative methods have been proposed over the years.",
        "One of the most successful methods used for this task, FDOOHG7LHUHG7DJJLQJ7XIL, 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions.",
        "A second phase is aimed at recovering the full set of morpho-syntactic features.",
        "In this paper we present an alternative method to Tiered Tagging, based on local optimizations with Neural Networks and we show how, by properly encoding the input sequence in a general Neural Network architecture, we achieve results similar to the Tiered Tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Part-of-speech tagging is a key process for various tasks such as `information extraction, text-to-speech synthesis, word sense disambiguation and machine translation.",
        "It is also known as lexical ambiguity resolution and it represents the process of assigning a uniquely interpretable label to every word inside a sentence.",
        "The labels are called POS tags and the entire inventory of POS tags is called a tagset.",
        "There are several approaches to part-of-speech tagging, such as Hidden Markov Models (HMM) (Brants, 2000), Maximum Entropy Classifiers (Berger et al., 1996; Ratnaparkhi, 1996), Bayesian Networks (Samuelsson, 1993), Neural Networks (Marques and Lopes, 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001).",
        "All these methods are primarily intended for English, which uses a relatively small tagset inventory, compared to highly inflectional languages.",
        "For the later mentioned languages, the lexicon tagsets (called morpho-syntactic descriptions (Calzolari and Monachini, 1995) or MSDs) may be 10-20 times or even larger than the best known tagsets for English.",
        "For instance Czech MSD tagset requires more than 3000 labels (Collins et al., 1999), Slovene more than 2000 labels (Erjavec and Krek, 2008), and Romanian more than 1100 labels (Tufi, 1999).",
        "The standard tagging methods, using such large tagsets, face serious data sparseness problems due to lack of statistical evidence, manifested by the non-robustness of the language models.",
        "When tagging new texts that are not in the same domain as the training data, the accuracy decreases significantly.",
        "Even tagging in-domain texts may not be satisfactorily accurate.",
        "One of the most successful methods used for this taVN FDOOHG 7LHUHG 7DJJLQJ 7XIL, 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions.",
        "According to the MULTEXT EAST lexical specifications (Erjavec and Monachini, 1997), the Romanian tagset consists of a number of 614 MSD tags (by exploiting the case and gender regular syncretism) for wordforms and 10 punctuation tags (Tufi et al., 1997), which is still significantly larger than the tagset of English.",
        "The MULTEX EAST version 4 (Erjavec, 2010) contains specifications for a total of 16 languages: Bulgarian, Croatian, Czech,",
        "In the case of out-of-vocabulary (OOV) words, both approaches use suffix analysis to determine the most probable tags that can be assigned to the current word.",
        "To clarify how these two methods work, if we want to train the network to label the current word, using a context window of 1 (previous tag, current possible tags, and possible tags for the next word) and if we have, say 100 tags in the tagset, the input is a real valued vector of 300 subunit elements and the output is a vector which contains 100 elements, also subunit real numbers.",
        "As mentioned earlier, each value in the output vector corresponds to a distinct tag from tagset and the tag assigned to the current word is chosen to correspond to the maximum value inside the output vector.",
        "The previously proposed methods still suffer from the same issue of data sparseness when applied to MSD tagging.",
        "However, in our approach, we overcome the problem through a different encoding of the input data (see section 2.1).",
        "The power of neural networks results mainly from their ability to attain activation functions over different patterns via their learning algorithm.",
        "By properly encoding the input sequence, the network chooses which input features contribute in determining the output features for MSDs (e.g. patterns composed of part of speech, gender, case, type etc.",
        "contribute independently in selecting the optimal output sequence).",
        "This way, we removed the need for explicit MSD to CTAG conversion and MSD recovery from CTAGs."
      ]
    },
    {
      "heading": "2.1 The MSD binary encoding scheme",
      "text": [
        "A MSD language independently encodes a part of speech (POS) with the associated lexical attribute values as a string of positional ordered character codes (Erjavec, 2004).",
        "The first character is an upper case character denoting the",
        "instantiations of the characteristic lexical attributes of the POS.",
        "For example, the MSD ?1FIVUQ?",
        "specifies a noun (the first character is ?1?",
        "the type of ZKLFK LV FRPPRQ ?F?",
        "WKH second character), feminine gender ?I?",
        "VLQJXODU number ?V?",
        "LQQRPLQDWLYHDFFXVDWLYHFDVH?U?",
        "and indefinite form ?Q?",
        "If a specific attribute is not relevant for a language, or for a given combination of feature-YDOXHVWKHFKDUDFWHU?-?LV used in the corresponding position.",
        "For a language which does not morphologically mark the gender and definiteness features, the earlier H[HPSOLILHG06'ZLOOEHHQFRGHGDV?1F-sr-?",
        "In order to derive a binary vector for each of the 614 MSDs of Romanian we proceeded to:",
        "1.",
        "List and sort all possible POSes of",
        "Romanian (16 POSes) and form a binary vector with 16 positions in which position k is equal 1 only if the respective MSD has the corresponding POS (i.e. the k-th POS in the sorted list of POSes);",
        "2.",
        "List and sort all possible values of all lexical attributes GLVUHJDUGLQJWKHZLOGFDUG?-?",
        "for all POSes (94 values) and form another binary vector with 94 positions such that the k-th position of this vector is 1 if the respective MSD has an attribute with the corresponding value; 3.",
        "Concatenate the vectors from steps 1 and 2 and obtain the binary codification of a MSD as a 110-position binary vector."
      ]
    },
    {
      "heading": "2.2 The training and tagging procedure",
      "text": [
        "The tagger automatically assigns four dummy tokens (two at the beginning and two at the end) to the target utterance and the neural network is trained to automatically assign a MSD given the context (two previously assigned tags and the possible tags for the current and following two words) of the current word (see below for details).",
        "In our framework a training example consists of the features extracted for a single word inside an utterance as input and it's MSD within that utterance as output.",
        "The features are extracted from a window of 5 words centered on the current word.",
        "A single word is characterized by a vector that encodes either its assigned MSD or its possible MSDs.",
        "To encode the possible MSDs we use equation 2, where each possible attribute a, has a single corresponding position inside the encoded vector.",
        "Note that we changed the probability estimates to account for attributes not tags.",
        "To be precise, for every word wk, we obtain its input features by concatenating a number of 5 vectors.",
        "The first two vectors encode the MSDs assigned to the previous two words (wk-1 and wk",
        "2).The next three vectors are used to encode the possible MSDs for the current word (wk) and the following two words (wk+1 and wk+2).",
        "During training, we also compute a list of suffixes with associated MSDs, which is used at runtime to build the possible MSDs vector for unknown words.",
        "When such words are found within the test data, we approximate their possible MSDs vector using a variation of the method proposed by Brants (2000).",
        "When the tagger is applied to a new utterance, the system iteratively calculates the output MSD for each individual word.",
        "Once a label has been assigned to a word, the ZRUG?VDVVRFLDWHGYHFWRU is edited so it will have the value of 1 for each attribute present in its newly assigned MSD.",
        "As a consequence of encoding each individual attribute separately for MSDs, the tagger can assign new tags (that were never associated with the current word in the training corpus).",
        "Although this is a nice behavior for dealing with unknown words it is often the case that it assigns attribute values that are not valid for the wordform.",
        "To overcome these types of errors we use an additional list of words with their allowed MSDs.",
        "For an OOV word, the list is computed as a union from all MSDs that appeared with the suffixes that apply to that word.",
        "When the tagger has to assign a MSD to a given word, it selects one from the possible wordform?V MSDs in its wordform/MSDs associated list using a simple distance function:"
      ]
    },
    {
      "heading": "3 Network hyperparameters",
      "text": [
        "In our experiments, we used a fully connected, feed forward neural network with 3 layers (1 input layer, 1 hidden layer and 1 output layer) and a sigmoid activation function (equation 3).",
        "While other network architectures such as recurrent neural networks may prove to be more suitable for this task, they are extremely hard to train, thus, we traded the advantages of such architectures for the robustness and simplicity of the feed-forward networks.",
        "The weighted sum of all the neuron outputs from the previous layer Based on the size of the vectors used for MSD encoding, the output layer has 110 neurons and the input layer is composed of 550 (5 x 110) neurons.",
        "In order to fully characterize our system, we took into account the following parameters: accuracy, runtime speed, training speed, hidden layer configuration and the number of optimal training iterations.",
        "These parameters have complex dependencies and relations among each other.",
        "For example, the accuracy, the optimal number of training iterations, the training and the runtime speed are all highly dependent on the hidden layer configuration.",
        "Small hidden layer give high training and runtime speeds, but often under-fit the data.",
        "If the hidden layer is too large, it can easily over-fit the data and also has a negative impact on the training and runtime speed.",
        "The number of optimal training iterations changes with the size of the hidden layer (larger layers usually require more training iterations).",
        "To obtain the trade-offs between the above mentioned parameters we devised a series of experiments, in all of which we used WKH??",
        "MSD annotated corpus, which is composed of 118,025 words.",
        "We randomly kept out approximately 1/10 (11,960 words) of the training corpus for building a cross-validation set.",
        "The baseline accuracy on the cross-validation set (i.e. returning the most probable tag) is 93.29%.",
        "We also used an additional inflectional wordform/MSD lexicon composed of approximately 1 million hand-validated entries.",
        "The first experiment was designed to determine the trade-off between the runtime speed and the size of the hidden layer.",
        "We made a series of experiments disregarding the tagging accuracy.",
        "the hidden layer Because, for a given number of neurons in the hidden layer, the tagging speed is independent on the tagging accuracy, we partially trained (using one iteration and only 1000 training sentences) several network configurations.",
        "The first network only had 50 neurons in the hidden layer and for the next networks, we incremented the hidden layer size by 20 neurons until we reached 310 neurons.",
        "The total number of tested networks is 14.",
        "After this, we measured the time it took to tag the 1984 test corpus (11,960 words) for each individual network, as an average of 3 tagging runs in order to reduce the impact of the operating system load on the tagger (Table 1 shows the figures).",
        "Determining the optimal size of the hidden layer is a very delicate subject and there are no perfect solutions, most of them being based on trial and error: small-sized hidden layers lead to under-fitting, while large hidden layers usually cause over-fitting.",
        "Also, because of the trade-off between runtime speed and the size of hidden layers, and if runtime speed is an important factor in a particular NLP application, then hidden layers with smaller number of neurons are preferable, as they surely do not over-fit the data and offer a noticeable speed boost.",
        "As shown in Table 1, the runtime speed of our system shows a constant decay when we increase the hidden layer size.",
        "The same decay can be seen in the training speed, only this time by an order of magnitude larger.",
        "Because training a single network takes a lot of time, this experiment was designed to estimate the size of the hidden layer which offers good performance in tagging.",
        "To do this, we individually trained a number of networks in 30 iterations, using various hidden layer configurations (50, 70, 90,",
        "110, 130, 150, 170, 190, and 210 neurons) and 5 initial random initializations of the weights.",
        "For each configuration, we stored the accuracy of reproducing the learning data (the tagging of the training corpus) and the accuracy on the unseen data (test sets).",
        "The results are shown in Table 2.",
        "Although a hidden layer of 210 neurons did not seem to over-fit the data, we stopped the experiment, as the training time got significantly longer.",
        "The next experiment was designed to see how the number of training iterations influences the tagging performance of networks with different hidden layer configurations.",
        "Intuitively, the training process must be stopped when the network begins to over-fit the data (i.e. the train set accuracy increases, but the test set accuracy drops).",
        "Our experiments indicate that this is not always the case, as in some situations the continuation of the training process leads to better results on the cross-validation data (as shown in Figure 2).",
        "So, the problem comes to determining which is the most stable configuration of the neural network (i.e. which hidden unit size will be most likely to return good results on the test set) and establish the number of iterations it takes for the system to be trained.",
        "To do this, we ran the training procedure for 100 iterations and for each training iteration, we computed the accuracy rate of every individual network on the cross-validation set (see Table 3 for the averaged values).",
        "As shown, the network configuration using 130 neurons on the hidden layer is most likely to produce better results on the cross-validation set regardless of the number of iterations.",
        "Although, some other configurations provided better figures for the maximum accuracy, their average accuracy is lower than that of the 130 hidden unit network.",
        "Other good candidates are the 90 and 110 hidden unit networks, but not the larger valued ones, which display a lower average accuracy and also significantly slower tagging speeds.",
        "The most suitable network configuration for a given task depends on the language, MSD encoding size, speed and accuracy requirements.",
        "In our own daily applications we use the 130 hidden unit network.",
        "After observing the behavior of the various networks on the cross-validation set we determined that a good choice is to stop the training procedure after 40 iterations.",
        "Hidden units Avg.",
        "acc.",
        "Max.",
        "acc.",
        "St. dev.",
        "hidden layer configuration calculated over 100 training iterations on the test set To obtain the accuracy of the system, in our last experiment we used the 130 hidden unit network and we performed the training/testing procedure on the 1984 corpus, using 10-fold validation and 30 random initializations.",
        "The final accuracy was computed as an average between all the accuracy figures measured at the end of the training process (after 40 iterations).",
        "The first 1/10 of the 1984 corpus on which we tuned the hyperparameters was not included in the test data, but was used for training.",
        "The mean accuracy of the system (98.41%) was measured as an average of 270 values."
      ]
    },
    {
      "heading": "4 Comparison to other methods",
      "text": [
        ",Q KLV ZRUN &HDXu (2006) presents a different approach to MSD tagging using the Maximum Entropy framework.",
        "He presents his results on the same corpus we used for training and testing (the 1984 corpus) and he compares his method (98.45% accuracy) with the Tiered Tagging methodology (97.50%) (Tufi and Dragomirescu, 2004).",
        "Our Neural Network approach obtained similar (slightly lower) results (98.41%), although it is arguable that our split/train procedure is not identical to the one used in his work (no details were given as how the 1/10 of the training corpus was selected).",
        "Also, our POS tagger detected cases where the annotation in the Gold Standard was erroneous.",
        "One such example LV LQ ?lame de ras?",
        "(QJOLVK ?UD]RU EODGHV?",
        "ZKHUH?ODPH?English ?EODGHV?",
        "LVDQRXQ?GH?",
        "?for?",
        "LVDSUHSRVLWLRQDQG?UDV??VKDYLQJ?)",
        "is a supine verb (with a past participle form) which was incorrectly annotated as a noun."
      ]
    },
    {
      "heading": "5 Network pattern analysis",
      "text": [
        "Using feed-forward neural networks gives the ability to outline what input features contribute to the selection of various MSD attribute values in the output layer which might help in reducing the tagset and thus, redesigning the network topology with beneficial effects both on the speed and accuracy.",
        "To determine what input features contribute to the selection of certain MSD attribute values, one can analyze the weights inside the neural network and extract the input ?",
        "output links that are formed during training.",
        "We used the network with 130 units on the hidden layer, which was previously trained for 100 iterations.",
        "Based on the input encoding, we divided the features into 5 groups (one group for each MSD inside the local context ?",
        "two previous MSDs, current and following two possible MSDs).",
        "For a target attribute value (noun, gender feminine, gender masculine, etc.)",
        "and for each input group, we selected the top 3 input values which support the decision of assigning the target value to the attribute (features that increase the output value) and the top 3 features which discourage this decision (features that decrease the output value).",
        "For clarity, we will use the following notations for the groups: x G -2: group one ?",
        "the assigned MSD for the word at position i-2 x G -1: group two ?",
        "the assigned MSD for the word at position i-1 x G0: group three ?",
        "the possible MSDs for the word at position i x G1: group four?",
        "the possible MSDs for the word at position i+1 x G2: group five ?",
        "the possible MSDs for the word at position i+2 where i corresponds to the position of the word which is currently being tagged.",
        "Also, we classify the attribute values into two categories (C): (P) want to see (support the decision) and",
        "conjunctive particle, future particle, nominative/accusative (of a noun/adjective) Table 4 ?",
        "P/N features for various attribute values.",
        "For instance, when deciding on whether to give a noun (N) label to current position (G0), we can see that the neural network has learned some interesting dependencies: at position G 1 we find an article (which frequently determines a noun) and at the current position it is very important for the word being tagged to actually be a common or proper noun (either by lexicon lookup or by suffix guessing) and not be an adverb, pronoun or numeral (POSes that cannot be found in the typical ambiguity class of a noun).",
        "At the next position of the target (G1) we also find a noun in genitive or dative, corresponding to a frequent construction in Romanian, HJ ?PDina ELDWXOXL?",
        "EHLQJ D VHTXHQFH RI WZR nouns, the second at genitive/dative.",
        "If the neural network outputs the feminine gender to its current MSD, one may see that it",
        "has actually learned the agreement rules (at least locally): the feminine gender is present both before (G -1) the target word as well as after it (G1)."
      ]
    },
    {
      "heading": "6 Conclusions and future work",
      "text": [
        "We presented a new approach for large tagset part-of-speech tagging using neural networks.",
        "An advantage of using this methodology is that it does not require extensive knowledge about the grammar of the target language.",
        "When building a new MSD tagger for a new language one is only required to provide the training data and create an appropriate MSD encoding system and as shown, the MSD encoding algorithm is fairly simple and our proposed version works for any other MSD compatible encoding, regardless of the language.",
        "Observing which features do not participate in any decision helps design custom topologies for the Neural Network, and provides enhancements in both speed and accuracy.",
        "The configurable nature of our system allows users to provide their own MSD encodings, which permits them to mask certain features that are not useful for a given NLP application.",
        "If one wants to process a large amount of text and is interested only in assigning grammatical categories to words, he can use a MSD encoding in which he strips off all unnecessary features.",
        "Thus, the number of necessary neurons would decrease, which assures faster training and tagging.",
        "This is of course possible in any other tagging approaches, but our framework supports this by masking attributes inside the MSD encoding configuration file, without having to change anything else in the training corpus.",
        "During testing the system only verifies if the MSD encodings are identical and the displayed accuracy directly reflects the performance of the system on the simplified tagging schema.",
        "We also proposed a methodology for selecting a network configurations (i.e. number of hidden units), which best suites the application requirements.",
        "In our daily applications we use a network with 130 hidden units, as it provides an optimal speed/accuracy trade-off (approx.",
        "3400 words per second with very good average accuracy).",
        "The tagger is implemented as part of a larger application that is primarily intended for text-to-speech (TTS) synthesis.",
        "The system is free for non-commercial use and we provide both web and desktop user-interfaces.",
        "It is part of the METASHARE platform and available online 2 .",
        "Our primary goal was to keep the system language independent, thus all our design choices are based on the necessity to avoid using language specific knowledge, when possible.",
        "The application supports various NLP related tasks such as lexical stress prediction, syllabification, letter-to-sound conversion, lemmatization, diacritic restoration, prosody prediction from text and the speech synthesizer uses unit-selection.",
        "From the tagging perspective, our future plans include testing the system on other highly inflectional languages such as Czech and Slovene and investigating different methods for automatically determining a more suitable custom network topology, such as genetic algorithms."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The work reported here was funded by the project METANET4U by the European"
      ]
    }
  ]
}
