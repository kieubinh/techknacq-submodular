{
  "info": {
    "authors": [
      "Aobo Wang",
      "Min-Yen Kan"
    ],
    "book": "ACL",
    "id": "acl-P13-1072",
    "title": "Mining Informal Language from Chinese Microtext: Joint Word Recognition and Segmentation",
    "url": "https://aclweb.org/anthology/P13-1072",
    "year": 2013
  },
  "references": [
    "acl-C00-2137",
    "acl-C02-1049",
    "acl-C04-1081",
    "acl-C08-1056",
    "acl-D08-1108",
    "acl-D11-1090",
    "acl-D12-1039",
    "acl-I05-3013",
    "acl-I05-3025",
    "acl-I08-4017",
    "acl-J05-4005",
    "acl-J90-1003",
    "acl-P11-1038",
    "acl-P12-1027",
    "acl-W00-1207",
    "acl-W03-1730",
    "acl-W06-2808",
    "acl-W11-0704",
    "acl-W12-2106"
  ],
  "sections": [
    {
      "heading": "Substitutions",
      "text": [
        "i??",
        "(hai2 zhi3 men) iP?",
        "(hai2 zi men) w??i??",
        "Get up kids bs ?",
        "(bi shi) bs` I despise you",
        "2) Abbreviation L8 Lb8 eL8' Let's play board gamesg g?2 ?g?",
        "Don't tell (me) the spoilers 3) Neologisms ??",
        "??",
        "?",
        "?J So awesome!?",
        "@ ?-p ??@?",
        "Quickly purchase it",
        "derive joint solutions for both problems of CWS and IWR.",
        "We also propose novel features for input to the joint inference.",
        "Our techniques significantly outperform both research and commercial state-of-the-art for these problems, including two-step linear CRF baselines which perform the two tasks sequentially.",
        "We detail our methods in Section 2.",
        "In Section 3, we first describe the details of our dataset and baseline systems, followed by demonstrating two sets of experiments for CWS and IWR, respectively.",
        "Section 4 offers the discussion on error analysis and limitations.",
        "We discuss related work in Section 5, before concluding our paper."
      ]
    },
    {
      "heading": "2 Methodology",
      "text": [
        "Given an input Chinese microblog post, our method simultaneously segments the sentences into words (the Chinese Word Segmentation, CWS, task), and marks the component words as informal or formal ones (the Informal Word Re-congition, IWR, task)."
      ]
    },
    {
      "heading": "2.1 Problem Formalization",
      "text": [
        "The two tasks are simple to formalize.",
        "The IWR task labels each Chinese character with either an F (part of a formal word) or IF (informal word).",
        "For the CWS task, we follow the widely-used BIES coding scheme (Low et al., 2005; Hai et al., 2006), where B, I, E and S stand for beginning of a word, inside a word, end of a word and single-character word, respectively.",
        "As a result, we have two (hidden) labels to associate with each (observable) character.",
        "Figure 1 illustrates an example microblog post graphically, where the labels are in circles and the observations are in squares.",
        "The two informal words in the example post are ?",
        "( ?",
        "(normalized form: ??",
        "?",
        "; English gloss: ?no?)",
        "and ?rp?",
        "(???<?",
        "; ?luck?",
        ")."
      ]
    },
    {
      "heading": "2.2 Conditional Random Field Models",
      "text": [
        "Given the general performance and discriminative framework, Conditional Random Fields (CRFs) (Lafferty et al., 2001) is a suitable framework for tackling sequence labeling problems.",
        "Other alternative frameworks such as Markov Logic Networks (MLNs) and Integer Linear Programming (ILP) could also be considered.",
        "However, we feel that for this task, formulating efficient global formulas (constraints) for MLN (ILP) is comparatively less straightforward than in other tasks (e.g, compared to Semantic Role Labeling, where the rules may come directly from grammatical constraints).",
        "CRFs represent a basic, simple and well-understood framework for sequence la-beling, making it a suitable framework for adapting to perform joint inference.",
        "A linear-chain CRF (LCRF; Figure 2a) predicts the output label based on feature functions provided by the scientist on the input.",
        "In fact, the LCRF has been used for the exact problem of CWS (Sun and Xu, 2011), garnering state-of-the-art performance, and as such, validate it as a strong baseline for comparison.",
        "To properly model the interplay between the two sub-problems, we employ the factorial CRF (FCRF) model, which is based on the dynamic CRF (DCRF) (Sutton et al., 2007).",
        "By introducing a pairwise factor between different variables at each position, the FCRF model results as a special case of the DCRF.",
        "A FCRF captures the joint distribution among various layers and jointly predicts across layers.",
        "Figure 2 illustrates both the LCRF and FCRF models, where cliques include within-chain edges (e.g., yt, yt+1) in both LCRF and FCRF models, and the between-chain edges (e.g., yt, zt) only in the FCRF.",
        "xt denotes the observation sequence.",
        "Although the FCRF can be collapsed into a LCRF whose state space is the crossproduct of the outcomes of the state variables (i.e., 8 labels in this case), Sutton et al. (2007) noted that such a LCRF requires not only more parameters in the number of variables, but also more training data to achieve equivalent performance with an FCRF.",
        "Given the limited scale of the state space and training data, we follow the FCRF model, using exact Junction Tree (Jensen, 1996) inference and decoding algorithm to perform prediction."
      ]
    },
    {
      "heading": "2.3 CRF Features",
      "text": [
        "We use three broad feature classes ?",
        "lexical, dictionary-based and statistical features ?",
        "aiming to distinguish the output classes for the CWS and IRW problems.",
        "Character-based sequence label-ing is employed for word segmentation due to its simplicity and robustness to the unknown word problem (Xue, 2003).",
        "A key contribution of our work is also to propose novel features for joint inference.",
        "We propose new features for the dictionary-based and statistical feature classes, which we have marked in the discussion below with ?(*)?.",
        "We later examine their efficacy in Section 3.",
        "Lexical Features.",
        "As a foundation, we employ lexical (n-gram) features informed by the previous state-of-the-art for CWS (Sun and Xu, 2011; Low et al., 2005).",
        "These features are listed below2:",
        "?",
        "Character 1-gram: Ck(i?",
        "4 < k < i+ 4) ?",
        "Character 2-gram: CkCk+1(i?",
        "4 < k < i+ 3) ?",
        "Character 3-gram: CkCk+1Ck+2(i ?",
        "3 < k < i+ 2) ?",
        "Character lexicon: C?1C1",
        "This feature is used to capture the common indicators in Chinese interrogative sentences.",
        "This feature is used to capture the words of employing character doubling in Chinese.",
        "(e.g., ????",
        "(?see you?",
        "), ?))?",
        "(?every",
        "day?))",
        "Dictionary-based Features.",
        "We use features that indicate whether the input character sequence 2For notational convenience, we denote a candidate character token Ci as having a context ...Ci?1CiCi+1.... We use Cm:n to express a subsequence starting at the position m and ending at n. len stands for the length of the subsequence, and offset denotes the position offset of Cm:n from the current character Ci.",
        "We use b (beginning), m (middle) and e (ending) to indicate the position of Ck (m ?",
        "k ?",
        "n) within the string Cm:n.",
        "matches entries in certain lexica.",
        "We use the on-line dictionary from Peking University as the formal lexicon and the compiled informal word list from our training instances as the informal lexicon.",
        "In addition, we employ additional online word lists3 to distinguish named entities and function words from potential informal words.",
        "As shown in Table 1, alphabetic sequences in microblogs may refer to Chinese Pinyin or Pinyin abbreviations, rather than English (e.g., ?bs?",
        "for bi shi; ?to despise?).",
        "Hence, we added dictionary-based features to indicate the presence of Pinyin initials, finals and standard Pinyin expansions, using a UK English word list4.",
        "The final list of dictionary-based features employed are:",
        "1990) to account for global, corpus-wide information.",
        "This measures the difference between the observed probability of an event (i.e., several characters combined as an informal word) and its expectation, based on the probabilities of the individual events (i.e., the probability of the individual characters occurring in the corpus).",
        "Compared with other standard association measures such as MI, PMI tends to assign rare events higher scores.",
        "This makes it a useful signal for IWR, as it is sensitive to informal words which often have low frequency.",
        "However, the word frequency alone is not reliable enough to distinguish informal words from uncommon but formal words.",
        "In response to these difficulties in differentiating linguistic registers, we compute two different PMI scores for character-based bigrams from two large corpora representing news and microblogs as features.",
        "We also use the difference between the two PMI scores as a differential feature.",
        "In addition, we also convert all the character-based bigrams into Pinyin-based bigrams (ignoring tones5) and compute the Pinyin-level PMI in the same way.",
        "These features capture inconsistent use of the bigram across the two domains, which assists to distinguish informal words.",
        "Note that we eschew smoothing in our computation of PMI, as it is important to capture the inconsistent character bigrams usage between the two domains.",
        "For example, the word ?rp?",
        "appears in the microblog domain, but not in news.",
        "If smoothing is conducted, the character bigram ?rp?",
        "will be given a non-zero probability in both domains, not reflective of actual use.",
        "For each character Ci, we incorporate the PMI of the character bigrams as follows: ?",
        "(*) If CkCk+1 (i ?",
        "4 < k < i + 4) is not a Chinese word recorded in dictionaries:"
      ]
    },
    {
      "heading": "3 Experiment",
      "text": [
        "We discuss the dataset, baseline systems and experiments results in detail in the following."
      ]
    },
    {
      "heading": "3.1 Data Preparation",
      "text": [
        "We utilize the Chinese social media archive, PrEV (Cui et al., 2012), to obtain Chinese mi",
        "croblog posts from the public timeline of Sina Weibo6.",
        "Sina Weibo is the largest microblogging in China, where over 100 million Chinese microblog posts are posted daily (Cao, 2012), likely the largest public source of informal and daily Chinese language use.",
        "Our dataset has a total of 6,678,021 messages, covering two months from June to July of 2011.",
        "To annotate the corpus, we employ Zhubajie7, one of China mainland's largest crowdsourcing (Wang et al., 2010) platforms to obtain informal word annotations.",
        "In total, we spent US$110 on assembling a subset of 5, 500 posts (12, 446 sentences) in which 1, 658 unique informal words are annotated within five weeks via Zhubajie.",
        "Each post was annotated by three annotators with moderate (0.57) inter-annotator agreement measured by Fleiss?",
        "?",
        "(Joseph, 1971), and conflicts were resolved by majority voting.",
        "We divided the annotated corpus, taking 4, 000 posts for training, and the remainder (1, 500) for testing.",
        "Through inspection, we note that 79.8% of the informal words annotated in the testing set are not covered by the training set.",
        "We also follow Wang et al. (2012)'s conventions and apply rule-sets to preprocess the corpus?",
        "URLs, emoticons, ?@usernames?",
        "and Hashtags as pre-segmented words, before input to CWS and IWR.",
        "For the CWS task, the first author manually labelled the same corpus following the segmentation guidelines published with the SIGHAN-58 MSR dataset."
      ]
    },
    {
      "heading": "3.2 Baseline Systems",
      "text": [
        "We implemented several baseline systems to compare with proposed FCRF joint inference method.",
        "Existing Systems.",
        "We reimplemented Xia and Wong (2008)'s extended Support Vector Machine (SVM) based microtext IWR system to compare with our method.",
        "Their system only does IWR, using the CWS and POS tagging otuput of the ICTCLAS segmenter (Zhang et al., 2003) as input.",
        "To compare our joint inference versus other learning models, we also employed a decision tree (DT) learner, equipped with the same feature set as our FCRF.",
        "Both the SVM and DT models are provided by the Weka3 (Hall et al., 2009) toolkit, using its default configuration.",
        "To evaluate CWS performance, we compare with two recent segmenters.",
        "Sun and Xu (2011)?s",
        "work achieves state-of-the-art performance and is publicly available.",
        "They employ a LCRF taking as input both lexical and statistical features derived from unlabeled data.",
        "As a second baseline, we also evaluate against a widely-used, commercially-available alternative, the recently released 2011 ICTCLAS segmenter9."
      ]
    },
    {
      "heading": "Two-stage Sequential Systems. To benchmark",
      "text": [
        "the improvement that the factorial CRF model has by doing the two tasks jointly, we compare with a LCRF solution that chains these two tasks together.",
        "For completeness, we test pipelin-ing in both directions ?",
        "CWS feeding features for IWR (LCRFcwsLCRFiwr), and the reverse (LCRFiwrLCRFcws).",
        "We modify the open-source Mallet GRMM package (Sutton, 2006) to implement both this sequential LCRF model and our proposed FCRF model.",
        "Both models take the whole feature set described in Section 2.3.",
        "Upper Bound Systems.",
        "To measure the upper-bound achievable with perfect support from the complementary task, we also provided gold standard labels of one task (e.g., IWR) as an input feature to the other task (e.g., CWS).",
        "These systems (hereafter denoted as LCRFLCRF-UB and FCRF-UB) are meant for reference only, as they have access to answers for the opposing tasks.",
        "Adapted SVM for Joint Classification.",
        "For completeness, we also compared our work against the standard SVM classification model that performs both tasks by predicting the crossproduct of the CWS and IWR individual classes (in total, 8 classes).",
        "We train the SVM classifier on the same set of features as the FCRF, by providing the crossproduct of two layer labels as gold labels.",
        "This system (hereafter denoted as SVM-JC) was implemented using the LibSVM package (Chang and Lin, 2011)."
      ]
    },
    {
      "heading": "3.3 Evaluation Metrics",
      "text": [
        "We use the standard metrics of precision, recall and F1 for the IWR task.",
        "Only words that exactly match the manually-annotated labels are considered correct.",
        "For example given the sentence ?",
        "H?H}b?",
        "(?H?H}b?",
        "; ?How delicious it is?",
        "), if the IWR component identifies ??",
        "H?",
        "as an informal word, it will be considered correct, whereas both ??H}?",
        "and ???",
        "are deemed incorrect.",
        "For CWS evaluation, we employ the conventional scoring script provided in SIGHAN",
        "5, which also provides out-of-vocabulary recall (OOVR).",
        "To determine statistical significance of the improvements, we also compute paired, one-tailed t tests.",
        "As pointed out by Yeh and Alexander (2000), the randomization method is more reliable in measuring the significance of F1 through handling non-linear functions of random variables.",
        "Thus we employ Pado?",
        "(2006)'s implementation of randomization algorithm to measure the significance of F1."
      ]
    },
    {
      "heading": "3.4 Experimental Results",
      "text": [
        "The goal of our experiments is to answer the following research questions:",
        "RQ1 Do the two tasks of CWS and IWR benefit from each other?",
        "RQ2 Is jointly modeling both tasks more efficient than conducting each task separately or sequentially?",
        "RQ3 What is the upper bound improvement that can be achieved with perfect support from the opposing task?",
        "RQ4 Are the features we designed for the joint inference method effective?",
        "RQ5 Is there a significant difference between the performance of the joint inference of a crossproduct SVM and our proposed FCRF?",
        "task.",
        "The two bottommost rows show upper bound performance.",
        "???(???)",
        "in the top four lines indicates statistical significance at p < 0.001 (0.05) when compared with the previous row.",
        "Symbols in the bottom two lines indicate significant difference between upper bound systems and their corresponding counterparts.",
        "In general, our FCRF yields the best performance among all systems (top portion of Table 2), answering RQ1.",
        "Given microblog posts as test data, the F1 of ICTCLAS drops from 0.98510 to 0.698, clearly showing the difficulty of processing microtext.",
        "The sequential LCRF model and FCRF model both outperform the baselines, which means with the novel features shared by the two tasks, CWS benefits significantly from the results of IWR.",
        "Hence our segmenter outperforms the existing segmenters by tackling one of the bottlenecks of recognizing informal words in Chinese microtext.",
        "To illustrate, the sequence ?...",
        "( ?...?",
        "(?...",
        "?",
        "?...?",
        "; ?...is there anyone...?",
        "), is correctly labeled as BIES by our FCRF model but mislabeled by baseline systems as SSBE.",
        "This is likely due to the ignorance of the informal word ?",
        "( ?, leading baseline systems to keep the formal word ?",
        "??",
        "(?someone?)",
        "as a segment.",
        "More importantly, by jointly optimizing the probabilities of labels on both layers, the FCRF model slightly but significantly improves over the sequential LCRF method, answering RQ2.",
        "Thus we conclude that jointly modeling both tasks is more effective than performing the tasks sequentially.",
        "For RQ3, the last two rows presents the upper-bound systems that have access to gold standard labels for IWR.",
        "Both upper-bound systems statistically outperform their counterparts, indicating that there is still room to improve CWS performance with better IWR as input.",
        "This also validates our assumption that CWS can benefit from joint consideration of IWR.",
        "Taking the best previous work as our lower bound (0.69 F1), we see that our FCRF methodology (0.77) makes significant progress towards the upper bound (0.82).",
        "For RQ1 and RQ2, Table 3 compares the performance of our method with the baseline systems on the IWR task.",
        "Overall, the FCRF method again outperforms all the baseline systems.",
        "We note that the CRF based models achieve much higher precision score than baseline systems, which means that the CRF based models can make accurate predictions without enlarging the scope of prospective informal words.",
        "Compared with the CRF based models, the SVM and DT both over-predict informal words, incurring a larger precision penalty.",
        "Studying this phe",
        "task.",
        "???",
        "or ???",
        "in the top four rows indicates statistical significance at p < 0.001 or < 0.05 compared with the previous row.",
        "Symbols in the bottom two rows indicate differences between upper bound systems and their counterparts.",
        "nomenon more closely, we find it is difficult for the baseline systems to classify segments mixed with formal and informal characters.",
        "Taking the microblog ?H?H}b?",
        "(?H?H} b?",
        "; ?how delicious it is?)",
        "as an example, without considering the possible word boundaries suggested by the contextual formal words ?",
        "i.e., ?",
        "H?",
        "(?how?)",
        "and ?}?",
        "(?delicious?)",
        "?",
        "the baselines chunk the informal words (i.e., ??H?)",
        "together with adjacent characters mistakenly as ??",
        "H}?",
        "or, ?H?H?.",
        "As indicated by the bold figures in Table 3, the FCRF performs slightly better than the sequential LCRF (p < 0.05) ?",
        "a weaker trend when compared with the CWS case.",
        "As an example, the sequential LCRF method fails to recognize ?1??",
        "(?iPhone?)",
        "as an informal word in the sentence ??1?}??",
        "(?my iPhone is fun?",
        "), where the FCRF succeeds.",
        "Inspecting the output, the LCRF segmenter mislabels ?1??",
        "as SS.",
        "By jointly considering the probabilities of the two layers, the FCRF model infers better quality segmentation labels, which in turn enhances the FCRF's capability to recognize the sequence of two characters as an informal word.",
        "This is further validated by the significant performance gulf between the upper bound and the basic system shown in the lower half of the table.",
        "For RQ3, interestingly, the difference in performance between the LCRF and FCRF upper-bound systems is not significant.",
        "However, these are upper bounds, and we expect on real-world data that CWS performance will not be perfect.",
        "As such, we still recommend using the FCRF model, as the joint process is more robust to noisy input from one channel.",
        "For RQ4, to evaluate the effectiveness of our newly-introduced feature sets (those marked with ?*?",
        "in Section 2.3), we also test a FCRF (FCRF?new) without our new features.",
        "According to Table 4, performance drops by a significant amount: 0.088 F1 on CWS and 0.198 F1 on IWR.",
        "FCRF?new makes many mistakes identical to the baselines: segmenting informal words into several single-character words and chunking adjacent characters from informal and formal words",
        "For RQ5, according to Table 5, our SVM trained to predict the crossproduct CWS/IWR classification (SVM-JC) performs quite well on its own.",
        "Unsurprisingly, it does not outperform our proposed FCRF, which has access to more structural correlation among the CWS and IWR labels.",
        "SVM-JC significantly (p < 0.001) outperforms the baseline SVM system by 0.151 in the IWR task, which we think is partially explained by its good performance (0.761) on the CWS task.",
        "The over-prediction tendency of the individual SVM is largely solved by simultaneously modeling the CWS task, whereas FCRF turns out to be more effective in solving joint inference problem, although in a weaker trend in terms of the statistical significance (p < 0.05).",
        "We conclude that the use of the FCRF model and the addition of our new features are both essential for the high performance of our system."
      ]
    },
    {
      "heading": "4 Discussion",
      "text": [
        "We wish to understand the causes of errors in our models so that we may better understand its weaknesses.",
        "Manually inspecting the errors of our system, we found three major categories of errors which we dissect here.",
        "For IWR, the major source of error, accounting for more than 60% of all errors, is caused by what we term the partially observed informal word phenomenon.",
        "This refers to informal words containing multiple characters, where some of its components have appeared in the training data as informal words individually.",
        "For instance, the single-character informal word, ???",
        "(???",
        "; ?very?)",
        "appears in training multiple times, thus the unseen informal word ??E?",
        "(??E?",
        "; ?long time?)",
        "is a partially observed informal word.",
        "In this case, the model incorrectly labels the known, single character ???",
        "with IF S as an informal word, instead of labeling the unseen sequence ??E?",
        "with correct labels IF B IF E. Errors then result in both tasks.",
        "This observation motivates the use of the relation between the known informal word and its formal counterpart in order to inform the model to better predict in cases of partial observations.",
        "Following the same example, given that ???",
        "is an informal word, if the model also considers the probability of normalizing ???",
        "to ??",
        "?, while considering the higher probability that the character sequence ??E?",
        "could be a formal word, there would be a higher likelihood of correctly predicting the sequence ??E?",
        "as an informal word.",
        "So informal word normalization is also an intrinsic component of IWR and CWS, and we believe it is an interesting direction for future work.",
        "Another source of error is a side effect of microtext being extremely short.",
        "For example, in the sentence ???*/??",
        "(???",
        "* /??",
        "; ?Go home!",
        "Exhausted.?",
        "), the unseen informal word ????",
        "itself forms a short sentence.",
        "Although it has a subsequent sentence ?*/??",
        "(?Exhausted?)",
        "as context, and the two are pragmatically related, (i.e., ?I am exhausted!",
        "[And as a result,] I want to go home.?",
        "), the lexical relationship between the sentences is weak; i.e., ?*/??",
        "appears frequently as the context of various sentences, making the context difficult to utilize.",
        "These phenomena makes it difficult to recognize ????",
        "as an informal word.",
        "A possible solution could factor in proximity, similar to density-based matching, as in Tellex et",
        "?dj?e?, ??pp?",
        "Usernames mixed of Chinese and alphabetic characters al.",
        "(2003).",
        "We can assign a higher weight to features related to characters closer to the current target character.",
        "In particular, for this example, given the current target character ??",
        "?, we can assign higher weight to features generated from features from the proximal context ???",
        "?, and lower weight to features extracted from distal contexts.",
        "Another major group of errors come from what we term freestyle named entities as exemplified in Table 6; i.e., person names in the form of user IDs and nicknames, that have less constraint on form in terms of length, canonical structure (not surnames with given names; as is standard in Chinese names) and may mix alphabetic characters.",
        "Most of these belong to the category of Person Name (PER), as defined in CoNLL-200311 Named Entity Recognition shared task.",
        "Such freestyle entities are often misrecognized as informal words, as they share some of the same stylistic markings, and are not marked by features used to recognize previous Chinese named entity recognition methods (Gao et al., 2005; Zhao and Kit, 2008) that work on news or general domain text.",
        "We recognize this as a challenge in Chinese microtext, but beyond the scope of our current work."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "In English, IWR has typically been investigated alongside normalization.",
        "Several recent works (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) aim to produce informal/formal word lexicons and mappings.",
        "These works are based on distributional similarity and string similarity that address concerns of lexical variation and spelling.",
        "These methods propose two-step unsupervised approaches to first detect and then normalize detected informal words using dictionaries.",
        "In processing Chinese informal language, work",
        "of in bulletin board system (BBS) chats.",
        "They employ pattern matching and SVM-based classification to recognize Chinese informal sentences (not individual words) chat (Xia et al., 2005).",
        "Both methods had their advantages: the learning-based method did better on recall, while the pattern matching performed better on precision.",
        "To obtain consistent performance on new unseen data, they further employed an error-driven method which performed more consistently over time-varying data (Xia and Wong, 2006).",
        "In contrast, our work identifies individual informal words, a finer-grained (and more difficult) task.",
        "While seminal, we feel that the difference in scope (informal sentence detection rather than word detection) shows the limitation of their work for microblog IWR.",
        "Their chats cover only 651 unique informal words, as opposed to our study covering almost triple the word types (1, 658).",
        "Our corpus demonstrates a higher ratio of informal word use (a new informal word appears in",
        "12,446 = 13% of sentences, as opposed to 65122,400 = 2% in their BBS corpus).",
        "Further analysis of their corpus reveals that phonetic substitution is the primary origin of informal words in their corpus ?",
        "99.2% as reported in (Wong and Xia, 2008).",
        "In contrast, the origin for informal words in microblogs is more varied, where phonetic substitutions abbreviations and neologisms, account for 53.1%, 21.4% and 18.7% of the informal word types, respectively.",
        "Their method is best suited for phonetic substitution, thus performing well on their corpus but poorly on ours.",
        "More closely related, Li and Yarowsky (2008) tackle Chinese IWR.",
        "They bootstrap 500 informal/formal word pairs by using manually-tuned queries to find definition sentences on the Web.",
        "The resulting noisy list is further re-ranked based on n-gram co-occurrence.",
        "However, their method makes a basic assumption that informal/formal word pairs co-occur within a definition sentence (i.e., ?<informal word> means <formal word>?)",
        "may not hold in microblog data, as microbloggers largely do not define the words they use.",
        "Closely related to our work is the task of Chinese new word detection, normally treated as a separate process from word segmentation in most previous works (Chen and Bai, 1998; Wu and Jiang, 2000; Chen and Ma, 2002; Gao et al., 2005).",
        "Aiming to improve both tasks, work by Peng et al. (2004) and Sun et al. (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint.",
        "This is a weakness as their linear CRF model requires re-training.",
        "Their method also requires thresholds to be set through heuristic tuning, as to whether the segmented words are indeed new words.",
        "We note that the task of new word detection refers to out-of-vocabulary (OOV) detection, and is distinctly different from IWR (new words could be both formal or informal words)."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "There is a close dependency between Chinese word segmentation (CWS) and informal word recognition (IWR).",
        "To leverage this, we employ a factorial conditional random field to perform both tasks of CWS and IWR jointly.",
        "We propose novel features including statistical and lexical features that improve the performance of the inference process.",
        "We evaluate our method on a manually-constructed data set and compare it with multiple research and industrial baselines that perform CWS and IWR individually or sequentially.",
        "Our experimental results show our joint inference model yields significantly better F1 for both tasks.",
        "For analysis, we also construct upper bound systems to assess the potential maximal improvement, by feeding one task with the gold standard labels from the complementary task.",
        "These experiments further verify the necessity and effectiveness of modeling the two tasks jointly, and point to the possibility of even better performance with improved per-task performance.",
        "Analyzing the classes of errors made by our system, we identify a promising future work topic to handle errors arising from partially observed informal words ?",
        "where parts of a multi-character informal word have been observed before.",
        "We believe incorporating informal word normalization into the inference process may help address this important source of error."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank the anonymous reviewers for their valuable comments.",
        "We also appreciate the proofreading effort made by Tao Chen, Xi-angnan He, Ning Fang, Yushi Wang and Haochen Zhan from WING.",
        "This work also benefits from the discussion with Yang Liu, associate professor from Tsinghua University."
      ]
    }
  ]
}
