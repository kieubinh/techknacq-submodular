{
  "info": {
    "authors": [
      "Jinho D. Choi",
      "Andrew McCallum"
    ],
    "book": "ACL",
    "id": "acl-P13-1104",
    "title": "Transition-based Dependency Parsing with Selectional Branching",
    "url": "https://aclweb.org/anthology/P13-1104",
    "year": 2013
  },
  "references": [
    "acl-D08-1059",
    "acl-D09-1058",
    "acl-D10-1001",
    "acl-D10-1004",
    "acl-D12-1019",
    "acl-D12-1029",
    "acl-D12-1030",
    "acl-D12-1133",
    "acl-E06-1011",
    "acl-J08-4003",
    "acl-J93-2004",
    "acl-N06-2033",
    "acl-N10-1115",
    "acl-N12-1015",
    "acl-N12-1054",
    "acl-P05-1012",
    "acl-P05-1013",
    "acl-P06-2041",
    "acl-P08-1068",
    "acl-P08-1108",
    "acl-P09-1040",
    "acl-P10-1001",
    "acl-P10-1110",
    "acl-P11-2033",
    "acl-P11-2121",
    "acl-P12-2071",
    "acl-W02-1001",
    "acl-W04-0308",
    "acl-W06-2920",
    "acl-W06-2932",
    "acl-W06-2933",
    "acl-W08-1301",
    "acl-W08-2102"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach.",
        "Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately.",
        "We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing.",
        "With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transition-based parser that uses beam search."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Transition-based dependency parsing has gained considerable interest because it runs fast and performs accurately.",
        "Transition-based parsing gives complexities as low as O(n) and O(n2) for projective and non-projective parsing, respectively (Nivre, 2008).1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing.",
        "Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of non-projective dependencies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projec-tivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011).",
        "1We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing.",
        "Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012).",
        "These approaches generate multiple transition sequences given a sentence, and pick one with the highest confidence.",
        "Coupled with dynamic programming, transition-based dependency parsing with beam search can be done very efficiently and gives significant improvement to parsing accuracy.",
        "One downside of beam search is that it always uses a fixed size of beam even when a smaller size of beam is sufficient for good results.",
        "In our experiments, a greedy parser performs as accurately as a parser that uses beam search for about 64% of time.",
        "Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions that a greedy parser makes, in which case, fewer transition sequences need to be explored to produce the same or similar parse output.",
        "We first present a new transition-based parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing.",
        "We then introduce selectional branching that uses confidence estimates to decide when to employ a beam.",
        "With our new approach, we achieve a higher parsing accuracy than the current state-of-the-art transition-based parser that uses beam search and a much faster speed."
      ]
    },
    {
      "heading": "2 Transition-based dependency parsing",
      "text": [
        "We introduce a transition-based dependency parsing algorithm that is a hybrid between Nivre's arc-eager and list-based algorithms (Nivre, 2003; Nivre, 2008).",
        "Nivre's arc-eager is a projective parsing algorithm showing a complexity of O(n).",
        "Nivre's list-based algorithm is a non-projective parsing algorithm showing a complexity of O(n2).",
        "Table 1 shows transitions in our algorithm.",
        "The top 4 and"
      ]
    },
    {
      "heading": "Transition Preconditions",
      "text": [
        "the bottom 3 transitions are inherited from Nivre's arc-eager and list-based algorithms, respectively.2 Each parsing state is represented as a tuple (?, ?, ?, A), where ?",
        "is a stack containing processed tokens, ?",
        "is a deque containing tokens popped out of ?",
        "but will be pushed back into ?",
        "in later parsing states to handle non-projectivity, and ?",
        "is a buffer containing unprocessed tokens.",
        "A is a set of labeled arcs.",
        "(i, j) represent indices of their corresponding tokens (wi, wj), l is a dependency label, and the 0 identifier corresponds to w0, introduced as the root of a tree.",
        "The initial state is ([0], [ ], [1, .",
        ".",
        ".",
        ", n], ?",
        "), and the final state is (?, ?, [ ], A).",
        "At any parsing state, a decision is made by comparing the top of ?, wi, and the first element of ?, wj .",
        "This decision is consulted by gold-standard trees during training and a classifier during decoding.",
        "LEFTl-?",
        "and RIGHTl-?",
        "are performed when wj is the head ofwi with a dependency label l, and vice versa.",
        "After LEFTl-?",
        "or RIGHTl-?, an arc is added to A. NO-?",
        "is performed when no dependency is found for wi and wj .",
        "?-SHIFT is performed when no dependency is found for wj and any token in ?",
        "other than wi.",
        "After ?-SHIFT, all tokens in ?",
        "as well as wj are pushed into ?.",
        "?-REDUCE is performed when wi already has the head, and wi is not the head of any token in ?.",
        "After ?-REDUCE, wi is popped out of ?.",
        "?-PASS is performed when neither ?-SHIFT nor ?-REDUCE can be performed.",
        "After ?-PASS, wi is moved to the front of ?",
        "so it 2The parsing complexity of a transition-based dependency parsing algorithm is determined by the number of transitions performed with respect to the number of tokens in a sentence, say n (K?bler et al., 2009).",
        "can be compared to other tokens in ?",
        "later.",
        "Each transition needs to satisfy certain preconditions to ensure the properties of a well-formed dependency graph (Nivre, 2008); they are described in Table 2.",
        "(i?",
        "j) and (i ??",
        "j) indicate that wj is the head and an ancestor of wi with any label, respectively.",
        "When a parser is trained on only projective trees, our algorithm learns only the top 4 transitions and produces only projective trees during decoding.",
        "In this case, it performs at most 2n ?",
        "1 transitions per sentence so the complexity is O(n).",
        "When a parser is trained on a mixture of projective and non-projective trees, our algorithm learns all transitions and produces both kinds of trees during decoding.",
        "In this case, it performs at most n(n+1)2 transitionsso the complexity is O(n2).",
        "However, because of the presence of ?-SHIFT and ?-REDUCE, our algorithm is capable of skipping or removing tokens during non-projective parsing, which allows it to show a linear time parsing speed in practice.",
        "Figure 1 shows the total number of transitions performed during training with respect to sentence lengths for Dutch.",
        "Among all languages distributed by the CoNLL-X shared task (Buchholz and Marsi, 2006), Dutch consists of the highest number of non-projective dependencies (5.4% in arcs, 36.4% in trees).",
        "Even with such a high number of non-projective dependencies, our parsing algorithm still shows a linear growth in transitions.",
        "Table 3 shows a transition sequence generated by our parsing algorithm using gold-standard decisions.",
        "Afterw3 andw4 are compared, w3 is popped out of ?",
        "(state 4) so it is not compared to any other token in ?",
        "(states 9 and 13).",
        "After w2 and w4 are compared, w2 is moved to ?",
        "(state 5) so it can be compared to other tokens in ?",
        "(state 10).",
        "After w4 and w6 are compared, RIGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in ?",
        "(state 10).",
        "After w6 and w7 are compared, w6 is popped out of ?",
        "(state 12) because it is not needed for later parsing states."
      ]
    },
    {
      "heading": "3 Selectional branching",
      "text": []
    },
    {
      "heading": "3.1 Motivation",
      "text": [
        "For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012).",
        "Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the one-best sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output.",
        "Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence.",
        "The selectional branching method presented here performs at most d ?",
        "t?",
        "e transitions, where t is the maximum number of transitions performed to generate a transition sequence, d = min(b, |?|+1), b is the beam size, |?",
        "|is the number of low confidence predictions made for the one-best sequence, and e = d(d?1)2 .",
        "Compared to beam search that alwaysperforms b ?",
        "t transitions, selectional branching is guaranteed to perform fewer transitions given the same beam size because d ?",
        "b and e > 0 except for d = 1, in which case, no branching happens.",
        "With selectional branching, our parser shows slightly",
        "higher parsing accuracy than the current state-of-the-art transition-based parser using beam search, and performs about 3 times faster."
      ]
    },
    {
      "heading": "3.2 Branching strategy",
      "text": [
        "Figure 2 shows an overview of our branching strategy.",
        "sij represents a parsing state, where i is the index of the current transition sequence and j is the index of the current parsing state (e.g., s12 represents the 2nd parsing state in the 1st transition sequence).",
        "pkj represents the k?th best prediction (in our case, it is a predicted transition) given s1j (e.g., p21 is the 2nd-best prediction given s11).",
        "s",
        "Each sequence Ti>1 branches from T1.",
        "Initially, the one-best sequence T1 = [s11, ... , s1t] is generated by a greedy parser.",
        "While generating T1, the parser adds tuples (s1j , p2j), ... , (s1j , pkj) to a list ?",
        "for each low confidence prediction p1j given s1j .4 Then, new transition sequences are generated by using the b highest scoring predictions in ?, where b is the beam size.",
        "If |?",
        "|< b, all predictions in ?",
        "are used.",
        "The same greedy parser is used to generate these new sequences although it now starts with s1j instead of an initial parsing state, applies pkj to s1j , and performs further transitions.",
        "Once all transition sequences are generated, a parse tree is built from a sequence with the highest score.",
        "For our experiments, we set k = 2, which gave noticeably more accurate results than k = 1.",
        "We also experimented with k > 2, which did not show significant improvement over k = 2.",
        "Note that assigning a greater k may increase |?",
        "|but not the total number of transition sequences generated, which is restricted by the beam size, b.",
        "Since each sequence Ti>1 branches from T1, selectional branching performs fewer transitions than beam search: at least d(d?1)2 transitions are inherited from T1, 4?",
        "is initially empty, which is hidden in Figure 2. where d = min(b, |?",
        "|+ 1); thus, it performs that many transitions less than beam search (see the left lower triangle in Figure 2).",
        "Furthermore, selectional branching generates a d number of sequences, where d is proportional to the number of low confidence predictions made by T1.",
        "To sum up, selectional branching generates the same or fewer transition sequences than beam search and each sequence Ti>1 performs fewer transitions than T1; thus, it performs faster than beam search in general given the same beam size."
      ]
    },
    {
      "heading": "3.3 Finding low confidence predictions",
      "text": [
        "For each parsing state sij , a prediction is made by generating a feature vector xij ?",
        "X , feeding it into a classifier C1 that uses a feature map ?",
        "(x, y) and a weight vector w to measure a score for each label y ?",
        "Y , and choosing a label with the highest score.",
        "When there is a tie between labels with the highest score, the first one is chosen.",
        "This can be expressed as a logistic regression:",
        "To find low confidence predictions, we use the margins (score differences) between the best prediction and the other predictions.",
        "If all margins are greater than a threshold, the best prediction is considered highly confident; otherwise, it is not.",
        "Given this analogy, the k-best predictions can be found as follows (m ?",
        "0 is a margin threshold):",
        "s.t.",
        "f(x,C1(x))?",
        "f(x, y) ?",
        "m ?K arg max?",
        "returns a set of k?",
        "labels whose margins to C1(x) are smaller than any other label's margin to C1(x) and also ?",
        "m, where k?",
        "?",
        "k. When m = 0, it returns a set of the highest scoring labels only, including C1(x).",
        "When m = 1, it returns a set of all labels.",
        "Given this, a prediction is considered not confident if |Ck(x,m) |> 1.",
        "3.4 Finding the best transition sequence Let Pi be a list of all predictions that lead to generate a transition sequence Ti.",
        "The predictions in Pi are either inherited from T1 or made specifically for Ti.",
        "In Figure 2, P3 consists of p11 as its first prediction, p22 as its second prediction, and",
        "further predictions made specifically for T3.",
        "The score of each prediction is measured by f(x, y) in Section 3.3.",
        "Then, the score of Ti is measured by averaging scores of all predictions in Pi.",
        "Unlike Zhang and Clark (2008), we take the average instead of the sum of all prediction scores.",
        "This is because our algorithm does not guarantee the same number of transitions for every sequence, so the sum of all scores would weigh more on sequences with more transitions.",
        "We experimented with both the sum and the average, and taking the average led to slightly higher parsing accuracy."
      ]
    },
    {
      "heading": "3.5 Bootstrapping transition sequences",
      "text": [
        "During training, a training instance is generated for each parsing state sij by taking a feature vector xij and its true label yij .",
        "To generate multiple transition sequences during training, the bootstrapping technique of Choi and Palmer (2011) is used, which is described in Algorithm 1.5 Algorithm 1 Bootstrapping Input: Dt: training set, Dd: development set.",
        "Output: A model M .",
        "1: r ?",
        "0 2: I ?",
        "getTrainingInstances(Dt) 3: M0 ?",
        "buildModel(I) 4: S0 ?",
        "getScore(Dd,M0) 5: while (r = 0) or (Sr?1 < Sr) do 6: r ?",
        "r + 1 7: I ?",
        "getTrainingInstances(Dt,Mr?1) 8: Mr ?",
        "buildModel(I) 9: Sr ?",
        "getScore(Dd,Mr) 10: return Mr?1",
        "First, an initial model M0 is trained on all data by taking the one-best sequences, and its score is measured by testing on a development set (lines 2-4).",
        "Then, the next model Mr is trained on all data but this time, Mr?1 is used to generate multiple transition sequences (line 7-8).",
        "Among all transition sequences generated by Mr?1, training instances from only T1 and Tg are used to trainMr, where T1 is the one-best sequence and Tg is a sequence giving the most accurate parse output compared to the gold-standard tree.",
        "The score of Mr is measured (line 9), and repeat the procedure if Sr?1 < Sr; otherwise, return the previous model Mr?1.",
        "5Alternatively, the dynamic oracle approach of Goldberg and Nivre (2012) can be used to generate multiple transition sequences, which is expected to show similar results."
      ]
    },
    {
      "heading": "3.6 Adaptive subgradient algorithm",
      "text": [
        "To build each model during bootstrapping, we use a stochastic adaptive subgradient algorithm called ADAGRAD that uses per-coordinate learning rates to exploit rarely seen features while remaining scalable (Duchi et al., 2011).This is suitable for NLP tasks where rarely seen features often play an important role and training data consists of a large number of instances with high dimensional features.",
        "Algorithm 2 shows our adaptation of ADAGRAD with logistic regression for multi-class classification.",
        "Note that when used with logistic regression, ADAGRAD takes a regular gradient instead of a subgradient method for updating weights.",
        "For our experiments, ADAGRAD slightly outperformed learning algorithms such as average perceptron (Collins, 2002) or Liblinear SVM (Hsieh et al., 2008).",
        "1: w?",
        "0, where w ?",
        "Rd 2: G?",
        "0, where G ?",
        "Rd 3: for t?",
        "1 .",
        ".",
        ".",
        "T do 4: for i?",
        "1 .",
        ".",
        ".",
        "n do 5: Q?y?Y ?",
        "I(yi, y)?",
        "f(xi, y), s.t.",
        "Q ?",
        "R|Y| 6: ?",
        "??y?Y(?",
        "(xi, y) ?Qy) 7: G?",
        "G + ?",
        "?",
        "?",
        "8: for j ?",
        "1 .",
        ".",
        ".",
        "d do 9: wj ?",
        "wj + ?",
        "?",
        "1?+?Gj ?",
        "?j",
        "The algorithm takes three hyper-parameters; T is the number of iterations, ?",
        "is the learning rate, and ?",
        "is the ridge (T > 0, ?",
        "> 0, ?",
        "?",
        "0).",
        "G is our running estimate of a diagonal covariance matrix for the gradients (per-coordinate learning rates).",
        "For each instance, scores for all labels are measured by the logistic regression function f(x, y) in Section 3.3.",
        "These scores are subtracted from an output of the indicator function I(y, y?",
        "), which forces our model to keep learning this instance until the prediction is 100% confident (in other words, until the score of yi becomes 1).",
        "Then, a subgradient is measured by taking all feature vectors together weighted by Q (line 6).",
        "This subgradient is used to update G and w, where ?",
        "is the Hadamard product (lines 7-9).",
        "?",
        "is a ridge term to keep the inverse covariance well-conditioned."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "4.1 Corpora",
      "text": [
        "For projective parsing experiments, the Penn English Treebank (Marcus et al., 1993) is used with the standard split: sections 2-21 for training, 22 for development, and 23 for evaluation.",
        "All constituent trees are converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).",
        "For non-projective parsing experiments, four languages from the CoNLL-X shared task are used: Danish, Dutch, Slovene, and Swedish (Buchholz and Marsi, 2006).",
        "These languages are selected because they contain non-projective trees and are publicly available from the CoNLL-X webpage.6 Since the CoNLL-X data we have does not come with development sets, the last 10% of each training set is used for development."
      ]
    },
    {
      "heading": "4.2 Feature engineering",
      "text": [
        "For English, we mostly adapt features from Zhang and Nivre (2011) who have shown state-of-the-art parsing accuracy for transition-based dependency parsing.",
        "Their distance features are not included in our approach because they do not seem to show meaningful improvement.",
        "Feature selection is done on the English development set.",
        "For the other languages, the same features are used with the addition of morphological features provided by CoNLL-X; specifically, morphological features from the top of ?",
        "and the front of ?",
        "are added as unigram features.",
        "Moreover, all POS tag features from English are duplicated with coarse-grained POS tags provided by CoNLL-X.",
        "No more feature engineering is done for these languages; it is possible to achieve higher performance by using different features, especially when these languages contain non-projective dependencies whereas English does not, which we will explore in the future."
      ]
    },
    {
      "heading": "4.3 Development",
      "text": [
        "Several parameters need to be optimized during development.",
        "For ADAGRAD, T , ?, and ?",
        "need to be tuned (Section 3.6).",
        "For bootstrapping, the number of iterations, say r, needs to be tuned (Section 3.5).",
        "For selectional branching, the margin threshold m and the beam size b need to be tuned (Section 3.3).",
        "First, all parameters are tuned on the English development set by using grid search on T = [1, .",
        ".",
        ".",
        ", 10],",
        "As a result, the following parameters are found: ?",
        "= 0.02, ?",
        "= 0.1, m = 0.88, and b = 64|80.",
        "For this development set, the beam size of 64 and 80 gave the exact same result, so we kept the one with a larger beam size (b = 80).",
        "gins and beam sizes on the English development set.",
        "b = 64|80: the black solid line with solid circles, b = 32: the blue dotted line with hollow circles, b = 16: the red dotted line with solid circles.",
        "Figure 3 shows parsing accuracies with respect to different margins and beam sizes on the English development set.",
        "These parameters need to be tuned jointly because different margins prefer different beam sizes.",
        "For instance, m = 0.85 gives the highest accuracy with b = 32, but m = 0.88 gives the"
      ]
    },
    {
      "heading": "5 ADAGRAD iterations before bootstrapping, the",
      "text": [
        "range 6-9 shows results of 4 iterations during the",
        "first bootstrapping, and the range 10-14 shows results of 5 iterations during the second bootstrapping.",
        "Thus, the number of bootstrap iteration is 2 where each bootstrapping takes a different number of ADAGRAD iterations.",
        "Using an Intel Xeon 2.57GHz machine, it takes less than 40 minutes to train the entire Penn Treebank, which includes times for IO, feature extraction and bootstrapping.",
        "during decoding with respect to beam sizes on the English development set.",
        "Figure 5 shows the total number of transitions performed during decoding with respect to beam sizes on the English development set (1,700 sentences, 40,117 tokens).",
        "With selectional branching, the number of transitions grows logarithmically as the beam size increases whereas it would have grown linearly if beam search were used.",
        "We also checked how often the one best sequence is chosen as the final sequence during decoding.",
        "Out of 1,700 sentences, the one best sequences are chosen for 1,095 sentences.",
        "This implies that about 64% of time, our greedy parser performs as accurately as our non-greedy parser using selectional branching.",
        "For the other languages, we use the same values as English for ?, ?, m, and b; only the ADAGRAD and bootstrap iterations are tuned on the development sets of the other languages."
      ]
    },
    {
      "heading": "4.4 Projective parsing experiments",
      "text": [
        "Before parsing, POS tags were assigned to the training set by using 20-way jackknifing.",
        "For the automatic generation of POS tags, we used the domain-specific model of Choi and Palmer (2012a)'s tagger, which gave 97.5% accuracy on the English evaluation set (0.2% higher than Collins (2002)'s tagger).",
        "Table 4 shows comparison between past and current state-of-the-art parsers and our approach.",
        "The first block shows results from transition-based dependency parsers using beam search.",
        "The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition).",
        "The third block shows results from parsers using external data.",
        "The last block shows results from our approach.",
        "The Time column show how many seconds per sentence each parser takes.7",
        "glish evaluation set, excluding tokens containing only punctuation.",
        "bt and bd indicate the beam sizes used during training and decoding, respectively.",
        "UAS: unlabeled attachment score, LAS: labeled attachment score, Time: seconds per sentence.",
        "For evaluation, we use the model trained with b = 80 and m = 0.88, which is the best setting found during development.",
        "Our parser shows higher accuracy than Zhang and Nivre (2011), which is the current state-of-the-art transition-based parser that uses beam search.",
        "Bohnet and Nivre (2012)'s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours.",
        "Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach.",
        "In terms of speed, our parser outperforms all other transition-based parsers; it takes about 9 milliseconds per 7Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods.",
        "sentence using the beam size of 80.",
        "Our parser is implemented in Java and tested on an Intel Xeon 2.57GHz.",
        "Note that we do not include input/output time for our speed comparison.",
        "For a proof of concept, we run the same model, trained with bt = 80, but decode with different beam sizes using the same margin.",
        "Surprisingly, our parser gives the same accuracy (0.01% higher for labeled attachment score) on this data even with bd = 16.",
        "More importantly, bd = 16 shows about the same parsing speed as bd = 80, which indicates that selectional branching automatically reduced down the beam size by estimating low confidence predictions, so even if we assigned a larger beam size for decoding, it would have performed as efficiently.",
        "This implies that we no longer need to be so conscious about the beam size during decoding.",
        "Another interesting part is that (bt = 80, bd = 1) shows higher accuracy than (bt = 1, bd = 1); this implies that our training method of bootstrapping transition sequences can improve even a greedy parser.",
        "Notice that our greedy parser shows higher accuracy than many other greedy parsers (Hall et al., 2006; Goldberg and Elhadad, 2010) because it uses the non-local features of Zhang and Nivre (2011) and the bootstrapping technique of Choi and Palmer (2011) that had not been used for most other greedy parsing approaches."
      ]
    },
    {
      "heading": "4.5 Non-projective parsing experiments",
      "text": [
        "Table 5 shows comparison between state-of-the-art parsers and our approach for four languages with non-projective dependencies.",
        "Nivre et al. (2006) uses a pseudo-projective transition-based parsing approach.",
        "McDonald et al. (2006) uses a 2nd-order maximum spanning tree approach.",
        "Nivre (2009) and Fern?ndez-Gonz?lez and G?mez-Rodr?guez (2012) use different non-projective transition-based parsing approaches.",
        "Nivre and McDonald (2008) uses an ensemble model between transition-based and graph-based parsing approaches.",
        "Martins et al.",
        "(2010) uses integer linear programming for the optimization of their parsing model.",
        "Some of these approaches use greedy parsers, so we include our results from models using (bt = 80, bd = 1, m = 0.88), which finds only the one-best sequences during decoding although it is trained on multiple transition sequences (see Section 4.4).",
        "Our parser shows higher accuracies for most languages except for unlabeled attachment scores in Danish and Slovene.",
        "Our greedy approach outperforms both Nivre (2009) and Fern?ndez-Gonz?lez and G?mez-Rodr?guez (2012) who use different non-projective parsing algorithms.",
        "coding with respect to sentence lengths for Dutch.",
        "Figure 6 shows the number of transitions performed during decoding with respect to sentence lengths for Dutch using bd = 1.",
        "Our parser still shows a linear growth in transition during decoding."
      ]
    },
    {
      "heading": "5 Related work",
      "text": [
        "Our parsing algorithm is most similar to Choi and Palmer (2011) who integrated our LEFT-REDUCE transition into Nivre's list-based algorithm.",
        "Our algorithm is distinguished from theirs because ours gives different parsing complexities of O(n) and O(n2) for projective and non-projective parsing, respectively, whereas their algorithm gives O(n2)",
        "for both cases; this is possible because of the new integration of the RIGHT-SHIFT and NO-REDUCE transitions.",
        "There are other transition-based dependency parsing algorithms that take a similar approach; Nivre (2009) integrated a SWAP transition into Nivre's arc-standard algorithm (Nivre, 2004) and Fern?ndez-Gonz?lez and G?mez-Rodr?guez (2012) integrated a buffer transition into Nivre's arc-eager algorithm to handle non-projectivity.",
        "Our selectional branching method is most relevant to Zhang and Clark (2008) who introduced a transition-based dependency parsing model that uses beam search.",
        "Huang and Sagae (2010) later applied dynamic programming to this approach and showed improved efficiency.",
        "Zhang and Nivre (2011) added non-local features to this approach and showed improved parsing accuracy.",
        "Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS tagging and dependency parsing.",
        "Our work is distinguished from theirs because we use selectional branching instead."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We present selectional branching that uses confidence estimates to decide when to employ a beam.",
        "Coupled with our new hybrid parsing algorithm, ADAGRAD, rich non-local features, and bootstrapping, our parser gives higher parsing accuracy than most other transition-based dependency parsers in multiple languages and shows faster parsing speed.",
        "It is interesting to see that our greedy parser outperformed most other greedy dependency parsers.",
        "This is because our parser used both bootstrapping and Zhang and Nivre (2011)'s non-local features, which had not been used by other greedy parsers.",
        "In the future, we will experiment with more advanced dependency representations (de Marneffe and Manning, 2008; Choi and Palmer, 2012b) to show robustness of our approach.",
        "Furthermore, we will evaluate individual methods of our approach separately to show impact of each method on parsing performance.",
        "We also plan to implement the typical beam search approach to make a direct comparison to our selectional branching.8"
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Special thanks are due to Luke Vilnis of the University of Massachusetts Amherst for insights on 8Our parser is publicly available under an open source project, ClearNLP (clearnlp.googlecode.com).",
        "the ADAGRAD derivation.",
        "We gratefully acknowledge a grant from the Defense Advanced Research"
      ]
    }
  ]
}
