{
  "info": {
    "authors": [
      "Jiwei Tan",
      "Xiaojun Wan",
      "Jianguo Xiao"
    ],
    "book": "ACL",
    "id": "acl-P13-2016",
    "title": "Learning to Order Natural Language Texts",
    "url": "https://aclweb.org/anthology/P13-2016",
    "year": 2013
  },
  "references": [
    "acl-C04-1108",
    "acl-C10-2170",
    "acl-I05-1055",
    "acl-N04-1015",
    "acl-N09-3014",
    "acl-P03-1069",
    "acl-W06-1662"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Ordering texts is an important task for many NLP applications.",
        "Most previous works on summary sentence ordering rely on the contextual information (e.g. adjacent sentences) of each sentence in the source document.",
        "In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information.",
        "We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences.",
        "We also propose to use the genetic algorithm to determine the total order of all sentences.",
        "Evaluation results on a news corpus show the effectiveness of our proposed method."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Ordering texts is an important task in many natural language processing (NLP) applications.",
        "It is typically applicable in the text generation field, both for concept-to-text generation and text-to-text generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on.",
        "However, ordering a set of sentences into a coherent text is still a hard and challenging problem for computers.",
        "Previous works on sentence ordering mainly focus on the MDS task (Barzilay et al., 2002; Okazaki et al., 2004; Nie et al., 2006; Ji and Pulman, 2006; Madnani et al., 2007; Zhang et al., 2010; He et al., 2006; Bollegala et al., 2005; Bollegala et al., 2010).",
        "In this task, each summary sentence is extracted from a source document.",
        "The timestamp of the source documents and the adjacent sentences in the source documents can be used as important clues for ordering summary sentences.",
        "In this study, we investigate a more challenging and more general task of ordering a set of unordered sentences (e.g. randomly shuffle the * Xiaojun Wan is the corresponding author.",
        "sentences in a text paragraph) without any contextual information.",
        "This task can be applied to almost all text generation applications without restriction.",
        "In order to address this challenging task, we first introduce a few useful features to characterize the order and coherence of natural language texts, and then propose to use the learning to rank algorithm to determine the order of two sentences.",
        "Moreover, we propose to use the genetic algorithm to decide the overall text order.",
        "Evaluations are conducted on a news corpus, and the results show the prominence of our method.",
        "Each component technique or feature in our method has also been validated."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts.",
        "Experimental evaluation indicated the importance of several learned lexical and syntactic features.",
        "However, the model only works well when using single feature, but unfortunately, it becomes worse when multiple features are combined.",
        "Barzilay and Lee (2004) investigated the utility of domain-specific content model for representing topic and topic shifts and the model performed well on the five selected domains.",
        "Nahnsen (2009) employed features which were based on discourse entities, shallow syntactic analysis, and temporal precedence relations retrieved from VerbOcean.",
        "However, the model does not perform well on datasets describing the consequences of events."
      ]
    },
    {
      "heading": "3 Our Proposed Method",
      "text": []
    },
    {
      "heading": "3.1 Overview",
      "text": [
        "The task of text ordering can be modeled like (Cohen et al., 1998), as measuring the coherence of a text by summing the association strength of any sentence pairs.",
        "Then the objective of a text ordering model is to find a permutation which can maximize the summation.",
        "Formally, we define an association strength function PREF( , ) Ru v ?",
        "to measure how strong it is that sentence u should be arranged before sentence v (denoted as u v; ).",
        "We then define function AGREE( ,PREF)?",
        "as:",
        "where ?",
        "denotes a sentence permutation and ( ) ( )u v?",
        "?> means u v; in the permutation ?",
        ".",
        "Then the objective of finding an overall order of the sentences becomes finding a permutation ?",
        "to maximize AGREE( ,PREF)?",
        ".",
        "The main framework is made up of two parts: defining a pairwise order relation and determining an overall order.",
        "Our study focuses on both the two parts by learning a better pairwise relation and proposing a better search strategy, as described respectively in next sections."
      ]
    },
    {
      "heading": "3.2 Pairwise Relation Learning",
      "text": [
        "The goal for pairwise relation learning is defining the strength function PREF for any sentence pair.",
        "In our method we define the function PREF by combining multiple features.",
        "Method: Traditionally, there are two main methods for defining a strength function: integrating features by a linear combination (He et al., 2006; Bollegala et al., 2005) or by a binary classifier (Bollegala et al., 2010).",
        "However, the binary classification method is very coarse-grained since it considers any pair of sentences either ?positive?",
        "or ?negative?.",
        "Instead we propose to use a better model of learning to rank to integrate multiple features.",
        "In this study, we use Ranking SVM implemented in the svmrank toolkit (Joachims, 2002; Joachims, 2006) as the ranking model.",
        "The examples to be ranked in our ranking model are sequential sentence pairs like u v; .",
        "The feature values for a training example are generated by a few feature functions ( , )if u v , and we will introduce the features later.",
        "We build the training examples for svmrank as follows: For a training query, which is a paragraph with n sequential sentences as 1 2 ... ns s s; ; ; , we can get 2 ( 1)nA n n= ?",
        "training examples.",
        "For pairs like ( 0)a a ks s k+ >; the target rank values are set to n k?",
        ", which means that the longer the distance between the two sentences is, the smaller the target value is.",
        "Other pairs like a k as s+ ; are all set to 0.",
        "In order to better capture the order information of each feature, for every sentence pair u v; , we derive four feature values from each function ( , )if u v , which are listed as follows:",
        "where S is the set of all sentences in a paragraph and S is the number of sentences in S .",
        "The three additional feature values of (3) (4) (5) are defined to measure the priority of u v; to v u; , u v; to { , }u y S u v?",
        "?",
        "?",
        "; and u v; to { , }x S u v v?",
        "?",
        "?",
        "; respectively, by calculating the proportion of ( , )if u v in respective summations.",
        "The learned model can be used to predict target values for new examples.",
        "A paragraph of unordered sentences is viewed as a test query, and the predicted target value for u v; is set as PREF( , )u v .",
        "Features: We select four types of features to characterize text coherence.",
        "Every type of features is quantified with several functions distinguished by i in the formulation of ( , )if u v and normalized to [0,1] .",
        "The features and definitions of ( , )if u v are introduced in Table 1.",
        "As in Table 1, function sim( , )u v denotes the cosine similarity of sentence u and v ; latter( )u and former( )v denotes the latter half part of u and the former part of v respectively, which are separated by the most centered comma (if exists) or word (if no comma exits); overlap ( , )j u v denotes the number of mutual words of u and v , for 1,2,3j = representing lemmatized noun, verb and adjective or adverb respectively; ||u is the number of words of sentence u .",
        "The value will be set to 0 if the denominator is 0.",
        "For the coreference features we use the ARK-ref 1 tool.",
        "It can output the coreference chains containing words which represent the same entity for two sequential sentences u v; .",
        "The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency."
      ]
    },
    {
      "heading": "3.3 Overall Order Determination",
      "text": [
        "Cohen et al. (1998) proved finding a permutation ?",
        "to maximize AGREE( ,PREF)?",
        "is NP-complete.",
        "To solve this, they proposed a greedy algorithm for finding an approximately optimal order.",
        "Most later works adopted the greedy search strategy to determine the overall order.",
        "However, a greedy algorithm does not always lead to satisfactory results, as our experiment shows in Section 4.2.",
        "Therefore, we propose to use the genetic algorithm (Holland, 1992) as the search strategy, which can lead to better results.",
        "Genetic Algorithm: The genetic algorithm (GA) is an artificial intelligence algorithm for optimization and search problems.",
        "The key point of using GA is modeling the individual, fitness function and three operators of crossover, mutation and selection.",
        "Once a problem is modeled, the algorithm can be constructed conventionally.",
        "In our method we set a permutation ?",
        "as an individual encoded by a numerical path, for example a permutation 2 1 3s s s; ; is encoded as (2 1 3).",
        "Then the function AGREE( ,PREF)?",
        "is just the fitness function.",
        "We adopt the order-based crossover operator which is described in (Davis, 1985).",
        "The mutation operator is a random inversion of two sentences.",
        "For selection operator we take a tournament selection operator which randomly selects two individuals to choose the one with the greater fitness value AGREE( ,PREF)?",
        ".",
        "After several generations of evolution, the individual with the greatest fitness value will be a close solution to the optimal result."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "4.1 Experiment Setup Data Set and Evaluation Metric: We con",
      "text": [
        "ducted the experiments on the North American News Text Corpus2.",
        "We trained the model on 80 thousand paragraphs and tested with 200 shuffled paragraphs.",
        "We use Kendall's ?",
        "as the evaluation metric, which is based on the number of inversions in the rankings.",
        "Comparisons: It is incomparable with other methods for summary sentence ordering based on special summarization corpus, so we implemented Lapata's probability model for comparison, which is considered the state of the art for this task.",
        "In addition, we implemented a random ordering as a baseline.",
        "We also tried to use a classification model in place of the ranking model.",
        "In the classification model, sentence pairs like 1a as s +; were viewed as positive examples and all other pairs were viewed as negative examples.",
        "When deciding the overall order for either ranking or classification model we used three search strategies: greedy, genetic and exhaustive (or brutal) algorithms.",
        "In addition, we conducted a series of experiments to evaluate the effect of each feature.",
        "For each feature, we tested in two experiments, one of which only contained the single feature and the other one contained all the other features.",
        "For comparative analysis of features, we tested with an exhaustive search algorithm to determine the overall order."
      ]
    },
    {
      "heading": "4.2 Experiment Results",
      "text": [
        "The comparison results in Table 2 show that our Ranking SVM based method improves the performance over the baselines and the classification based method with any of the search algorithms.",
        "We can also see the greedy search strategy does not perform well and the genetic algorithm can provide a good approximate solution to",
        "Ranking vs.",
        "Classification: It is not surprising that the ranking model is better, because when using a classification model, an example should be labeled either positive or negative.",
        "It is not very reasonable to label a sentence pair like ( 1)a a ks s k+ >; as a negative example, nor a positive one, because in some cases, it is easy to conclude one sentence should be arranged after another but hard to decide whether they should be adjacent.",
        "As we see in the function AGREE , the value of PREF( , )a a ks s + also contributes to the summation.",
        "In a ranking model, this information can be quantified by the different priorities of sentence pairs with different distances.",
        "Single Feature Effect: The effects of different types of features are shown in Table 3.",
        "Prob denotes Lapata's probability model with different features.",
        "It can be seen in Table 3 that all these features contribute to the final result.",
        "The two features of noun probability and dependency probability play an important role as demonstrated in (Lapata, 2003).",
        "Other features also improve the final performance.",
        "A paragraph which is ordered entirely right by our method is shown in Figure 1.",
        "Sentences which should be arranged together tend to have a higher similarity and overlap.",
        "Like sentence (3) and (4) in Figure 1, they have a highest cosine similarity of 0.2240 and most overlap words of ?Israel?",
        "and ?nuclear?.",
        "However, the similarity or overlap of the two sentences does not help to decide which sentence should be arranged before another.",
        "In this case the overlap and similarity of half part of the sentences may help.",
        "For example latter((3)) and former((4)) share an overlap of ?Israel?",
        "while there is no overlap for latter((4)) and former((3)).",
        "Coreference is also an important clue for ordering natural language texts.",
        "When we use a pronoun to represent an entity, it always has occurred before.",
        "For example when conducting coreference resolution for (1) (2); , it will be found that ?He?",
        "refers to ?Vanunu?.",
        "Otherwise for (2) (1); , no coreference chain will be found."
      ]
    },
    {
      "heading": "4.3 Genetic Algorithm",
      "text": [
        "There are three main parameters for GA including the crossover probability (PC), the mutation probability (PM) and the population size (PS).",
        "There is no definite selection for these parameters.",
        "In our study we experimented with a wide range of parameter values to see the effect of each parameter.",
        "It is hard to traverse all possible combinations so when testing a parameter we fixed the other two parameters.",
        "The results are shown in Table 4.",
        "As we can see in Table 4, when adjusting the three parameters the average ?",
        "values are all close to the exhaustive result of 0.5768 and their standard deviations are low.",
        "Table 4 shows that in our case the genetic algorithm is not very sensible to the parameters.",
        "In the experiments, we set PS to 30, PC to 0.5 and PM to 0.05, and reached a value of 0.5747, which is very close to the theoretical upper bound of 0.5768."
      ]
    },
    {
      "heading": "5 Conclusion and Discussion",
      "text": [
        "In this paper we propose a method for ordering sentences which have no contextual information by making use of Ranking SVM and the genetic algorithm.",
        "Evaluation results demonstrate the good effectiveness of our method.",
        "In future work, we will explore more features such as semantic features to further improve the performance."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "(1) Vanunu, 43, is serving an 18-year sentence for treason.",
        "(2) He was kidnapped by Israel's Mossad spy agency in Rome in 1986 after giving The Sunday Times of London photographs of the inside of the Dimona reactor.",
        "(3) From the photographs, experts determined that Israel had the world's sixth largest stockpile of nuclear weapons.",
        "(4) Israel has never confirmed or denied that it"
      ]
    }
  ]
}
