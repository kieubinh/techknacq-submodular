{
  "info": {
    "authors": [
      "Jan Šnajder",
      "Sebastian Padó",
      "Željko Agić"
    ],
    "book": "ACL",
    "id": "acl-P13-2137",
    "title": "Building and Evaluating a Distributional Memory for Croatian",
    "url": "https://aclweb.org/anthology/P13-2137",
    "year": 2013
  },
  "references": [
    "acl-D07-1060",
    "acl-J10-4006",
    "acl-J10-4007",
    "acl-L08-1078",
    "acl-W06-2932"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We report on the first structured distributional semantic model for Croatian, DM.HR.",
        "It is constructed after the model of the English Distributional Memory (Ba-roni and Lenci, 2010), from a dependency-parsed Croatian web corpus, and covers about 2M lemmas.",
        "We give details on the linguistic processing and the design principles.",
        "An evaluation shows state-of-the-art performance on a semantic similarity task with particularly good performance on nouns.",
        "The resource is freely available."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Most current work in lexical semantics is based on the Distributional Hypothesis (Harris, 1954), which posits a correlation between the degree of words?",
        "semantic similarity and the similarity of the contexts in which they occur.",
        "Using this hypothesis, word meaning representations can be extracted from large corpora.",
        "Words are typically represented as vectors whose dimensions correspond to context features.",
        "The vector similarities, which are interpreted as semantic similarities, are used in numerous applications (Turney and Pantel, 2010).",
        "Most vector spaces in current use are either word-based (co-occurrence defined by surface window, context words as dimensions) or syntax-based (co-occurrence defined syntactically, syntactic objects as dimensions).",
        "Syntax-based models have several desirable properties.",
        "First, they are model to fine-grained types of semantic similarity such as predicate-argument plausibility (Erk et al., 2010).",
        "Second, they are more versatile ?",
        "Baroni and Lenci (2010) have presented a generic framework, the Distributional Memory (DM), which is applicable to a wide range of tasks beyond word similarity.",
        "Third, they avoid the ?syntactic assumption?",
        "inherent in word-based models, namely that context words are relevant iff they are in an n-word window around the target.",
        "This property is particularly relevant for free word order languages with many long distance dependencies and non-projective structure (Ku?bler et al., 2009).",
        "Their obvious problem, of course, is that they require a large parsed corpus.",
        "In this paper, we describe the construction of a Distributional Memory for Croatian (DM.HR), a free word order language.",
        "To do so, we parse hrWaC (Ljubes?ic?",
        "and Erjavec, 2011), a 1.2B-token Croatian web corpus.",
        "We evaluate DM.HR on a synonym choice task, where it outperforms the standard bag-of-word model for nouns and verbs."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Vector space semantic models have been applied to a number of Slavic languages, including Bulgarian (Nakov, 2001a), Czech (Smrz?",
        "and Rychly?, 2001), Polish (Piasecki, 2009; Broda et al., 2008; Broda and Piasecki, 2008), and Russian (Nakov, 2001b; Mitrofanova et al., 2007).",
        "Previous work on distributional semantic models for Croatian dealt with similarity prediction (Ljubes?ic?",
        "et al, 2008; Jankovic?",
        "et al, 2011) and synonym detection (Karan et al., 2012), however using only word-based and not syntactic-based models.",
        "So far the only DM for a language other than English is the German DM.DE by Pado?",
        "and Utt (2012), who describe the process of building DM.DE and the evaluation on a synonym choice task.",
        "Our work is similar, though each language has its own challenges.",
        "Croatian, like other Slavic languages, has rich inflectional morphology and free word order, which lead to errors in linguistic processing and affect the quality of the DM."
      ]
    },
    {
      "heading": "3 Distributional Memory",
      "text": [
        "DM represents co-occurrence information in a general, non-task-specific manner, as a tensor, i.e., a three-dimensional matrix, of weighted word-link-word tuples.",
        "Each tuple is mapped onto a number by scoring function ?",
        ": W ?",
        "L ?W ?",
        "R+, that reflects the strength of the association.",
        "When a particular task is selected, a vector space for this task can be generated from the tensor by matricization.",
        "Regarding the examples from Section 1, synonym discovery would use a word by link-word space (W ?",
        "LW ), which contains vectors for words w represented by pairs ?l, w?",
        "of a link and a context word.",
        "Analogy discovery would use a word-word by link space (WW ?",
        "L), which represents word pairs ?w1, w2?",
        "by vectors over links l. The links can be chosen to model any relation of interest between words.",
        "However, as noted by Pado?",
        "and Utt (2012), dependency relations are the most obvious choice.",
        "Baroni and Lenci (2010) introduce three dependency-based DM variants: De-pDM, LexDM, and TypeDM.",
        "DepDM uses links that correspond to dependency relations, with sub-categorization for subject (subj tr and subj intr) and object (obj and iobj).",
        "Furthermore, all prepositions are lexicalized into links (e.g., ?sun, on, Sunday?).",
        "Finally, the tensor is symmetrized: for each tuple ?w1, l, w2?, its inverse ?w2, l?1, w1?",
        "is included.",
        "The other two variants are more complex: LexDM uses more lexicalized links, encoding, e.g., lexical material between the words, while TypeDM extends LexDM with a scoring function based on lexical variability.",
        "Following the work of Pado?",
        "and Utt (2012), we build a DepDM variant for DM.HR.",
        "Although Baroni and Lenci (2010) show that TypeDM can outperform the other two variants, DepDM often performs at a comparable level, while being much simpler to build and more efficient to compute."
      ]
    },
    {
      "heading": "4 Building DM.HR",
      "text": [
        "To build DM.HR, we need to collect co-occurrence counts from a corpus.",
        "Since no sufficiently large suitable corpus exists for Croatian, we first explain how we preprocessed, tagged, and parsed the data.",
        "Corpus and preprocessing.",
        "We adopted hrWaC, the 1.2B-token Croatian web corpus (Ljubes?ic?",
        "and Erjavec, 2011), as starting point.",
        "hrWaC was built with the aim of obtaining a cleaner-than-usual web corpus.",
        "To this end, a conservative boilerplate removal procedure was used; Ljubes?ic?",
        "and Erjavec (2011) report a precision of 97.9% and a recall of 70.7%.",
        "Nonetheless, our inspection revealed that, apart from the unavoidable spelling and grammatical errors, hrWaC still contains non-textual content (e.g., code snippets and formatting structure), encoding errors, and foreign-language content.",
        "As this severely affects linguistic processing, we additionally filtered the corpus.",
        "First, we removed from hrWaC the content crawled from main discussion forum and blog websites.",
        "This content is highly ungrammatical and contains a lot of non-diacriticized text, typical for user-generated content.",
        "This step alone removed one third of the data.",
        "We processed the remaining content with a tokenizer and a sentence segmenter based on regular expressions, obtaining 66M sentences.",
        "Next, we applied a series of heuristic filters at the document-and sentence-level.",
        "At the document level, we discard all documents (1) whose length is below a specified threshold, (2) contain no diacritics, (3) contain no words from a list of frequent Croatian words, or (4) contain a single word from lists of distinctive foreign-language words (for Serbian).",
        "The last two steps serve to eliminate foreign-language content.",
        "In particular, the last step serves to filter out the text in Serbian, which at the sentence-level is difficult to automatically discriminate from Croatian.",
        "At the sentence-level, we discard sentences that are (1) shorter than a specified threshold, (2) contain non-standard symbols,",
        "(3) contain non-diacriticized Croatian words, or (4) contain too many foreign words from a list of",
        "foreign-language words (for English and Slovene).",
        "The last step filters out specifically the sentences in English and Slovene, as we found that these often occur mixed with text in Croatian.",
        "The final filtered version of hrWaC contains 51M sentences and 1.2B tokens.",
        "The corpus is freely available for download, along with a more detailed description of the preprocessing steps.1 Tagging, lemmatization, and parsing.",
        "For mor-phosyntactic (MSD) tagging, lemmatization, and dependency parsing of hrWaC, we use freely available tools with models trained on the new SETimes Corpus of Croatian (SETIMES.HR), based on the Croatian part of the SETimes parallel corpus.2 SE-TIMES.HR and the derived tools are prototypes",
        "racy that are about to be released as parts of another work.",
        "Here we give a general description and a re-evaluation that we consider relevant for building DM.HR.",
        "SETIMES.HR consists of 90K tokens and 4K sentences, manually lemmatized and MSD-tagged according to Multext East v4 tagset (Erjavec, 2012), with the help of the Croatian Lemmatization Server (Tadic?, 2005).",
        "It is used also as a basis for a novel formalism for syntactic annotation and dependency parsing of Croatian (Agic?",
        "and Merkler, 2013).",
        "On the basis of previous evaluation for Croatian (Agic?",
        "et al, 2008; Agic?",
        "et al, 2009; Agic?, 2012) and availability and licensing considerations, we chose HunPos tagger (Hala?csy et al., 2007), CST lemmatizer (Ingason et al., 2008), and MSTParser (McDonald et al., 2006) to process hrWaC.",
        "We evaluated the tools on 100-sentence test sets from SETIMES.HR and Wikipedia; performance on Wikipedia should be indicative of the performance on a cross-domain dataset, such as hrWaC.",
        "In Table 1 we show lemmatization and tagging accuracy, as well as dependency parsing accuracy in terms of labeled attachment score (LAS).",
        "The results show that lemmatization, tagging and parsing accuracy improves on the state of the art for Croatian.",
        "The SETIMES.HR dependency parsing models are publicly available.3 Syntactic patterns.",
        "We collect the co-occurrence counts of tuples using a set of syntactic patterns.",
        "The patterns effectively define the link types, and hence the dimensions of the semantic space.",
        "Similar to previous work, we use two sorts of links: unlexicalized and lexicalized.",
        "For unlexicalized links, we use ten syntactic patterns.",
        "These correspond to the main dependency relations produced by our parser: Pred for predicates, Atr for attributes, Adv for adverbs, Atv for verbal complements, Obj for objects, Prep for prepositions, and Pnom for nominal predicates.",
        "We sub-categorized the subject relation into Sub tr (sub",
        "jects of transitive verbs) and Sub intr (subject of intransitive verbs).",
        "The motivation for this is better modeling of verb semantics by capturing diathesis alternations.",
        "In particular, for many Croatian verbs reflexivization introduces a meaning shift, e.g., predati (to hand in/out) vs. predati se (to surrender).",
        "With subject subcategorization, reflexive and irreflexive readings will have different tensor representations; e.g., ?student, Subj tr, zadac?a?",
        "(?student, Subj tr, homework?)",
        "vs. ?trupe, Subj intr, napadac??",
        "(?troops, Subj intr, invadors?).",
        "Finally, similar to Pado?",
        "and Utt (2012), we use Verb as an underspecified link between subjects and objects linked by non-auxiliary verbs.",
        "For lexicalized links, we use two more extraction patterns for prepositions and verbs.",
        "Prepositions are directly lexicalized as links; e.g., ?mjesto, na, sunce?",
        "(?place, on, sun?).",
        "The same holds for non-auxiliary verbs linking subjects to objects; e.g., ?drz?ava, kupiti, kolic?ina?",
        "(?state, buy, amount?).",
        "Tuple extraction and scoring.",
        "The overall quality of the DM.HR depends on the accuracy of extracted tuples, which is affected by all preprocessing steps.",
        "We computed the performance of tuple extraction by evaluating a sample of tuples extracted from a parsed version of SETIMES.HR against the tuples extracted from the SETIMES.HR gold annotations (we use the same sample as for tagging and parsing performance evaluation).",
        "Table 2 shows Precision, Recall, and F1 score.",
        "Overall, we achieve the best performance on the Atr links, followed by Pred links.",
        "The performance is generally higher on unlexicalized links than on lexicalized links (note that performance on unlexical",
        "kupiti (to buy) ized Verb links is identical to overall performance on lexicalized verb links).",
        "The overall F1 score of tuple extraction is 74.6%.",
        "Following DM and DM.DE, we score each extracted tuple using Local Mutual Information (LMI) (Evert, 2005): LMI(i, j, k) = f(i, j, k) log P (i, j, k)P (i)P (j)P (k) For a tuple (w1, l, w2), LMI scores the association strength between word w1 and word w2 via link l by comparing their joint distribution against the distribution under the independence assumption, multiplied with the observed frequency f(w1, l, w2) to discount infrequent tuples.",
        "The probabilities are computed from tuple counts as maximum likelihood estimates.",
        "We exclude from the tensor all tuples with a negative LMI score.",
        "Finally, we symmetrize the tensor by introducing inverse links.",
        "Model statistics.",
        "The resulting DM.HR tensor consists of 2.3M lemmas, 121M links and 165K link types (including inverse links).",
        "On average, each lemma has 53 links.",
        "This makes DM.HR more sparse than English DM (796 link types), but less sparse than German DM (220K link types; 22 links per lemma).",
        "Table 3 shows an example of the extracted tuples for the verb kupiti (to buy).",
        "DM.HR tensor is freely available for download.4"
      ]
    },
    {
      "heading": "5 Evaluating DM.HR",
      "text": [
        "Task.",
        "We present a pilot evaluation DM.HR on a standard task from distributional semantics, namely synonym choice.",
        "In contrast to tasks like predicting word similarity We use the dataset created by Karan et al. (2012), with more than 11,000 synonym choice questions.",
        "Each question consists of one target word (nouns, verbs, and adjectives) with",
        "four synonym candidates (one is correct).",
        "The questions were extracted automatically from a machine-readable dictionary of Croatian.",
        "An example item is tez?ak (farmer): poljoprivrednik (farmer), um-jetnost (art), radijacija (radiation), bod (point).",
        "We sampled from the dataset questions for nouns, verbs, and adjectives, with 1000 questions each.5 Additionally, we manually corrected some errors in the dataset, introduced by the automatic extraction procedure.",
        "To make predictions, we compute pairwise cosine similarities of the target word vectors with the four candidates and predict the candidate(s) with maximal similarity (note that there may be ties).",
        "Evaluation.",
        "Our evaluation follows the scheme developed by Mohammad et al. (2007), who define accuracy as the average number of correct predictions per covered question.",
        "Each correct prediction with a single most similar candidate receives a full credit (A), while ties for maximal similarity are discounted (B: two-way tie, C: three-way tie, D: four-way tie): A+ 12B+ 13C+ 14D.",
        "We consider aquestion item to be covered if the target and at least one answer word are modeled.",
        "In our experiments, ties occur when vector similarities are zero for all word pairs (due to vector sparsity).",
        "Note that a random baseline would perform at 0.25 accuracy.",
        "As baseline to compare against the DM.HR, we build a standard bag-of-word model from the same corpus.",
        "It uses a ?5-word within-sentence context window, and the 10,000 most frequent context words (nouns, adjectives, and verbs) as dimensions.",
        "We also compare against BOW-LSA, a state-of-the-art synonym detection model from Karan et al.",
        "(2012), which uses 500 latent dimensions and paragraphs as contexts.",
        "We determine the significance of differences between the models by computing 95% confidence intervals with bootstrap re-sampling (Efron and Tibshirani, 1993).",
        "Results.",
        "Table 4 shows the results for the three",
        "and verbs (V).",
        "The performance of BOW-LSA differs slightly from that reported by Karan et al. (2012), because we evaluate on a sample of their dataset.",
        "DM.HR outperforms the baseline BOW model for nouns and verbs (differences are significant at p < 0.05).",
        "Moreover, on these categories DM.HR performs slightly better than BOW-LSA, but the differences are not statistically significant.",
        "Conversely, on adjectives BOW-LSA performs slightly better than DM.HR, but the difference is again not statistically significant.",
        "All models achieve comparable and almost perfect coverage on this dataset (BOW-LSA achieves complete coverage because of the way how the original dataset was filtered).",
        "Overall, the biggest improvement over the baseline is achieved for nouns.",
        "Nouns occur as heads and dependents of many link types (unlexicalized and lexicalized), and are thus well represented in the semantic space.",
        "On the other hand, adjectives seem to be less well modeled.",
        "Although the majority of adjectives occur as heads or dependents of the Atr relation, for which extraction accuracy is the highest (cf. Table 2), it is likely that a single link type is not sufficient.",
        "As noted by a reviewer, more insight could perhaps be gained by comparing the predictions of BOW-LSA and DM.HR models.",
        "The generally low performance on verbs suggests that their semantic is not fully covered in word-and syntax-based spaces."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We have described the construction of DM.HR, a syntax-based distributional memory for Croatian built from a dependency-parsed web corpus.",
        "To the best of our knowledge, DM.HR is the first freely available distributional memory for a Slavic language.",
        "We have conducted a preliminary evaluation of DM.HR on a synonym choice task, where DM.HR outperformed the bag-of-word model and performed comparable to an LSA model.",
        "This work provides a starting point for a systematic study of dependency-based distributional semantics for Croatian and similar languages.",
        "Our first priority will be to analyze how corpus preprocessing and the choice of link types relates to model performance on different semantic tasks.",
        "Better modeling of adjectives and verbs is also an important topic for future research."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The first author was supported by the Croatian Science Foundation (project 02.03/162: ?Deriva-tional Semantic Models for Information Retrieval?).",
        "We thank the reviewers for their constructive comments.",
        "Special thanks to Hiko Schamoni, Tae-Gil Noh, and Mladen Karan for their assistance."
      ]
    }
  ]
}
