{
  "info": {
    "authors": [
      "Johann-Mattis List",
      "Steven Moran"
    ],
    "book": "ACL",
    "id": "acl-P13-4003",
    "title": "An Open Source Toolkit for Quantitative Historical Linguistics",
    "url": "https://aclweb.org/anthology/P13-4003",
    "year": 2013
  },
  "references": [
    "acl-A00-2038",
    "acl-W09-0303",
    "acl-W12-0216"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Given the increasing interest and development of computational and quantitative methods in historical linguistics, it is important that scholars have a basis for documenting, testing, evaluating, and sharing complex workflows.",
        "We present a novel open-source toolkit for quantitative tasks in historical linguistics that offers these features.",
        "This toolkit also serves as an interface between existing software packages and frequently used data formats, and it provides implementations of new and existing algorithms within a homogeneous framework.",
        "We illustrate the toolkit's functionality with an exemplary workflow that starts with raw language data and ends with automatically calculated phonetic alignments, cognates and borrowings.",
        "We then illustrate evaluation metrics on gold standard datasets that are provided with the toolkit."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Since the turn of the 21st century, there has been an increasing amount of research that applies computational and quantitative approaches to historical-comparative linguistic processes.",
        "Among these are: phonetic alignment algorithms (Kondrak, 2000; Proki?",
        "et al, 2009), statistical tests for genealogical relatedness (Kessler, 2001), methods for phylogenetic reconstruction (Holman et al., 2011; Bouckaert et al., 2012), and automatic detection of cognates (Turchin et al., 2010; Steiner et al., 2011), borrowings (Nelson-Sathi et al., 2011), and proto-forms (Bouchard-C?t?",
        "et al, 2013).",
        "In contrast to traditional approaches to language comparison, quantitative methods are often emphasized as advantageous with regard to objectivity, transparency and replicability of results.",
        "It is striking then that given the multitude of new approaches, very few are publicly available as executable code.",
        "Thus in order to replicate a study, researchers have to rebuild workflows from published descriptions and reimplement their approaches and algorithms.",
        "These challenges make the replication of results difficult, or even impossible, and they hinder not only the evaluation and comparison of existing algorithms, but also the development of new approaches that build on them.",
        "Another problem is that quantitative approaches that have been released as software are largely incompatible with each other and they show great differences in regard to their input and out formats, application range and flexibility.1 Given the breadth of research questions involved in determining language relatedness, this is not surprising.",
        "Furthermore, the linguistic datasets upon which many analyses and tools are based are only ?",
        "if at all ?",
        "available in disparate formats that need manual or semi-automatic re-editing before they can be used as input elsewhere.",
        "Scholars who want to analyze a dataset with different approaches often have to (time-consumingly) convert it into various input formats and they have to familiarize themselves with many different kinds of software.",
        "As a result, errors may occur during data conversion processes and the output from different tools must also be converted into a comparable format.",
        "For the comparison of different output formats or 1There is the STARLING database program for lexicosta-tistical and glottochronological analyses (Starostin, 2000).",
        "TheRug/L04 software aligns sound sequences and calculates phonetic distances using the Levensthein distance (Kleiweg, 2009; Levenshtein, 1966).",
        "The ASJP-Software also computes the Levenshtein distance (Holman et al., 2011), but its results are based on previously executed phonetic analyses.",
        "The ALINE software carries out pairwise alignment analyses (Kondrak, 2000).",
        "There are also software packages from evolutionary biology, which are adapted for linguistic purposes, such as MrBayes (Ronquist and Huelsenbeck, 2003), PHYLIP (Felsenstein, 2005), and SplitsTree (Huson, 1998).",
        "for the evaluation of competing quantitative approaches, gold standard datasets are desirable.",
        "Towards a solution to these problems, we have developed a toolkit that (a) serves as an interface between existing software packages and data formats frequently used in quantitative approaches, (b) provides high-quality implementations of new and existing approaches within a homogeneous framework, and (c) offers a solid basis for testing, documenting, evaluating, and sharing complex workflows in quantitative historical linguistics.",
        "We call this open source toolkit LingPy."
      ]
    },
    {
      "heading": "2 Lingpy",
      "text": [
        "LingPy is written in Python3 and is freely available online.",
        "The Lingpy website contains an API, documentation, tutorials, example scripts, workflows, and datasets that can be used for training, testing, and comparing results from different algorithms.",
        "We use Python because it is flexible and object-oriented, it is easy to write C extensions for scientific computing, and it is approachable to non-programmers (Knight et al., 2007).",
        "Apart from a large number of different functions for common automatic tasks, LingPy offers specific modules for implementing general workflows that are used in historical linguistics and which partially mimic the basic aspects of the traditional comparative method (Trask, 2000, 64-67).",
        "Figure 1 illustrates the interaction between different modules along with the data they produce.",
        "In the following subsections, these modules will be introduced in the order of a typical workflow to illustrate the basic capabilities of the LingPy toolkit in more detail."
      ]
    },
    {
      "heading": "2.1 Input Formats",
      "text": [
        "The basic input format read by LingPy is a tab-delimited text file in which the first line (the header) indicates the values of the columns and all words are listed in the following rows.",
        "The format is very flexible.",
        "No specific order of columns or rows is required.",
        "Any additional data can be specified by the user, as long as it is in a separate column.",
        "Each row represents a word that has to be characterized by a minimum of four values that are given in separate columns: (1) ID, an integer that is used to uniquely identify the word during calculations, (2) CONCEPT, a gloss which indicates the meaning of the word and which is used to align the words semantically, (3) WORD, the orthographic",
        "representation of the word,3 and (4) TAXON, the name of the language (or dialect) inwhich theword occurs.",
        "Basic output formats are essentially the same, the difference being that the results of calculations are added as separate columns.",
        "Table 1 illustrates the basic structure of the input format for a dataset covering 325 concepts translated into 18 Dogon language varieties taken from the Dogon comparative lexical spreadsheet (Heath et al.,"
      ]
    },
    {
      "heading": "2.2 Parsing and Unicode Handling",
      "text": [
        "Given a dataset in the basic LingPy input format, the first step towards sound-based normalization for automatically identifying cognates and sound changes with quantitative methods is to parse words into tokens.",
        "Orthographic tokenization is a non-trivial task, but it is needed to at"
      ]
    },
    {
      "heading": "ID CONCEPT WORD TAXON",
      "text": [
        "tain interoperability across different orthographies or transcription systems and to enable the comparative analysis of languages.",
        "LingPy includes a parser that takes as input a dataset and an optional orthography profile, i.e. a description of the Unicode code points, characters, graphemes and orthographic rules that are needed to adequately model a writing system for a language variety as described in a particular document (Moran, 2012, 331).",
        "The LingPy parser first normalizes all strings into UnicodeNormalization FormD,which decomposes all character sequences and reorders them into one canonical order.",
        "This step is necessary because sequences of Unicode characters may differ in their visual and logical orders.",
        "Next, if no orthography profile is specified, the parser will use a regular expression match \\X for Unicode grapheme clusters, i.e. combining character sequences typified by a base character followed by one or more Combing Diacritical Marks.",
        "However, another layer of tokenization is usually required to match linguistic graphemes, or what Unicode calls ?tailored grapheme clusters?.",
        "Table 2 illustrates the different technological and linguistic levels involved in orthographic parsing.5 code points t s h o ?",
        "?",
        "?",
        "?",
        "?",
        "?",
        "s h i ?characters?",
        "t s h o???",
        "s h i graphemes tsh o???",
        "sh i Table 2: Tokens for the string <tsh??",
        "?shi> So, given the dataset illustrated in Table 1 and an orthography profile that defines the phonemic units in the Dogon comparative lexicon, the 5Note that even when a linguist transcribes a word with the International Phonetic Alphabet (IPA; a transcription system with one-to-one symbol-to-sound correspondences), explicit definitions for phonemes are needed because some IPA diacritics are encoded as Unicode Spacing Modifier Letters, i.e. characters that are not specified as how they combine with a base character, such as aspiration.",
        "LingPy parser produces the IPA tokenized output shown in Table 3."
      ]
    },
    {
      "heading": "2.3 Phonetic Alignments",
      "text": [
        "Although less common in traditional historical linguistics, phonetic alignment plays a crucial role in automatic approaches, with alignment analyses being currently used in many different subfields, such as dialectology (Proki?",
        "et al, 2009), phylogenetic reconstruction (Holman et al., 2011) and cognate detection (List, 2012a).",
        "Furthermore, alignment analyses are very useful for data visualization, since they directly show which sound segments correspond in cognate words.",
        "LingPy offers implementations for many different approaches to pairwise and multiple phonetic alignment.",
        "Among these, there are standard approaches that are directly taken from evolutionary biology and can be applied to linguistic data with only slight modifications, such as the Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) and the Smith-Waterman algorithm (Smith and Waterman, 1981).",
        "Furthermore, there are novel approaches that use more complex sequence models in order to meet linguistic-specific requirements, such as the Sound-Class-based phonetic Alignment (SCA) method (List, 2012b).",
        "Figure 2 shows a plot of the multiple alignment of the counterparts of the concept ?stool?",
        "in eight Dogon languages.",
        "The color scheme for the sound segments follows the sound class distinction of Dolgopolsky (1964)."
      ]
    },
    {
      "heading": "2.4 Automatic Cognate Detection",
      "text": [
        "The identification of cognates plays an important role in both traditional and quantitative approaches in historical linguistics.",
        "Most quantitative approaches dealing with phylogenetic reconstruction are based on previously identified cognate sets distributed over the languages being in",
        "vestigated (Bouckaert et al., 2012; Bouchard-C?t?",
        "et al, 2013).",
        "Since the traditional approach to cognate detection within the framework of the comparative method is very time-consuming and difficult to evaluate for the non-expert, automatic approaches to cognate detection can play an important role in objectifying phylogenetic reconstructions.",
        "Currently, LingPy offers four alternative approaches to cognate detection in multilingual wordlists.",
        "Themethod by Turchin et al. (2010) employs sound classes as proposed by Dolgopolsky (1964) and assigns words that match in their first two consonant classes to the same cognate set.",
        "The NED method calculates the normalized edit distance between words and groups them into cognate sets using a flat cluster algorithm.",
        "The SCA and the LexStat methods (List, 2012a) use the same strategy for clustering, but the distances for the SCA method are calculated with help of the SCA alignment method (List, 2012b), and the distances for the LexStat method are derived from previously identified regular sound correspondences.",
        "Table 4 shows a small section of the results from the LexStat analysis of the Dogon data.",
        "As shown, LingPy follows the STARLING approach in displaying cognate judgments by assigning cognate words the same cognate ID (COGID).",
        "In Table 4, the words judged to be cognate are shaded in the same color.",
        "The full results are posted on the LingPy website."
      ]
    },
    {
      "heading": "2.5 Automatic Borrowing Detection",
      "text": [
        "Automatic approaches for borrowing detection are still in their infancy in historical linguistics.",
        "LingPy provides a full reimplementation (along with specifically linguistic modifications) of the minimal lateral network (MLN) approach (Nelson-Sathi et al., 2011).",
        "This approach searches for cognate sets which are not compatible with a given ref-6The normalized edit distance is calculated by dividing the edit distance (Levenshtein, 1966) by the length of the smaller sequence, see Holman et al. (2011) for details.",
        "erence tree topology.",
        "Incompatible (patchy) cognate sets often point to either borrowings or wrong cognate assessments in the data.",
        "The results can be visualized by connecting all taxa of the reference tree for which patchy cognate sets can be inferred with lateral links.",
        "In Figure 3, the method has been applied again to the Dogon dataset.",
        "Cognate judgments for this analysis were carried out with help of LingPy's LexStat method.",
        "The tree topology was calculated using MrBayes."
      ]
    },
    {
      "heading": "2.6 Output Formats",
      "text": [
        "The output formats supported by LingPy can be divided into three different classes.",
        "The first class consists of text-based formats that can be used for manual correction and inspection by importing the data into spreadsheet programs, or simply editing and reviewing the results in a text editor.",
        "The second class consists of specific formats for third-party toolkits, such as PHYLIP, SplitsTree, MrBayes, or STARLING.",
        "LingPy currently offers support for PHYLIP's distance calculations (DST-format), for tree-representation (Newick-format), for complex representations of character data (Nexus-format), and for the import into STARLING databases (CSV with STARLING markup).",
        "The third class consists of new approaches to the visualization of phonetic alignments, cognate sets, and phylogenetic networks.",
        "In fact, all plots in this paper were created with LingPy's output formats."
      ]
    },
    {
      "heading": "3 Evaluation",
      "text": [
        "In order to improve the performance of quantitative approaches, it is of crucial importance to test and evaluate them.",
        "Evaluation is usually done by comparing how well a given approach performs on a reference dataset, i.e. a gold standard, where the results of the analysis are known in advance.",
        "LingPy comes with a module for the evaluation of",
        "basic tasks in historical linguistics, such as phonetic alignment and cognate detection.",
        "This module offers both common evaluation measures that are used to assess the accuracy of the respective methods and gold standard datasets encoded in the LingPy input format.",
        "In Figure 4, the performance of the four above-mentioned approaches to automatic cognate detection are compared with the gold standard cognate judgments of a dataset covering 207 concepts translated into 20 Indo-European languages taken from the Indo-European Lexical Cognacy (IELex) database (Bouckaert et al., 2012).7 The pair scores, implemented in LingPy after the description in Bouchard-C?t?",
        "et al (2013), were used as an evaluation measure.",
        "For all approaches we chose the respective thresholds that tend to yield the best results on all of the gold standards.",
        "As shown in Figure, both the SCA and LexStat methods show a higher accuracy than the Turchin and NED methods, with LexStat slightly outperforming SCA.",
        "However, the generally bad performance 7Gold standard here means that the cognate judgments were carried out manually by the compilers of the IELex database.",
        "of all approaches on this dataset shows that there is a clear need for improving automatic cognate detection approaches, especially in cases of remote relationship, such as Indo-European."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "Quantitative approaches in historical linguistics are still in their infancy, far away from being able to compete with the intuition of trained historical",
        "linguists.",
        "The toolkit we presented is a first attempt to close the gap between quantitative and traditional methods by providing a homogeneous framework that serves as an interface between existing packages and at the same time provides high-quality implementations of new approaches."
      ]
    }
  ]
}
