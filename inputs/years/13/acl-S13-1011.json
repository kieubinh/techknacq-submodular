{
  "info": {
    "authors": [
      "Tamara Polajnar",
      "Laura Rimell",
      "Douwe Kiela"
    ],
    "book": "*SEM",
    "id": "acl-S13-1011",
    "title": "UCAM-CORE: Incorporating structured distributional similarity into STS",
    "url": "https://aclweb.org/anthology/S13-1011",
    "year": 2013
  },
  "references": [
    "acl-P03-1054",
    "acl-S12-1051",
    "acl-S12-1059",
    "acl-S12-1088",
    "acl-W11-1302"
  ],
  "sections": [
    {
      "text": [
        "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 85?89, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics UCAM-CORE: Incorporating structured distributional similarity into STS"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "This paper describes methods that were submitted as part of the *SEM shared task on Semantic Textual Similarity.",
        "Multiple kernels provide different views of syntactic structure, from both tree and dependency parses.",
        "The kernels are then combined with simple lexical features using Gaussian process regression, which is trained on different subsets of training data for each run.",
        "We found that the simplest combination has the highest consistency across the different data sets, while introduction of more training data and models requires training and test data with matching qualities."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The Semantic Textual Similarity (STS) shared task consists of several data sets of paired passages of text.",
        "The aim is to predict the similarity that human annotators have assigned to these aligned pairs.",
        "Text length and grammatical quality vary between the data sets, so our submissions to the task aimed to investigate whether models that incorporate syntactic structure in similarity calculation can be consistently applied to diverse and noisy data.",
        "We model the problem as a combination of kernels (Shawe-Taylor and Cristianini, 2004), each of which calculates similarity based on a different view of the text.",
        "State-of-the-art results on text classification have been achieved with kernel-based classification algorithms, such as the support vector machine (SVM) (Joachims, 1998), and the methods here can be adapted for use in multiple kernel classification, as in Polajnar et al. (2011).",
        "The kernels are combined using Gaussian process regression (GPR) (Rasmussen and Williams, 2006).",
        "It is important to note that the combination strategy described here is only a different way of viewing the regression-combined mixture of similarity measures approach that is already popular in STS systems, including several that participated in previous SemEval tasks (Croce et al., 2012; Ba?r et al., 2012).",
        "Likewise, others, such as Croce et al. (2012), have used tree and dependency parse information as part of their systems; however, we use a tree kernel approach based on a novel encoding method introduced by Zanzotto et al. (2011) and from there derive two dependency-based methods.",
        "In the rest of this paper we will describe our system, which consists of distributional similarity (Section 2.1), several kernel measures (Section 2.2), and a combination method (Section 2.3).",
        "This will be followed by the description of our three submissions (Section 3), and a discussion of the results (Section 4)."
      ]
    },
    {
      "heading": "2 Methods",
      "text": [
        "At the core of all the kernel methods is either surface, distributional, or syntactic similarity between sentence constituents.",
        "The methods themselves encode sentences into vectors or sets of vectors, while the similarity between any two vectors is calculated using cosine."
      ]
    },
    {
      "heading": "2.1 Distributional Similarity",
      "text": [
        "Target words are the non-stopwords that occur within our training and test data.",
        "The two distributional methods we use here both represent target",
        "words as vectors that encode word occurrence within a set of contexts.",
        "The first method is a variation on BEAGLE (Jones and Mewhort, 2007), which considers contexts to be words that surround targets.",
        "The second method is based on ESA (Gabrilovich and Markovitch, 2007), which considers contexts to be Wikipedia documents that contain target words.",
        "To gather the distributional data with both of these approaches we used 316,305 documents from the September 2012 snapshot of Wikipedia.",
        "The training corpus for BEAGLE is generated by pooling the top 20 documents retrieved by querying the Wikipedia snapshot index for each target word in the training and test data sets."
      ]
    },
    {
      "heading": "2.1.1 BEAGLE",
      "text": [
        "Random indexing (Kaski, 1998) is a technique for dimensionality reduction where pseudo-orthogonal bases are generated by randomly sampling a distribution.",
        "BEAGLE is a model where random indexing is used to represent word co-occurrence vectors in a distributional model.",
        "Each context word is represented as a D-dimensional vector of normally distributed random values drawn from the Gaussian distribution N (0, ?2), where ?",
        "=",
        "A target word is represented as the sum of the vectors of all the context words that occur within a certain context window around the target word.",
        "In BEAGLE this window is considered to be the sentence in which the target word occurs; however, to avoid segmenting the entire corpus, we assume the window to include 5 words to either side of the target.",
        "This method has the advantage of keeping the dimensionality of the context space constant even if more context words are added, but we limit the context words to the top 10,000 most frequent non-stopwords in the corpus."
      ]
    },
    {
      "heading": "2.1.2 ESA",
      "text": [
        "ESA represents a target word as a weighted ranked list of the top N documents that contain the word, retrieved from a high quality collection.",
        "We used the BM25F (Robertson et al., 2004) weighting function and the topN = 700 documents.",
        "These parameters were chosen by testing on the WordSim353 dataset.",
        "The list of retrieved documents can be represented as a very sparse vector whose dimensions match the number of documents in the collection, or in a more computationally efficient manner as a hash map linking document identifiers to the retrieval weights.",
        "Similarity between lists was calculated using the cosine measure augmented to work on the hash map data type."
      ]
    },
    {
      "heading": "2.2 Kernel Measures",
      "text": [
        "In our experiments we use six basic kernel types, which are described below.",
        "Effectively we have eight kernels, because we also use the tree and dependency kernels with and without distributional information.",
        "Each kernel is a function which is passed a pair of short texts, which it then encodes into a specific format and compares using a defined similarity function.",
        "LK uses the regular cosine similarity function, but LEK, TK, DK, MDK, DGK use the following cosine similarity redefined for sets of vectors.",
        "If the texts are represented as sets of vectors X and Y , the set similarity kernel function is:",
        "and normalisation is accomplished in the standard way for kernels by:",
        "tween the tokens that occur in each of the paired texts, where the tokens consist of Porter stemmed (Porter, 1980) non-stopwords.",
        "Each text is represented as a frequency vector of tokens that occur within it and the similarity between the pair is calculated using cosine.",
        "LEK - The lexical ESA kernel represents each example in the pair as the set of words that do not occur in the intersection of the two texts.",
        "The similarity is calculated as in Equation (3) with X and Y being the ESA vectors of each word from the first and second text representations, respectively.",
        "TK - The tree kernel representation is based on the definition by Zanzotto et al. (2011).",
        "Briefly,",
        "each piece of text is parsed2; the non-terminal nodes of the parse tree, stopwords, and out-of-dictionary terms are all assigned a new random vector (Equation 1); while the leaves that occurred in the BEAGLE training corpus are assigned their learned distributional vectors (Section 2.1.1).",
        "Each subtree of a tree is encoded recursively as a vector, where the distributional vectors representing each node are combined using the circular convolution operator (Plate, 1994; Jones and Mewhort, 2007).",
        "The whole tree is represented as a set of vectors, one for each subtree.",
        "DK - The dependency kernel representation encodes each dependency pair as a separate vector, discounting the labels.",
        "The non-stopword terminals are represented as their distributional vectors, while the stopwords and out-of-dictionary terms are given a unique random vector.",
        "The vector for the dependency pair is obtained via a circular convolution of the individual word vectors.",
        "MDK - The multiple dependency kernel is constructed like the dependency kernel, but similarity is calculated separately between all the the pairs that share the same dependency label.",
        "The combined similarity for all dependency labels in the parse is then calculated using least squares linear regression.",
        "While at the later stage we use GPR to combine all of the different kernels, for MDK we found that linear regression provided better performance.",
        "DGK - The depgram kernel represents each dependency pair as an ESA vector obtained by searching the ESA collection for the two words in the dependency pair joined by the AND operator.",
        "The DGK representation only contains the dependencies that occur in one similarity text or the other, but not in both."
      ]
    },
    {
      "heading": "2.3 Regression",
      "text": [
        "Each of the kernel measures above is used to calculate a similarity score between a pair of texts.",
        "The different similarity scores are then combined using",
        "grammatical sentences, we had to approximate some parses.",
        "The parsing was done using the Stanford parser (Klein and Manning, 2003), which failed on some overly long sentences, which we therefore segmented at conjunctions or commas.",
        "Since our methods only compared subtrees of parses, we simply took the union of all the partial parses for a given sentence.",
        "Gaussian process regression (GPR) (Rasmussen and Williams, 2006).",
        "GPR is a probabilistic regression method where the weights are modelled as Gaussian random variables.",
        "GPR is defined by a covariance function, which is akin to the kernel function in the support vector machine.",
        "We used the squared exponential isotropic covariance function (also known as the radial basis function):",
        "found that training for parameters increased overfit-ting and produced worse results in validation experiments."
      ]
    },
    {
      "heading": "3 Submitted Runs",
      "text": [
        "We submitted three runs.",
        "This is not sufficient for a full evaluation of the new methods we proposed here, but it gives us an inkling of general trends.",
        "To choose the composition of the submissions, we used STS 2012 training data for training, and STS 2012 test data for validation (Agirre et al., 2012).",
        "The final submitted runs also used some of the STS 2012 test data for training.",
        "Basic - With this run we were examining if a simple introduction of syntactic structure can improve over the baseline performance.",
        "We trained a GPR combination of the linear and tree kernels (LK-TK) on the MSRpar training data.",
        "In validation experiments we found that this data set in general gave the most consistent performance for regression training.",
        "Custom - Here we tried to approximate the best training setup for each type of data.",
        "We only had training data for OnWN and for this dataset we were able to improve over the LK-TK setup; however, the settings for the rest of the data sets were guesses based on observations from the validation experiments and overall performed poorly.",
        "OnWN was trained on MSRpar train with LK and DK.",
        "The headlines model was trained on MSRpar train and Europarl test, with LK-LEK-TK-DK-TKND-DKNDMDK (trained on Europarl).3 FNWN was trained on MSRpar train and OnWN test with LK-LEK-DGKTK-DK-TKND-DKND.",
        "Finally, the SMT model",
        "All - As in the LK-TK experiment, we used the same model on all of the data sets.",
        "It was trained on all of the training data except MSRvid, using all eight kernel types defined above.",
        "In summary we used the LK-LEK-TK-TKND-DK-DKNDMDK-DGK kernel combination.",
        "MDK was trained on the 2012 training portion of MSRpar."
      ]
    },
    {
      "heading": "4 Discussion",
      "text": [
        "From the shared task results in Table 1, we can see that Basic is our highest ranked run.",
        "It has also achieved the best performance on all data sets.",
        "The LK on its own improves slightly on the task baseline by removing stop words and using stemming, while the introduction of TK contributes syntactic and distributional information.",
        "With the Custom run, we were trying to manually estimate which training data would best reflect properties of particular test data, and to customise the kernel combination through validation experiments.",
        "The only data set for which this led to an improvement is OnWN, indicating that customised settings can be beneficial, but that a more scientific method for matching of training and test data properties is required.",
        "In the All run, we were examining the effects that maximising the amount of training data and the number of kernel",
        "gold standard measures has on the output predictions.",
        "The results show that swamping the regression with models and training data leads to overly normalised output and a decrease in performance.",
        "While the evaluation measure, Pearson correlation, does not take into account the shape of the output distribution, Figure 1 shows that this information may be a useful indicator of model quality and behaviour.",
        "In particular, the role of the regression component in our approach is to learn a transformation from the output distributions of the models to the distribution of the training data gold standard.",
        "This makes it sensitive to the choice of training data, which ideally would have similar characteristics to the individual kernels, as well as a similar gold standard distribution to the test data.",
        "We can see in Figure 1 that the training data and choice of kernels influence the output distribution.",
        "Analysis of the minimum, first quartile, median, third quartile, and maximum statistics of the distributions in Figure 1 demonstrates that, while it is difficult to visually evaluate the similarities of the different distributions, the smallest squared error is between the gold standard and the Custom run.",
        "This suggests that properties other than the rank order may also be good indicators in training and testing of STS methods."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": []
    }
  ]
}
