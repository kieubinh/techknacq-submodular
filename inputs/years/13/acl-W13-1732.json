{
  "info": {
    "authors": [
      "Shibamouli Lahiri",
      "Rada Mihalcea"
    ],
    "book": "BEA",
    "id": "acl-W13-1732",
    "title": "Using N-gram and Word Network Features for Native Language Identification",
    "url": "https://aclweb.org/anthology/W13-1732",
    "year": 2013
  },
  "references": [
    "acl-C04-1139",
    "acl-D11-1148",
    "acl-D12-1064",
    "acl-N01-1031",
    "acl-N12-1033",
    "acl-P12-2038",
    "acl-W04-3252",
    "acl-W13-1706"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We report on the performance of two different feature sets in the Native Language Identification Shared Task (Tetreault et al., 2013).",
        "Our feature sets were inspired by existing literature on native language identification and word networks.",
        "Experiments show that word networks have competitive performance against the baseline feature set, which is a promising result.",
        "We also present a discussion of feature analysis based on information gain, and an overview on the performance of different word network features in the Native Language Identification task."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Native Language Identification (NLI) is a well-established problem in NLP, where the goal is to identify a writer's native language (L1) from his/her writing in a second language (L2), usually English.",
        "NLI is generally framed as a multi-class classification problem (Koppel et al., 2005; Brooke and Hirst, 2011; Wong and Dras, 2011), where native languages (L1) are considered class labels, and writing samples in L2 are used as training and test data.",
        "The NLI problem has recently seen a big surge in interest, sparked in part by three influential early papers on this problem (Tomokiyo and Jones, 2001; van Halteren and Oostdijk, 2004; Koppel et al., 2005).",
        "Apart from shedding light on the way non-native learners (also called ?L2 learners?)",
        "learn a new language, the NLI task allows constrastive analysis (Wong and Dras, 2009), study of different types of errors that people make while learning a new language (Kochmar, 2011; Bestgen et al., 2012; Jarvis et al., 2012), and identification of language transfer patterns (Brooke and Hirst, 2012a; Jarvis and Crossley, 2012), thereby helping L2-students improve their writing styles and expediting the learning process.",
        "It also helps L2 educators to concentrate their efforts on particular areas of a language that cause the most learning difficulty for different L1s.",
        "The NLI task is closely related to traditional NLP problems of authorship attribution (Juola, 2006; Sta-matatos, 2009; Koppel et al., 2009) and author profiling (Kes?elj et al., 2003; Estival et al., 2007a; Estival et al., 2007b; Bergsma et al., 2012), and shares many of the same features.",
        "Like authorship attribution, NLI is greatly benefitted by having function words and character n-grams as features (Brooke and Hirst, 2011; Brooke and Hirst, 2012b).",
        "Native languages form a part of an author's sociocultural and psychological profiles, thereby being related to author profiling (van Halteren and Oostdijk, 2004; Torney et al., 2012).",
        "Researchers have used different types of features for the NLI problem, including but not limited to function words (Brooke and Hirst, 2012b); character, word and POS n-grams (Brooke and Hirst, 2012b); spelling and syntactic errors (Koppel et al., 2005); CFG productions (Brooke and Hirst, 2012b); Tree Substitution Grammar productions (Swanson and Charniak, 2012); dependencies (Brooke and Hirst, 2012b); Adaptor Grammar features (Wong et al., 2012); L1-influence (Brooke and Hirst, 2012a); stylometric features (Golcher and Reznicek, 2011;",
        "Crossley and McNamara, 2012; Jarvis et al., 2012); recurrent n-grams on words and POS (Bykh and Meurers, 2012); and features derived from topic models (Wong et al., 2011).",
        "State-of-the-art results are typically in the 80%-90% range, with results above 90% reported in some cases (Brooke and Hirst, 2012b).",
        "Note, however, that results vary greatly across different datasets, depending on the number of languages being considered, size and difficulty of data, etc."
      ]
    },
    {
      "heading": "2 Our Approach",
      "text": [
        "The NLI 2013 Shared Task (Tetreault et al., 2013) marks an effort in bringing together the NLI research community to share and compare their results and evaluations on a common dataset - TOEFL11 (Blanchard et al., 2013) - consisting of 12,100 unique English essays written by non-native learners of eleven different languages.",
        "The dataset has 9,900 essays for training, 1,100 essays for test, and 1,100 essays for development.",
        "Each of the three sets is balanced across different L1s.",
        "Inspired by previous work in NLI, in our different NLI systems submissions we used several different types of character, word, and POS n-gram features (cf.",
        "Section 2.1).",
        "Although not included in the systems submitted, we also experimented with a family of new features derived from a word network representation of natural language text (cf.",
        "Section 2.2).",
        "We used Weka (Hall et al., 2009) for all our classification experiments.",
        "The systems that were submitted gave best 10-fold cross-validation accuracy on training data among different feature-classifier combinations (Section 3).",
        "Word network features - although competitive against the baseline n-gram features - were not able to beat the baseline features on the training set, so we did not submit that system for evaluation.",
        "Section 2.1 discusses our n-gram features, followed by a discussion of word network features in Section 2.2."
      ]
    },
    {
      "heading": "2.1 N-gram Features",
      "text": [
        "We used several baseline n-gram features based on words, characters, and POS.",
        "We experimented with the raw frequency, normalized frequency, and binary",
        "1Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu and Turkish.",
        "presence/absence indicator on top 100, 200, 500 and 1000 n-grams:2 1. word n-grams (n = 1, 2, 3), with and without punctuation.",
        "2. character n-grams (n = 1, 2, 3), with and without space characters.",
        "3.",
        "POS n-grams (n = 1, 2, 3), with and without",
        "We experimented with punctuation because previous research indicates that punctuation is helpful (Wong and Dras, 2009; Kochmar, 2011).",
        "In total, there are 216 types of n-gram feature vectors (with dimensions 100, 200, 500 and 1000) for a particular document.",
        "Because of size restrictions (e.g., some n-gram dictionaries are smaller than the specified feature vector dimensions), we ended up with 168 types of feature vectors per document (cf.",
        "Tables 2 to 4)."
      ]
    },
    {
      "heading": "2.2 Word Networks",
      "text": [
        "A ?word network?",
        "of a particular document is a network (graph) of unique words found in that document.",
        "Each node (vertex) in this network is a word.",
        "Edges between two nodes (unique words) can be constructed in several different ways.",
        "The simplest type of edge connects word A to word B, if word A is followed by word B in the document at least once.",
        "In our work, we have assumed a directed edge with direction from word A to word B.",
        "Note that we could have used undirected edges as well (cf. (Mihalcea and Tarau, 2004)).",
        "Moreover, edges can be weighted/unweighted.",
        "We assumed unweighted edges.",
        "A deeper issue with this network construction process concerns what we should do with stopwords.",
        "Should we keep them, or should we remove them?",
        "Since stopwords and function words have proved to be of special importance in previous native language identification studies (Wong and Dras, 2009; Brooke and Hirst, 2012b), we chose to keep them in our",
        "fox jumped over the lazy dog?.",
        "and word co-occurrence.",
        "Word networks can be constructed either by respecting sentence boundaries (where the last word of sentence 1 does not link to the first word of sentence 2), or by disregarding them.",
        "In our case, we disregarded all sentence boundaries.",
        "Moreover, a network edge can either link two words that appeared side-by-side in the original document, or it can link two words that appeared within a window of n words in the document (cf. (Mihalcea and Tarau, 2004)).",
        "In our case, we chose the first option - linking unique words that appeared side-by-side at least once.",
        "Finally, we did not perform any stemming/morphological analysis to retain subtle cues that might be revealed from inflected/derived words.",
        "The word network of an example sentence (?the quick brown fox jumped over the lazy dog?)",
        "is shown in Figure 1.",
        "Note that the word ?the?",
        "appeared twice in this sentence, so the corresponding network contains a cycle that starts at ?the?",
        "and ends at ?the?.",
        "In a realistic word network of a large document, there can be many such cycles.",
        "In addition, it is observed that such word networks show power-law degree distribution and a small-world structure (i Cancho and Sole?, 2001; Matsuo et al., 2001).",
        "Once the word networks have been constructed, we extract a set of simple features from these networks4 that represent local properties of individual nodes.",
        "We have extracted ten local features for each",
        "node in a word network: 1. in-degree, out-degree and degree 2. in-coreness, out-coreness and coreness5 3. in-neighborhood size (order 1), out-neighborhood size (order 1) and neighborhood size (order 1) 4. local clustering coefficient",
        "We take a set of representative words, and convert a document into a local feature vector - each local feature pertaining to one word in the set of representative words.",
        "For example, when we use the top 200 most frequent words as the representative set, a document can be represented as the degree vector of these 200 words in the document's word network, or as the local clustering coefficient vector of these words in the word network, or as the coreness vector of the words (and so on).",
        "A document can also be represented as a concatenation (mixture) of these vectors.",
        "For example, it can be represented as concat(degree vector, coreness vector) of top 200 most frequent words.",
        "We are yet to explore how such mixed feature sets perform in the NLI task, and this constitutes a part of our future work (Section 4).",
        "We experimented with top k most frequent words (with k = 100, 200, 500, 1000) on train-ing+development data as our representative word-set."
      ]
    },
    {
      "heading": "3 Results",
      "text": [
        "Table 1 describes the three systems we submitted.",
        "The first two systems (UNT-closed-1.csv and UNT-closed-2.csv) were based on a bag of words model using all the words from the training set.",
        "The systems used a home-grown implementation of the Na?",
        "?ve Bayes classifier, and achieved 10-fold cross-validation accuracy of 64.5% and 65.1% respectively, on the training set.",
        "The first system used raw",
        "term frequency of all words including stopwords as features, and the second system used raw term frequency of all words except stopwords.",
        "These two systems achieved test set accuracy of 63.2% and 63.7%, respectively.",
        "The third system we submitted (UNT-closed3.csv) was based on n-gram features (cf.",
        "Section 2.1).",
        "We used the raw frequency of top 1000 word unigrams, including punctuation, as features.",
        "The Weka SMO implementation of SVM (Hall et al., 2009) was used as classifier with default parameter settings.",
        "This system gave us the best 10-fold cross-validation accuracy of 62.46% in the training set, among all n-gram features.",
        "Note that this system was also the top performer among the systems we submitted in NLI evaluation, with a test set accuracy of 64.5%, and a 10-fold CV accuracy of 63.77% on the training+development set folds specified by the organizers.",
        "We will now describe in the following two subsections how our n-gram features and word network features performed on the training set.",
        "All results reported here reflect best 10-fold cross-validation accuracy in the training set among different classifiers (SVM, Na?",
        "?ve Bayes, 1-nearest-neighbor (1NN), J48 decision tree, and AdaBoost).",
        "SVM and Na?",
        "?ve Bayes gave best results in our experiments, so only these two are shown in Tables 2 to 5."
      ]
    },
    {
      "heading": "3.1 Performance of N-gram Features",
      "text": [
        "Recall from Section 2.1 that we extracted 168 different n-gram feature vectors corresponding to the raw frequency, normalized frequency, and binary presence/absence indicator of top k n-grams (with k = 100, 200, 500, 1000) in the training+development set.",
        "Performance of these n-gram features is given in Tables 2 to 4.",
        "A general observation with Tables 2 to 4 is that cross-validation performance improves as k increases, although there are a few exceptions.",
        "We marked those exceptions with an asterisk (?*?).",
        "It is interesting to note that top k word unigrams with punctuation were the top performers in most of the cases.",
        "Also interesting is the fact that SVM mostly gave best performance on n-gram features among different classifiers.",
        "Note that Na?",
        "?ve Bayes was best performer in a few cases (Table 4).",
        "Performance of raw and normalized frequency features were mostly comparable (Tables 2 and 3), whereas binary presence/absence indicator achieved worse accuracy values in general than raw and normalized frequency features (Table 4).",
        "Among different n-grams, word unigrams performed better than bigrams and trigrams, POS bi-grams performed better than POS trigrams, and character bigrams and character trigrams performed comparably well (Tables 2 and 3).",
        "Exceptions to this observation are seen in Table 4, where character trigrams performed better than character bigrams, and word bigrams sometimes performed better than word unigrams.",
        "In general, word n-grams performed the best, followed by POS and character n-grams."
      ]
    },
    {
      "heading": "3.2 Performance of Word Network Features",
      "text": [
        "Word networks and word network features were described in Section 2.2.",
        "We extracted ten local features on four different representative sets of words - the top k most frequent words (k = 100, 200, 500, 1000) on the training+development set, respectively.",
        "Performance of these features is given in Table 5.",
        "Note that in general, word network features per",
        "training set are shown, along with the classifiers that achieved these accuracy values.",
        "Best results in different columns are boldfaced.",
        "Rank Word Network Feature Information Gain 1 Degree of the word a 0.1058 2 Neighborhood size of the word a 0.1054 3 Out-neighborhood size of the word a 0.1050 4 Outdegree of the word a 0.1049 5 In-neighborhood size of the word a 0.1017 6 Indegree of the word a 0.1016 7 Neighborhood size of the word however 0.0928 8 Degree of the word however 0.0928 9 Indegree of the word however 0.0928 10 In-neighborhood size of the word however 0.0928 11 Outdegree of the word however 0.0916 12 Out-neighborhood size of the word however 0.0916 13 Out-coreness of the word however 0.0851 14 Coreness of the word however 0.0851 15 In-coreness of the word however 0.0850 16 Outdegree of the word the 0.0793 17 Out-neighborhood size of the word the 0.0790 18 Degree of the word the 0.0740 19 Neighborhood size of the word the 0.0740 20 Coreness of the word a 0.0710",
        "frequent words on the training+development set, and collected all their word network features in a single file.",
        "This ranking reflects the top 20 features in that file, along with their information gain values.",
        "formed quite well, with the best result (60.41% CV accuracy on the train set) being competitive against (but slightly worse than) the baseline n-gram features (62.46% CV accuracy on the train set).",
        "Performance improved with increasing k, thereby corroborating our general observation from Tables 2 to 4.",
        "Clustering coefficient performed poorly, and seems rather unsuitable for the NLI task.",
        "But degree, coreness, and neighborhood size performed good.",
        "Here also, SVM turned out to be the best classifier, giving best CV accuracy in all cases.",
        "We experimented with the in-, out-, and overall versions of degree, coreness and neighborhood size.",
        "Their performance was mostly comparable with each other (Table 5).",
        "To investigate which word network features are the most discriminatory in this task, we collected all ten word network features of the top 1000 words in a single file, and then ranked those features on the training set based on Information Gain (IG).",
        "The 20 top-ranking features are shown in Table 6, along with their corresponding IG values.",
        "Note that the words a, the, and however were among the most discriminatory, and different versions of degree, neighborhood size and coreness appeared among the top, which is in line with our earlier observation that clustering coefficients were not very discriminatory at the native language classification task."
      ]
    },
    {
      "heading": "4 Conclusions and Future Work",
      "text": [
        "In this paper, we described experiments with the NLI task using a baseline set of n-gram features, and a set of novel features derived from a word network representation of text documents.",
        "Useful and less useful n-gram features were identified, along with the fact that SVM was the best classifier in most of the cases.",
        "We learned that when using raw or normalized frequency, lower-order n-grams perform at least as good as higher-order n-grams; moreover, Na?",
        "?ve Bayes sometimes give good results when binary presence/absence indicator variables are used as features.",
        "We described the construction of our word networks in detail, and discussed experiments with word network features.",
        "These features are competitive against the baseline n-gram features, and we need to fine-tune our classifiers to see if they can exceed the performance of the baseline.",
        "Clustering coefficients were found to be less useful for the NLI task, and feature ranking based on information",
        "gain helped us identify the most important word network features in a collection of top 1000 words in the training+development set.",
        "Future work consists of experimenting with combined word network features; mixed word network features and baseline n-gram features; and the one-vs-all classification scheme instead of the multiclass classification scheme."
      ]
    }
  ]
}
