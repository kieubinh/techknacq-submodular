{
  "info": {
    "authors": [
      "Stéphane Huet",
      "Elena Manishina",
      "Fabrice Lefèvre"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W13-2218",
    "title": "Factored Machine Translation Systems for Russian-English",
    "url": "https://aclweb.org/anthology/W13-2218",
    "year": 2013
  },
  "references": [
    "acl-D07-1091",
    "acl-J93-2004",
    "acl-L08-1539",
    "acl-P07-2045",
    "acl-W08-0509",
    "acl-W12-3130"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe the LIA machine translation systems for the Russian-English and English-Russian translation tasks.",
        "Various factored translation systems were built using MOSES to take into account the morphological complexity of Russian and we experimented with the romanization of untranslated Russian words."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "This paper presents the factored phrase-based Machine Translation (MT) systems (Koehn and Hoang, 2007) developed at LIA, for the Russian-English and English-Russian translation tasks at WMT?13.",
        "These systems use only data provided for the evaluation campaign along with the LDC English Gigaword corpus.",
        "We summarize in Section 2 the resources used and the main characteristics of the systems based on the MOSES toolkit (Koehn et al., 2007).",
        "Section 3 reports experiments on the use of factored translation models.",
        "Section 4 describes the transliteration process used to improve the Russian to English task.",
        "Finally, we conclude in Section 5."
      ]
    },
    {
      "heading": "2 System Architecture",
      "text": []
    },
    {
      "heading": "2.1 Pre-processing",
      "text": [
        "The corpora available for the workshop were preprocessed using an in-house script that normalizes quotes, dashes, spaces and ligatures.",
        "Long sentences or sentences with many numeric or non-alphanumeric characters were also discarded.",
        "Since the Yandex corpus is provided as lower-cased, we decided to lowercase all the other corpora.",
        "The same pipeline was applied to the LDC Gigaword; also only the documents classified as ?story?",
        "were retained.",
        "Table 1 summarizes the used data and introduces designations that we follow in the remainder of this paper to refer to these corpora.",
        "Russian is a morphologically rich language with nouns, adjectives and verbs inflected for case, number and gender.",
        "This property requires introducing morphological information inside the MT system to handle the lack of many inflectional forms inside training corpora.",
        "For this purpose, each corpus was previously tagged with Part-of-Speech (PoS) tags.",
        "The tagger TREE-TAGGER (Schmid, 1995) was selected for its good performance on several comparable tasks.",
        "The Russian tagger associates each word (e.g.",
        "?????",
        "(boxes)) with a complex PoS including morphological information (e.g. ?Ncmpnn?",
        "for ?Noun Type=common Gender=masculine Num-ber=plural Case=nominative Animate=no?)",
        "and its lemma (e.g.",
        "????",
        "(box)).",
        "A description of the Russian tagset can be found in (Sharoff et al., 2008).",
        "The English tagger provides also a lemma-tization and outputs PoS from the Penn Treebank tagset (Marcus et al., 1993) (e.g. ?NNS?",
        "for ?Noun plural?).",
        "In order to simplify the comparison of different setups, we used the tokenizer included in the TREETAGGER tool to process all the corpora."
      ]
    },
    {
      "heading": "2.2 Language Models",
      "text": [
        "Kneser-Ney discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002).",
        "5-gram LMs were trained for words, 7-gram LMs for lemmas and PoS.",
        "A LM was built separately on each monolingual corpus: mono-news-c and news-s.",
        "Since ldc was too large to be processed as one file, it was split into three parts according to the original publication year of the document.",
        "These LMs were combined through linear interpolation.",
        "Weights were fixed by optimizing the perplexity on a corpus made of the WMT test sets from 2008 to 2011 for English and on the WMT 2012 test set for Russian (the"
      ]
    },
    {
      "heading": "2.3 Alignment and Translation Models",
      "text": [
        "All parallel corpora were aligned using MGIZA++ (Gao and Vogel, 2008).",
        "Our translation models are phrase-based models (PBMs) built with MOSES using default settings.",
        "Weights of LM, phrase table and lexicalized reordering model scores were optimized on test12, thanks to the MERT algorithm (Gao and Vogel, 2008).",
        "Since only one development corpus was made available for Russian, we used a 3-fold cross-validation so that MERT is repeated three times for each translation model on a 2,000-sentence subsample of test12.",
        "To recase the corpora, translation models were trained using a word-to-word translation model trained on the parallel corpora aligning lowercased and cased sentences of the monolingual corpora mono-news-c and news-s."
      ]
    },
    {
      "heading": "3 Experiments with Factored Translation Models",
      "text": [
        "The evaluation was performed using case-insensitive BLEU and was computed with the mteval-v13a.pl script provided by NIST.",
        "The BLEU scores shown in the tables below are all averaged on the test parts obtained from the 3 fold cross validation process.",
        "In the remainder of the paper, we employ the notation proposed by Bojar et al. (2012) to refer to factored translation models.",
        "For example, tW-W:tL-L+tP-P+gLaP-W, where ?t?",
        "and ?g?",
        "stand for ?translation?",
        "and ?generation?, denotes a translation system with two decoding paths:",
        "?",
        "a first one directly translates words to words (tW-W), ?",
        "a second one is divided into three steps: 1. translation from lemmas to lemmas (tL-L), 2. translation from PoS to PoS (tP-P) and 3. generation of target words from target lemmas and PoS (gLaP-W)."
      ]
    },
    {
      "heading": "3.1 Baseline Phrase-Based Systems",
      "text": [
        "Table 2 is populated with the results of PBMs which use words as their sole factor.",
        "When LMs are built on mono-news-c and news-s, an improvement of BLEU is observed each time a training parallel corpus is used, both for both translation directions (columns 1 and 3).",
        "We can also notice an absolute increase of 0.4 BLEU score when the English LM is additionally trained on ldc (column 2)."
      ]
    },
    {
      "heading": "3.2 Decomposition of factors",
      "text": [
        "Koehn and Hoang (2007) suggested from their experiments for English-Czech systems that ?it is beneficial to carefully consider which morphological information to be used.?",
        "We therefore tested various decompositions of the complex Russian PoS tagset (P) output by TREETAGGER.",
        "We considered the grammatical category alone (C), morphological information restrained to case, number",
        "and gender (M1), the fields included in M1 along with additional information (lemmas) for conjunctions, particles and adpositions (M2), and finally the information included in M2 enriched with person for pronouns and person, tense and aspect for verbs (M3).",
        "Table 3 provides the number of tags and shows examples for each used tagset.",
        "To speed up the training of translation models, we experimented with various setups for factor decomposition from news-c.",
        "The results displayed on Table 4 show that factors with morphological information lead to better results than a PBM trained on word forms (line 1) but that finally the best system is achieved when the complex PoS tag output by TREETAGGER is used without any decomposition (last line).",
        "as training parallel corpus."
      ]
    },
    {
      "heading": "3.3 Experimental Results for Factored Models",
      "text": [
        "The many inflections for Russian induce a hight out-of-vocabulary rate for the PBMs, which generates many untranslated Russian words for Russian to English.",
        "We experimented with the training of a PMB on lemmatized Russian corpora (Table 5, line 1) but observed a decrease in BLEU score w.r.t.",
        "a PBM trained on words (line 2).",
        "With two decoding paths ?",
        "one from words, one from lemmas (line 4) ?",
        "using the MOSES ability to manage multiple decoding paths for factored translation models, an absolute improvement of 0.2 BLEU score was observed.",
        "Another interest of factored models is disambiguating translated words according to their PoS.",
        "Translating a (word, PoS) pair results in an absolute increase of 0.3 BLEU (line 5), and of 0.4 BLEU when considering two decoding paths (last line).",
        "Disambiguating source words with PoS did not seem to help the translation process (line 3).",
        "The Russian inflections are far more problematic in the other translation direction since morphological information, including case, gender and number, has to be induced from the English words and PoS, which are restrained for that language to the grammatical category and knowledge about number (singular/plural for nouns, 3rd person singular or not for verbs).",
        "Disambiguating translated Russian words with their PoS resulted in a dramatic increase of BLEU by 1.6 points (Table 6, last line vs line 3).",
        "The model that translates independently PoS and lemmas, before generating words, albeit appealing for its potential to deal with data sparsity, turned out to be very disappointing (first line).",
        "We additionally led experiments training generation models gLaP-W on monolingual corpora instead of the less voluminous parallel corpora, but we did not observed a gain in terms of BLEU."
      ]
    },
    {
      "heading": "4 Transliteration",
      "text": [
        "Words written in Cyrillic inside the English translation output were transliterated into Latin letters.",
        "We decided to restrain the use of transliteration for the English to Russian direction since we found that many words, especially proper names, are intentionally used in Latin letters in the Russian reference.",
        "Transliteration was performed in two steps.",
        "Firstly, untranslated words in Cyrillic are looked up in the guessed-names.ru-en file provided for the workshop and built from Wikipedia.",
        "Secondly, the remaining words are romanized with rules of the BGN/PCGN romanization method for Russian (on Geographic Names, 1994).",
        "Transliterating words in Cyrillic resulted in an absolute improvement of 0.3 BLEU for our two best factor-based system (Table 7, last column).",
        "The factored model with the tW-WaP:tLWaP translation path and a transliteration post-processing step is the final submission for the Russian-English workshop translation task, while the tW-WaP is the final submission for the other translation direction."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "This paper presented experiments carried out with factored phrase-based translation models for the two-way Russian-English translation tasks.",
        "A minor gain was observed after romanizing Russian words (+0.3 BLEU points for RU ?",
        "EN) and higher improvements using word forms, PoS integrating morphological information and lemma as factors (+0.4 BLEU points for RU?",
        "EN and +1.6 for EN ?",
        "RU w.r.t.",
        "to a phrase-based restrained to word forms).",
        "However, these improvements were observed with setups which disambiguate words according to their grammatical category or morphology, while results integrating a generation step and dealing with data sparsity were disappointing.",
        "It seems that further work should be done to fully exploit the potential of this option inside MOSES."
      ]
    }
  ]
}
