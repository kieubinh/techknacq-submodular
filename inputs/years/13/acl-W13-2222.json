{
  "info": {
    "authors": [
      "Tsuyoshi Okita",
      "Qun Liu",
      "Josef Genabith"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W13-2222",
    "title": "Shallow Semantically-Informed PBSMT and HPBSMT",
    "url": "https://aclweb.org/anthology/W13-2222",
    "year": 2013
  },
  "references": [
    "acl-J03-1002",
    "acl-P03-1021",
    "acl-P05-1033",
    "acl-P06-1124",
    "acl-P07-1040",
    "acl-P07-2045",
    "acl-P11-1087",
    "acl-W04-3250",
    "acl-W10-1720",
    "acl-W10-4006",
    "acl-W12-2702",
    "acl-W12-3141"
  ],
  "sections": [
    {
      "text": [
        "and HPBSMT decoders with multiple LMs, but will run only one decoding path decided before starting translation.",
        "Therefore the paper does not present a multi-engine system combination.",
        "We investigate three types of shallow semantics: (i) Quality Estimation (QE) score, (ii) genre ID, and (iii) context ID derived from context-dependent language models.",
        "Our results show that the improvement is 0.8 points absolute (BLEU) for EN-ES and 0.7 points for ES-EN compared to the standard PBSMT system (single best system).",
        "It is important to note that we developed this method when the standard (confusion network-based) system combination is ineffective such as in the case when the input is only two."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "This paper describes shallow semantically-informed Hierarchical Phrase-based SMT (HPBSMT) and Phrase-Based SMT (PBSMT) systems developed at Dublin City University for participation in the translation task between EN-ES and ES-EN at WMT 13.",
        "Our objectives are to incorporate several shallow semantics into SMT systems.",
        "The first semantics is the QE score for a given input sentence which can be used to select the decoding path either of HPBSMT or PBSMT.",
        "Although we call this a QE score, this score is not quite a standard one which does not have access to translation output information.",
        "The second semantics is genre ID which is intended to capture domain adaptation.",
        "The third semantics is context ID: this context ID is used to adjust the context for the local words.",
        "Context ID is used in a continuous-space LM (Schwenk, 2007), but is implicit since the context does not appear in the construction of a continuous-space LM.",
        "Note that our usage of the term semantics refers to meaning constructed by a sentence or words.",
        "The QE score works as a sentence level switch to select HPBSMT or PBSMT, based on the semantics of a sentence.",
        "The genre ID gives an indication that the sentence is to be translated by genre ID-sensitive MT systems, again based on semantics on a sentence level.",
        "The context-dependent LM can be interpreted as supplying the local context to a word, capturing semantics on a word level.",
        "The architecture presented in this paper is substantially different from multi-engine system combination.",
        "Although the system has multiple paths, only one path is chosen at decoding when processing unseen data.",
        "Note that standard multi-engine system combination using these three semantics has been presented before (Okita et al., 2012b; Okita et al., 2012a; Okita, 2012).",
        "This paper also compares the two approaches.",
        "The remainder of this paper is organized as follows.",
        "Section 2 describes the motivation for our approach.",
        "In Section 3, we describe our proposed systems, while in Section 4 we describe the experimental results.",
        "We conclude in Section 5."
      ]
    },
    {
      "heading": "2 Motivation",
      "text": [
        "Model Difference of PBSMT and HPBSMT Our motivation is identical with a system combination strategy which would obtain a better translation if we can access more than two translations.",
        "Even though we are limited in the type of MT sys",
        "tems, i.e. SMT systems, we can access at least two systems, i.e. PBSMT and HPBSMT systems.",
        "The merit that accrues from accessing these two translation is shown in Figure 1.",
        "In this example between EN-ES, the skirts of the distribution shows that around 20% of the examples obtain the same BLEU score, 37% are better under PBSMT, and 42% under HPBSMT.",
        "Moreover, around 10% of sentences show difference of 10 BLEU points.",
        "Even a selection of outputs would improve the results.",
        "Unfortunately, some pitfall of system combination (Rosti et al., 2007) impact on the process when the number of available translation is only two.",
        "If there are only two inputs, (1) the mismatch of word order and word selection would yield a bad combination since system combination relies on monolingual word alignment (or TER-based alignment) which seeks identical words, and (2) Minimum Bayes Risk (MBR) decoding, which is a first step, will not work effectively since it relies on voting.",
        "(In fact, only selecting one of the translation outputs is even effective: this method is called system combination as well (Specia et al., 2010).)",
        "Hence, although the aim is similar, we do not use a system combination strategy, but we de",
        "based performance between PBSMT and HPBSMT systems.",
        "Relation of Complexity of Source Sentence and Performance of HPBSMT and PBSMT It is interesting to note that PBSMT tends to be better than HPBSMT for European language pairs as the recent WMT workshop shows, while HPBSMT shows often better performance for distant language pairs such as EN-JP (Okita et al., 2010b) and EN-ZH in other workshops.",
        "Under the assumption that we use the same training corpus for training PBSMT and HPBSMT systems, our hypothesis is that we may be able to predict the quality of translation.",
        "Note that although this is the analogy of quality estimation, the setting is slightly different in that in test phase, we will not be given a translation output, but only a source sentence.",
        "Our aim is to predict whether HPBSMT obtains better translation output than PBSMT or not.",
        "Hence, our aim does not require that the quality prediction here is very accurate compared to the standard quality estimation task.",
        "We use a feature set consisting of various characteristics of input sentences."
      ]
    },
    {
      "heading": "3 Our Methods: Shallow Semantics",
      "text": [
        "Our system accommodates PBSMT and HPBSMT with multiple of LMs.",
        "A decoder which handles shallow semantic information is shown in Table 3.1."
      ]
    },
    {
      "heading": "3.1 QE Score",
      "text": [
        "Quality estimation aims to predict the quality of translation outputs for unseen data (e.g. by building a regressor or a classifier) without access to references: the inputs are translation outputs and source sentences in a test phase, while in a training phase the corresponding BLEU or HTER scores are used.",
        "In this subsection, we try to build a re-gressor with the similar settings but without supplying the translation outputs.",
        "That is, we supply only the input sentences.",
        "(Since our method is not a quality estimation for a given translation output, quality estimation may not be an entirely appropriate term.",
        "However, we borrow this term for this paper.)",
        "If we can build such a regressor for PBSMT and HPBSMT systems, we would be able to select a better translation output without actually translating them for a given input sentence.",
        "Note that we translate the training set by PBSMT and HPBSMT in a training phase only to supply their BLEU scores to a regressor (since a regressor is a supervised learning method).",
        "Then, we use these regressors for a given unseen source sentence (which has no translation output attached) to predict their BLEU scores for PBSMT and HPBSMT.",
        "Our motivation came from the comparison of a sequential learning system and a parser-based system.",
        "The typical decoder of the former is a",
        "Viterbi decoder while that of the latter is a Cocke-Younger-Kasami (CYK) decoder (Younger, 1967).",
        "The capability of these two systems provides an intuition about the difference of PBSMT and HPBSMT: the CYK decoder-based system has some capability to handle syntactic constructions while the Viterbi decoder-based system has only the capability of learning a sequence.",
        "For ex-Input: Foreign sent f=f1,...,f1f , language model, translation model, rule table.",
        "Output: English translation e ceScore = predictQEScore(fi) if (ceScore == HPBSMTBetter) for span length l=1 to 1f do for start=0..1f 1 do",
        "forall seq s of entries and words in span [start,end] do forall rules r do if rule r applies to chart seq s then create new chart entry c with LM(genreID) add chart entry c to chart return e from best chart entry in span [0,1f ] else: genreID = predictGenreID(fi) place empty hypothesis into stack 0 for all stacks 0...n-1 do for all hypotheses in stack do for all translation options do if applicable then create new hyp with LM(ID) place in stack recombine with existing hyp if possible prune stack if too big",
        "of PBSMT and HPBSMT are from (Koehn, 2010).",
        "The modification is related to predictQEScore(), predictGenreID(), and predictContextID().",
        "ample, the (context-free) grammar-based system has the capability of handling various difficulties caused by inserted clauses, coordination, long Multiword Expressions, and parentheses, while the sequential learning system does not (This is since this is what the aim of the context-free grammar-based system is.)",
        "These difficulties are manifest in input sentences.",
        "true BLEU difference of PBSMT and HPBSMT predicted BLEU difference of PBSMT and HPBSMT",
        "ference between PBSMT and HPBSMT (y-axis) where x-axis is the sample IDs reordered in descending order (blue), while green dots show the BLEU absolute difference (y-axis) of the typical samples where x-axis is shared with the above.",
        "This example is sampled 300 points from new-stest2013 (ES-EN).",
        "Even if the regressor does not achieve a good performance, the bottom line of the overall performance is already really high in this tricky problem.",
        "Roughly, even if we plot randomly we could achieve around 80 - 90% of correctness.",
        "Around 50% of samples (middle of the curve) do not care (since the true performance of PBSMT and HPBSMT are even), there is a slope in the left side of the curve where random plot around this curve would achieve 15 - 20% among 25% of correctness (the performance of PBSMT is superior), and there is another slope in the right side of the curve where random plot would achieve again 15 - 20% among 25% (the performance of HPBSMT is superior).",
        "In this case, accuracy is 86%.",
        "If we assume that this is one major difference between these two systems, the complexity of the input sentence will correlate with the difference of translation quality of these two systems.",
        "In this subsection, we assume that this is one major difference of these two systems and that the complexity of the input sentence will correlate with the difference of translation quality of these two systems.",
        "Based on these assumptions, we build a regressor",
        "for each system for a given input sentence where in a training phase we supply the BLEU score measured using the training set.",
        "One remark is that the BLEU score which we predict is only meaningful in a relative manner since we actually generate a translation output in preparation phase (there is a dependency to the mean of BLEU score in the training set).",
        "Nevertheless, this is still meaningful as a relative value if we want to talk about their difference, which is what we want in our settings to predict which system, either PBSMT or HPBSMT, will generate a better output.",
        "The main features used for training the regressor are as follows: (1) number of / length of inserted clause / coordination / multiword expressions, (2) number of long phrases (connection by ?of?",
        "; ordering of words), (3) number of OOV words (which let it lower the prediction quality), (4) number of / length of parenthesis, etc.",
        "We obtained these features using parser (de Marneffe et al., 2006) and multiword extractor (Okita et al., 2010a)."
      ]
    },
    {
      "heading": "3.2 Genre ID",
      "text": [
        "Genre IDs allow us to apply domain adaptation technique according to the genre ID of the testset.",
        "Among various methods of domain adaptation, we investigate unsupervised clustering rather than already specified genres.",
        "We used (unsupervised) classification via Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to obtain genre ID.",
        "LDA represents topics as multinomial distributions over the W unique word-types in the corpus and represents documents as a mixture of topics.",
        "Let C be the number of unique labels in the corpus.",
        "Each label c is represented by a W - dimensional multinomial distribution ?c over the vocabulary.",
        "For document d, we observe both the words in the document w(d) as well as the docu",
        "ment labels c(d).",
        "Given the distribution over topics ?d, the generation of words in the document is captured by the following generative model.",
        "1.",
        "For each label c ?",
        "{1, .",
        ".",
        ".",
        "C}, sample a distribution over word-types ?c ?",
        "Dirichlet(?|?)",
        "2.",
        "For each document d ?",
        "{1, .",
        ".",
        ".",
        ", D} (a) Sample a distribution over its observed labels ?d ?",
        "Dirichlet(?|?)",
        "(b) For each word i ?",
        "{1, .",
        ".",
        ".",
        ", NWd } i.",
        "Sample a label z(d)i ?",
        "Multinomial(?d) ii.",
        "Sample a word w(d)i ?",
        "Multinomial(?c) from the label c = z(d)i",
        "Using topic modeling (or LDA) as described above, we perform the in-domain data partitioning as follows, building LMs for each class, and running a decoding process for the development set,",
        "which will obtain the best weights for cluster i.",
        "1.",
        "Fix the number of clusters C, we explore values from small to big.1 2.",
        "Do unsupervised document classification (or LDA) on the source side of the training, development and test sets.",
        "3.",
        "Separate each class of training sets and build LM for each cluster i (1 ?",
        "i ?",
        "C).",
        "4.",
        "Separate each class of development set (keep the original index and new index in the allocated separated dataset).",
        "5.",
        "(Using the same class of development set):",
        "Run the decoder on each class to obtain the n-best lists, run a MERT process to obtain the best weights based on the n-best lists, (Repeat the decoding / MERT process several iterations.",
        "Then, we obtain the best weights for a particular class.)",
        "For the test phase,",
        "1.",
        "Separate each class of the test set (keep the original index and new index in the allocated separated dataset).",
        "2.",
        "Suppose the test sentence belongs to cluster i, run the decoder of cluster i.",
        "3.",
        "Repeat the previous step until all the test sentences are decoded."
      ]
    },
    {
      "heading": "3.3 Context ID",
      "text": [
        "Context ID semantics is used through the re-ranking of the n-best list in a MERT process (Schwenk, 2007; Schwenk et al., 2012; Le et al., 2012).",
        "2-layer ngram-HMM LM is a two layer version of the 1-layer ngram-HMM LM (Blun-som and Cohn, 2011) which is a nonparametric 1Currently, we do not have a definite recommendation on this.",
        "It needs to be studied more deeply.",
        "Bayesian method using hierarchical Pitman-Yor prior.",
        "In the 2-layer LM, the hidden sequence of the first layer becomes the input to the higher layer of inputs.",
        "Note that such an architecture comes from the Restricted Boltzmann Machine (Smolen-sky, 1986) accumulating in multiple layers in order to build deep belief networks (Taylor and Hinton, 2009).",
        "Although a 2-layer ngram-HMM LM is inferior in its performance compared with other two LMs, the runtime cost is cheaper than these.",
        "ht denotes the hidden word for the first layer, h?t denotes the hidden word for the second layer, wi denotes the word in output layer.",
        "The generative model for this is shown below.",
        "where ?",
        "is a concentration parameter, ?",
        "is a strength parameter, and Gi is a base measure.",
        "Note that these terms belong to the hierarchical Pitman-Yor language model (Teh, 2006).",
        "We used a blocked inference for inference.",
        "The performance of 2-layer LM is shown in Table 3."
      ]
    },
    {
      "heading": "4 Experimental Settings",
      "text": [
        "We used Moses (Koehn et al., 2007) for PBSMT and HPBSMT systems in our experiments.",
        "The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4.",
        "For phrase extraction the grow-diag-final heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments.",
        "We then perform MERT process (Och, 2003) which optimizes the BLEU metric, while a 5-gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002).",
        "For the HPBSMT system, the chart-based decoder of Moses (Koehn et al., 2007) is used.",
        "Most of the procedures are identical with the PBSMT systems except the rule extraction process (Chiang, 2005).",
        "The procedures to handle three kinds of semantics are implemented using the already mentioned algorithm.",
        "We use libSVM (Chang and Lin, 2011), and Mallet (McCallum, 2002) for Latent Dirichlet Allocation (LDA) (Blei et al., 2003).",
        "For the corpus, we used all the resources provided for the translation task at WMT13 for lan",
        "space language model (Schwenk, 2007).",
        "The lower-left shows ours, i.e. the 2-layer ngram-HMM LM.",
        "The lower-right shows the 2-layer conditional Restricted Boltzmann Machine LM (Taylor and Hinton, 2009).",
        "guage model, that is parallel corpora (Europarl V7 (Koehn, 2005), Common Crawl corpus, UN corpus, and News Commentary) and monolingual corpora (Europarl V7, News Commentary, and News Crawl from 2007 to 2012).",
        "Experimental results are shown in Table 2.",
        "The leftmost column (sem-inform) shows our results.",
        "The sem-inform made a improvement of 0.8 BLEU points absolute compared to the PBSMT results in EN-ES, while the standard system combination lost 0.1 BLEU points absolute compared to the single worst.",
        "For ES-EN, the sem-inform made an improvement of 0.7 BLEU points absolute compared to the PBSMT results.",
        "These improvements over both of PBSMT and HPBSMT are statistically significant by a paired bootstrap test (Koehn, 2004)."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "This paper describes shallow semantically-informed HPBSMT and PBSMT systems developed at Dublin City University for participation in the translation task at the Workshop on Statistical Machine Translation (WMT 13).",
        "Our system has",
        "official score.",
        "?syscomb?",
        "denotes the confusion-network-based system combination using BLEU, while ?aug-syscomb?",
        "uses three shallow semantics described in QE score (Okita et al., 2012a), genre ID (Okita et al., 2012b), and context ID (Okita, 2012).",
        "Note that the inputs for syscomb and aug-syscomb are the output of HPBSMT and PBSMT.",
        "HPBSMT from ES to EN has marked with ?, which indicates that this is trained only with Europarl V7.",
        "dependent language models, which is 2-layer ngram HMM LM, and that of SRILM (Stolcke, 2002) in terms of newstest09 to 12.",
        "PBSMT and HPBSMT decoders with multiple LMs, but our system will execute only one path, which is different from multi-engine system combination.",
        "We consider investigate three types of shallow semantic information: (i) a Quality Estimate (QE) score, (ii) genre ID, and (iii) a context ID through context-dependent language models.",
        "Our experimental results show that the improvement is 0.8 points absolute (BLEU) for EN-ES and 0.7 points for ES-EN compared to the standard PBSMT system (single best system).",
        "We developed this method when the standard (confusion network-based) system combination is ineffective such as in the case when the input is only two.",
        "A further avenue would be the investigation of other semantics such as linguistic semantics, including co-reference resolution or anaphora resolution, hyper-graph decoding, and text understanding.",
        "Some of which are investigated in the context of textual entailment task (Okita, 2013b) and we would like to extend this to SMT task.",
        "Another investigation would be the integration of genre ID into the context-dependent LM.",
        "The preliminary work shows that such integration would decrease the overall perplexity (Okita, 2013a)."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank Antonio Toral and Santiago Corte's Va?",
        "?lo for providing parts of their processing data.",
        "This research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the Centre for Next Generation Localisation (http://www.cngl.ie) at Dublin City University."
      ]
    }
  ]
}
