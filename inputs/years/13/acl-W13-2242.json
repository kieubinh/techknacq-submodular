{
  "info": {
    "authors": [
      "Ergun Bicici"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W13-2242",
    "title": "Referential Translation Machines for Quality Estimation",
    "url": "https://aclweb.org/anthology/W13-2242",
    "year": 2013
  },
  "references": [
    "acl-J93-2003",
    "acl-N10-1069",
    "acl-N12-1021",
    "acl-P02-1040",
    "acl-P07-2045",
    "acl-S12-1059",
    "acl-S13-1034",
    "acl-S13-2098",
    "acl-W02-1001",
    "acl-W09-0441",
    "acl-W11-2103",
    "acl-W11-2131",
    "acl-W11-2137",
    "acl-W12-3102",
    "acl-W13-2201",
    "acl-W13-2206"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We introduce referential translation machines (RTM) for quality estimation of translation outputs.",
        "RTMs are a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for estimating the quality of translation outputs, judging the semantic similarity between text, and evaluating the quality of student answers.",
        "RTMs achieve top performance in automatic, accurate, and language independent prediction of sentence-level and word-level statistical machine translation (SMT) quality.",
        "RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations.",
        "We develop novel techniques for solving all subtasks in the WMT13 quality estimation (QE) task (QET 2013) based on individual RTM models.",
        "Our results achieve improvements over last year's QE task results (QET 2012), as well as our previous results, provide new features and techniques for QE, and rank 1st or 2nd in all of the subtasks."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Quality Estimation Task (QET) (Callison-Burch et al., 2012; Callison-Burch et al., 2013) aims to develop quality indicators for translations and predictors without access to the references.",
        "Prediction of translation quality is important because the expected translation performance can help in estimating the effort required for correcting the translations during post-editing by human translators.",
        "Bicici et al. (2013) develop the Machine Translation Performance Predictor (MTPP), a state-of-the-art, language independent, and SMT system extrinsic machine translation performance predictor, which achieves better performance than the competitive QET baseline system (Callison-Burch et al., 2012) by just looking at the test source sentences and becomes the 2nd overall after also looking at the translation outputs in QET 2012.",
        "In this work, we introduce referential translation machines (RTM) for quality estimation of translation outputs, which is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain.",
        "RTMs reduce our dependence on any task dependent resource.",
        "In particular, we do not use the baseline software or the SMT resources provided with the QET 2013 challenge.",
        "We believe having access to glass-box features such as the phrase table or the n-best lists is not realistic especially for use-cases where translations may be provided by different MT vendors (not necessarily from SMT products) or by human translators.",
        "Even the prior knowledge of the training corpora used for building the SMT models or any other model used when generating the translations diverges from the goal of independent and unbiased prediction of translation quality.",
        "Our results show that we do not need to use any SMT system dependent information to achieve the top performance when predicting translation output quality."
      ]
    },
    {
      "heading": "2 Referential Translation Machine (RTM)",
      "text": [
        "Referential translation machines (RTMs) provide a computational model for quality and semantic similarity judgments using retrieval of relevant training data (Bic?ici and Yuret, 2011a; Bic?ici, 2011) as interpretants for reaching shared semantics (Bic?ici, 2008).",
        "RTMs achieve very good performance in judging the semantic similarity of sentences (Bic?ici and van Genabith, 2013a) and we can also use RTMs to automatically assess the",
        "correctness of student answers to obtain better results (Bic?ici and van Genabith, 2013b) than the state-of-the-art (Dzikovska et al., 2012).",
        "RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain.",
        "RTM can be used for predicting the quality of translation outputs.",
        "An RTM model is based on the selection of common training data relevant and close to both the training set and the test set of the task where the selected relevant set of instances are called the interpretants.",
        "Interpretants allow shared semantics to be possible by behaving as a reference point for similarity judgments and providing the context.",
        "In semiotics, an interpretant I interprets the signs used to refer to the real objects (Bic?ici, 2008).",
        "RTMs provide a model for computational semantics using interpretants as a reference according to which semantic judgments with translation acts are made.",
        "Each RTM model is a data translation model between the instances in the training set and the test set.",
        "We use the FDA (Feature Decay Algorithms) instance selection model for selecting the interpretants (Bic?ici and Yuret, 2011a) from a given corpus, which can be monolingual when modeling paraphrasing acts, in which case the MTPP model (Section 2.1) is built using the interpretants themselves as both the source and the target side of the parallel corpus.",
        "RTMs map the training and test data to a space where translation acts can be identified.",
        "We view that acts of translation are ubiquitously used during communication: Every act of communication is an act of translation (Bliss, 2012).",
        "Translation need not be between different languages and paraphrasing or communication also contain acts of translation.",
        "When creating sentences, we use our background knowledge and translate information content according to the current context.",
        "Given a training set train, a test set test, and some monolingual corpus C, preferably in the same domain as the training and test sets, the RTM steps are:",
        "1.",
        "T = train ?",
        "test.",
        "2. select(T, C)?",
        "I 3.",
        "MTPP(I,train)?",
        "Ftrain 4.",
        "MTPP(I,test)?",
        "Ftest 5. learn(M,Ftrain)?M 6. predict(M,Ftest)?",
        "q?",
        "Step 2 selects the interpretants, I, relevant to the instances in the combined training and test data.",
        "Steps 3 and 4 use I to map train and test to a new space where similarities between translation acts can be derived more easily.",
        "Step 5 trains a learning model M over the training features, Ftrain, and Step 6 obtains the predictions.",
        "RTM relies on the representativeness of I as a medium for building translation models for translating between train and test.",
        "Our encouraging results in the QET challenge provides a greater understanding of the acts of translation we ubiquitously use when communicating and how they can be used to predict the performance of translation, judging the semantic similarity between text, and evaluating the quality of student answers.",
        "RTM and MTPP models are not data or language specific and their modeling power and good performance are applicable across different domains and tasks.",
        "RTM expands the applicability of MTPP by making it feasible when making monolingual quality and similarity judgments and it enhances the computational scalability by building models over smaller but more relevant training data as interpretants."
      ]
    },
    {
      "heading": "2.1 The Machine Translation Performance Predictor (MTPP)",
      "text": [
        "In machine translation (MT), pairs of source and target sentences are used for training statistical MT (SMT) models.",
        "SMT system performance is affected by the amount of training data used as well as the closeness of the test set to the training set.",
        "MTPP (Bic?ici et al., 2013) is a state-of-the-art and top performing machine translation performance predictor, which uses machine learning models over features measuring how well the test set matches the training set to predict the quality of a translation without using a reference translation.",
        "MTPP measures the coverage of individual test sentence features and syntactic structures found in the training set and derives feature functions measuring the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation."
      ]
    },
    {
      "heading": "2.2 MTPP Features for Translation Acts",
      "text": [
        "MTPP uses n-gram features defined over text or common cover link (CCL) (Seginer, 2007) structures as the basic units of information over which similarity calculations are made.",
        "Unsupervised",
        "parsing with CCL extracts links from base words to head words, resulting in structures representing the grammatical information instantiated in the training and test data.",
        "Feature functions use statistics involving the training set and the test sentences to determine their closeness.",
        "Since they are language independent, MTPP allows quality estimation to be performed extrinsically.",
        "We extend MTPP (Bic?ici et al., 2013) in its learning module, the features included, and their representations.",
        "Categories for the 308 features (S for source, T for target) used are listed below where the number of features are given in {#} and the detailed descriptions for some of the features are presented in (Bic?ici et al., 2013).",
        "?",
        "Coverage {110}: Measures the degree to which the test features are found in the training set for both S ({56}) and T ({54}).",
        "?",
        "Synthetic Translation Performance {6}: Calculates translation scores achievable according to the n-gram coverage.",
        "?",
        "Length {7}: Calculates the number of words and characters for S and T and their average token lengths and their ratios.",
        "?",
        "Feature Vector Similarity {16}: Calculates similarities between vector representations.",
        "?",
        "Perplexity {90}: Measures the fluency of the sentences according to language models (LM).",
        "We use both forward ({30}) and backward ({15}) LM features for S and T.",
        "?",
        "Entropy {9}: Calculates the distributional similarity of test sentences to the training set over top N retrieved sentences.",
        "?",
        "Retrieval Closeness {24}: Measures the degree to which sentences close to the test set are found in the selected training set, I, using FDA (Bic?ici and Yuret, 2011a).",
        "?",
        "Diversity {6}: Measures the diversity of co-occurring features in the training set.",
        "?",
        "IBM1 Translation Probability {16}: Calculates the translation probability of test",
        "sentences using the selected training set, I, (Brown et al., 1993).",
        "?",
        "IBM2 Alignment Features {11}: Calculates the sum of the entropy of the distribution of alignment probabilities for S (?s?S ?p log p for p = p(t|s) where s and t are tokens) and T, their average for S and T, the number of entries with p ?",
        "0.2 and p ?",
        "0.01, the entropy of the word alignment between S and T and",
        "its average, and word alignment log probability and its value in terms of bits per word.",
        "?",
        "Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances.",
        "?",
        "Sentence Translation Performance {3}: Calculates translation scores obtained according to q(T,R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic?ici and Yuret, 2011b) for q. ?",
        "Character n-grams {4}: Calculates cosine between character n-grams (for n=2,3,4,5) obtained for S and T (Ba?r et al., 2012).",
        "?",
        "LIX {2}: Calculates the LIX readability score (Wikipedia, 2013; Bjo?rnsson, 1968) for",
        "S and T. 1 For retrieval closeness, we use FDA instead of dice for sentence selection.",
        "We also improve FDA's instance selection score by scaling with the length of the sentence (Bic?ici and Yuret, 2011a).",
        "IBM2 alignments and their probabilities are obtained by first obtaining IBM1 alignments and probabilities, which become the starting point for the IBM2 model.",
        "Both models are trained for 25 to 75 iterations or until convergence."
      ]
    },
    {
      "heading": "3 Quality Estimation Task Results",
      "text": [
        "We participate in all of the four challenges of the quality estimation task (QET) (Callison-Burch et al., 2013), which include English to Spanish (en-es) and German to English translation directions.",
        "There are two main categories of challenges: sentence-level prediction (Task 1.",
        "*) and word-level prediction (Task 2).",
        "Task 1.1 is about predicting post-editing effort (PEE), Task 1.2 is about ranking translations from different systems, Task 1.3 is about predicting post-editing time (PET), and Task 2 is about binary or multi-class classification of word-level quality.",
        "For each task, we develop RTM models using the parallel corpora and the LM corpora distributed by the translation task (WMT13) (Callison-Burch et al., 2013) and the LM corpora provided by LDC for English and Spanish 2.",
        "The parallel corpora contain 4.3M sentences for de-en with 106M words for de and 111M words for en and 15M sentences for en-es with 406M words for en and 455M words for 1LIX=AB + C 100A , where A is the number of words, C iswords longer than 6 characters, B is words that start or end with any of ?.",
        "?, ?",
        ":?, ?!",
        "?, ???",
        "similar to (Hagstro?m, 2012).",
        "es.",
        "We do not use any resources provided by QET including data, software, or baseline features since they are SMT system dependent or language specific.",
        "Instance selection for the training set and the language model (LM) corpus is handled by a parallel implementation of FDA (Bic?ici, 2013).",
        "We tokenize and true-case all of the corpora.",
        "The true-caser is trained on all of the training corpus using Moses (Koehn et al., 2007).",
        "We prepare the corpora by following this procedure: tokenize ?",
        "train the true-caser ?",
        "true-case.",
        "Table 1 lists the statistics of the data used in the training and test sets for the tasks.",
        "number of words is listed after tokenization.",
        "Since we do not know the best training set size that will maximize the performance, we rely on previous SMT experiments (Bic?ici and Yuret, 2011a; Bic?ici and Yuret, 2011b) and quality estimation challenges (Bic?ici and van Genabith, 2013a; Bic?ici and van Genabith, 2013b) to select the proper training set size.",
        "For each training and test sentence provided in each subtask, we choose between 65 and 600 sentences from the parallel training corpora to be added to the training set, which creates roughly 400K sentences for training.",
        "We add the selected training set to the 8 million sentences selected for each LM corpus.",
        "The statistics of the training data selected by the parallel FDA and used as interpretants in the RTM models is given in Table 2.",
        "terpretants in the RTM models in thousands (K) of sentences or millions (M) of words."
      ]
    },
    {
      "heading": "3.1 Evaluation",
      "text": [
        "In this section, we describe the metrics we use to evaluate the learning performance.",
        "Let yi represent the actual target value for instance i, y?",
        "the mean of the actual target values, y?i the value estimated by the learning model, and ?",
        "?y the mean of the estimated target values, then we use the following metrics to evaluate the learning models:",
        "the average quality difference between the scores for the top n ?",
        "1 quartiles and the overall quality for the test set.",
        "Relative absolute error measures the error relative to the error when predicting the actual mean.",
        "We use the coefficient of determina",
        "during optimization where the models are regression based and higher R2 values are better."
      ]
    },
    {
      "heading": "3.2 Task 1: Sentence-level Prediction of Quality",
      "text": [
        "In this subsection, we develop techniques for the prediction of quality at the sentence-level.",
        "We first discuss the learning models we use and how we optimize them and then provide the results for the individual subtasks and the settings used.",
        "The learning models we use for predicting the translation quality include the ridge regression (RR) and support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Scho?lkopf, 2004).",
        "Both of these models learn a regression function using the features to estimate a numerical target value such as the HTER score, the F1 score (Bic?ici and Yuret, 2011b), or the PET score.",
        "We also use these learning models after a feature subset selection with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), both of which are described in (Bic?ici et al., 2013).",
        "The learning parameters that govern the behavior of RR and SVR are the regularization ?",
        "for RR and the C, ?, and ?",
        "parameters for SVR.",
        "We optimize",
        "the learning parameters, the number of features to select, and the number of dimensions used for PLS.",
        "More detailed description of the optimization process is given in (Bic?ici et al., 2013).",
        "In our submissions, we only used the results we obtained from SVR and SVR after PLS (SVRPLS) since they perform the best during training.",
        "Optimization can be a challenge for SVR due to the large number of parameter settings to search.",
        "In this work, we decrease the search space by selecting ?",
        "close to the theoretically optimal values.",
        "We select ?",
        "close to the standard deviation of the noise in the training set since the optimal value for ?",
        "is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998).",
        "We use RMSE of RR on the training set as an estimate for the noise level (?",
        "of noise) and the following formulas to obtain the ?",
        "with ?",
        "= 3:",
        "Since the C obtained could be low (Chalimourda et al., 2004), we use a range of C values in addition to the obtained C value including C values with a couple of ?y values larger.",
        "Table 3 lists the RMSE of the RR model on the training set and the corresponding ?",
        "and C values for different subtasks.",
        "We also present the optimized parameter values for SVR and SVRPLS.",
        "Table 3 shows that, empirically, Equation 1 and Equation 2 gives results close to the best parameters found after optimization.",
        "tion 1 and Equation 2 and the optimized parameter values, C?",
        "and ?",
        "for SVR and SVRPLS and the number of dimensions (# dim) for SVRPLS.",
        "3.2.2 Task 1.1: Scoring and Ranking for"
      ]
    },
    {
      "heading": "Post-Editing Effort",
      "text": [
        "Task 1.1 involves the prediction of the case insensitive translation edit rate (TER) scores obtained by TERp (Snover et al., 2009) and their ranking.",
        "In contrast, we derive features over sentences that are true-cased.",
        "We obtain the rankings by sorting according to the predicted TER scores.",
        "the training set using the optimized parameters.",
        "We are able to significantly improve the results when compared with the QET 2012 (Callison-Burch et al., 2012) and our previous results (Bic?ici et al., 2013) especially in terms of MAE and RAE.",
        "The results on the test set are given in Table 5.",
        "Rank lists the overall ranking in the task.",
        "RTMs"
      ]
    },
    {
      "heading": "3.2.3 Task 1.2: Ranking Translations from Different Systems",
      "text": [
        "Task 1.2 involves the prediction of the ranking among up to 5 translation outputs produced by different MT systems.",
        "Evaluation is done against the human rankings using the Kendall's ?",
        "correlation (Callison-Burch et al., 2013): ?",
        "= (c ?",
        "d)/n(n?1)2 = c?dc+d where a pair is concordant, c, ifthe ordering agrees, discordant, d, if their ordering disagrees, and neither concordant nor discordant if their rankings are equal.",
        "We use sentence-level F1 scores (Bic?ici and Yuret, 2011b) as the target to predict.",
        "We use F1 because it can be easily interpreted and it correlates well with human judgments (more than TER) (Bic?ici and Yuret, 2011b; Callison-Burch et al., 2011).",
        "We also found that the ?",
        "of the rankings obtained according to the F1 score over the",
        "training set (0.2040) is better than BLEU (Pap-ineni et al., 2002) (0.1780) and NIST (Doddington, 2002) (0.1907) for de-en.",
        "Table 6 presents the learning performance on the training set using the optimized parameters.",
        "Learning F1 becomes an easier task than learning TER as observed from the results but we have significantly more training instances.",
        "We use the SVR model for predicting the F1 scores on the training set and the test set.",
        "MAE is a more important performance metric here since we want to be as precise as possible when predicting the actual performance.",
        "Our next goal is to learn a threshold for judging if two translations are equal over the predicted F1 scores.",
        "This threshold is used to determine whether we need to alter the ranking.",
        "We try to mimic the human decision process when determining whether two translations are equivalent.",
        "On some occasions where the sentences are close enough, humans give them equal ranking.",
        "This is also related to the granularity of the differences visible with a 1 to 5 ranking schema.",
        "We compared different threshold formulations and used the following condition in our submissions to decide whether the ranking of item i in a set S of translations, i ?",
        "S, should be different:",
        "where t is the optimized threshold minimizing the following loss for n training instances:",
        "where f(t, qi) is a function returning rankings based on the threshold t and the quality scores for instance i, qi and ?",
        "(rj , ri) calculates the ?",
        "score based on the rankings rj and ri.",
        "For both de-en and en-es subtasks, we found the thresholds obtained to be very similar or the same.",
        "The optimized values are given in Table 7.",
        "On the test set, we used the same threshold, t = 0.00275 for both de-en and en-es, which is a little higher than the optimal t to prevent overfitting.",
        "corresponding comparisons that were found to be equal (# same) over all comparisons (# all).",
        "We believe that human judgments of linguistic equality and the corresponding thresholds we learned in this work can be useful for developing better automatic evaluation metrics and can improve the correlation of the scores obtained with human judgments (as we did here).",
        "The results on the test set are given in Table 8.",
        "We are also able to achieve the top ranking in this task.",
        "Task 1.3 involves the prediction of the post-editing time (PET) for a translator to post-edit the MT output.",
        "Table 9 presents the learning performance on the training set using the optimized parameters.",
        "The results on the test set are given in Table 10.",
        "We are able to become the 2nd best system according to MAE in this task."
      ]
    },
    {
      "heading": "3.3 Task 2: Word-level Prediction of Quality",
      "text": [
        "In this subsection, we develop a learning model, global linear models with dynamic learning rate (GLMd), for the prediction of quality at the word-level where the word-level quality is a binary (K: keep, C: change) or multi-class classification problem (K: keep, S: substitute, D: delete).",
        "We first discuss the GLMd learning model, then we present",
        "the word-level features we use, and then present our results on the test set.",
        "Learning (GLMd) Collins (2002) develops global learning models (GLM), which rely on Viterbi decoding, percep-tron learning, and flexible feature definitions.",
        "We extend the GLM framework by parallel percep-tron training (McDonald et al., 2010) and dynamic learning with adaptive weight updates in the per-ceptron learning algorithm:",
        "where ?",
        "returns a global representation for instance i and the weights are updated by ?",
        "= exp(log10(3?1/0)) with ?1 and 0 representing the error of the previous and first iteration respectively.",
        "?",
        "decays the amount of the change during weight updates at later stages and prevents large fluctuations with updates.",
        "We used both the GLM model and the GMLd models in our submissions.",
        "We introduce a number of novel features for the prediction of word-level translation quality.",
        "In broad categories, these word-level features are:",
        "?",
        "CCL: Uses CCL links.",
        "?",
        "Word context: Surrounding words.",
        "?",
        "Word alignments: Alignments, their probabilities, source and target word contexts.",
        "?",
        "Length: Word lengths, n-grams over them.",
        "?",
        "Location: Location of the words.",
        "?",
        "Prefix and Suffix: Word prefixes, suffixes.",
        "?",
        "Form: Capital, contains digit or punctuation.",
        "We found that CCL links are the most discriminative feature among these.",
        "In total, we used 511K features for binary and 637K for multi-class classification.",
        "The learning curve is given in Figure 1.",
        "The results on the test set are given in Table 11.",
        "P, R, and A stand for precision, recall, and accuracy respectively.",
        "We are able to become the 2nd"
      ]
    },
    {
      "heading": "4 Contributions",
      "text": [
        "Referential translation machines achieve top performance in automatic, accurate, and language independent prediction of sentence-level and word-level statistical machine translation (SMT) quality.",
        "RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations.",
        "We develop novel techniques for solving all subtasks in the quality estimation (QE) task (QET 2013) based on individual RTM models.",
        "Our results achieve improvements over last year's QE task results (QET 2012), as well as our previous results, provide new features and techniques for QE, and rank 1st or 2nd in all of the subtasks."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work is supported in part by SFI (07/CE/I1142) as part of the Centre for Next Generation Localisation (www.cngl.ie) at Dublin City University and in part by the European Commission through the QTLaunchPad FP7 project (No: 296347).",
        "We also thank the SFI/HEA Irish Centre for High-End Computing (ICHEC) for the provision of computational facilities and support."
      ]
    }
  ]
}
