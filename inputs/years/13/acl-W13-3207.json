{
  "info": {
    "authors": [
      "Marton Makrai",
      "David Mark Nemeskey",
      "Andras Kornai"
    ],
    "book": "CVSC",
    "id": "acl-W13-3207",
    "title": "Applicative structure in vector space models",
    "url": "https://aclweb.org/anthology/W13-3207",
    "year": 2013
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 59?63, Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics Applicative structure in vector space models"
      ]
    },
    {
      "heading": "Andra's Kornai Abstract",
      "text": [
        "We introduce a new 50-dimensional embedding obtained by spectral clustering of a graph describing the conceptual structure of the lexicon.",
        "We use the embedding directly to investigate sets of antonymic pairs, and indirectly to argue that function application in CVSMs requires not just vectors but two transformations (corresponding to subject and object) as well."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Commutativity is a fundamental property of vector space models.",
        "As soon as we encode king by ~k, queen by ~q, male by ~m, and female by ~f , if we expect ~k ?",
        "~q ?",
        "~m ?",
        "~f , as suggested in Mikolov et al. (2013), we will, by commutativity, also expect ~k ?",
        "~m ?",
        "~q ?",
        "~f ?ruler, gender unspecified?.",
        "When the meaning decomposition involves function application, commutativity no longer makes sense: consider Victoria as ~qmEngland and Victor as ~kmItaly.",
        "If the function application operator m is simply another vector to be added to the representation, the same logic would yield that Italy is the male counterpart of female England.",
        "To make matters worse, performing the same operations on Albert, ~kmEngland and Elena, ~qmItaly would yield that Italy is the female counterpart of male England.",
        "Section 2 offers a method to treat antonymy in continuous vector space models (CVSMs).",
        "Section 3 describes a new embedding, 4lang, obtained by spectral clustering from the definitional framework of the Longman Dictionary of Contemporary English (LDOCE, see Chapter 13 of McArtur 1998), and Section 4 shows how to solve the problem outlined above by treating m and n not as a vectors but as transformations."
      ]
    },
    {
      "heading": "2 Diagnostic properties of additive",
      "text": [
        "decomposition The standard model of lexical decomposition (Katz and Fodor, 1963) divides lexical meaning in a systematic component, given by a tree of (generally binary) features, and an accidental component they call the distinguisher.",
        "Figure 1 gives an example.",
        "This representation has several advantages: for example bachelor3 ?holder of a BA or BSc degree?",
        "neatly escapes being male by definition.",
        "We tested which putative semantic features like GENDER are captured by CVSMs.",
        "We assume that the difference between two vectors, for antonyms, dis-tills the actual property which is the opposite in each member of a pair of antonyms.",
        "So, for example, for a set of male and female words, such as xking, queeny, xactor, actressy, etc., the difference between words in each pair should represent the idea of gender.",
        "To test the hypothesis, we as"
      ]
    },
    {
      "heading": "GOOD VERTICAL",
      "text": [
        "safe out raise level peace war tall short pleasure pain rise fall ripe green north south defend attack shallow deep conserve waste ascending descending affirmative negative superficial profound"
      ]
    },
    {
      "heading": "and VERTICAL",
      "text": [
        "sociated antonymic word pairs from the WordNet (Miller, 1995) to 26 classes e.g. END/BEGINNING, GOOD/BAD, .",
        ".",
        ".",
        ", see Table 1 and Table 3 for examples.",
        "The intuition to be tested is that the first member of a pair relates to the second one in the same way among all pairs associated to the same feature.",
        "For k pairs ~xi, ~yi we are looking for a common vector ~a such that ~xi ?",
        "~yi ?",
        "~a (1) Given the noise in the embedding, it would be naive in the extreme to assume that (1) can be a strict identity.",
        "Rather, our interest is with the best ~a which minimizes the error",
        "As is well known, E will be minimal when ~a is chosen as the arithmetic mean of the vectors ~xi ?",
        "~yi.",
        "The question is simply the following: is the minimal Em any better than what we could expect from a bunch of random ~xi and ~yi?",
        "Since the sets are of different sizes, we took 100 random pairings of the words appearing on either sides of the pairs to estimate the error distribution, computing the minima of",
        "For each distribution, we computed the mean and the variance ofErrrand, and checked whether the error of the correct pairing, Err is at least 2 or 3 's away from the mean.",
        "Table 2 summarizes our results for three embeddings: the original and the scaled HLBL (Mnih and Hinton, 2009) and SENNA (Collobert et al., 2011).",
        "The first two columns give the number of pairs considered for a feature and the name of the"
      ]
    },
    {
      "heading": "PRIMARY ANGULAR",
      "text": [
        "leading following square round preparation resolution sharp flat precede follow curved straight intermediate terminal curly straight antecedent subsequent angular rounded precede succeed sharpen soften question answer angularity roundness",
        "feature.",
        "For each of the three embeddings, we report the errorErr of the unpermuted arrangement, the mean m and variance ?",
        "of the errors obtained under random permutations, and the ratio",
        "Horizontal lines divide the features to three groups: for the upper group, r ?",
        "3 for at least two of the three embeddings, and for the middle group r ?",
        "2 for at least two.",
        "For the features above the first line we conclude that the antonymic relations are well captured by the embeddings, and for the features below the second line we assume, conservatively, that they are not.",
        "(In fact, looking at the first column of Table 2 suggests that the lack of significance at the bottom rows may be due primarily to the fact that WordNet has more antonym pairs for the features that performed well on this test than for those features that performed badly, but we didn't want to start creating antonym pairs manually.)",
        "For example, the putative sets in Table 3 does not meet the criterion and gets rejected."
      ]
    },
    {
      "heading": "3 Embedding based on conceptual",
      "text": [
        "representation The 4lang embedding is created in a manner that is notably different from the others.",
        "Our input is a graph whose nodes are concepts, with edges running from A to B iff B is used in the definition of A.",
        "The base vectors are obtained by the spectral clustering method pioneered by (Ng et al., 2001): the incidence matrix of the conceptual network is replaced by an affinity matrix whose ij-th element is formed by computing the cosine distance of the ith and jth row of the original matrix, and the first few (in our case, 100) eigenvectors are used as a basis.",
        "Since the concept graph includes the entire Longman Defining Vocabulary (LDV), each LDV",
        "with 100 random pairings, and the ratio r ?",
        "|Err?m|?",
        "for different features and embeddings element wi corresponds to a base vector bi.",
        "For the vocabulary of the whole dictionary, we simply take the Longman definition of any word w, strip out the stopwords (we use a small list of 19 elements taken from the top of the frequency distribution), and form V pwq as the sum of the bi for the wis that appeared in the definition of w (with multiplicity).",
        "We performed the same computations based on this embedding as in Section 2: the results are presented in Table 4.",
        "Judgment columns under the four three embeddings in the previous section and 4lang are highly correlated, see table 5.",
        "Unsurprisingly, the strongest correlation is between the original and the scaled HLBL results.",
        "Both the original and the scaled HLBL correlate notably better with 4lang than with SENNA, making the latter the odd one out."
      ]
    },
    {
      "heading": "4 Applicativity",
      "text": [
        "So far we have seen that a dictionary-based embedding, when used for a purely semantic task, the analysis of antonyms, does about as well as the more standard embeddings based on cooccurrence data.",
        "Clearly, a CVSM could be obtained by the same procedure from any machine-readable dic",
        "different embeddings tionary.",
        "Using LDOCE is computationally advantageous in that the core vocabulary is guaranteed to be very small, but finding the eigenvectors for an 80k by 80k sparse matrix would also be within CPU reach.",
        "The main advantage of starting with a conceptual graph lies elsewhere, in the possibility of investigating the function application issue we started out with.",
        "The 4lang conceptual representation relies on a small number of basic elements, most of which correspond to what are called unary predicates in logic.",
        "We have argued elsewhere (Kornai, 2012) that meaning of linguistic expressions can be formalized using predicates with at most two arguments (there are no ditransitive or higher arity predicates on the semantic side).",
        "The x and y slots of binary elements such as x has y or x kill y, (Kornai and Makrai 2013) receive distinct labels called NOM and ACC in case grammar (Fillmore, 1977); 1 and 2 in relational grammar (Perlmutter, 1983); or AGENT and PATIENT in linking theory (Ostler, 1979).",
        "The label names themselves are irrelevant, what matters is that these elements are not part of the lexicon the same way as the words are, but rather constitute transformations of the vector space.",
        "Here we will use the binary predicate x has y to reformulate the puzzle we started out with, an-alyzing queen of England, king of Italy etc.",
        "in a compositional (additive) manner, but escaping the commutativity problem.",
        "For the sake of concreteness we use the traditional assumption that it is the king who possesses the realm and not the other way around, but what follows would apply just as well if the roles were reversed.",
        "What we are interested in is the asymmetry of expressions like Albert has England or Elena has Italy, in contrast to largely symmetric predicates.",
        "Albert marries Victoria will be true if and only if Victoria marries Albert is true, but from James has a martini it does not follow that ?A martini has James.",
        "While the fundamental approach of CVSM is quite correct in assuming that nouns (unaries) and verbs (binaries) can be mapped on the same space, we need two transformations T1 and T2 to regulate the linking of arguments.",
        "A form like James kills has James as agent, so we compute V (James)`T1V (kill), while kills James is obtained as V (James)`T2V (kill).",
        "The same two transforms can distinguish agent and patient relatives as in the man that killed James versus the man that James killed.",
        "Such forms are compositional, and in languages that have overt case markers, even ?surface compositional?",
        "(Hausser, 1984).",
        "All input and outputs are treated as vectors in the same space where the atomic lexical entries get mapped, but the applicative paradox we started out with goes away.",
        "As long as the transforms T1 (n) and T2 (m) take different values on kill, has, or any other binary, the meanings are kept separate."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Makrai did the work on antonym set testing, Nemeskey built the embedding, Kornai advised.",
        "We would like to thank Zso?fia Tardos (BUTE) and the anonymous reviewers for useful comments.",
        "Work supported by OTKA grant #82333."
      ]
    }
  ]
}
