{
  "info": {
    "authors": [
      "Karl Stratos",
      "Alexander Rush",
      "Shay B. Cohen",
      "Michael Collins"
    ],
    "book": "CoNLL",
    "id": "acl-W13-3507",
    "title": "Spectral Learning of Refinement HMMs",
    "url": "https://aclweb.org/anthology/W13-3507",
    "year": 2013
  },
  "references": [
    "acl-D07-1094",
    "acl-E12-1042",
    "acl-N13-1015",
    "acl-P05-1010",
    "acl-P06-1055",
    "acl-P12-1024",
    "acl-P92-1017"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We derive a spectral algorithm for learning the parameters of a refinement HMM.",
        "This method is simple, efficient, and can be applied to a wide range of supervised sequence labeling tasks.",
        "Like other spectral methods, it avoids the problem of local optima and provides a consistent estimate of the parameters.",
        "Our experiments on a phoneme recognition task show that when equipped with informative feature functions, it performs significantly better than a supervised HMM and competitively with EM."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Consider the task of supervised sequence labeling.",
        "We are given a training set where the j?th training example consists of a sequence of observations x(j)1 ...x(j)N paired with a sequence of labels a(j)1 ...a(j)N and asked to predict the correct labels on a test set of observations.",
        "A common approach is to learn a joint distribution over sequences p(a1 .",
        ".",
        ".",
        "aN , x1 .",
        ".",
        ".",
        "xN ) as a hidden Markov model (HMM).",
        "The downside of HMMs is that they assume each label ai is independent of labels before the previous label ai?1.",
        "This independence assumption can be limiting, particularly when the label space is small.",
        "To relax this assumption we can refine each label ai with a hidden state hi, which is not observed in the training data, and model the joint distribution p(a1 .",
        ".",
        ".",
        "aN , x1 .",
        ".",
        ".",
        "xN , h1 .",
        ".",
        ".",
        "hN ).",
        "This refinement HMM (R-HMM), illustrated in figure 1, is able to propagate information forward through the hidden state as well as the label.",
        "Unfortunately, estimating the parameters of an",
        "R-HMM is complicated by the unobserved hidden variables.",
        "A standard approach is to use the expectation-maximization (EM) algorithm which",
        "representation where labels and hidden states are intertwined.",
        "has no guarantee of finding the global optimum of its objective function.",
        "The problem of local optima prevents EM from yielding statistically consistent parameter estimates: even with very large amounts of data, EM is not guaranteed to estimate parameters which are close to the ?correct?",
        "model parameters.",
        "In this paper, we derive a spectral algorithm for learning the parameters of R-HMMs.",
        "Unlike EM, this technique is guaranteed to find the true parameters of the underlying model under mild conditions on the singular values of the model.",
        "The algorithm we derive is simple and efficient, relying on singular value decomposition followed by standard matrix operations.",
        "We also describe the connection of R-HMMs to L-PCFGs.",
        "Cohen et al. (2012) present a spectral algorithm for L-PCFG estimation, but the na?",
        "?ve transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque.",
        "We therefore work through the non-trivial derivation the spectral algorithm for R-HMMs.",
        "We note that much of the prior work on spectral algorithms for discrete structures in NLP has shown limited experimental success for this family of algorithms (see, for example, Luque et al., 2012).",
        "Our experiments demonstrate empirical",
        "success for the R-HMM spectral algorithm.",
        "The spectral algorithm performs competitively with EM on a phoneme recognition task, and is more stable with respect to the number of hidden states.",
        "Cohen et al. (2013) present experiments with a parsing algorithm and also demonstrate it is competitive with EM.",
        "Our set of experiments comes as an additional piece of evidence that spectral algorithms can function as a viable, efficient and more principled alternative to the EM algorithm."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Recently, there has been a surge of interest in spectral methods for learning HMMs (Hsu et al., 2012; Foster et al., 2012; Jaeger, 2000; Siddiqi et al., 2010; Song et al., 2010).",
        "Like these previous works, our method produces consistent parameter estimates; however, we estimate parameters for a supervised learning task.",
        "Balle et al. (2011) also consider a supervised problem, but our model is quite different since we estimate a joint distribution p(a1 .",
        ".",
        ".",
        "aN , x1 .",
        ".",
        ".",
        "xN , h1 .",
        ".",
        ".",
        "hN ) as opposed to a conditional distribution and use feature functions over both the labels and observations of the training data.",
        "These feature functions also go beyond those previously employed in other spectral work (Siddiqi et al., 2010; Song et al., 2010).",
        "Experiments show that features of this type are crucial for performance.",
        "Spectral learning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b).",
        "Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs.",
        "This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm.",
        "For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992).",
        "Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing."
      ]
    },
    {
      "heading": "3 The R-HMM Model",
      "text": [
        "We decribe in this section the notation used throughout the paper and the formal details of R-HMMs."
      ]
    },
    {
      "heading": "3.1 Notation",
      "text": [
        "We distinguish row vectors from column vectors when such distinction is necessary.",
        "We use a superscript > to denote the transpose operation.",
        "We write [n] to denote the set {1, 2, .",
        ".",
        ".",
        ", n} for any integer n ?",
        "1.",
        "For any vector v ?",
        "Rm, diag(v) ?",
        "Rm?m is a diagonal matrix with entries v1 .",
        ".",
        ".",
        "vm.",
        "For any statement S , we use [[S]] to refer to the indicator function that returns 1 if S is true and 0 otherwise.",
        "For a random variable X , we use E[X] to denote its expected value.",
        "A tensor C ?",
        "Rm?m?m is a set of m3 values Ci,j,k for i, j, k ?",
        "[m].",
        "Given a vector v ?",
        "Rm, we define C(v) to be the m ?",
        "m matrix"
      ]
    },
    {
      "heading": "3.2 Definition of an R-HMM",
      "text": [
        "An R-HMM is a 7-tuple ?l,m, n, pi, o, t, f?",
        "for integers l,m, n ?",
        "1 and functions pi, o, t, f where ?",
        "[l] is a set of labels.",
        "?",
        "[m] is a set of hidden states.",
        "?",
        "[n] is a set of observations.",
        "?",
        "pi(a, h) is the probability of generating a ?",
        "[l] and h ?",
        "[m] in the first position in the labeled sequence.",
        "?",
        "o(x|a, h) is the probability of generating x ?",
        "[n], given a ?",
        "[l] and h ?",
        "[m].",
        "?",
        "t(b, h?|a, h) is the probability of generating b ?",
        "[l] and h?",
        "?",
        "[m], given a ?",
        "[l] and h ?",
        "[m].",
        "?",
        "f(?|a, h) is the probability of generating the stop symbol ?, given a ?",
        "[l] and h ?",
        "[m].",
        "See figure 1(b) for an illustration.",
        "At any time step of a sequence, a label a is associated with a hidden state h. By convention, the end of an R-HMM sequence is signaled by the symbol ?.",
        "For the subsequent illustration, let N be the length of the sequence we consider.",
        "A full sequence consists of labels a1 .",
        ".",
        ".",
        "aN , observations",
        "t(ai+1, hi+1|ai, hi)?",
        "f(?|aN , hN )",
        "Input: a sequence of observations x1 .",
        ".",
        ".",
        "xN ; operators?",
        "A skeletal sequence consists of labels a1 .",
        ".",
        ".",
        "aN and observations x1 .",
        ".",
        ".",
        "xN without hidden states.",
        "Under the model, it has probability",
        "An equivalent definition of an R-HMM is given by organizing the parameters in matrix form.",
        "Specifically, an R-HMM has parameters?",
        "pia, oax, T b|a, fa ?",
        "where pia ?",
        "Rm is a column vector, oax is a row vector, T b|a ?",
        "Rm?m is a matrix, and fa ?",
        "Rm is a row vector, defined for all a, b ?",
        "[l] and x ?",
        "[n].",
        "Their entries are set to",
        "?",
        "[pia]h = pi(a, h) for h ?",
        "[m] ?",
        "[oax]h = o(x|a, h) for h ?",
        "[m] ?",
        "[T b|a]h?,h = t(b, h?|a, h) for h, h?",
        "?",
        "[m] ?",
        "[fa]h = f(?|a, h) for h ?",
        "[m]"
      ]
    },
    {
      "heading": "4 The Forward-Backward Algorithm",
      "text": [
        "Given an observation sequence x1 .",
        ".",
        ".",
        "xN , we want to infer the associated sequence of labels under an R-HMM.",
        "This can be done by computing the",
        "for all labels a ?",
        "[l] and positions i ?",
        "[N ].",
        "Then the most likely label at each position i is given by a?i = arg maxa?",
        "[l] ?",
        "(a, i) The marginals can be computed using a tensor variant of the forward-backward algorithm, shown in figure 2.",
        "The algorithm takes additional quanti",
        "ties ?Cb|a, C?|a, c1a, cax ?",
        "called the operators: ?",
        "Tensors Cb|a ?",
        "Rm?m?m for a, b ?",
        "[l] ?",
        "Tensors C?|a ?",
        "R1?m?m for a ?",
        "[l] ?",
        "Column vectors c1a ?",
        "Rm for a ?",
        "[l] ?",
        "Row vectors cax ?",
        "Rm for a ?",
        "[l] and x ?",
        "[n]",
        "The following proposition states that these operators can be defined in terms of the R-HMM parameters to guarantee the correctness of the algorithm.",
        "Proposition 4.1.",
        "Given an R-HMM with parameters ?pia, oax, T b|a, fa ?, for any vector v ?",
        "Rm define the operators:",
        "Then the algorithm in figure 2 correctly computes marginals ?",
        "(a, i) under the R-HMM.",
        "The proof is an algebraic verification and deferred to the appendix.",
        "Note that the running time of the algorithm as written is O(l2m3N).1 Proposition 4.1 can be generalized to the following theorem.",
        "This theorem implies that the operators can be linearly transformed by some invertible matrices as long as the transformation leaves the embedded R-HMM parameters intact.",
        "This observation is central to the derivation of the spectral algorithm which estimates the linearly transformed operators but not the actual R-HMM parameters.",
        "Theorem 4.1.",
        "Given an R-HMM with parameters?",
        "pia, oax, T b|a, fa ?, assume that for each a ?",
        "[l] we have invertible m ?m matrices Ga and Ha.",
        "For any vector v ?",
        "Rm define the operators:",
        "Then the algorithm in figure 2 correctly computes marginals ?",
        "(a, i) under the R-HMM.",
        "The proof is similar to that of Cohen et al. (2012).",
        "1We can reduce the complexity to O(l2m2N) by pre-computing the matricesCb|a(cax) for all a, b ?",
        "[l] and x ?",
        "[n] after parameter estimation."
      ]
    },
    {
      "heading": "5 Spectral Estimation of R-HMMs",
      "text": [
        "In this section, we derive a consistent estimator for the operators ?Cb|a, C?|a, c1a, cax ?",
        "in theorem 4.1 through the use of singular-value decomposition (SVD) followed by the method of moments.",
        "Section 5.1 describes the decomposition of the R-HMM model into random variables which are used in the final algorithm.",
        "Section 5.2 can be skimmed through on the first reading, especially if the reader is familiar with other spectral algorithms.",
        "It includes a detailed account of the derivation of the R-HMM algorithm.",
        "For a first reading, note that an R-HMM sequence can be seen as a right-branching L-PCFG tree.",
        "Thus, in principle, one can convert a sequence into a tree and run the inside-outside algorithm of Cohen et al. (2012) to learn the parameters of an R-HMM.",
        "However, projecting this transformation into the spectral algorithm for L-PCFGs is cumbersome and unintuitive.",
        "This is analogous to the case of the Baum-Welch algorithm for HMMs (Rabiner, 1989), which is a special case of the inside-outside algorithm for PCFGs (Lari and Young, 1990)."
      ]
    },
    {
      "heading": "5.1 Random Variables",
      "text": [
        "We first introduce the random variables underlying the approach then describe the operators based on these random variables.",
        "From p(a1 .",
        ".",
        ".",
        "aN , x1 .",
        ".",
        ".",
        "xN , h1 .",
        ".",
        ".",
        "hN ), we draw an R-HMM sequence (a1 .",
        ".",
        ".",
        "aN , x1 .",
        ".",
        ".",
        "xN , h1 .",
        ".",
        ".",
        "hN ) and choose a time step i uniformly at random from [N ].",
        "The random variables are then defined as",
        "Figure 3 shows the relationship between the random variables.",
        "They are defined in such a way that the future is independent of the past and the present is independent of the destiny conditioning on the current node's label and hidden state.",
        "Next, we require a set of feature functions over the random variables.",
        "random variables over observed quantities so that conditioning on the current node, (a) the future F1 is independent of the past P and (b) the present R is independent of the density D.",
        "We will see that the feature functions should be chosen to capture the influence of the hidden states.",
        "For instance, they might track the next label, the previous observation, or important combinations of labels and observations.",
        "Finally, we require projection matrices",
        "defined for all labels a ?",
        "[l].",
        "These matrices will project the feature vectors of ?, ?, ?, and ?",
        "from (d1, d2, d3, d4)-dimensional spaces to an m-dimensional space.",
        "We refer to this reduced dimensional representation by the following random variables:",
        "Note that they are all vectors in Rm."
      ]
    },
    {
      "heading": "5.2 Estimation of the Operators",
      "text": [
        "Since F 1, F 2, P , R, and D do not involve hidden variables, the following quantities can be directly estimated from the training data of skeletal sequences.",
        "For this reason, they are called observable blocks:",
        "The main result of this paper is that under certain conditions, matrices ?a and ?a are invertible and the operators ?Cb|a, C?|a, c1a, cax ?",
        "in theorem 4.1 can be expressed in terms of these observable blocks.",
        "To derive this result, we use the following definition to help specify the conditions on the expectations of the feature functions.",
        "Definition.",
        "For each a ?",
        "[l], define matrices",
        "In addition, let ?a ?",
        "Rm?m be a diagonal matrix with [?a]h,h = P (H1 = h|A1 = a).",
        "We now state the conditions for the correctness of Eq.",
        "(1-4).",
        "For each label a ?",
        "[l], we require that Condition 6.1 Ia, Ja,Ka,W a have rank m. Condition 6.2 [?a]h,h > 0 for all h ?",
        "[m].",
        "The conditions lead to the following proposition.",
        "Proposition 5.1.",
        "Assume Condition 6.1 and 6.2 hold.",
        "For all a ?",
        "[l], define matrices",
        "Let ua1 .",
        ".",
        ".",
        "uam ?",
        "Rd1 and va1 .",
        ".",
        ".",
        "vam ?",
        "Rd2 be the top m left and right singular vectors of ?a.",
        "Similarly, let la1 .",
        ".",
        ".",
        "lam ?",
        "Rd3 and ra1 .",
        ".",
        ".",
        "ram ?",
        "Rd4 be the top m left and right singular vectors of ?a.",
        "Define projection matrices",
        "are invertible.",
        "The proof resembles that of lemma 2 of Hsu et al. (2012).",
        "Finally, we state the main result that shows?",
        "Cb|a, C?|a, c1a, cax ?",
        "in Eq.",
        "(1-4) using the projections from proposition 5.1 satisfy theorem 4.1.",
        "A sketch of the proof is deferred to the appendix.",
        "Theorem 5.1.",
        "Assume conditions 6.1 and 6.2 hold.",
        "Let ??a,?a,?a,?a?",
        "be the projection matrices from proposition 5.1.",
        "Then the operators in Eq.",
        "(1-4) satisfy theorem 4.1.",
        "In summary, these results show that with the proper selection of feature functions, we can construct projection matrices ??a,?a,?a,?a?",
        "to obtain operators ?Cb|a, C?|a, c1a, cax ?",
        "which satisfy the conditions of theorem 4.1."
      ]
    },
    {
      "heading": "6 The Spectral Estimation Algorithm",
      "text": [
        "In this section, we give an algorithm to estimate the operators ?Cb|a, C?|a, c1a, cax ?",
        "from samples of skeletal sequences.",
        "Suppose the training set consists of M skeletal sequences (a(j), x(j)) for j ?",
        "[M ].",
        "ThenM samples of the random variables can be derived from this training set as follows ?",
        "At each j ?",
        "[M ], choose a position ij uniformly at random from the positions in (a(j), x(j)).",
        "Sample the random variables (X,A1, A2, F1, F2, P,R,D,B) using the procedure defined in section 5.1.",
        "This process yields M samples",
        "and obtain their singular vectors via an SVD.",
        "Use the top m singular vectors to construct projections?",
        "The algorithm in figure 4 shows how to derive estimates of the observable representations from these samples.",
        "It first computes the projection matrices ??a,?a,?a,?a?",
        "for each label a ?",
        "[l] by computing empirical estimates of ?a1 and ?a2 in proposition 5.1, calculating their singular vectors via an SVD, and setting the projections in terms of these singular vectors.",
        "These projection matrices are then used to project (d1, d2, d3, d4)",
        "for all j ?",
        "[M ].",
        "It then computes correlation between these vectors in this lower dimensional space to estimate the observable blocks which are used to obtain the operators as in Eq.",
        "(1-4).",
        "These operators can be used in algorithm 2 to compute marginals.",
        "As in other spectral methods, this estimation algorithm is consistent, i.e., the marginals ??",
        "(a, i) computed with the estimated operators approach the true marginal values given more data.",
        "For details, see Cohen et al. (2012) and Foster et al. (2012)."
      ]
    },
    {
      "heading": "7 Experiments",
      "text": [
        "We apply the spectral algorithm for learning R-HMMs to the task of phoneme recognition.",
        "The goal is to predict the correct sequence of phonemes a1 .",
        ".",
        ".",
        "aN for a given a set of speech frames x1 .",
        ".",
        ".",
        "xN .",
        "Phoneme recognition is often modeled with a fixed-structure HMM trained with EM, which makes it a natural application for spectral training.",
        "We train and test on the TIMIT corpus of spoken language utterances (Garofolo and others, 1988).",
        "The label set consists of l = 39 English phonemes following a standard phoneme set (Lee and Hon, 1989).",
        "For training, we use the sx and si utterances of the TIMIT training section made up of",
        "recognition.",
        "The simplest features look only at the current label and observation.",
        "Other features indicate the previous phoneme type used before ai (pp), the next phoneme type used after ai (np), and the relative position (beginning, middle, or end) of ai within the current phoneme (pos).",
        "The figure gives a typical segment of the phoneme sequence a1 .",
        ".",
        ".",
        "aN M = 3696 utterances.",
        "The parameter estimate is smoothed using the method of Cohen et al. (2013).",
        "Each utterance consists of a speech signal aligned with phoneme labels.",
        "As preprocessing, we divide the signal into a sequence of N overlapping frames, 25ms in length with a 10ms step size.",
        "Each frame is converted to a feature representation using MFCC with its first and second derivatives for a total of 39 continuous features.",
        "To discretize the problem, we apply vector quantization using euclidean k-means to map each frame into n = 10000 observation classes.",
        "After preprocessing, we have 3696 skeletal sequence with a1 .",
        ".",
        ".",
        "aN as the frame-aligned phoneme labels and x1 .",
        ".",
        ".",
        "xN as the observation classes.",
        "For testing, we use the core test portion of TIMIT, consisting of 192 utterances, and for development we use 200 additional utterances.",
        "Accuracy is measured by the percentage of frames labeled with the correct phoneme.",
        "During inference, we calculate marginals ?",
        "for each label at each position i and choose the one with the highest marginal probability, a?i = arg maxa?",
        "[l] ?",
        "(a, i).",
        "The spectral method requires defining feature functions ?, ?, ?, and ?.",
        "We use binary-valued feature vectors which we specify through features templates, for instance the template ai ?",
        "xi corresponds to binary values for each possible label and output pair (ln binary dimensions).",
        "Figure 6 gives the full set of templates.",
        "These feature functions are specially for the phoneme labeling task.",
        "We note that the HTK baseline explicitly models the position within the current"
      ]
    },
    {
      "heading": "Method Accuracy",
      "text": [
        "ure 5).",
        "The improvement of the spectral method over the EM baseline is significant at the p ?",
        "0.05 level (and very close to significant at p ?",
        "0.01, with a precise value of p ?",
        "0.0104).",
        "phoneme as part of the HMM structure.",
        "The spectral method is able to encode similar information naturally through the feature functions.",
        "We implement several baseline for phoneme recognition: UNIGRAM chooses the most likely label, arg maxa?",
        "[l] p(a|xi), at each position; HMM is a standard HMM trained with maximum-likelihood estimation; EM(m) is an R-HMM with m hidden states estimated using EM; and SPECTRAL(m) is an R-HMM with m hidden states estimated with the spectral method described in this paper.",
        "We also compare to HTK, a fixed-structure HMM with three segments per phoneme estimated using EM with the HTK speech toolkit.",
        "See Young et al. (2006) for more details on this method.",
        "An important consideration for both EM and the spectral method is the number of hidden states m in the R-HMM.",
        "More states allow for greater label refinement, with the downside of possible overfit-ting and, in the case of EM, more local optima.",
        "To determine the best number of hidden states, we optimize both methods on the development set for a range of m values between 1 to 32.",
        "For EM,",
        "we run 200 training iterations on each value of m and choose the iteration that scores best on the development set.",
        "As the spectral algorithm is non-iterative, we only need to evaluate the development set once per m value.",
        "Figure 5 shows the development accuracy of the two method as we adjust the value of m. EM accuracy peaks at 4 hidden states and then starts degrading, whereas the spectral method continues to improve until 24 hidden states.",
        "Another important consideration for the spectral method is the feature functions.",
        "The analysis suggests that the best feature functions are highly informative of the underlying hidden states.",
        "To test this empirically we run spectral estimation with a reduced set of features by ablating the templates indicating adjacent phonemes and relative position.",
        "Figure 7 shows that removing these features does have a significant effect on development accuracy.",
        "Without either type of feature, development accuracy drops by 1.5%.",
        "We can interpret the effect of the features in a more principled manner.",
        "Informative features yield greater singular values for the matrices ?a1 and ?a2, and these singular values directly affect the sample complexity of the algorithm; see Cohen et al. (2012) for details.",
        "In sum, good feature functions lead to well-conditioned ?a1 and ?a2, which in turn require fewer samples for convergence.",
        "Figure 8 gives the final performance for the baselines and the spectral method on the TIMIT test set.",
        "For EM and the spectral method, we use best performing model from the development data, 4 hidden states for EM and 24 for the spectral method.",
        "The experiments show that R-HMM models score significantly better than a standard HMM and comparatively to the fixed-structure HMM.",
        "In training the R-HMM models, the spectral method performs competitively with EM while avoiding the problems of local optima."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "This paper derives a spectral algorithm for the task of supervised sequence labeling using an R-HMM.",
        "Unlike EM, the spectral method is guaranteed to provide a consistent estimate of the parameters of the model.",
        "In addition, the algorithm is simple to implement, requiring only an SVD of the observed counts and other standard matrix operations.",
        "We show empirically that when equipped with informative feature functions, the spectral method performs competitively with EM on the task of phoneme recognition.",
        "Appendix Proof of proposition 4.1.",
        "At any time step i ?",
        "[N ] in the algorithm in figure 2, for all label a ?",
        "[l] we have a column vector ?ia ?",
        "Rm and a row vector ?ia ?",
        "Rm.",
        "The value of these vectors at each index h ?",
        "[m] can be verified as",
        "which is the value of the marginal ?",
        "(a, i).",
        "Proof of theorem 5.1.",
        "It can be verified that c1a = Gapia.",
        "For the others, under the conditional independence illustrated in figure 3 we can decompose the observable blocks in terms of the R-HMM parameters and invertible matrices",
        "using techniques similar to those sketched in Cohen et al. (2012).",
        "By proposition 5.1, ?a and ?a are invertible, and these observable blocks yield the operators that satisfy theorem 4.1 when placed in Eq.",
        "(1-3)."
      ]
    }
  ]
}
