{
  "info": {
    "authors": [
      "Diarmuid Ó Séaghdha",
      "Simone Teufel"
    ],
    "book": "COLING",
    "id": "acl-C14-1002",
    "title": "Unsupervised learning of rhetorical structure with un-topic models",
    "url": "https://aclweb.org/anthology/C14-1002",
    "year": 2014
  },
  "references": [
    "acl-D08-1035",
    "acl-D08-1036",
    "acl-D10-1111",
    "acl-D10-1114",
    "acl-D11-1025",
    "acl-I08-1050",
    "acl-J00-3005",
    "acl-J02-4002",
    "acl-N07-1040",
    "acl-N09-1042",
    "acl-N10-1020",
    "acl-N12-1003",
    "acl-N13-1019",
    "acl-P06-1003",
    "acl-P09-1024",
    "acl-W10-1913"
  ],
  "sections": [
    {
      "text": [
        "Unsupervised learning of rhetorical structure with un-topic models Diarmuid ?",
        "O S ?",
        "eaghdha Computer Laboratory University of Cambridge Cambridge, UK do242@cam.ac.uk Simone Teufel Computer Laboratory University of Cambridge Cambridge, UK sht25@cam.ac.uk",
        "Abstract",
        "In this paper we investigate whether unsupervised models can be used to induce conventional aspects of rhetorical language in scientific writing.",
        "We rely on the intuition that the rhetorical language used in a document is general in nature and independent of the document's topic.",
        "We describe a Bayesian latent-variable model that implements this intuition.",
        "In two empirical evaluations based on the task of argumentative zoning (AZ), we demonstrate that our generality hypothesis is crucial for distinguishing between rhetorical and topical language and that features provided by our unsupervised model trained on a large corpus can improve the performance of a supervised AZ classifier."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Scientific writing has many conventions.",
        "Some exist at the level of sentence construction, such as a preference for the passive voice or for deverbal nominalisations.",
        "Others relate to the high-level organisation of a paper: a typical paper at an NLP conference may be divided into sections covering the introduction, related work, methods, experimental results and conclusion.",
        "There are also intermediate levels of convention that use lexical and phrasal items to signal the role played by each part of the text in the argument the authors wish to construct.",
        "The theory of argumentative zoning (AZ) describes how a scientific article can be analysed in terms of text blocks (or zones) that share a rhetorical function (Teufel, 2010).",
        "For example: part of the article may consist of background information, another part may describe the aim of the research, other parts may report the authors?",
        "own work or compare that work to alternative approaches in the literature.",
        "Supervised computational systems can be trained to mark up the AZ structure of a text automatically (see Section 2); the output of such systems has been shown to aid summarisation and human browsing of the scientific literature (Teufel and Moens, 2002; Guo et al., 2011a; Contractor et al., 2012).",
        "However, supervised systems require manually annotated training data that must be created anew for each discipline (and language) before they can be deployed, while large quantities of unannotated text are often available.",
        "For this reason, there is considerable value in developing unsupervised systems that induce aspects of rhetorical structure from unannotated text.",
        "In this paper we advance a hypothesis about the generality of rhetorical language.",
        "We propose that the words and linguistic constructs used to express rhetorical function in a scientific paper are independent of the paper's topic.",
        "Naturally there will be some variation across research areas and there may be large differences across disciplines, but within a discipline we do not expect that the specific subject of a paper plays a significant role in how the authors construct their argument.",
        "For example, the following template could be used to generate an abstract for very many papers in NLP and other fields: The problem of has received a lot of attention because of its relevance to .",
        "CITATION proposed an approach based on the method of .",
        "In this paper we present a method for that has the following advantages over prior work: .",
        "We demonstrate the empirical effectiveness of our method by reporting experiments on data, where it outperforms the approach of CITATION by %.",
        "2 This leads us to the idea of two-stage ?recipes?",
        "for scientific papers, whereby the authors start with a framework of boilerplate text that matches the rhetorical argument they wish to make.",
        "The authors can then fill in the gaps with the substance of their research contribution.",
        "The two-stage model is of course an idealisation of how scientists construct their papers, but it is useful as an inspiration for a computational model that implements the generality hypothesis.",
        "We propose BOILERPLATE-LDA, a generative model that assigns responsibility for generating each word in an abstract to a document-specific topic model or to a rhetorical language model that is not specific to the document.",
        "Essentially, we induce argumentative structure from the parts of the text that are not well-explained by the topic model.",
        "Hence we describe BOILERPLATE-LDA as an ?un-topic model?.",
        "We evaluate our model in two settings: a clustering evaluation that treats BOILERPLATE-LDA as performing unsupervised argumentative zoning, and a downstream evaluation where the induced structure is not taken as explicitly modelling argumentative zones but is used to provide informative features for a supervised AZ classifier.",
        "In both cases, we show that BOILERPLATE-LDA performs well on a very challenging task.",
        "2 Related work There has been great interest in unsupervised learning among NLP researchers due to the availability of large amounts of unprocessed text through the Web, newswire providers, scientific repositories and other sources in contrast to the onerous requirements of creating task-specific manually annotated data for training supervised analysers.",
        "Particularly relevant to our work is the field of topic modelling, where Bayesian latent-variable models are used to induce meaningful generalisations from observations of co-occurrences.",
        "Blei et al. (2003) introduced Latent Dirichlet Allocation (LDA) as a model of thematic structure in documents, but subsequent work has adapted the general framework to many different purposes in modelling text as well as other kinds of data.",
        "This includes research on modelling aspects of document structure such as topic segmentation, implementing the intuitions that neighbouring blocks of text are coherent in the sense of lexical similarity (Purver et al., 2006; Gruber et al., 2007; Eisenstein and Barzilay, 2008; Du et al., 2013).",
        "The model most similar to ours (that we are aware of) is the model of Ritter et al. (2010), which captures dialogue acts and transitions between them in Twitter conversations.",
        "Despite the general popularity of unsupervised approaches, rhetorical analysis has generally been treated as a problem for supervised machine learning.",
        "Classification-based approaches to argumentative zoning typically use a sequence classifier such as a maximum-entropy Markov model or conditional random field (Teufel and Moens, 2002; Siddharthan and Teufel, 2007; Hirohata et al., 2008; Guo et al., 2010).",
        "Guo et al. (2011b) take a semi-supervised approach based on active learning and self-training.",
        "Two unsupervised approaches in the literature are Varga et al. (2012) and Reichart and Korhonen (2012).",
        "Varga et al. use a topic model variant called ZONE-LDA that assigns each sentence a latent variable index or ?topic?",
        "and assumes that the words in the sentence are generated from a distribution particular to the topic; in this situation each topic is assumed to correspond to a distinct argumentative zone.",
        "Such a model will have the effect of clustering sentences that share lexical items.",
        "Varga et al. also propose a model they call ZONE-LDA-B, in which some common words are assigned to a ?background?",
        "distribution that is independent of the sentence category; this model performs worse than ZONE-LDA in their evaluation.",
        "Reichart and Korhonen take an approach based on Markov random fields.",
        "They construct a graphical model in which sentence vertices are connected by potentials weighted according to adjacency and sentence similarity, as well as hand-defined rules about passivisation and sentence location.",
        "The papers cited in the two preceding paragraphs have focused on rhetorical analysis in scientific writing, yet there are many other textual genres where argumentation is conventionalised.",
        "For example, Burstein et al. (2003) identify building blocks analogous to AZ zones in the writing of English language learners and demonstrate that a supervised classification approach can be used to mark up their essays.",
        "Also in the educational domain, Madnani et al. (2012) train a supervised classifier to detect the ?shell?",
        "language that learners use to organise the high-level structure of their compositions; this is quite close to our idea of ?templates?",
        "or ?recipes?",
        "for scientific papers.",
        "Sauper and Barzilay (2009) and Chen et al. (2009) both present models that learn structural conventions in Wikipedia articles without relying on human annotation.",
        "Sauper and Barzilay's model induces the typical section structure of Wikipedia articles 3 about a specific entity type (e.g., Actors or Diseases) and retrieves web snippets relevant to each section for a target entity, before performing multidocument summarisation to produce a new entry for posting to Wikipedia.",
        "Chen et al. take a Bayesian segmentation approach to implicitly learn the topical section structure of articles and use a generalised Mallows model, a distribution over permutations, to identify a canonical ordering for sections.",
        "1 Other forms of general rhetorical analysis include Rhetorical Structure Theory (Mann and Thompson, 1988; Marcu, 2000), which captures local discourse relations between segments of text; RST provides a layer of analysis that is separate and complementary to more global schemes such as argumentative zoning.",
        "3 Intuitions The performance of unsupervised learning depends on how intuitions about the task are incorporated in the statistical model.",
        "Our approach relies on three main intuitions: Sentence similarity: All else being equal, we expect that lexically similar sentences will have similar purposes.",
        "At the same time, lexical similarity alone is not sufficient to capture shared argumentative function: all sentences in a paper about parsing will be similar to each other, while the introductory sentences of a parsing paper and a machine translation paper may share few similar lexical items.",
        "Adjacency: The theory of argumentative zones suggests that sentences with the same rhetorical function will often be grouped together into blocks.",
        "Additionally, we expect that authors will follow general conventions about the order of zones, e.g., starting with background and goal statements and progressing to results and conclusions.",
        "Generality: We expect that the language used to convey rhetorical function is independent of the topical content of the paper.",
        "Sentence similarity can be captured using standard lexical similarity measures or through the clustering effects of a topic model.",
        "The adjacency assumption can be implemented using a linear-chain sequence model such as a Hidden Markov Model.",
        "The ZONE-LDA approach of Varga et al. (2012) relies on sentence similarity alone.",
        "Reichart and Korhonen's (2012) model combines sentence similarity and adjacency.",
        "To the best of our knowledge, the generality hypothesis has not previously been investigated.",
        "The model we describe in Section 4 incorporates all three intuitions in its structure.",
        "4 Models The model we propose assumes that each word in a sentence is generated either from an LDA-style topic model or from a distribution associated with the rhetorical category assigned to the sentence.",
        "The former captures the subject matter of the document; the latter captures conventional language that is independent of the document's subject matter.",
        "The sentence categories are generated from a first-order Markov model.",
        "The assignment of responsibility for a word is implemented through a so-called ?switching variable?, a binary-valued latent variable.",
        "This is a commonly used mechanism for interpolating language models (Griffiths et al., 2004; Reisinger and Mooney, 2010; Ahmed and Xing, 2010); in many cases, the goal is to assign common words to a ?background?",
        "distribution that is not considered an object of interest from a topic modelling perspective.",
        "In our case it is this non-topical part of the text that is the object of interest.",
        "The dependencies between variables in our full BOILERPLATE-LDA model are shown by the plate diagram in Figure 1.",
        "The corresponding ?generative story?",
        "is as follows: 1 It would be interesting to swap in Chen et al.'s generalised Mallows model for the HMM-style ordering model in BOILERPLATE-LDA.",
        "The former has the advantage of capturing non-local ordering effects, while the latter has the advantage of not assuming a single canonical ordering.",
        "4 for topic t ?",
        "{1 .",
        ".",
        ".",
        "|T |} do (Draw a distribution over words) ?",
        "t ?",
        "Dirichlet(?)",
        "end for for zone z ?",
        "{1 .",
        ".",
        ".",
        "|Z|} do (Draw a distribution over words) ?",
        "z ?",
        "Dirichlet(?)",
        "(Draw a transition distribution) ?",
        "z ?",
        "Dirichlet(?)",
        "end for (Draw the switch distribution) ?",
        "?",
        "Beta(?",
        "0 , ?",
        "1 ) for doc d ?",
        "{1 .",
        ".",
        ".",
        "|D|} do (Draw a distribution over topics) ?",
        "d ?",
        "Dirichlet(?)",
        "for sentence s ?",
        "Sentences(d) do z s ?Multinomial(?",
        "z s?1 ) for word i ?Words(s) do (Draw a switch indicator) b i = Beta(?)",
        "if b i = 0 then (Draw a word from the zone-word distribution) w i ?Multinomial(?",
        "z s ) else (Draw a topic) t i ?Multinomial(?",
        "d ) (Draw a word from the topic-word distribution) w i ?Multinomial(?",
        "t i ) end if end for end for end for We train the model using Gibbs sampling.",
        "Due to Dirichlet-multinomial and beta-Bernoulli conjugacy it is relatively straightforward to integrate out the multinomial and Bernoulli distribution parameters ?, ?, ?",
        "and ?",
        "and derive update rules for a collapsed Gibbs sampler.",
        "Each iteration of the sampler visits each sentence in the corpus in turn, first sampling the sentence label assignment z s and then sampling for each word in the sentence the switch indicator b i and (if b i = 1) the topic assignment t i .",
        "The sentence label update is performed using what Gao and Johnson (2008) call a pointwise collapsed Gibbs sampler.",
        "Omitting hyperparameters for clarity, the sampling probabilities can be written as P (z i = z|z ?i ,w, b) ?",
        "f z i?1 ?z + ?",
        "z f z i?1 + ?",
        "z ?",
        "?",
        "z ?",
        "f z?z i+1 + I(z = z i+1 ) + ?",
        "z i+1 f ?i z + I(z = z i+1 ) + ?",
        "z ?",
        "?",
        "z ?",
        "?",
        "v?V ?",
        "(f ?i zv,b=0 + f s i v,b=0 + ?)",
        "?",
        "(f ?i z + f s i + ?|V |) (1) where f z?>z ?",
        "is the transition frequency from zone z to zone z ?",
        ", f z is the number of sentences assigned zone z; I(z = z i+1 ) has value 1 if the two zone assignments are equal and 0 otherwise; V is the vocabulary of word types; f zv,b=0 is the number of words of type z that appear in sentences assigned zone z and whose corresponding switch variable has value 0; f s i v,b=0 is the number of words of type v that appear in sentence s i and whose corresponding switch variable has value 0; the superscript ?i indicates that the frequency is calculated over all sentences except s i .",
        "We introduce observed start and end state variables z s and z e to handle the boundaries at the beginning and end of each document.",
        "5 zs z 1 z 2 z n z e w w w t t t Words(s 1 ) Words(s 2 ) Words(s n ) b b b ?",
        "?",
        "D ?",
        "?",
        "0 ?",
        "1 ??",
        "Z ?",
        "?",
        "Z ?",
        "?",
        "T Figure 1: Plate diagram for BOILERPLATE-LDA The topic and switch variables for each word are sampled in a blocked fashion; the sampling probabilities are similar to the standard LDA updates: P (b j = 0, t j = ?|z ?j ,b ?j , t,w) ?",
        "(f ?j b=0 + ?",
        "0 ) f ?j z i w j ,b=0 + ?",
        "f z i ,b=0 + |V |?",
        "P (b j = 1, t j = t|z ?j ,b ?j , t,w) ?",
        "(f ?j b=1 + ?",
        "1 ) f ?j tw j + ?",
        "z f ?j w j ,b=1 + ?",
        "z ?",
        "?",
        "z ?",
        "f ?j zw j + ?",
        "f ?j z + |V |?",
        "P (b i = 0, t i 6= ?|z ?j ,b ?j , t,w) = 0 P (b i = 1, t i = ?|z ?j ,b ?j , t,w) = 0 (2) where we use j to index words and i to index sentences; f tw j is the number of words of type w j that are assigned topic t; the superscript ?j indicates that the frequency is calculated over all words except j.",
        "5 Experiments 5.1 Data For evaluation, we use a collection of abstracts compiled by Guo et al. (2010).",
        "These abstracts had originally been collected in the context of semi-automated cancer risk assessment by searching PubMed for abstracts mentioning one or more of a list of chemicals known to have carcinogenic properties (Korhonen et al., 2009).",
        "Guo et al. annotated abstracts for five of these chemicals using an AZ scheme with seven categories: Background, Objective, Method, Result, Conclusion, Related work and Future work.",
        "2 In order to test whether our models can also perform over a large, heterogeneous dataset, we also used a collection of 129,595 abstracts taken from a collection of open-access journal articles.",
        "Preprocessing involved sentence splitting, tokenisation and part-of-speech tagging using the Stanford CoreNLP toolkit 3 and the removal of all tokens containing non-alphanumeric characters, all tokens of character length one 2 The annotated dataset has been made available at http://www.cl.cam.ac.uk/ ?",
        "yg244/abstract_az.html.",
        "3 http://nlp.stanford.edu/software/corenlp.shtml 6 and a small set of stop words.",
        "4 This left a training corpus of 16,841,280 tokens.",
        "5.2 Clustering Evaluation 5.2.1 Evaluation Our first quantitative evaluation investigates whether the zones induced by BOILERPLATE-LDA correspond to the argumentative zones identified by human theorists.",
        "We treat this as a clustering task with the gold standard provided by Guo et al.'s (2010) dataset.",
        "The clustering evaluation measures we use are the Adjusted Rand Index (Hubert and Arabie, 1985) and Adjusted Mutual Information (Vinh et al., 2010); both measures are normalised to have a maximum value of 1 and are adjusted for chance so that the expected score given to a random clustering is 0.",
        "This second property makes them conservative in comparison to other evaluation measures.",
        "We report results with the number of zones |Z| ?",
        "{10, 20, 50} and number of topics |T | ?",
        "{10, 20, 50, 100}; for each combination of settings we report the average evaluation score attained by three independent runs of the learning algorithm.",
        "5.2.2 Models For our evaluation, we test the following models: BOILERPLATE-LDA: Our full model, as described in Section 4.",
        "BOILERPLATE-LDA-MULT: A simplified model where the Markov dependencies between zone assignments are replaced by a flat multinomial; the probability of a zone is independent of the adjacent sentences.",
        "BOILERPLATE-LDA-NOTOPICS: A simplified model where all words in a sentence are generated from the zone distribution ?",
        "z s ; this is almost identical to Varga et al.'s (2012) ZONE-LDA model.",
        "K-MEANS: A standard k-means clustering model run until convergence.",
        "The features for each sentence consist of tf-idf-transformed lexical frequencies, part-of-speech tags and a location feature computed by dividing the abstract into 5 bins.",
        "The BOILERPLATE-LDA models are all trained for 1000 iterations of Gibbs sampling.",
        "The Dirichlet hyperparameters are re-estimated every 10 iterations; the topic hyperparameters ?",
        "are optimised using a fixed-point iteration to maximise the log-evidence (Minka, 2003; Wallach, 2008), while the other hyperparameters are sampled using Hamiltonian Monte Carlo (Neal, 2010).",
        "K-MEANS was run until convergence.",
        "5.2.3 Results Figure 2 gives an illustration of the zone representation induced at the end of one run of BOILERPLATE-LDA with the settings |Z| = 10, |T | = 100.",
        "Firstly, we list the most probable words for each zone (2a).",
        "While the model may not find a perfect match for the gold-standard inventory of argumentative zones, we can see that some induced zones describe standard methodology (8,9), others describe results and implications (1,3,7) and others describe motivations (2,5,6).",
        "Inspection of the transition matrix (2b) confirms our expectation that self-transitions have the highest probability; we also observed that the zones most frequently transitioned to from the start state are the motivational zones and the zones most frequently transitioned from to the end state are the results/implications zones.",
        "The example abstracts in Figure 3 illustrate how BOILERPLATE-LDA can be used to mark up the text of an abstract as ?boilerplate?",
        "or ?non-boilerplate?",
        "based on the values of the switch variables b i .",
        "4 The part-of-speech tags are not used by BOILERPLATE-LDA but they are used as features for other models.",
        "7 1 results, suggest, our, data, study, role, findings, we, between, indicate, important, studies 2 study, we, using, used, investigated, determine, present, between, investigate, analysis, aim 3 increased, significantly, levels, showed, found, observed, significant, after, compared, higher 4 two, sequence, we, found, region, sequences, we, three, identified, between, different, analysis 5 use, more, studies, study, used, however, important, health, most, treatment, clinical, potential 6 role, important, known, studies, however, shown, including, involved, mechanisms, cell 7 case, we, patient, report, rare, most, common, reported, presented, disease, associated, cause 8 CI, significantly, respectively, significant, between, group, mean, higher, compared, more, found 9 study, years, using, two, patients, included, total, group, three, data, after, used, collected, age 10 we, data, analysis, used, using, new, approach, based, method, information, developed, more (a) Most probable words for each zone From To Start 1 2 3 4 5 6 7 8 9 10 End Start 0.00 0.00 0.10 0.01 0.08 0.24 0.36 0.10 0.00 0.03 0.08 0.00 1 0.00 0.37 0.01 0.04 0.01 0.03 0.01 0.00 0.00 0.00 0.01 0.50 2 0.00 0.02 0.26 0.25 0.07 0.01 0.02 0.00 0.05 0.29 0.01 0.00 3 0.00 0.27 0.02 0.59 0.02 0.02 0.02 0.00 0.02 0.00 0.01 0.04 4 0.00 0.12 0.02 0.09 0.62 0.00 0.03 0.00 0.01 0.00 0.05 0.05 5 0.00 0.01 0.10 0.00 0.00 0.55 0.03 0.02 0.00 0.06 0.04 0.18 6 0.00 0.06 0.21 0.05 0.05 0.05 0.50 0.01 0.00 0.01 0.05 0.02 7 0.00 0.02 0.02 0.01 0.01 0.15 0.04 0.63 0.01 0.02 0.00 0.08 8 0.00 0.09 0.01 0.14 0.01 0.07 0.00 0.02 0.61 0.04 0.01 0.01 9 0.00 0.01 0.11 0.05 0.01 0.05 0.00 0.02 0.20 0.54 0.01 0.00 10 0.00 0.05 0.02 0.01 0.07 0.02 0.01 0.00 0.01 0.01 0.68 0.12 End 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 (b) Zone transition probabilities between adjacent sentences Figure 2: Zones induced by one run of BOILERPLATE-LDA (|Z| = 10, |T | = 100) 8 VASP: A Volumetric Analysis of Surface Properties Yields Insights into Protein-Ligand Binding Specificity Many algorithms that compare protein structures can reveal similarities that suggest related biological functions, even at great evolutionary distances.",
        "Proteins with related function often exhibit differences in binding specificity, but few algorithms identify structural variations that effect specificity.",
        "To address this problem, we describe the Volumetric Analysis of Surface Properties (VASP), a novel volumetric analysis tool for the comparison of binding sites in aligned protein structures.",
        "VASP uses solid volumes to represent protein shape and the shape of surface cavities, clefts and tunnels that are defined with other methods.",
        "Our approach, inspired by techniques from constructive solid geometry, enables the isolation of volumetrically conserved and variable regions within three dimensionally superposed volumes.",
        "We applied VASP to compute a comparative volumetric analysis of the ligand binding sites formed by members of the steroidogenic acute regulatory protein (StAR)-related lipid transfer (START) domains and the serine proteases.",
        "Within both families, VASP isolated individual amino acids that create structural differences between ligand binding cavities that are known to influence differences in binding specificity.",
        "Also, VASP isolated cavity subregions that differ between ligand binding cavities which are essential for differences in binding specificity.",
        "As such, VASP should prove a valuable tool in the studyof protein-ligand binding specificity.",
        "A new usage of functionalized oligodeoxynucleotide probe for site-specific modification of a guanine base within RNA Site-specific modification of RNA is of great significance to investigate RNA structure, function and dynamics.",
        "Recently, we reported a new method for sequence- and cytosine-selective chemical modification of RNA based on the functional group transfer reaction of the 1-phenyl-2-methylydene-1,3-diketone unit of the 6-thioguanosine base incorporated in the oligodeoxynucleotide probe.",
        "In this study, we describe that the functionality transfer rate is greatly enhanced and the selectivity is shifted to the guanine base when the reaction is performed under alkaline conditions.",
        "Detailed investigation indicated that the 2-amino group of the enolate form of rG is the reactant of the functionality transfer reaction.",
        "As a potential application of this efficient functionality transfer reaction, a pyrene group as a relatively large fluorescent group was successfully transferred to the target guanine base of RNA with a high guanine and site selectivity.",
        "This functionality transfer reaction with high efficiency and high site-selectivity would provide a new opportunity as a unique tool for the study of RNA.",
        "Figure 3: Examples of abstracts marked up for boilerplate (underlined) and non-boilerplate (faded text) by BOILERPLATE-LDA 9 Model |T | |Z| = 10 |Z| = 20 |Z| = 50 ARI NMI ARI NMI ARI NMI BOILERPLATE-LDA 10 0.19 0.15 0.09 0.09 0.04 0.07 20 0.20 0.16 0.03 0.10 0.03 0.08 50 0.26 0.21 0.18 0.16 0.05 0.10 100 0.32 0.28 0.20 0.20 0.07 0.14 BOILERPLATE-LDA-MULT 10 0.13 0.11 0.08 0.08 0.04 0.06 20 0.10 0.13 0.04 0.09 0.03 0.07 50 0.21 0.16 0.13 0.14 0.06 0.10 100 0.18 0.16 0.14 0.14 0.07 0.11 BOILERPLATE-LDA-NOTOPICS 0 0.00 0.02 0.04 0.05 0.06 0.05 K-MEANS 0 0.05 0.05 0.03 0.06 0.03 0.04 Table 1: Results of the clustering evaluation.",
        "|Z| is the number of zones; |T | is the number of topics.",
        "The results of the clustering evaluation are presented in Table 1.",
        "Clearly, this is a challenging task; the BOILERPLATE-LDA-NOTOPICS and K-MEANS models, which do not filter out topic-specific vocabulary, perform little better than chance in terms of identifying argumentative zones (recall that for the ARI and AMI measures, zero means ?not greater than expected by chance?",
        "rather than ?no correlation at all?).",
        "BOILERPLATE-LDA-MULT performs better than those models though not as well as the full BOILERPLATE-LDA model, indicating that sequential structure is important for inducing rhetorical regularities.",
        "In general, the best results are attained with low settings of |Z| and high settings of |T |; this seems to create the ?bottleneck?",
        "effect needed to focus the model on purely rhetorical information.",
        "The highest scores (ARI = 0.32, AMI = 0.28) are attained by BOILERPLATE-LDA with the settings |Z| = 10, |T | = 100.",
        "5.3 Supervised Evaluation 5.3.1 Evaluation A second evaluation of BOILERPLATE-LDA's usefulness is to test whether it can yield features that improve the performance of a supervised argumentative zoning system.",
        "It is possible for an unsupervised model to induce structure that does not map exactly onto a pre-existing set of labels but still captures valuable information about the underlying phenomenon that can be of use to a supervised classifier when combined with other information sources.",
        "To this end, we train and evaluate supervised models on the same dataset of Guo et al. (2010) that we used for the clustering evaluation.",
        "We perform 10-fold cross-validation and report Accuracy (proportion of sentences labelled correctly) as well as macro-averaged Precision, Recall and F-Score.",
        "To measure statistical significance we use two-tailed paired t-tests, following Dietterich (1998).",
        "5 5.3.2 Models We use two supervised sequence classification algorithms for training models: LR: A logistic regression classifier with a ?history?",
        "feature encoding the previous sentence's label, trained with L 1 regularisation, using the implementation in LibLinear.",
        "6 CRF: A first-order conditional random field classifier, trained with L 1 regularisation, using the implementation in Mallet.",
        "7 In both cases, the predicted labelling for a test document is given by the most probable (Viterbi) sequence according to the trained model.",
        "We use the following feature sets: 5 In order to address concerns about the suitability of the t-tests under non-normality, we replicated the tests using Wilcoxon's signed-ranks test as recommended by Dem?sar (2006); the results were identical.",
        "6 http://www.csie.ntu.edu.tw/ ?",
        "cjlin/liblinear/ 7 http://mallet.cs.umass.edu/ 10 LR CRF Model Acc P R F Acc P R F BASELINE 0.83 0.71 0.70 0.70 0.85 0.75 0.64 0.67 +BOILERPLATE-LDA 0.84 0.72 0.71 0.71 0.86 0.74 0.65 0.68 +LDA-BAG (50) 0.83 0.69 0.68 0.68 0.84 0.73 0.62 0.64 +LDA-BAG (100) 0.83 0.69 0.69 0.69 0.84 0.72 0.64 0.66 +LDA-MAX (50) 0.83 0.71 0.69 0.69 0.85 0.72 0.64 0.66 +LDA-MAX (100) 0.84 0.71 0.69 0.70 0.85 0.74 0.63 0.66 Table 2: Results of the supervised evaluation BASELINE: Our baseline set of features is a standard set for supervised argumentative zoning: all unigrams and bigrams in the sentence, all part-of-speech tags in the sentence and a location feature computed by dividing the abstract into 5 bins.",
        "+BOILERPLATE-LDA: The baseline model with additional features corresponding to the zone index assigned by BOILERPLATE-LDA to the sentence.",
        "We set |Z| = 10, |T | = 100 since that setting performed best in the clustering evaluation.",
        "As before, we use the output of three independently learned sampling chains, giving each sentence three zone features; the classifier should learn which chains are better than others during training.",
        "+LDA-BAG: The baseline model with additional features derived from standard Latent Dirichlet Allocation models trained on the same corpus as BOILERPLATE-LDA.",
        "As LDA assigns a topic to each word in a sentence, we add all topics assigned to all words in the sentence as additional features.",
        "As above, we use the output of three sampling chains.",
        "We report results for models with 50 topics and 100 topics.",
        "+LDA-MAX: The baseline model with additional features derived from LDA models.",
        "Here each model assigns each sentence the single topic assigned to the greatest number of words in the sentence (ties are broken randomly).",
        "5.3.3 Results Results for the supervised evaluation are presented in Table 2.",
        "+BOILERPLATE-LDA is the only augmented feature set that consistently gives an improvement over the baseline features.",
        "The improvements in accuracy are statistically significant (p < 0.01).",
        "In every case but one (which is not statistically significant), the LDA models fail to improve on the baseline in either accuracy or F-Score, showing that the latent structure induced by BOILERPLATE-LDA captures aspects of rhetorical language that are not captured by topical word clustering.",
        "6 Conclusion We consider the work presented in this paper to be a first step towards the ambitious goal of inducing latent descriptions of the templates used by scientists and writers in other fields.",
        "We have shown how our hypothesis about the generality of rhetorical language allows the construction of models that can separate out topical and rhetorical language use.",
        "One focus for future work will be to enrich the model structure; an approach based on adaptor grammars (Johnson et al., 2006) could be used to break the reductive unigram assumption in BOILERPLATE-LDA and identify multiword collocations that carry rhetorical information.",
        "Another focus will be to broaden our understanding of how unsupervised rhetorical models trained on large corpora can improve the robustness of supervised systems.",
        "For example, we have observed that lexicalised AZ classifiers trained on texts from one scientific domain will often perform poorly on texts from another domain; unsupervised models have the potential to induce relevant lexical commonalities across domains.",
        "11 Acknowledgements This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (DoI/NBC) contract number D11PC20153.",
        "The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.",
        "Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.",
        "References"
      ]
    }
  ]
}
