{
  "info": {
    "authors": [
      "Xipeng Qiu",
      "ChaoChao Huang",
      "Xuanjing Huang"
    ],
    "book": "COLING",
    "id": "acl-C14-1109",
    "title": "Automatic Corpus Expansion for Chinese Word Segmentation by Exploiting the Redundancy of Web Information",
    "url": "https://aclweb.org/anthology/C14-1109",
    "year": 2014
  },
  "references": [
    "acl-C04-1081",
    "acl-D11-1090",
    "acl-E03-1008",
    "acl-I08-4017",
    "acl-J04-1004",
    "acl-J09-4006",
    "acl-J96-1002",
    "acl-N06-1020",
    "acl-P02-1064",
    "acl-P06-1027",
    "acl-P06-2056",
    "acl-P07-1034",
    "acl-P07-1078",
    "acl-P08-1076",
    "acl-P12-1083",
    "acl-P12-2075",
    "acl-P13-1075",
    "acl-P95-1026",
    "acl-W02-1001",
    "acl-W02-1033",
    "acl-W03-0407",
    "acl-W03-1728",
    "acl-W10-2606"
  ],
  "sections": [
    {
      "text": [
        "Abstract",
        "Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus.",
        "However, these supervised methods do not work well when we deal with a new different domain without enough annotated corpus.",
        "In this paper, we propose a method to automatically expand the training corpus for the out-of-domain texts by exploiting the redundant information on Web.",
        "We break up a complex and uncertain segmentation by resorting to Web for an ample supply of relevant easy-to-segment sentences.",
        "Then we can pick out some reliable segmented sentences and add them to corpus.",
        "With the augmented corpus, we can re-train a better segmenter to resolve the original complex segmentation.",
        "The experimental results show that our approach can more effectively and stably improve the performance of CWS.",
        "Our method also provides a new viewpoint to enhance the performance of CWS by automatically expanding corpus rather than developing complicated algorithms or features."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word segmentation is a fundamental task for Chinese language processing.",
        "In recent years, Chinese word segmentation (CWS) has undergone great development.",
        "The popular method is to regard word segmentation as a sequence labeling problems (Xue, 2003; Peng et al., 2004).",
        "The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as Maximum Entropy (ME) (Berger et al., 1996), Conditional Random Fields (CRF)(Lafferty et al., 2001).",
        "After years of intensive researches, Chinese word segmentation achieves a quite high precision.",
        "However, the performance of segmentation is not so satisfying for the practical demands to analyze Chinese texts.",
        "The key reason is that most of annotated corpora are drawn from news texts.",
        "Therefore, the system trained on these corpora cannot work well with the out-of-domain texts.",
        "Since these supervised approaches often has a high requirement on the quality and quantity of annotated corpus, which is always not easy to create.",
        "As a result, many methods were proposed to utilize the information of unlabeled data.",
        "There are three kinds of methods for domain adaptation problem in CWS.",
        "The first is to use unsupervised learning algorithm to segment texts, like branching entropy (BE) (Jin and Tanaka-Ishii, 2006), normalized variation of branching entropy (nVBE)(Magistry and Sagot, 2012).",
        "License details: http://creativecommons.org/licenses/by/4.",
        "0/ 1154 The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011) The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the difference in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and Isozaki, 2008).",
        "Although these methods improve the performance of out-of-domain texts, the performance is still worse than that of in-domain texts obviously.",
        "We firstly investigate the reasons of lower performance in new domain for state-of-the-art CWS systems and find that most of error segmentation were caused by out-of-vocabulary (OOV) words, also called new words or unknown words (see details in Section 3).",
        "It is difficult to devote efforts to building a corpus for out-of-domain texts, since new words are produced frequently as the development of the society, especially the Internet society.",
        "It is also impractical to manually maintain an up-to-date corpus to include all geographical names, person names, organization names, technical terms, etc.",
        "In this paper, we propose a method to automatically expand the training corpus for the out-of-domain texts by exploiting the redundant information on Web.",
        "When we meet a complex and potentially difficult-to-segment sentence, we do not expect to solve it with more complicated learning algorithm or elaborate features.",
        "We assume that there are some relevant sentences that are relatively easy to process.",
        "These simple sentences can help to solve the complex one.",
        "For example, the sentence ???????",
        "(L?Oreal, Maybelline) -- is difficult to segment if both ????",
        "(L?Oreal) -- and ????",
        "(Maybelline)?are unknown words.",
        "However, we can always find some easy-to-segment sentences, such as???????",
        "(I use Maybelline)?,???",
        "????",
        "(production of L?Oreal)?, and so on.",
        "When we use these simple sentences to re-train the segmenter, we can solve the previous complex sentence.",
        "Our method relies on breaking up the complex problems into relevant smaller, simpler problems that can be solved easily.",
        "Fortunately, we can resort to the scale and redundancy of the web for an ample supply of simple sentences that are relatively easy to process.",
        "Our method is very easy to implement upon a trainable base segmenter.",
        "Given the out-of-domain texts, we firstly choose some uncertain segmentations and select the candidate expansion seeds.",
        "Secondly, we use these seeds to get the relevant texts from Web search engine.",
        "Then we segment these texts and add the texts with high confidence to training corpus.",
        "Finally, we can get a better segmenter with the new corpus.",
        "The rest of the paper is organized as follows: we review the related works in section 2.",
        "In section 3, we analyze the influence factor for CWS.",
        "Then we describe our method in section 4.",
        "Section 5 introduces the base segmenter.",
        "Section 6 gives the experimental results.",
        "Finally we conclude our work in section 7.",
        "2 Related Works The idea of exploring information redundancy on Web was introduced in question answering system (Kwok et al., 2001; Clarke et al., 2001; Banko et al., 2002) and the famous information extraction system KNOWITALL(Etzioni et al., 2004).",
        "However, this idea is rarely mentioned in Chinese word segmentation.",
        "Nonetheless, there are three kinds of related methods on Chinese word segmentation.",
        "One is active learning.",
        "Both (Li et al., 2012) and (Sassano, 2002) try to use active learning method to expand annotated corpus, but they still need to manually label some new raw texts in order to enlarge the training corpus.",
        "Different with these methods, our method do not require any manual oracle labeling at all.",
        "Another is self-training, also called bootstrapping or self-teaching (Zhu, 2005).",
        "Self-training is a general semi-supervised learning approach.",
        "In self-training, a classifier is first trained with the small amount of labeled data.",
        "The classifier is then used to classify the unlabeled data.",
        "1155 0.7 0.75 0.8 0.85 0.9 0.95 1 0 1 2 3 4 5 F1 Number of Continuous OOV Words (a) Number of continuous OOV words 0.100.20 0.300.40 0.500.60 0.700.80 0.901.00 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.6 0.7 1 F1 OOV Rate (b) OOV rate 0.100.20 0.300.40 0.500.60 0.700.80 0.901.00 1 2 3 4 5 6 7 8 9 10 F1 Word Length (c) Word Length The blue horizontal line is the overall F1 score, and the red line is the F1 scores with different values of the factor.",
        "Figure 1: Analysis of Influence Factors Typically the most confident unlabeled points, together with their predicted labels, are added to the training set.",
        "The classifier is re-trained and the procedure repeated.",
        "Note that the classifier uses its own predictions to teach itself.",
        "Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POS-tagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on.",
        "It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process.",
        "However, the data on target domain cannot always help itself (Steedman et al., 2003).",
        "The third is weakly supervised learning.",
        "(Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve the performance of CWS.",
        "However, these natural annotations are just partial annotations and their roles depend on the qualities of the selected resource, such as Wikipedia.",
        "In this paper, we wish to propose a method to obtain new fully-annotated data in more aggressive way, which can combine the advantages of the above works.",
        "3 Analysis of Influence Factors for CWS Before describing our method, we give an analysis of the impact of out-of-vocabulary (OOV) words for segmentation.",
        "We first conduct experiments on the Chinese Treebank (CTB6.0) dataset (Xue et al., 2005) (The detailed information of dataset is shown in Section 6).",
        "Table 1 shows the performance of base segmenter.",
        "The F1 score of OOV words is significantly lower than that of in-vocabulary (INV) words.",
        "Precision Recall F1 INV 95.86 96.58 96.21 OOV 74.12 66.77 70.25 Total 94.64 94.73 94.69 Table 1: Performances of INV and OOV words We also investigate the impacts of three different factors: number of continuous OOV words, OOV rate and word length.",
        "Figure 1 shows the F1 scores with the changes of the different factors.",
        "We find that OOV words significantly improve the difficulty of segmentation, while the word length does not always harm the accuracy.",
        "These findings also indicate that we can improve the performance of CWS if we have a dictionary or annotated corpus including these OOV words.",
        "With the redundancy of the Web information, it is not difficult to automatically obtain the expected dictionary or corpus.",
        "1156 4 Our Method In this section, we describe our method to automatically expand the training corpus.",
        "4.1 Framework of Automatic Corpus Expansion Our framework of automatic corpus expansion is similar to standard process self-training or active learning for domain adaptation.",
        "Given a trainable base segmenter, the texts in out-of-domain, we firstly choose some uncertain segmentations and select the candidate expansion seeds.",
        "Secondly, we use these seeds to get the relevant texts from Web search engine.",
        "Then we segment these texts and add the texts with high confidence to training corpus.",
        "Finally, we can get a better segmenter with the new corpus.",
        "Algorithm 1 illustrates the framework of automatic corpus expansion.",
        "Algorithm 1 Framework of Automatic Corpus Expansion Input: Annotated Corpus C A Unannotated Corpus in Target domain C T Uncertainty Threshold T u Seed Extraction Threshold T se Acceptation Threshold T a Maximum Iteration Number: M Output: Expanded Annotated Corpus C A 1: for i = 1 to M do 2: Train a basic segmenter using current C A with base learner 3: Use the basic segmenter to do segmentation for each sentence in C T and calculate its confidence.",
        "4: Choose out the sentences collection C TS , in which the segmentation confidence of each sentence is less than T u .",
        "5: Extract the expansion seeds collection C seeds from C TS and use search engine to acquire relevant raw texts C RRT .",
        "6: Segment and calculate the confidence for each sentence in C RRT .",
        "7: Pick the reliable segmentations C new with confidence more than T a from C RRT .",
        "8: Add C new into C A .",
        "9: end for 10: return C A ; 4.2 Uncertainty Sampling The first key step in our method is to find the uncertain segmentations.",
        "There are many proposed uncertainty measures in the literature of active learning (Settles, 2010), such as entropy and query-by-committee (QBC) algorithm.",
        "In our works, we investigate four following uncertainty measures for each sentence x.",
        "We use S 1 (x), S 2 (x), ?",
        "?",
        "?",
        ", S N (x) to represent the top N scores given by the segmenter.",
        "Normalized Score U NS The first measures is normalized score by the length of x, the normalized score U NS is calculated by U NS = S 1 (x) L (1) where L is the length of x.",
        "Standard Deviation U SD 1157 The standard deviation is calculate with the top N scores.",
        "U SD = ?",
        "?",
        "?",
        "?",
        "1 N N ?",
        "i=1 (S i (x) ?",
        "?)",
        "2 (2) where ?",
        "= 1 N ?",
        "N i=1 S i (x) is the average or expected value of S i (x).",
        "Entropy U Entropy Entropy is a measure of unpredictability or information content.",
        "Since we use character-based method for word segmentation, each character is labeled as one of {B, M, E, S} to indicate the segmentation.",
        "{B, M, E} represent Begin, Middle, End of a multi-character segmentation respectively, and S represents a Single character segmentation.",
        "Given the top N labeled results for a sentence, each labeled sequence consists of the labels {B, M, E, S}.",
        "We define l ?",
        "{B,M,E, S} to represent the label variable, and count j (l) to be the number of occurrences of l on position j among the top N results.",
        "Thus, we can calculate the entropy for the labeling uncertainty of each character.",
        "The entropy H j (l) for the character on position j is calculated by H j (l) = ?",
        "?",
        "l count j (l) N log countj(l) N , (3) where ?",
        "l count j (l) = N .",
        "The entropy of sentence U Entropy is the sum of the entropies of all the characters in the sentence.",
        "U Entropy = L ?",
        "j=1 H j (l).",
        "(4) Margin U Margin Margin is the deviation of top 2 scores, which is often used in machine learning algorithms, such as support vector machine (Cristianini and Shawe-Taylor, 2000) and passive-aggressive algorithm (Crammer et al., 2006).",
        "U Margin = S 1 (x) ?",
        "S 2 (x) (5) Among the above four measures, the larger the entropy is, the more uncertain the result is.",
        "For the rest three factors, the less the score is, the more uncertain the result is.",
        "We test these four uncertainty measures on the development set in order to choose the best one as our confidence measure.",
        "In figure 2, we illustrate the relationship between each uncertainty measure and the OOV count.",
        "We assume that the more OOV words are, the more uncertainty is.",
        "Meanwhile, a steep learning curve imply a good ability to distinguish whether the result is uncertain.",
        "Obviously, the entropy is not helpful according to our assumption.",
        "The normalized score is okay but not good, and both the standard deviation and margin seem to be useful because they can give a better threshold to distinguish uncertain segmentation.",
        "Finally, we choose margin as our uncertainty measure.",
        "4.3 Expansion Seeds Extraction For the uncertain segmentation, not every word is unreliable.",
        "We just pick the suspicious fragments.",
        "Therefore, we need to extract some seed phrases to get the relevant texts.",
        "It is notable that these seed phrases do not need to be words.",
        "They can be the combinations of several words or only parts of words.",
        "Take the following sentence for example.",
        "1158 0.3450.35 0.3550.36 0.3650.37 0.3750.38 0.385 0 2 4 6 8 10 U NS OOV count (a) NScore 00.001 0.0020.003 0.0040.005 0.0060.007 0 2 4 6 8 10 U SD OOV count (b) STDEV 00.5 11.5 22.5 33.5 44.5 5 0 2 4 6 8 10 U Entropy OOV count (c) Entropy 0 0.002 0.004 0.006 0.008 0.01 0.012 0 2 4 6 8 10 U Margin OOV count (d) Margin Figure 2: Different Uncertainty Measures ??????????????",
        "(L?Oreal, Maybelline, Lancome are good brands) The first fragment?????????",
        " -- is difficult to segment if these words does not appear in training corpus.",
        "Conversely, the second fragment is easy to segment since the containing words are very common.",
        "We use base segmenter to get the top five results as follows: ?",
        "?",
        "?",
        "?",
        "?",
        "?",
        "?",
        "?",
        "?",
        "?",
        "?",
        "?",
        "?",
        "?",
        "1 B M M M E B M E S B E S B E 2 B M M E B E B E S B E S B E 3 B M E B E B M E S B E S B E 4 S B M M E B E S S B E S B E 5 S B M M M E B E S B E S B E (Li et al., 2012) proposed a good way to select the candidate words for active learning with diversity measurement to avoid duplicate annotation.",
        "However, their method is not suitable for our work.",
        "The reason is that they regarded CWS as a binary classification problem, while our base segmenter uses 1st-order sequence labeling.",
        "In our work, we choose the expansion seeds by calculating the entropy of each character.",
        "If the entropy of the character is larger than threshold T se , we say that this character may be in an uncertain context.",
        "Thus, we extract the consecutive uncertain characters and their contexts as the expansion seeds.",
        "For the above example, we select the ?????????",
        "(L?Oreal, Maybelline, Lancome)?",
        "and its context ??",
        "(is)?as a seed ???????????.",
        "4.4 Collect relevance texts by using Web Search Engines After obtaining the expansion seeds, we collect the relevant texts on multiple search engines including Google, Baidu and Bing.",
        "For the seed ??????????",
        "?, we can get the following relevance sentence, which is easy to segment.",
        "???????????????????",
        "500 ????",
        "(L?Oreal owns more than 500 brands, including Lancome, L?Oreal, Maybelline, Vichy, etc.)",
        "In our work, we just get the top 100 relevant texts returned by each search engine without manual intervention.",
        "We do not use any search API and directly use the returned webpages by search engine, then extract the snippets and titles.",
        "Therefore, we just write a simple program to collect the webpages and clean them.",
        "4.5 Expand Training Corpus Since the qualities of these relevant texts are spotty, we just pick the reliable texts with high confidence scores.",
        "In contrast to uncertainty sampling, we find the certain segmentations from the collecting raw texts and add them to training corpus.",
        "Here, we also use a margin to find the reliable ones as new training data.",
        "In our experiments, the number of selected sentence is 1 ?",
        "5 for each seed.",
        "Thus, we can re-train a new segmenter on the expanded corpus.",
        "After several iteration, we will get a segmenter with the best performance.",
        "1159 5 Base Segmenter We use discriminative character-based sequence labeling for base word segmentation.",
        "Each character is labeled as one of {B, M, E, S} to indicate the segmentation.",
        "We use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters.",
        "Following (Collins, 2002), the average strategy is used to avoid the overfitting problem.",
        "6 Experiment To evaluate our algorithm, we use both CTB6.0 and CTB7.0 datasets in our experiments.",
        "CTB is a segmented, part-of-speech tagged, and fully bracketed corpus in the constituency formalism.",
        "It is also a popular data set to evaluate word segmentation methods, such as (Sun and Xu, 2011).",
        "Since CTB dataset is collected from different sources, such as newswire, magazine, broadcast news and web blogs, it is suitable to evaluate the performance of CWS systems on different domains.",
        "We conduct two experiments on different divisions of datasets.",
        "1.",
        "The first experiment is performed on CTB6.0 for comparison with state-of-the-art systems which also utilize the unlabeled data for word segmentation.",
        "2.",
        "The second experiment is performed on CTB7.0 for better evaluation on out-of-domain texts.",
        "CTB7.0 contains some newer news texts and web blogs texts, which is more suitable to evaluate our method for out-of-domain data.",
        "In our experiments, we set C = 0.01 for PA algorithm.",
        "We also try to use the different values of C, and found that larger values of C imply a more aggressive update step and result to fast convergence, but it has little influence on the final accuracy.",
        "The maximum iteration number M ?",
        "of PA algorithm is set to 50.",
        "The feature templates are C i T 0 , (i = ?1, 0, 1),C ?1,0 T 0 , C 0,1 T 0 , C ?1,1 T 0 , T ?1,0 .",
        "C represents a Chinese character, and the subscript of C indicates its position relative to the current character, whose subscript is 0.",
        "T represents the character-based tag.",
        "The evaluation measure are reported are precision, recall, and an evenly-weighted F 1 .",
        "6.1 Experiments on CTB6.0 Train Dev Test 81-325, 400-454, 500-554, 590-596, 600-885, 900, 1001-1017, 1019, 1021-1035, 1037-1043, 1045-1059, 1062-1071, 1073-1078, 1100-1117, 1130-1131 1133-1140, 1143-1147, 1149-1151,2000-2139, 2160-2164, 2181-2279,2311-2549, 2603-2774, 2820-3079 41-80, 1120-1129, 2140-2159, 2280-2294, 2550-2569, 2775-2799, 3080-3109 (1-40,901-931 newswire) (1018, 1020, 1036, 1044,1060-1061, 1072, 1118-1119, 1132,1141-1142, 1148 magazine) (2165-2180, 2295-2310, 2570-2602, 2800- 2819, 3110-3145 broadcast news) Table 2: CTB6.0 Dataset Division On CTB 6.0, we divide the training, development and test sets according to (Yang and Xue, 2012).",
        ", which are shown in Table 2 The detailed statistical information is shown in Table 3.",
        "Firstly, We use the development set to determine the parameters in Algorithm 1.",
        "For T u , T se and T a , we have three rounds to determine the parameters.",
        "In first round, we find the best value t1 in the range to 0 ?",
        "1 with the interval of 0.1.",
        "In second round, we find the best value t2 in range t1?",
        "0.1 ?",
        "t1+0.1 with the interval of 0.01.",
        "In third round, we find the final best value t3 1160 94.594.794.9 95.195.395.5 95.795.996.1 0 1 2 3 4 5 F1 Iterations (a) F1 score 7072 7476 7880 8284 0 1 2 3 4 5 Roov Iterations (b) OOV Recall Figure 3: Iterative Learning Curve on CTB6.0 92.592.792.9 93.193.393.5 93.793.994.1 94.394.5 0 1 2 3 4 5 F1 Iterations (a) F1 score 60.562.564.5 66.568.570.5 72.574.576.5 78.5 0 1 2 3 4 5 Roov Iterations (b) OOV Recall Figure 4: Iterative Learning Curve on CTB7.0 in the range to t2?",
        "0.01 ?",
        "t1+ 0.01 with the interval of 0.001.",
        "The maximum iteration number M is just determined based on convergence with the range 1 ?",
        "10.",
        "Finally, we set these parameters as following: uncertainty threshold T u = 0.003, seed extraction threshold T se = 0.65, acceptation threshold T a = 0.004 and maximum iteration number M = 5.",
        "Figure 3 shows the changing curve of F1 and OOV recall in the process of corpus expansion.",
        "The performance of the baseline segmenter is shown at iteration 0.",
        "The curve shows that the F1 score and OOV recall have continuous improvement with the increasing of train corpus.",
        "The maximum performance is achieved at the 5th iteration.",
        "The detailed results are shown in Table 4.",
        "Compared with the baseline, the expanded corpus leads to a segmenter with significantly higher accuracy.",
        "The relative error reductions are 26.37% and 43.63% in terms of the balanced F-score and the recall of OOV words respectively.",
        "Dataset Sents Words Chars OOV Rate Train.",
        "22757 639506 1053426 - Dev.",
        "2003 59764 100038 5.45% Test 2694 81304 133798 5.58% Table 3: Corpus Information of CTB 6.0 Test P R F1 R oov Baseline 94.64 94.73 94.69 70.25 Final 95.66 96.51 96.09 83.23 (Sun and Xu, 2011) 95.86 95.62 95.74 79.28 Table 4: Performance on CTB6.0 6.2 Experiments on CTB7.0 CTB7.0 includes documents from newswire, magazine articles, broadcast news, broadcast con-versations, newsgroups and weblogs.",
        "The newly added documents contains texts from web blogs, which is very different with news texts.",
        "Therefore, we use the documents (No.",
        "4198 4411, weblogs) as test dataset, and the rest as training dataset.",
        "The detailed statistical information is shown in Table 5.",
        "We can see that the OOV rate is higher than the dataset in the first experiment.",
        "Dataset Sents Words Chars OOV Rate Train.",
        "40425 987307 1601142 Test 10177 209827 342061 7.09% Table 5: Corpus Information of CTB 7.0 Test P R F1 R oov Baseline 93.58 92.40 92.98 60.72 Final 94.47 94.40 94.43 79.24 Table 6: Performance on CTB7.0 Figure 4 shows the changing curve of F1 and OOV recall in the process of corpus expansion.",
        "The performance of the baseline segmenter is shown at iteration 0.",
        "The curve shows that the F1 score and OOV recall have continuous improvement with the increasing of train corpus.",
        "The maximum performance is achieved at iteration 5.",
        "The detailed results are shown in Table 6.",
        "Compared with the baseline, the expanded corpus leads to a segmenter with significantly higher accuracy.",
        "The relative error reductions are 20.66% and 47.15% in terms of the balanced F-score and the recall of OOV words respectively.",
        "1161 6.3 Analysis The experimental results show that our method is very effective to improve the performance of Chinese word segmentation.",
        "Especially, our method gives a significant boost on OOV words.",
        "For the words such as ????????",
        "(Borussia Moenchengladbach)?, ??????",
        "(catalase)?,????",
        "(Yi ZhongTian, a Chinese person name)?and????",
        "(prime time)?, it is still difficult to segment them correctly even if we can obtain useful features from unlabeled data.",
        "When we take advantage of the redundant information from Web, we can easily collect the relevant easy-to-segment sentences to expand the training corpus.",
        "Our method can result to a segmenter significantly better than the systems which finds the informative features derived from unlabeled data, such as (Sun and Xu, 2011).",
        "This also suggests that expanding corpus is more effective than developing complicated algorithm or well-design features.",
        "Of course, our method is compatible with these technologies, which can further improve the performance of CWS by combining the Web redundancy.",
        "7 Conclusion In this paper, we propose a method to automatically expand the training corpus for the out-of-domain texts.",
        "Given the out-of-domain texts, we first choose some uncertain segmentations as candidate expansion seeds, and use these seeds to get the relevant texts from search engine.",
        "Then we segment the texts and add the texts with high confidence to training corpus.",
        "We can always obtain some easily-segmented texts due to the large amount of redundancy texts on Web, especially for new words.",
        "Our experimental results show that our proposed method can more effectively and stably utilize the unlabeled examples to improve the performance.",
        "Our method also provides a new viewpoint to enhance the performance of CWS by expanding corpus rather than developing complicated algorithms or features.",
        "The long term goal of our method is to build an online and constant learning system, which can identify the difficult tasks and seek help from crowdsourcing.",
        "Search engines are special cases of crowdsourcing.",
        "In the future, we wish to investigate our method for other NLP tasks, such as POS tagging, Named Entity Recognition, and so on.",
        "Acknowledgments We would like to thank the anonymous reviewers for their valuable comments.",
        "This work was funded by NSFC (No.61003091), Science and Technology Commission of Shanghai Municipality (14ZR1403200) and Shanghai Leading Academic Discipline Project (B114).",
        "References"
      ]
    }
  ]
}
