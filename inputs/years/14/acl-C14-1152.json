{
  "info": {
    "authors": [
      "Tao Qian",
      "Donghong JI",
      "Mingyao Zhang",
      "Chong Teng",
      "Congling Xia"
    ],
    "book": "COLING",
    "id": "acl-C14-1152",
    "title": "Word Sense Induction Using Lexical Chain based Hypergraph Model",
    "url": "https://aclweb.org/anthology/C14-1152",
    "year": 2014
  },
  "references": [
    "acl-C02-1114",
    "acl-C94-2121",
    "acl-D12-1086",
    "acl-E03-1020",
    "acl-E06-1018",
    "acl-E09-1013",
    "acl-E12-1060",
    "acl-J91-1002",
    "acl-J98-1004",
    "acl-N13-1119",
    "acl-P09-1002",
    "acl-P11-1148",
    "acl-P95-1026",
    "acl-S10-1011",
    "acl-S13-2049",
    "acl-S13-2050",
    "acl-S13-2051",
    "acl-W04-2406",
    "acl-W06-1669",
    "acl-W07-2002",
    "acl-W09-2419",
    "acl-W97-0703"
  ],
  "sections": [
    {
      "text": [
        "Abstract",
        "Word Sense Induction is a task of automatically finding word senses from large scale texts.",
        "It is generally considered as an unsupervised clustering problem.",
        "This paper introduces a hypergraph model in which nodes represent instances of contexts where a target word occurs and hyperedges represent higher-order semantic relatedness among instances.",
        "A lexical chain based method is used for discovering the hyperedges, and hypergraph clustering methods are used for finding word senses among the context instances.",
        "Experiments show that this model outperforms other methods in supervised evaluation and achieves comparable performance with other methods in unsupervised evaluation."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word sense induction (WSI) aims to automatically find senses of a given target word (Yarowsky, 1995) from large scale texts.",
        "Compared with existing manual word sense resources, WSI techniques use clustering algorithms to determine the possible senses for a word.",
        "Word sense induction is generally considered as an unsupervised clustering problem.",
        "The input for the clustering algorithm is context instances of a target word, represented by word bags or co-occurrence vectors, and the output is a grouping of these instances into classes, each corresponding to an induced sense.",
        "Traditional methods in WSI tend to adopt the vector space model, in which the context of each instance of a target word is represented as a vector of features based on frequency statistics and probability distributions, e.g., first-order or second-order vector (Schütze, 1998; Purandare and Pedersen, 2004; Cruys et al., 2011).",
        "These vectors are clustered and the resulting clusters represent the induced senses.",
        "Another family of employed approach is graph-based methods (Widdows and Dorow, 2002; V?ronis, 2004; Agirre et al., 2006; Klapaftis and Manandhar, 2007; Di Marco and Navigli, 2011; Hope and Keller, 2013), which have been recently explored successfully to some extent.",
        "Graph-based methods are considering the notion of a co-occurrence graph, assuming a binary relatedness between co-occurring words.",
        "One of the key challenges in WSI is learning the higher-order semantic relatedness among multiple context instances.",
        "Previous approaches (Klapaftis and Manandhar.",
        "2007; Bordag, 2006) for WSI are used to construct higher-order relatedness by counting co-occurrence frequency or collocation of muti-words, regardless of global semantic similarity.",
        "Lexical chain (Morris and Hirst, 1991) is defined as a sequence of semantically related words in text and provides important clues about the text structure and topic.",
        "It can be viewed as a global counterpart of the measures of semantic similarity (Navigli, 2009).",
        "For example, Figure 1 gives three context instances containing Apple.",
        "1601 Figure 1.",
        "Context instances of Apple Obviously, four Apples in Figure 1 all refer to the Apple Company.",
        "We can directly group three instances by the lexical chain: iPod-iTunes-hardware and software product-Inc.",
        "This lexical chain represents a higher-order semantic relatedness among the three instances.",
        "In this paper, we propose a hypergraph model from a global perspective, in which nodes represent instances of contexts where a target word occurs and hyperedges denote higher-order semantic relatedness among instances.",
        "A lexical chain based method is used for identifying the hyperedges.",
        "This method for lexical chain extraction is a knowledge-free method based on LDA topic model (Remus and Biemann, 2013).",
        "The remainder of this paper is structured as follows.",
        "Section 2 presents an overview of the related work.",
        "Section 3 describes our model in details.",
        "Section 4 provides a quantitative evaluation and comparison with other algorithms in the SemEval-2013 word sense induction task.",
        "Finally, section 5 draws conclusions and lays out some future research directions.",
        "2 Related Work 2.1 Word sense induction A number of diverse approaches to WSI have been proposed so far.",
        "Context features are often represented in a variety of forms such as co-occurrence of words within phrases (Pantel and Lin, 2002; Dorow and Widdows, 2003), parts of speeches (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003).",
        "The size of the context window also varies, such as two words before and after the target word, the sentence or even larger paragraph within which contains the target word.",
        "Most of the work in WSI is the vector space model, such as context-based vector algorithm (Schütze, 1998; Ide et al., 2001; Van de Cruys et al., 2011), substitute-based vector algorithm (Yatbaz et al., 2012; Baskaya et al., 2013).",
        "In this model, the context of each instance of a target word is represented as a vector of features based on frequency statistics or probability distributions (e.g., first-order or second-order vector).",
        "These vectors are clustered by various algorithms and the resulting clusters represent the induced senses.",
        "Another family of employed approach is graph-based methods, which have been successfully applied in the sense induction task with some better results achieved.",
        "In this framework words are represented as nodes in the graph and vertices are drawn between the target word and its co-occurrences.",
        "The co-occurrences between words can be obtained on the basis of grammatical (Wid- dows and Dorow, 2002) or collocational relations (V?ronis, 2004).",
        "Senses are induced by identifying highly dense sub-graphs (hubs) in the co-occurrence graph.",
        "Klapaftis (2007) uses hypergraph model for WSI, in which co-occurrences of two or more words are represented by using weighted hyperedges.",
        "This model fully exploits the existence of collocations or terms consisting of more than two words.",
        "In fact, the method converts the sense induction problem to the clustering of the contextual words, and the result relies on local word co-occurrence frequency.",
        "Our hypergraph model is constructed from a global perspective, where the whole context instance is regarded as a node.",
        "WSI evaluation also is an important issue in WSI tasks.",
        "Previous WSI evaluations in SemEval (Agirre and Soroa, 2007; Manandhar et al., 2010) have approached sense induction in terms of finding the single most salient sense of a target word given its context.",
        "However, as shown in Erk and McCarthy (2009), multiple senses of the target word may be perceived by readers from different angles and a graded notion of sense labeling may be considered as the most appropriate.",
        "The SemEval-2013 WSI evaluation is designed to explore the possibility of finding all perceived senses of a target word in a single context instance.",
        "Our model is evaluated and verified on the SemEval-2013 WSI task.",
        "1602 2.2 Lexical chain extraction The Lexical chain method is an important technique in natural language processing.",
        "A lexical chain is a sequence of semantic related words in text and provides important clues about the text structure and topic.",
        "It has formed a theoretically well-founded building block in a lot of applications, such as word sense disambiguation (Manabu and Takeo, 1994), malapropism detection and correction (Hirst and St-Onge, 1998), summarization (Barzilay et al., 1997), topic tracking (Carthy, 2004), text segmentation (Stokes et al., 2004), and others.",
        "There are mainly two approaches for lexical chain extraction.",
        "One focuses on the use of knowledge resources like WordNet (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background information in order to quantify semantic relations between words.",
        "A major disadvantage of this strategy is that it relies on the resource, which has a direct impact on the quality of lexical chains.",
        "Another approach is based on statistical methods (Remus and Biemann, 2013).",
        "In this paper, we follow Remus and Biemann (2013) to automatically extract lexical chain by using LDA topic model.",
        "3 Hypergraph model In general, lexical chain based hypergraph model contains the following steps: i) Automatically extracting lexical chains based on topic model; ii) Constructing hypergraph with lexical chains; iii) Inducing word sense by hypergraph clustering.",
        "3.1 Lexical chain extraction The extraction technique of lexical chains is based on LDA topic model.",
        "LDA topic model (Blei et al., 2003) is a probabilistic model of text generation designed for revealing some hidden structure in large data collections.",
        "The key idea is that each document can be represented as a probability distribution over a fixed set of topics where each topic can be represented as a probability distribution over words.",
        "We use LDA topic model for estimating the semantic closeness of lexical terms, and explore a way of utilizing LDA's topic information in constructing lexical chains automatically.",
        "In our model, document is replaced with context instance of a target word.",
        "We adopt the idea of interpreting lexical chains as topics and placing all word tokens that share the same topic into the same chain.",
        "Lexical chains are usually extracted from the same paragraph or text, whose topic distributions are identical.",
        "However, in our experiment the context instances of a target word for WSI are derived from different articles, whose topic distributions are varied.",
        "Therefore both lexical and contextual topics are modeled.",
        "After training the LDA model, we use the information of the per-document topic distribution ?d= p(z|d), the per-topic word distribution?w=p(w|z) and the sampling topic of a word zw.",
        "The key work lies in how to assign a word to a topic in training LDA model.",
        "Since single samples of topics per word may exhibit a large variance (Riedl and Biemann, 2012), we sample several times and use the mode (most frequently assigned) topic ID per word as the topic assignment.",
        "Algorithm 1. lexical chains extraction algorithm Input: training set D of target word, hyper-parameters of LDA model; semantic threshold ?.",
        "Output: lexical chain set S 1 ?,?,Z LDA (D) 2 for each topic z 3 lc =\"\" // lc denotes a lexical chain 4 for each doc d 5 for each word w in doc d 6 if ( zw = z and p(w,d|z) > ? )",
        "7 lc.add (w) 8 S.add (lc) 9 return S 1603 The extraction algorithm is shown in algorithm 1.",
        "In order to improve the quality of identified lexical chains, a threshold ?",
        "is set to filter those invalid words whose generating probability of sampling topics in the document is lower than ?.",
        "( , | ) ( | ) ( | )p w d z p z d p w z ??",
        "> (1) The threshold ?",
        "is essential for the quality of lexical chains, which directly impacts on the performance of the model.",
        "Detailed analysis for the threshold ?",
        "will be given in section 4.5.",
        "3.2 Hypergraph creation A hypergraph H = (V, E) is a generalization of a graph whose edge can connect more than two vertices.",
        "Just as graphs represent many kinds of information in mathematical and computer science problems, hypergraphs also arise in important practical problems, including circuit layout, boolean satisfiability, numerical linear algebra, complex network and article co-citation, etc.",
        "Figure 2 shows an example of hypergraph creation in our model.",
        "We represent each context instance as a vertex and connect those context instances with a lexical chain across them by a hyperedge.",
        "A hyperedge weight equals to the weight of the corresponding lexical chain, defined as follows: ( | d ) (w | ) ( ) (2)| | i i i w C p z p z w e C ?",
        "= ?",
        "where lexical chain C corresponds to hyperedge e, |C| is the number of words in C, and z is the sampling topic of C. 3.3 Hypergraph clustering For hypergraph clustering, the hypergraph is usually transformed into induced graph.",
        "There are two transformation strategies: one is vertex expansions (2006; Zhou et al., 2006), i.e., clique expansion or star expansion, in which a hyperedge is transformed into a clique; the other is called hyperedge expansion (Pu and Faltings, 2012) based on a network flow technique, in which hyperedges are projected back to vertices through the adjacency information between hyperedges and vertices.",
        "Hypergraph clustering algorithm can be divided into two classes: one is based on minimal normalized cut, and the other is based on maximal density.",
        "We use three general hypergraph clustering algorithms to identify the context instance clusters.",
        "The three algorithms are simply shown in figure 3 and described as the follows.",
        "1) Normalized Hypergraph Cut (NHC) 1604 The NHC algorithm (Zhou et al., 2006) is a typical approach based on vertex expansion.",
        "The objective is to obtain a partition in which the connection among the vertices in the same cluster is dense while the connection between two clusters is sparse.",
        "The main steps include transforming the hypergraph into an induced graph first, and then adopting the normalized Laplacian to spectral partitioning.",
        "2) Hyperedge Expansion Clustering (HEC) Some works (e.g., Shashua et al., 2006; Bulo and Pelillo, 2012) have shown that the pairwise affinity relations after the projection to the induced graph would introduce information loss, and working directly on the hypergraph could produce better performance.",
        "The hyperedge expansion works as follows.",
        "It constructs a directed graph G = (V, E) that includes two vertices e+ and e-for each hyperedge e in the original hypergraph.",
        "Note that the vertices in G correspond to the hyperedges, but not the vertices in the original hypergraph.",
        "A directed edge is placed from e+ to e-with weight w(e) where w is the weighting function in the hypergraph.",
        "For every pair of overlapping hyperedges e1 and e2, two directed edges 1 2(e ,e )?",
        "+ and 2 1(e ,e )?",
        "+ are added to G with weights w(e2) and w(e1).",
        "After hypergraph expansion, it adopts spectral method for clustering.",
        "3) Schype The Schype (Michoel and Nachtergaele, 2012) is a maximal density cluster algorithm.",
        "According to the generalization of the Perron-Frobenius theorem, there exists a unique, positive vector, called the dominant eigenvector, over the set of vertices of the hypergraph, which produces a maximal density sub-graph with linear time.",
        "The procedure is as follow: i) Finding maximal density sub-graph by computing the dominant eigenvector.",
        "ii) Removing all vertices and hyperedges of the sub-graph from hypergraph.",
        "iii) Repeating above steps until no vertex in hypergraph occurs.",
        "This algorithm tends to generate many fine-grained clusters.",
        "We follow Tan and Kumar (2006) to merge clusters using two measures: cohesion and separation.",
        "The cohesion of a cluster Ci is defined as: , #( | , ) (C ) | C | i ix C y C i i e x y e cohesion ?",
        "?",
        "?",
        "= ?",
        "(3) where #( | , )e x y e?",
        "is the number of hyperedges containing nodes x and y in C and |Ci| is the number of vertices in Ci.",
        "Separation between two clusters Ci, Cj is defined as: , #( | , ) (C ,C ) 1 ( )| C | | C | i jx C y C i j i j e x y e separation ?",
        "?",
        "?",
        "= ?",
        "?",
        "?",
        "(4) 1605 We merge cluster pairs with high cohesion and low separation.",
        "The intuition is that context instances in such pairs will maintain a relatively high degree of semantic similarity.",
        "High cohesion is defined as greater than average cohesion of all clusters.",
        "Low separation is defined as a reciprocal relationship between two clusters: if a cluster Ci has the lowest separations to a cluster Cj and Cj has the lowest separation to Ci, then the two (high cohesion) clusters are merged.",
        "This merging process is iterated until it converges.",
        "4 Experiment and Evaluation 4.1 Dataset Our WSI evaluation is based on the dataset provided by the SemEval-2013 shared 13th task.",
        "Test data was drawn from the Open American National Corpus (OANC) (Ide and Suderman, 2004) across a variety of genres and from both the spoken and written portions of the corpus.",
        "It consists of 4,806 instances of 50 target words: 20 verbs, 20 nouns and 10 adjectives.",
        "Due to the unsupervised nature of the task, participants were not provided with sense-labeled training data.",
        "However, WSI systems were provided with the ukWac corpus (Baroni et al., 2009) to use in inducing senses.",
        "Additionally, we used the SemEval-2013 lexical trial data sets as development sets to tune parameters.",
        "4.2 Implementation details The training data is extracted from uKWac corpus.",
        "For each target word, we extracted 10K context instances, and each instance is a sentence window containing the target word.",
        "Additionally, we randomly selected 10K sentences as common auxiliary corpus, including none of the target word.",
        "The training data are tagged with POS tags and lemmatized with TreeTagger (Schmid, 1994).",
        "Removing stop words, nouns are taken as features.",
        "Meanwhile, we also removed words that co-occur with the target of word less than 50 times over the whole ukWac data.",
        "The training data in the model contains 20K instances: 10K instances of target word, 10K auxiliary instances.",
        "Specifically, we used the JGibbLDA1 framework for topic model estimation and inference, and examined the following LDA parameters: number of topics K, dirichlet hyperparameters for document-topic distribution ?",
        "and topic-term distribution ?.",
        "We tested combinations in the ranges K=1000?1500?2000, ?=5/K..50/K and ?=0.001..0.1.",
        "The highest performance of the WSI system was found for K = 2000, ?",
        "=0.025, ?",
        "= 0.001.",
        "Similar to tuning the dirichlet hyperparameters of LDA, the best parameter ?",
        "in lexical chain extraction is 0.0001 in the ranges ?",
        "=0.01.. 0.000001.",
        "We adopt the three clustering algorithms to cluster hypergraph2.",
        "The number of clusters is set as 10 for NHC and HEC, while Schype algorithm generates the number of the clusters (but requiring the edge-vertex ratio to be pre-defined), whose average number of senses is 31.8 after clusters are merged.",
        "Additionally, For Schype algorithm, we used the default values of parameters, except that the ?min- clustscore?",
        "parameter, a minimal score to output a cluster, being tuned to 0.1.",
        "The sense inventory acquired from the induction step can be used for disambiguation of individual instances.",
        "Each sense is represented as a vector, whose element is a word and the value of element is co-occurrence frequency with target word in the training set.",
        "Each test instance is also represented as a vector.",
        "The similarity between the instance and the induced sense is computed by using cosine function.",
        "For each test instance, it is compared with each sense separately, and finally the sense is selected if the cosine value is greater than a certain threshold ?.",
        "In experiment, ?",
        "is 0.04 for NHC and HEC, and is 0.1 for schype.",
        "4.3 Evaluation measures Evaluation in the SemEval-2013 WSI task can be divided into two categories: 1.",
        "A traditional WSD task for unsupervised WSD and WSI systems, 2.",
        "A clustering comparison setting that evaluates the similarity of the sense inventories for WSI systems.",
        "1 http://sourceforge.net/projects/jgibblda/ 2 The Hypergraph Analysis Toolbox (HAT) for NHC and HEC: http://lia.epfl.ch/index.php/research/relational-learning and the Schype's code: http://www.roslin.ed.ac.uk/tom-michoel/software/ 1606 In the first evaluation, we adopt a WSD task with three objectives: (1) detecting which senses are applicable, (2) ranking senses by their applicability, and (3) measuring agreement in applicability ratings with human annotators.",
        "Each objective uses a specific measurement: i): Jaccard Index: given two sets of sense labels for an instance, X and Y, the Jaccard Index is used to measure the agreement: X Y X Y ?",
        "?",
        ".",
        "The Jaccard Index is maximized when X and Y use identical labels, and is minimized when the sets of sense labels are disjoint.",
        "ii): Positionally-weighted Kendall's ?",
        "similarity: for graded sense evaluation, we consider an ranking scoring based on Kumar and Vassilvitskii(2010), which weights the penalty of reordering the lower positions less than the penalty of reordering the first ranks.",
        "iii): Weighted Normalized Discounted Cumulative Gain (WNDCG): NDCG (Moffat and Zobel, 2008) normally compares the rankings of two lists.",
        "It is extended to weighting the DCG by considering the relative difference in the two weights.",
        "Because the induced senses will likely vary in number and nature between systems, the WSD evaluation has to incorporate a sense alignment step, in which it performs by splitting the test in-stances into two sets: a mapping set and an evaluation set.",
        "The optimal mapping from induced senses to gold-standard senses is learned from the mapping set, and the sense alignment is used to map the predictions of the WSI system to pre-defined senses for the evaluation set.",
        "The particular split we use to calculate WSD effectiveness in this paper is 80%/20% (mapping/test), averaged across 5 random splits.",
        "In the clustering evaluation, similarity between participant's clusters and the gold standard clusters is measured by way of two metrics.",
        "i): Fuzzy Normalised Mutual information (NMI): it extends the method of (Lancichinetti et al., 2009) to compute NMI between overlapping clusters.",
        "Fuzzy NMI captures the alignment of the two clusters independent of the cluster sizes and therefore serves as an effective measure of the ability of an approach to accurately model rare senses.",
        "ii): Fuzzy B-Cubed: it adapts the overlapping B-Cubed measured defined in Amigo et al. (2009) to the fuzzy clustering setting, and provides an item-based evaluation that is sensitive to the cluster size skew and effectively captures the expected performance of the system on a dataset where the cluster distribution would be equivalent.",
        "4.4 Results We compared our models with four baselines and three benchmark systems from the SemEval-2013 task.",
        "Four baselines are described as follows.",
        "?",
        "Baseline MFS ?",
        "most frequent sense baseline, assigning all test instances to the MFS in the test data (regardless of what applicability rating it was given).",
        "?",
        "One sense ?",
        "labels each instance with the same induced sense.",
        "?",
        "One sense per instance (1clinst) ?",
        "labels each instance with its own induced.",
        "?",
        "Baseline Random-n ?",
        "randomly assigns each test instance to one of n randomly selected induced senses, where n is the number of senses for the target word in WordNet 3.1.",
        "Three benchmark systems as the following are those which achieved better results in the original SemEval-2013 task.",
        "?",
        "AI-KU is based on a lexical substitution method.",
        "?",
        "UoS uses dependency-parsed features from the corpus, which are then clustered into senses using the MaxMax algorithm (Hope and Keller, 2013).",
        "?",
        "Unimelb is a non-parameter topic model which uses a Hierarchical Dirichlet Process (Teh et al., 2006) to automatically infer the number of senses from contextual and positional features.",
        "4.4.1 Supervised evaluation In the supervised evaluation, the automatically induced clusters are mapped to gold standard senses, using one part of the test set.",
        "The obtained mapping is used to label the other part of test set with gold standard tags, which means that the methods are evaluated in a standard WSD task.",
        "In experiment, we follow the 80/20 setup: 80% for mapping and 20% for test.",
        "Table 1 shows the results of our systems on test data using all instances (including verbs, nouns and adjectives) for the WSD evaluation.",
        "As in previous WSD tasks, the MFS baseline on Jaccard Index measures is quite competitive, outperforming all systems in detecting which senses are applicable.",
        "1607 System JI-F1 WKT-F1 WNDCG-F1 NHC 0.325 0.692 0.375 HEC 0.327 0.693 0.376 Schype 0.376 0.753 0.345 AI-KU 0.244 0.642 0.332 UNIMELB 0.213 0.62 0.371 UoS 0.232 0.625 0.374 MFS 0.455 0.465 0.339 One sense 0.192 0.609 0.288 1c1inst 0.0 0.095 0.0 Random-n 0.29 0.638 0.286 Table 1.",
        "The supervised results over the SemEv-al-2013 dataset.",
        "System FNMI FBC NHC 0.046 0.406 HEC 0.037 0.400 Schype 0.042 0.377 AI-KU 0.039 0.451 UNIMELB 0.056 0.459 UoS 0.045 0.448 MFS - One sense 0 0.632 1c1inst 0.071 0 Random-n 0.016 0.245 Table 2.",
        "The unsupervised results over the Se-mEval-2013 dataset.",
        "However, most systems in this task were able to outperform the MFS baseline in ranking senses and quantifying their applicability.",
        "On the other hand, it also indicates that our three systems achieve better or comparable scores And the Schype gets the highest scores in detecting senses and ranking senses over all systems.",
        "4.4.2 Unsupervised evaluation Unsupervised evaluation aims to measure the similarity of the induced sense inventories for WSI systems.",
        "Unlike supervised metrics, it avoids potential loss of sense information since this setting does not require any sense mapping procedure to convert induced senses to WordNet senses.",
        "Table 2 shows the performance of our systems, benchmarks and baselines.",
        "It shows that the NMI-measure is biased towards the one sense per instance baseline and the FBC-measure one sense baseline.",
        "However, systems are capable of performing well in both the Fuzzy NMI and Fuzzy B-Cubed measures, thereby avoiding the extreme performance of either baseline.",
        "Generally, the performance of our model gets balanced scores.",
        "4.5 Discussion Topic models, such as LDA and HDP (Brody and Lapata, 2009; Lau et al., 2012), have been successfully adopted for WSI, in which one topic is viewed as one sense.",
        "Our work is motivated by lexical chain that represents the intrinsic semantic relatedness among context instances on the viewpoint of linguistics.",
        "Topic model is used to find lexical chains which are interpreted as topics.",
        "We have compared the Unimelb (Lau et al., 2013), a HDP topic model, with our model in the experiments.",
        "Addi-tionally, we also follow Lau et al. (2012) to train a LDA model with a fixed number of topics based on our training data for WSI3.",
        "Table 3 shows the supervised result compared to the Schype.",
        "These experiments show promising performance for our model, which captures richer semantic relatedness by using lexical chains.",
        "Lexical chains play a key role for the performance of our model.",
        "Intuitively, when lexical chains are too long, the higher-order relatedness would be mixed with some noises, while when lexical chains are too short, some higher-order relatedness will be missed.",
        "In order to verify the effectiveness of lexical chains, we tune the parameter ?",
        "in lexical chain extraction procedure and the results are shown in Figure 4.",
        "Another issue is the impact of POS labels of word for WSI task.",
        "The test data in SemeVal-2013 WSI task contain nouns, verbs and adjectives.",
        "We also test the performance based on different POS labels.",
        "Table 4 gives the supervised evaluation performance of our three systems on adjectives, verbs and nouns respectively.",
        "We found that the performance for adjectives in sense detection is the best, verbs followed and nouns worst, whereas it's reversed in sense ranking.",
        "The probable interpretation is 3 The LDA model parameters are set as follows: K=10, ?",
        "=0.025, ?",
        "= 0.001.",
        "1608 Figure 4.",
        "Performance analysis on Jaccard index measure for different threshold parameter ?",
        "in lexical chains extraction procedure.",
        "Type LDAK=10 Schype JI-F1 0.318 0.376 WKT-F1 0.692 0.753 WNDCG-F1 0.334 0.345 Table 3.",
        "The supervised results over the SemEv-al-2013 dataset between LDAk=10 and Schype for WSI.",
        "NHC HEC Schype POS JI WKT WNDCG JI WKT WNDCG JI WKT WNDCG nouns 0.306 0.697 0.370 0.313 0.702 0.367 0.363 0.767 0.246 verbs 0.336 0.686 0.374 0.336 0.690 0.380 0.384 0.749 0.245 adjs 0.347 0.697 0.396 0.342 0.678 0.390 0.394 0.733 0.248 Table 4.",
        "The supervised performance of three algorithms respectively on nouns, verbs and adjectives.",
        "that adjective's average sense number is the lowest, and the sense granularity is greater than verbs and nouns over the test data4.",
        "5 Conclusions and future work In this paper, we present a hypergraph model in which a node represents an instance and a hyperedge represents higher-order semantic relatedness among instances.",
        "Compared with other strategies based on binary local comparison, the model captures complex semantic relatedness among the instances from a global perspective.",
        "The evaluation results indicate that our model outperforms or reaches competitive performance comparable to other systems for the SemEval-2013 word sense induction task.",
        "Additionally, the experiments also show that both sense number and sense granularity of a target word affect the performance of WSI.",
        "For future work, we would like to explore better ways to extract and evaluate lexical chain for WSI task.",
        "In addition, for the three clustering algorithms, they generally require the number of clusters or edge-vertex ratio to be pre-defined, so we will seek more effective hypergraph clustering algorithms to automatically determine the parameters.",
        "Finally, the hypergraph model proposed in this work is not specific to the sense induction task, and can be adapted for other applications, such as document classification and clustering, information retrieval, etc.",
        "Acknowledgements This work is supported by the National Natural Science Foundation of China (No.",
        "61173062, 61373108, 61133012), the major program of the National Social Science Foundation of China (No.",
        "11&ZD189), and the High Performance Computing Center of Computer School, Wuhan University.",
        "Reference"
      ]
    }
  ]
}
