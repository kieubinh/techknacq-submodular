{
  "info": {
    "authors": [
      "Youzheng Wu",
      "Taro Watanabe",
      "Chiori Hori"
    ],
    "book": "COLING",
    "id": "acl-C14-1180",
    "title": "Recurrent Neural Network-based Tuple Sequence Model for Machine Translation",
    "url": "https://aclweb.org/anthology/C14-1180",
    "year": 2014
  },
  "references": [
    "acl-C04-1022",
    "acl-D07-1045",
    "acl-D13-1106",
    "acl-D13-1170",
    "acl-D13-1176",
    "acl-J03-1002",
    "acl-J06-4004",
    "acl-N03-1017",
    "acl-N12-1005",
    "acl-N13-1002",
    "acl-P06-2093",
    "acl-P07-2045",
    "acl-P11-1105",
    "acl-P11-2031",
    "acl-P13-1017",
    "acl-P13-2071",
    "acl-W09-0438"
  ],
  "sections": [
    {
      "text": [
        "Abstract",
        "In this paper, we propose a recurrent neural network-based tuple sequence model (RNNTSM) that can help phrase-based translation model overcome the phrasal independence assumption.",
        "Our RNNTSM can potentially capture arbitrary long contextual information during estimating probabilities of tuples in continuous space.",
        "It, however, has severe data sparsity problem due to the large tuple vocabulary coupled with the limited bilingual training data.",
        "To tackle this problem, we propose two improvements.",
        "The first is to factorize bilingual tuples of RNNTSM into source and target sides, we call factorized RNNTSM.",
        "The second is to decompose phrasal bilingual tuples to word bilingual tuples for providing fine-grained tuple model.",
        "Our extensive experimental results on the IWSLT2012 test sets 1 showed that the proposed approach essentially improved the translation quality over state-of-the-art phrase-based translation systems (baselines) and recurrent neural network language models (RNNLMs).",
        "Compared with the baselines, the BLEU scores on English-French and English-German tasks were greatly enhanced by 2.1%- 2.6% and 1.8%-2.1%, respectively."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The phrase-based translation systems (Koehn et al., 2003) rely on language model and lexicalized reordering model to capture lexical dependencies that span phrase boundaries.",
        "Their translation models, however, do not explicitly model context dependencies between translation units.",
        "To address this limi-tation, Marino et al. (2006) and Crego and Yvon (2010) proposed n-gram-based translation systems to capture dependencies across phrasal boundaries.",
        "The n-gram translation models have been shown to be effective in helping the phrase-based translation models overcome the phrasal independence assumption (Durrani et al., 2013; Zhang et al., 2013).",
        "Most of the n-gram translation models (Marino et al., 2006; Durrani et al., 2013; Zhang et al., 2013) employed Markov (n-gram) model over sequence of bilingual tuples also known as minimal translation units (MTUs).",
        "Recently, some pioneer studies (Schwenk et al., 2007; Son et al., 2012) proposed feed-forward neural networks with factorizations to model bilingual tuples in a continuous space.",
        "Although the authors reported some gains over the n-gram model in machine translation tasks, these models can only capture a limited amount of context and remain a kind of n-gram model.",
        "In language modeling, experimental results in (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013) showed that recurrent neural networks (RNNs) outperform feed-forward neural networks in both perplexity and word error rate in speech recognition even though it is harder to train properly.",
        "Therefore, in this paper we take the advantages of RNN and tuple sequence model and propose recurrent neural network-based tuple sequence models (RNNTSMs) to improve phrase-based translation system.",
        "Our RNNTSMs are capable of modeling long-span context and have better generalization.",
        "Compared with such related studies as (Schwenk et al., 2006; Son et al., 2012), our main contributions can This work is licenced under a Creative Commons Attribution 4.0 International License.",
        "1 The IWSLT workshop aims at translating TED speeches (http://www.ted.com), a collection of public lectures covering a variety of topics.",
        "1908 be summarized as: (i) our models can be regarded as deep neural network translation models because they can capture arbitrary-length context potentially, which are proven to estimate more accurate probabilities of bilingual tuples; (ii) we extend the conventional RNNTSM to factorized RNNTSMs that can significantly overcome the data sparseness problem caused by the large vocabularies of bilingual tuples by incorporating the factors from the source and the target sides in addition to bilingual tuples; (iii) we investigate heuristic rules to decompose phrasal bilingual tuples to word bilingual tuples for reducing the out-of-tuple-vocabulary rate and providing fine-grained tuple sequence model; (iv) we integrate the proposed models into the state-of-the-art phrase-based translation system (MOSES) as a supplement of the work in (Son et al., 2012) that is a complete n-gram translation system.",
        "2 Related Work The n-gram translation model (Marino et al., 2006) is a Markov model over phrasal bilingual tuples and can improve the phrase-based translation system (Koehn et al., 2003) by providing contextual dependencies between phrase pairs.",
        "To further improve the n-gram translation model, Crego and Yvon (2010) explored factored bilingual n-gram language models.",
        "Durrani et al. (2011) proposed a joint sequence model for the translation and reordering probabilities.",
        "Zhang et al. (2013) explored multiple decomposition structures as well as dynamic bidirectional decomposition.",
        "Since neural networks advance the state of the art in the fields of image processing, acoustic modeling (Seide et al., 2011), language modeling (Bengio et al., 2003), natural language processing (Collobert et al., 2011; Socher et al., 2013), machine transliteration (Deselaers et al., 2009), etc, some prior studies have been done on neural network-based translation models (NNTMs).",
        "One kind of the NNTMs relies on word-to-word alignment information or phrasal bilingual tuples.",
        "For example, Schwenk et al. (2007) investigated feed-forward neural networks to model bilingual tuples in continuous space.",
        "Son et al. (2012) improved this idea by decomposing tuple units, i.e., distinguishing the source and target sides of the tuple units, to address data sparsity issues.",
        "Although the authors reported some gains over the n-gram model in the BLEU scores on some tasks, these models can only capture a limited amount of context and remain a kind of n-gram model.",
        "In addition, a feed-forward neural network independent from bilingual tuples was proposed (Schwenk, 2012), which can infer meaningful translation probabilities for phrase pairs not seen in the training data.",
        "Another kind of the NNTMs do not rely on alignment.",
        "Auli et al. (2013) and Kalchbrenner and Blunsom (2013) proposed joint language and translation model with recurrent neural networks, in which latent semantic analysis and convolutional sentence model were used to model source-side sentence.",
        "Potentially, they can exploit an unbounded history of both source and target words thanks to recurrent connections.",
        "However, they only modestly observed gains over the recurrent neural network language model.",
        "Previous studies (Wu and Wang, 2007; Yang et al., 2013) showed that the performance of word alignment (alignment error rate) is nearly 80%.",
        "That means explicit word alignment may be more reliable as a way to represent the corresponding bilingual sentences compared with an implicit compressed vector representation (Auli et al., 2013).",
        "Our RNNTSM takes the advantages of the above NNTMs, that is, RNN enables our model to capture long-span contextual information, while tuple sequence model uses word alignment without much information loss.",
        "Furthermore, factorized RNN and word bilingual tuples are proposed to address data sparsity issue.",
        "To the best of our knowledge, few studies have been done on this aspect.",
        "3 Tuple Sequence Model In tuple sequence model, bilingual tuples are translation units extracted from word-to-word alignment.",
        "They are composed of source phrases and their aligned target phrases that are also known as minimal translation units (MTUs) and thus cannot be broken down any further without violating the constrains of the translation rules.",
        "This condition results in a unique segmentation of the bilingual sentence pair given its alignment.",
        "In our implementation, GIZA++ with grow-diag-final-and setting is used to conduct word-to-word alignments in both directions, source-to-target and target-to-source (Och and 1909 D \u0003ZRUG\u0010WR\u0010ZRUG\u0003DOLJQHG\u0003UHVXOW\u0003IURP\u0003*,=$\u000e\u000e\u0003ZLWK\u0003JURZ\u0010GLDJ\u0010ILQDO\u0010DQG\u0003 FRPSRVHUV\u0003DQG\u0003 RQW\u0003 ?W?\u0003 LQWHUURJ?V\u0003 DIULFDQ\u0003 DPHULFDQ\u0003 PXVLFLDQV\u0003 OHV\u0003 musiciens\u0003 DIUR\u0010DP?ULFDL\u0003 LQWHUYLHZHG\u0003ZHUH\u0003 HW\u0003 FRPSRVLWHXUV\u0003 X\u0014\u0003 PXVLFLDQV\u0003 OHV\u0003musiciens\u0003W\u0014\u0003 V\u0014\u0003 E \u0003VHTXHQFH\u0003RI\u0003SKUDVDO\u0003ELOLQJXDO\u0003WXSOHV\u0003 DIULFDQ\u0003DPHULFDQ\u0003 DIUR\u0010DP?ULFDL\u0003 V\u0015\u0003 X\u0015\u0003 W\u0015\u0003 X\u0016\u0003 DQG\u0003 HW\u0003 V\u0016\u0003 W\u0016\u0003 X\u0017\u0003 FRPSRVHUV\u0003 FRPSRVLWHXUV\u0003 V\u0017\u0003 W\u0017\u0003 X\u0018\u0003 ZHUH\u0003LQWHUYLHZHG\u0003 RQW\u0003?W?\u0003LQWHUURJ?V\u0003\u0003 V\u0018\u0003 W\u0018\u0003 DIULFDQ\u0003 DIUR\u0010DP?ULFDL\u0003 V\u0016\u0003 X\u0014\u0003 X\u0016\u0003 X\u0018\u0003 X\u0019\u0003 X\u001a\u0003 PXVLFLDQV\u0003 OHV\u0003W\u0014\u0003 W\u0016\u0003 DQG\u0003 HW\u0003 FRPSRVHUV\u0003 FRPSRVLWHXUV\u0003 ZHUH\u0003LQWHUYLHZHG\u0003 RQW\u0003?W?\u0003LQWHUURJ?V\u0003\u0003 V\u0018\u0003V\u0014\u0003 V\u0019\u0003 V\u001a\u0003 W\u0018\u0003 W\u001a\u0003W\u0019\u0003 X\u0015\u0003 PXVLFLDQV\u0003 PXVLFLHQV\u0003W\u0015\u0003 V\u0015\u0003 DPHULFDQ\u0003 DIUR\u0010DP?ULFDL\u0003 V\u0017\u0003 X\u0017\u0003 W\u0017\u0003 F \u0003VHTXHQFH\u0003RI\u0003ZRUG\u0010ELOLQJXDO\u0010WXSOH\u0010,\u0003 DIULFDQ\u0003 DIUR\u0010DP?ULFDL\u0003 V\u0016\u0003 X\u0014\u0003 X\u0016\u0003 X\u0018\u0003 X\u0019\u0003 X\u001a\u0003 PXVLFLDQV\u0003 OHV\u0003W\u0014\u0003 W\u0016\u0003 DQG\u0003 HW\u0003 FRPSRVHUV\u0003 FRPSRVLWHXUV\u0003 ZHUH\u0003 RQW\u0003 V\u0018\u0003V\u0014\u0003 V\u0019\u0003 V\u001a\u0003 W\u0018\u0003 W\u001a\u0003W\u0019\u0003 X\u0015\u0003 PXVLFLHQV\u0003W\u0015\u0003 V\u0015\u0003 DPHULFDQ\u0003V\u0017\u0003 X\u0017\u0003 X\u001b\u0003 LQWHUYLHZHG\u0003 ?W?\u0003 V\u001b\u0003 W\u001b\u0003 X \u0003 LQWHUURJ?V\u0003\u0003 V \u0003 W \u0003 G \u0003VHTXHQFH\u0003RI\u0003ZRUG\u0010ELOLQJXDO\u0010WXSOH\u0010,,\u0003 FRS\\\u0003 FRS\\\u0003 18//\u0003 18//\u0003 18//\u0003W\u0017\u0003 LQVHUW\u0003 LQVHUW\u0003 LQVHUW\u0003 Figure 1: An example of generating basic bilingual tuples from word alignment information.",
        "Ney, 2003).",
        "Ncode toolkit 2 is used to generate a unique bilingual segmentation of word-to-word aligned sentence.",
        "Figure 1(a)-(b) illustrates the process of generating bilingual tuple.",
        "As can be seen in Figure 1, bilingual tuple u 1 is composed of source phrase s?",
        "1 (musicians) and target phrase ?",
        "t 1 (les musiciens) linked to s?",
        "1 .",
        "Because this type of bilingual tuples are composed of one or more words from the source side and zero or more words from the target side, we call them phrasal bilingual tuples.",
        "The phrasal bilingual tuple is not able to provide translations for individual words that appear tied to other words unless they occur alone in some other tuple.",
        "For example, if target phrase ?",
        "t k =?les musiciens?",
        "is always aligned to source phrase s?",
        "k =?musicians?",
        "in the training corpus, then no word-to-word translation probability for ?musicians:musiciens?",
        "will exist.",
        "This becomes a serious drawback when a large number of phrasal bilingual tuples are extracted from one-to-many, many-to-one, and many-to-many alignments.",
        "To tackle the issue, we propose to decompose phrasal bilingual tuples into word bilingual tuples for providing fine-grained tuple sequence model.",
        "Suppose source phrase s?",
        "k , a sequence of source word s k1 , s k2 , ..., s kI , is aligned to target phrase ?",
        "t k , a sequence of target word t k1 , t k2 , ..., t kJ , in which I and J refer to the number of words in source phrase and that in target phrase.",
        "The following two types of heuristic rules are considered.",
        "(word-bilingual-tuple-I): For one-to-many alignments, we copy s kI J ?",
        "1 times to fill the short phrase s?",
        "k .",
        "For many-to-one alignments, we copy t kJ I ?",
        "1 times to fill the phrase ?",
        "t k .",
        "For many-to-many alignment, a maximum phrase length, we set it to 5, is used to avoid vocabulary explosion.",
        "That means, if I > 5; then s?",
        "k =?unk?, if J > 5; then ?",
        "t k =?unk?.",
        "(word-bilingual-tuple-II): For one-to-many, many-to-one, and many-to-many alignments, we insert a special token ?NULL?",
        "| J ?",
        "I | times to fill the short phrase, and map each word in the extended phrase monotonically to generate a word-wise tuple sequence.",
        "The Figure 1(c)-(d) demonstrate the decomposition results.",
        "As shown in Figure 1(c), the translation probability of ?musicians?",
        "being aligned to ?musiciens?",
        "can be learned in the word bilingual tuples.",
        "The word bilingual tuples enable our model use information from source-side of the tuples for computing translation probabilities of some tuples.",
        "For example, translating ?musicians:musiciens?",
        "benefits from its source word ?musicians?.",
        "Table 2 in Section 4 shows the sizes of the tuple vocabularies.",
        "We can see 2 http://ncode.limsi.fr/ 1910 hk-1 hidden layer, hk delay copy ?",
        "?",
        "?",
        "W U input layer, xk output layer, yk and:et and et uk-1 sk-1 tk-1 Distribution on source phrases ??",
        "?",
        "?",
        "?",
        "?",
        "?",
        "??",
        "??",
        "p(?|c(sk), xk) ??",
        "?",
        "?",
        "?",
        "Distribution on classes of source phrases P(?|xk) hk-1 hidden layer, hk delay copy ?",
        "?",
        "?",
        "W U input layer, xk and:et and et uk-1 sk-1 tk-1 output layer, yk Distribution on target phrases ??",
        "?",
        "?",
        "?",
        "?",
        "?",
        "??",
        "??",
        "??",
        "?",
        "?",
        "?",
        "Distribution on classes of target phrases P(?|xk) p(?|c(tk), xk) ?",
        "composers sk hk-1 hidden layer, hk delay copy ?",
        "?",
        "?",
        "W U input layer, xk output layer, yk and:et and et uk-1 sk-1 tk-1 Distribution on tuples ??",
        "?",
        "?",
        "?",
        "?",
        "?",
        "??",
        "??",
        "p(?|c(sk), xk) ??",
        "?",
        "?",
        "?",
        "Distribution on classes of tuples P(?|xk) (a) (b) (c) Figure 2: (a): factorized RNNTSM, called fRNNTSM for short, which will go back to the RNNTSM model when s?",
        "k?1 and ?",
        "t k?1 are dropped.",
        "(b): fRNNTSM source .",
        "(c) fRNNTSM target .",
        "that the word-bilingual-tuple-I has lower out-of-tuple-vocabulary (OOTV) rate, though it increases the tuple vocabulary.",
        "The word-bilingual-tuple-II greatly reduces the tuple vocabulary and the OOTV rate.",
        "Note that some words may not be aligned correctly, like ?NULL-musiciens?.",
        "However, generating these tuples can be viewed as a language model process that exploits previous source and target words, and current source word contained in previous tuples like ?les-musiciens?.",
        "Thus, given a target sentence t, a source sentence s, and its alignment a, the tuple sequence model can be defined over the sequence of bilingual tuples (u 1 , u 2 , ..., u m ) as follows.",
        "p(t, s, a) = m ?",
        "k=1 p(u k |u k?1 , u k?2 , ..., u 1 ) = m ?",
        "k=1 p(u k |u k?1 , u k?2 , ..., u k?n+1 ) (1) where u k denotes the k-th bilingual tuple of a given bilingual sentence pair.",
        "Each bilingual tuple u k contains a source phrase s?",
        "k and its aligned target phrase ?",
        "t k 3 .",
        "Formally, u k =s?",
        "k : ?",
        "t k .",
        "The tuple sequence model does not make any phrasal independence assumption and generates a tuple by looking at a context of previous tuples.",
        "The n-gram translation models are Markov models over sequences of tuples, they generate a tuple by looking at previous n-1 tuples.",
        "4 Recurrent Neural Network-based Tuple Sequence Model In order to use long-span context, this paper presents a recurrent neural network-based tuple sequence model (RNNTSM) to approximate the probability p(u i |u i?1 , ..., u 1 ).",
        "Our RNNTSM can potentially capture arbitrary long context rather than n-1 previous tuples.",
        "The input layer encodes bilingual tuples by using 1-of-n coding, and the output layer produces a probability distribution over all bilingual tuples.",
        "The hidden layer maintains a representation of the sentence history.",
        "This RNNTSM, however, has severe data sparsity problem due to the large tuple vocabulary coupled with the limited bilingual training data.",
        "4.1 Factorized RNNTSM To solve the problem, we extend the RNNTSM model with factorizing tuples in input layer, as shown in Figure 2(a).",
        "Specifically, it consists of an input layer x, a hidden layer h (state layer), and an output layer y.",
        "The connection weights among layers are denoted by matrixes U and W. Unlike the RNNTSM, which predicts probability p(u k |u k?1 , h k?1 ), the factorized RNNTSM predicts probability p(u k |u k?1 , s?",
        "k?1 , ?",
        "t k?1 , h k?1 ) of generating following tuple u k and is explicitly conditioned on the preceding tuple u k?1 , source-side of the tuple s?",
        "k?1 , and target-side of the tuple ?",
        "t k?1 .",
        "It is implicitly conditioned on the entire history by the delay copy of hidden layer h k?1 .",
        "For those tuples (approximate- ly 20% as shown in Table 2) that are not contained in the training data, i.e., co-occurrence (s i?1 , t i?1 ) 3 Phrases turn to words in the word bilingual tuples.",
        "For convenience, we do not distinguish them in our paper.",
        "1911 non-exist while either s i?1 or t i?1 exists, the factorized RNNTSM backs off to the source-(s i?1 ) or target-side (t i?1 ).",
        "This process resembles factored n-gram language model (Duh and Kirchhoff, 2004).",
        "However, the RNNTSM, computing p(u i |u i?1 , h i?1 ), cannot estimate the probabilities for those tuples.",
        "In the special case that s?",
        "k?1 and ?",
        "t k?1 are dropped, the factorized RNNTSM goes back to the RNNTSM.",
        "For convenience, u k?1 , s?",
        "k?1 and ?",
        "t k?1 are called features.",
        "In the input layer, each feature is encoded into a feature vector using the 1-of-n coding.",
        "The tuple u k?1 , the source phrase s?",
        "k?1 and the target phrase ?",
        "t k?1 are encoded into |u|-dimension feature vector v u k?1 , |s?|-dimension feature vector v s?",
        "k?1 and | ?",
        "t|-dimension feature vector v ?",
        "t k?1 , respectively.",
        "Here, |u|, |s?| and | ?",
        "t| stand for the sizes of the tuple, the source phrase, and the target phrase vocabularies.",
        "Finally, the input layer x k is formed by concatenating feature vectors and hidden layer h k?1 at the preceding time step, as shown in the following equation.",
        "x k = [v u k?1 , v s?",
        "k?1 , v ?",
        "t k?1 , h k?1 ] (2) The neurons in the hidden and output layers are computed as follows: h k = f(U ?",
        "x k ), y k = g(W ?",
        "h k ) f(z) = 1 1 + e ?z , g(z) = e z m ?",
        "k e z k (3) To speed-up both in the training and testing processes, we map bilingual tuples into classes with frequency binning and divide the output layer into two parts following (Mikolov et al., 2010).",
        "The first part estimates the posterior probability distribution over all classes.",
        "The second computes the posterior probability distribution over the tuples that belong to class c(u k ), the one that contains predicted tuple u k .",
        "Finally, translation probability p(u k |u k?1 , s?",
        "k?1 , ?",
        "t k?1 , h k?1 ) is calculated by, p(u k |u k?1 , s?",
        "k?1 , ?",
        "t k?1 , h k?1 ) = p(c(u k )|x k ) ?",
        "p(u k |c(u k ), x k ) (4) 4.2 Factorized RNNTSM on source and target phrases The above factorized RNNTSM is conditioned on the previous context during computing the probability for tuple u k .",
        "It does not exploit its source side s?",
        "k .",
        "For example, tuple ?composers:compositeurs?",
        "does not benefit from ?composers?.",
        "To address this limitation, we rewrite the probability in Equation 1. p(u k |u k?1 , u k?2 , ..., u 1 ) = p(s k , t k |u k?1 , u k?2 , ..., u 1 ) = p(s k |u k?1 , u k?2 , ..., u 1 ) ?",
        "p(t k |s k , u k?1 , u k?2 , ..., u 1 ) (5) The first sub-model p(s k |u k?1 , u k?2 , ..., u 1 ) computes the probability distribution over source phrases.",
        "This model, called fRNNTSM source for short, can be regarded as a reordering model.",
        "The second sub-model p(t k |s k , u k?1 , u k?2 , ..., u 1 ) is a translation model, abbreviated as fRNNTSM target , which computes the probability distribution over ?",
        "t k that are translated from s?",
        "k .",
        "The two sub-models are computed with the recurrent neural networks shown in Figure 2(b)-(c).",
        "Another advantage of using the factorized RNNTSM on source and target phrases separately is that their training become faster because the vocabulary sizes of the source and target phrases are much smaller than that of the tuples.",
        "4.3 Training Training can be performed by the back-propagation through time (BPTT) algorithm (Boden, 2002) by minimizing an error function defined in the following equations.",
        "L = 1 2 ?",
        "N ?",
        "i=1 (o i ?",
        "p i ) 2 + ?",
        "?",
        "( ?",
        "lk u 2 lk + ?",
        "tl w 2 tl ) (6) where N is the number of training instances, o i denotes the desired output; i.e., the probability should be 1.0 for the predicted tuple in the training sentence and 0.0 for all others.",
        "?",
        "is the regularization term's weight, which is determined experimentally using a validation set.",
        "The training algorithm randomly initializes the matrixes and updates them with Equation 7 over all the training instances in several iterations.",
        "1912 English-French English-German tst2010 tst2011 tst2012 tst2010 tst2011 tst2012 Baseline 30.15 35.97 35.48 20.29 21.48 19.30 +RNNTSM 30.51 (0.3) 36.11 (0.1) 36.44 (0.9) 20.67 (0.4) 21.85 (0.4) 19.56 (0.3) +fRNNTSM (1) 31.83 (1.6) 37.58 (1.6) 37.74 (2.2) 21.67 (1.4) 22.89 (1.4) 20.60 (1.3) +fRNNTSM source (2) +fRNNTSM target (3) 31.89 (1.7) 38.23 (2.2) 37.82 (2.3) 21.49 (1.2) 22.94 (1.4) 20.41 (1.1) +(1) +(2) +(3) 32.26 (2.1) 38.36 (2.4) 38.11 (2.6) 21.80 (1.5) 22.88 (1.4) 20.76 (1.5) Table 1: BLEU scores of the RNNTSMs, the factorized RNNTSM (fRNNTSM), the fRNNTSM source (sfRNNTSM), the fRNNTSM target with the word-bilingual-tuple-I and their combination.",
        "The numbers in the parentheses are the absolute improvements over the Baseline.",
        "In Equation 7, ?",
        "stands for one of the connection weights in the neural networks and ?",
        "is the learning rate.",
        "After each iteration, it uses validation data for stopping and controlling the learning rate.",
        "Usually, our RNNs needs 10 to 20 iterations.",
        "?",
        "new = ?",
        "previous ?",
        "?",
        "?",
        "?L ??",
        "(7) 5 Experiments We experiment with two language pairs on the IWSLT2012 data sets (Federico et al., 2012), with English as source and French, German as target.",
        "The IWSLT data comes from TED speecheds, given by leaders in various fields and covering an open set of topics in technology, entertainment, design, and many others.",
        "In the following experiments, the IWSLT dev2010 set is used as the tuning set, the tst2010, tst2011, and tst2012 as the test sets.",
        "Phrase-based translation systems are constructed as baselines using standard settings (GIZA++ align-ment, grow-diag-final-and, lexical reordering models, SRILM, and MERT optimizer) in the MOSES toolkit (Koehn et al., 2007).",
        "The proposed models are used to re-score n-best lists produced by the baseline systems.",
        "The n-best size is set to at most 1000 for each test sentence.",
        "During the n-best re-scoring, the weights are re-tuned on the dev2010 data set with MERT optimizer 4 .",
        "The proposed RNN-based models are evaluated on a small task and a large task.",
        "For the parameters of all the RNN-based models, we set the number of hidden neurons in the hidden layer to 480 and classes in the output layer to 300.",
        "5.1 Small Task In the small task, the training data only contains the speech-style bi-text, i.e., the human translation of TED speeches.",
        "Specially, the corpora for the English-French and English-German pairs contain 139K and 128K parallel sentences.",
        "The language model is a standard 4-gram language model with the Kneser-Ney discounting.",
        "Both the n-gram LM and the RNNLM are trained on the target side of the bi-text corpus.",
        "As the first experiment, we compare the proposed RNNTSMs with the word-bilingual-tuple-I.",
        "Table 1 summarizes the results.",
        "The main findings from this experiment are: (1) The RNNTSM yields modest improvements of 0.3%-0.4% over the baseline system onmost the test sets.",
        "(2) The factorized RNNTSMs essentially outperform the baseline and the RNNTSM on all the test sets.",
        "Specially, the improvements of the factorized RNNTSM and the combination of the fRNNTSM source and the fRNNTSM target over the baseline for the English-French task range 1.6%-2.2% and 1.7%-2.3%.",
        "For the English-German pair, these improvements are between 1.3%-1.4% and 1.1%-1.4%.",
        "The results indicate that the factorized RNNTSMs can well address the data sparsity problem of the RNNTSM.",
        "(3) The improvements for the English-German pair are comparatively smaller than that for the English-French pair.",
        "This is because German is a morphologically rich language (Fraser et al., 2013), its vocabulary is larger and the sparsity 4 To get statistically reliable comparison (Clark et al., 2011), replication of the MERT optimizer and test set evaluation are performed five times.",
        "We finally report the average BLEU scores in the following experiments.",
        "1913 English-French English-German #Tuple #Source/#Target OOTV #Tuple #Source/#Target OOTV Phrasal bilingual tuple 308K 130K/175K 26.0% 315K 148K/196K 23.4% word-bilingual-tuple-I 332K 100K/111K 23.7% 351K 104K/135K 22.2% word-bilingual-tuple-II 293K 44K/56K 14.9% 327K 43K/86K 14.8% Table 2: Vocabulary sizes.",
        "OOTV refers to the out-of-tuple-vocabulary rate on the dev2010 set.",
        "K stands for thousands.",
        "English-French English-German tst2010 tst2011 tst2012 tst2010 tst2011 tst2012 +fRNNTSM p 31.44 37.68 37.34 20.76 21.80 19.57 +fRNNTSM I 31.83 (0.4) 37.58 (?0.1) 37.74 (0.4) 21.67 (0.9) 22.89 (1.1) 20.60 (1.0) +fRNNTSM II 31.73 (0.3) 37.66 37.78 (0.4) 22.00 (1.2) 23.24 (1.4) 21.09 (1.5) +fRNNTSM I +fRNNTSM II 31.98 (0.6) 37.97 (0.3) 38.14 (0.6) 22.19 (1.4) 23.25 (1.4) 21.17 (1.7) Table 3: BLEU scores of the factorized RNNTSM with various types of bilingual tuples.",
        "fRNNTSM p refers to the fRNNTSM with phrasal bilingual tuples, fRNNTSM I to the fRNNTSM with the word-bilingual-tuple-I, etc.",
        "+ means these models are used with the baseline systems.",
        "The numbers in the parentheses are the absolute improvements over the +fRNNTSM p .",
        "problem is more serious.",
        "(4) There is no significant difference between the factorized RNNTSM and the combination of the fRNNTSM source and the fRNNTSM target on most of the test sets except for the tst2011 set of the English-French task.",
        "However, the BLEU scores are modestly improved by combining the three factorized RNNTSMs.",
        "The second experiment is to compare the phrasal bilingual tuples and the word bilingual tuples.",
        "Table 2 lists the vocabulary sizes of the tuples, source and target phrases.",
        "For the word-bilingual-tuple-I, the bilingual tuple vocabulary size increases by 10% in both the English-French and the English-German pairs.",
        "Compared with the phrasal bilingual tuples, the bilingual tuple vocabulary size in the word-bilingual-tuple-II slightly changes.",
        "In addition, decomposing the tuples is capable to greatly reduce the out-of-tuple-vocabulary rate by approximately 50% in the word-bilingual-tuple-II.",
        "Table 3 compares bilingual tuples in terms of BLEU scores.",
        "It can be clearly seen that both the word-bilingual-tuple-I and the word-bilingual-tuple-II achieve better performance than the phrasal bilingual tuple on most of the test sets.",
        "The BLEU improvements of the word-bilingual-tuple-II over the phrasal tuple range 1.2-1.5 points on the English-German task.",
        "The main reason may be lie in: the decomposition can provide word-to-word translation probabilities (such as ?musicians:musiciens?",
        "in the example of Section 2) for those non-one-to-one alignments.",
        "Thus the translation system will have a translation option for an isolated occurrence of such words.",
        "Another important observation is that the decomposition performs differently on the English-French and English-German tasks.",
        "For example, there exists slight difference between the word-bilingual-tuple-I and the word-bilingual-tuple-II for the English-French task.",
        "However, for the English-German task, the word-bilingual-tuple-II significantly outperforms the word-bilingual-tuple-I by 0.4 BLEU scores.",
        "Lastly, we achieve modest improvements by combining the two types of word bilingual tuples.",
        "This paper proposes three factorized RNNTSMs and two types of word bilingual tuples.",
        "In this ex-periment, we combine all of them (+Combination contains 6 models) and compare with RNN-based language model (Mikolov et al., 2010).",
        "Table 4 summarizes the results.",
        "As shown in Table 4 and Table 1, the combination can further enhance the performance on the English-German task.",
        "For example, the combination improves the factorized RNNTSM with the word-bilingual-tuple-I from 20.76 to 21.29 on the tst2012 set of the English-German task.",
        "Moreover, the combination significantly outperforms the RNNLM.",
        "The improvements over the RNNLMs on all test sets range 0.7-1.2 BLEU scores.",
        "The 1914 English-French English-German tst2010 tst2010 tst2012 tst2010 tst2010 tst2012 Baseline 30.15 (?1.2) 35.97 (?1.2) 35.48 (?1.5) 20.29 (?0.8) 21.48 (?0.9) 19.30 (?0.8) +RNNLM 31.43 37.23 37.04 21.14 22.39 20.08 +Combination 32.10 (0.7) 38.04 (0.8) 37.75 (0.8) 22.13 (1.0) 23.64 (1.2) 21.29 (1.2) Table 4: BLEU scores of the combination of our proposed models and RNNLM in the small task.",
        "The numbers in the parentheses are the absolute improvements over the RNNLM.",
        "English-French English-German tst2010 tst2010 tst2012 tst2010 tst2010 tst2012 Baseline 32.92 (?1.0) 38.67 (?1.2) 39.41 (?1.4) 22.29 (?0.5) 23.67 (?0.4) 20.83 (?0.7) +RNNLM 33.93 39.90 40.82 22.80 24.12 21.49 +Combination 34.24 (0.3) 40.37 (0.5) 40.92 (0.1) 23.61 (0.8) 25.18 (1.1) 22.64 (1.1) Table 5: BLEU scores of the combination of our proposed models and RNNLM.",
        "The numbers in the parentheses are the absolute improvements over the RNNLM.",
        "improvement over the baseline are between 1.9-2.3 BLEU points.",
        "5.2 Large Task In the large task, the training data includes both speech-style and text-style bi-text corpora.",
        "The text-style bi-text corpora are collected from the WMT2012 campaign 5 , including CommonCrawl, NewsCommen-tary, and Europarl.",
        "Totally, the numbers of the parallel sentences are 4.35M for the English-French task and 3.85M for the English-German task.",
        "The language model is obtained by linear interpolation of several 4-gram models trained on the target side of bi-text corpora and the LDC French Gigaword corpus.",
        "Table 5 reports the results.",
        "+Combination means the combination of six models, as described in Table 4.",
        "We can observe that: (1) The combination of the proposed RNNTSMs only trained on the speech-style data can essentially enhance the baselines by 1.2-1.8 BLEU points.",
        "(2) The improvements over the RNNLMs are significant on the English-German task but these improvements are modest on the English-French task.",
        "Note that the factorized RNNTSMs and the RNNLMs in the large task are also only trained the speech-style parallel corpus.",
        "In future work, we will train them on a bigger corpus, which can be expected to further increase the performance (Auli et al., 2013; Wu et al., 2012).",
        "6 Conclusion Most prior neural network-based translation models either employ feed-forward neural networks to explicitly integrate source information via word-to-word alignment, or use recurrent neural networks in which source information is implicitly represented with a compressed vector.",
        "In this paper, we present recurrent neural network-based tuple sequence models (RNNTSMs) to compute probabilities of bilingual tuples in continuous space.",
        "One of major advantages is their potential to capture long-span history compared with feed-forward neural networks.",
        "In addition, our models can well address the data sparsity problem thanks to the fine-grained word bilingual tuples and the factorized recurrent neural networks.",
        "As can be concluded from the experimental results on the IWSLT2012 test sets, our factorized RNNTSMs with the proposed bilingual tuples can essentially improve the BLEU scores for the English-French and English-German tasks.",
        "We plan to incorporate re-ordering and syntactic features into RNNTSMs and evaluate them on distant language pairs, such as English-Chinese (Japanese) tasks in the future.",
        "Moreover, we will prune large tuple vocabulary and speed up the training on bigger data.",
        "5 http://www.statmt.org/wmt12/translation-task.html 1915 References"
      ]
    }
  ]
}
