{
  "info": {
    "authors": [
      "Abdullah Alrajeh",
      "Mahesan Niranjan"
    ],
    "book": "EMNLP",
    "id": "acl-D14-1183",
    "title": "Large-scale Reordering Model for Statistical Machine Translation using Dual Multinomial Logistic Regression",
    "url": "https://aclweb.org/anthology/D14-1183",
    "year": 2014
  },
  "references": [
    "acl-D08-1089",
    "acl-H05-1021",
    "acl-N04-4026",
    "acl-N13-1003",
    "acl-P00-1056",
    "acl-P02-1038",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P06-1066",
    "acl-P07-2045",
    "acl-W04-3250",
    "acl-W05-0820",
    "acl-W06-3108",
    "acl-W11-1007"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1758?1763, October 25-29, 2014, Doha, Qatar.",
        "Abstract",
        "Phrase reordering is a challenge for statistical machine translation systems.",
        "Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model.",
        "However, Training this discriminative model using large-scale parallel corpus might be computationally expensive.",
        "In this paper, we explore recent advancements in solving large-scale classification problems.",
        "Using the dual problem to multinomial logistic regression, we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Phrase reordering is a common problem when translating between two grammatically different languages.",
        "Analogous to speech recognition sys-tems, statistical machine translation (SMT) systems relied on language models to produce more fluent output.",
        "While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like Arabic-English (Koehn, 2004a), many researchers considered lexicalized reordering models that attempted to learn orientation based on the training corpus (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005).",
        "Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria.",
        "The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011).",
        "Max-margin structure classifiers were also proposed (Ni et al., 2011).",
        "Alternatively, Cherry (2013) proposed recently using sparse features optimize the translation quality with the decoder instead of training a classifier independently.",
        "While large-scale parallel corpus is advantageous for improving such reordering model, this improvement comes at a price of computational complexity.",
        "This issue is particularly pronounced when discriminative models are considered such as maximum entropy-based model due to the required iterative learning.",
        "Advancements in solving large-scale classification problems have been shown to be effective such as dual coordinate descent method for linear support vector machines (Hsieh et al., 2008).",
        "Sim-ilarly, Yu et al. (2011) proposed a two-level dual coordinate descent method for maximum entropy classifier.",
        "In this work we explore the dual problem to multinomial logistic regression for building large-scale reordering model (section 3).",
        "One of the main advantages of solving the dual problem is providing a mechanism to shrink the training data which is a serious issue in building such large-scale system.",
        "We present empirical results comparing between the primal and the dual problems (section 4).",
        "Our approach is shown to be fast and memory-efficient.",
        "2 Baseline System In statistical machine translation, the most likely translation e best of an input sentence f can be found by maximizing the probability p(e|f), as follows: e best = arg max e p(e|f).",
        "(1) 1758 A log-linear combination of different models (features) is used for direct modeling of the posterior probability p(e|f) (Papineni et al., 1998; Och and Ney, 2002): e best = arg max e n ?",
        "i=1 ?",
        "i h i (f , e) (2) where the feature h i (f , e) is a score function over sentence pairs.",
        "The translation model and the language model are the main features in any system although additional features h(.)",
        "can be integrated easily (such as word penalty).",
        "State-of-the-art systems usually have around ten features.",
        "The language model, which ensures fluent translation, plays an important role in reordering; however, it has a bias towards short translations (Koehn, 2010).",
        "Therefore, a need for developing a specific model for the reordering problem.",
        "2.1 Lexicalized Reordering Model Adding a lexicalized reordering model consistently improved the translation quality for several language pairs (Koehn et al., 2005).",
        "Reordering modeling involves formulating phrase movements as a classification problem where each phrase position considered as a class (Tillmann, 2004).",
        "Some researchers classified phrase movements into three categories (monotone, swap, and discontinuous) but the classes can be extended to any arbitrary number (Koehn and Monz, 2005).",
        "In general, the distribution of phrase orientation is: p(o k | ?",
        "f i , e?",
        "i ) = 1 Z h( ?",
        "f i , e?",
        "i , o k ) .",
        "(3) This lexicalized reordering model is estimated by relative frequency where each phrase pair ( ?",
        "f i , e?",
        "i ) with such an orientation (o k ) is counted and then normalized to yield the probability as fol-lows: p(o k | ?",
        "f i , e?",
        "i ) = count( ?",
        "f i , e?",
        "i , o k ) ?",
        "o count( ?",
        "f i , e?",
        "i , o) .",
        "(4) The orientation of a current phrase pair is defined with respect to the previous target phrase.",
        "Galley and Manning (2008) extended the model to tackle long-distance reorderings.",
        "Their hierarchical model enables phrase movements that are more complex than swaps between adjacent phrases.",
        "3 Multinomial Logistic Regression Multinomial logistic regression (MLR), also known as maximum entropy classifier (Zens and Ney, 2006), is a probabilistic model for the multi-class problem.",
        "The class probability is given by: p(o k | ?",
        "f i , e?",
        "i ) = exp(w > k ?",
        "( ?",
        "f i , e?",
        "i )) ?",
        "k ?",
        "exp(w > k ?",
        "?",
        "( ?",
        "f i , e?",
        "i )) , (5) where ?",
        "( ?",
        "f i , e?",
        "i ) is the feature vector of the i-th phrase pair.",
        "An equivalent notation to w > k ?",
        "( ?",
        "f i , e?",
        "i ) is w > f(?",
        "( ?",
        "f i , e?",
        "i ), o k ) where w is a long vector composed of all classes parameters (i.e. w > = [w > 1 .",
        ".",
        ".w > K ] ) and f(., .)",
        "is a joint feature vector decomposed via the orthogonal feature representation (Rousu et al., 2006).",
        "This representation simply means there is no crosstalk between two different feature vectors.",
        "For example, f(?",
        "( ?",
        "f i , e?",
        "i ), o 1 ) > = [?",
        "( ?",
        "f i , e?",
        "i ) > 0 .",
        ".",
        ".",
        "0].",
        "The model's parameters can be estimated by minimizing the following regularized negative log-likelihood P(w) as follows (Bishop, 2006): min w 1 2?",
        "2 K ?",
        "k=1 ?w k ?",
        "2 ?",
        "N ?",
        "i=1 K ?",
        "k=1 p?",
        "ik log p(o k | ?",
        "f i , e?",
        "i ) (6) Here ?",
        "is a penalty parameter and p?",
        "is the empirical distribution where p?",
        "ik equals zero for all o k 6= o i .",
        "Solving the primal optimization problem (6) using the gradient: ?P(w) ?w k = w k ?",
        "2 ?",
        "N ?",
        "i=1 ( p?",
        "ik ?",
        "p(o k | ?",
        "f i , e?",
        "i ) ) ?",
        "( ?",
        "f i , e?",
        "i ), (7) do not constitute a closed-form solution.",
        "In our experiments, we used stochastic gradient decent method (i.e. online learning) to estimate w which is shown to be fast and effictive for large-scale problems (Bottou, 2010).",
        "The method approximates (7) by a gradient at a single randomly picked phrase pair.",
        "The update rule is: w ?",
        "k = w k ?",
        "?",
        "i ?",
        "k P i (w), (8) where ?",
        "i is a positive learning rate.",
        "1759 3.1 The Dual Problem Lebanon and Lafferty (2002) derived an equivalent dual problem to (6).",
        "Introducing Lagrange multipliers ?, the dual becomes min w 1 2?",
        "2 K ?",
        "k=1 ?w k (?)?",
        "2 + N ?",
        "i=1 K ?",
        "k=1 ?",
        "ik log?",
        "ik , s.t.",
        "K ?",
        "k=1 ?",
        "ik = 1 and ?",
        "ik ?",
        "0 ,?i, k, (9) where w k (?)",
        "= ?",
        "2 N ?",
        "i=1 (p?",
        "ik ?",
        "?",
        "ik )?",
        "( ?",
        "f i , e?",
        "i ) (10) As mentioned in the introduction, Yu et al. (2011) proposed a two-level dual coordinate descent method to minimize D(?)",
        "in (9) but it has some numerical difficulties.",
        "Collins et al. (2008) proposed simple exponentiated gradient (EG) algorithm for Conditional Random Feild (CRF).",
        "The algorithm is applicable to our problem, a special case of CRF.",
        "The rule update is: ?",
        "?",
        "ik = ?",
        "ik exp(??",
        "i ?",
        "ik D(?))",
        "?",
        "k ?",
        "?",
        "ik ?",
        "exp(??",
        "i ?",
        "ik ?",
        "D(?))",
        "(11) where ?",
        "ik D(?)",
        "?",
        "?D(?)",
        "??",
        "ik = 1 + log?",
        "ik + ( w y (?)",
        "> ?",
        "( ?",
        "f i , e?",
        "i )?w k (?)",
        "> ?",
        "( ?",
        "f i , e?",
        "i ) ) .",
        "(12) Here y represents the true class (i.e. o y = o i ).",
        "To improve the convergence, ?",
        "i is adaptively adjusted for each example.",
        "If the objective function (9) did not decrease, ?",
        "i is halved for number of trials (Collins et al., 2008).",
        "Calculating the function difference below is the main cost in EG algorithm, D(?",
        "?",
        ")?D(?)",
        "= K ?",
        "k=1 ( ?",
        "?",
        "ik log?",
        "?",
        "ik ?",
        "?",
        "ik log?",
        "ik ) ?",
        "K ?",
        "k=1 (?",
        "?",
        "ik ?",
        "?",
        "ik )w k (?)",
        "> ?",
        "( ?",
        "f i , e?",
        "i ) + ?",
        "2 2 ??",
        "( ?",
        "f i , e?",
        "i )?",
        "2 K ?",
        "k=1 (?",
        "?",
        "ik ?",
        "?",
        "ik ) 2 .",
        "(13) Clearly, the cost is affordable because w k (?)",
        "is maintained throughout the algorithm as follows: w k (?",
        "? )",
        "= w k (?)??",
        "2 (?",
        "?",
        "ik ??",
        "ik )?",
        "( ?",
        "f i , e?",
        "i ) (14) Following Yu et al. (2011), we initialize ?",
        "ik as follows: ?",
        "ik = { (1?",
        "\u000f) if o k = o i ; \u000f K?1 else.",
        "(15) where \u000f is a small positive value.",
        "This is because the objective function (9) is not well defined at ?",
        "ik = 0 due to the logarithm appearance.",
        "Finally, the optimal dual variables are achieved when the following condition is satisfied for all examples (Yu et al., 2011): max k ?",
        "ik D(?)",
        "= min k ?",
        "ik D(?)",
        "(16) This condition is the key to accelerate EG algorithm.",
        "Unlike the primal problem (6), the dual variables ?",
        "ik are associated with each example (i.e. phrase pair) therefore a training example can be disregarded once its optimal dual variables obtained.",
        "More data shrinking can be achieved by tolerating a small difference between the two values in (16).",
        "Algorithm 1 presents the overall procedure (shrinking step is from line 6 to 9).",
        "Algorithm 1 Shrinking stochastic exponentiated gradient method for training the dual problem Require: training set S = {?",
        "( ?",
        "f i , e?",
        "i ), o i } N i=1 1: Given ?",
        "and the corresponding w(?)",
        "2: repeat 3: Randomly pick i from S 4: Claculate?",
        "ik D(?)",
        "?k by (12) 5: v i = max k ?",
        "ik D(?",
        ")?min k ?",
        "ik D(?)",
        "6: if v i ?",
        "\u000f then 7: Remove i from S 8: Continue from line 3 9: end if 10: ?",
        "= 0.5 11: for t = 1 to maxTrial do 12: Calculate ?",
        "?",
        "ik ?k by (11) 13: if D(?",
        "?",
        ")?D(?)",
        "?",
        "0 then 14: Update ?",
        "and w(?)",
        "by (14) 15: Break 16: end if 17: ?",
        "= 0.5 ?",
        "18: end for 19: until v i ?",
        "\u000f ?i 1760 4 Experiments We used MultiUN which is a large-scale parallel corpus extracted from the United Nations website (Eisele and Chen, 2010).",
        "We have used Arabic and English portion of MultiUN where the English side is about 300 million words.",
        "We simplify the problem by classifying phrase movements into three categories (monotone, swap, discontinuous).",
        "To train the reordering models, we used GIZA++ to produce word alignments (Och and Ney, 2000).",
        "Then, we used the extract tool that comes with the Moses toolkit (Koehn et al., 2007) in order to extract phrase pairs along with their orientation classes.",
        "As shown in Table 1, each extracted phrase pair is represented by linguistic features as follows: ?",
        "Aligned source and target words in a phrase pair.",
        "Each word alignment is a feature.",
        "?",
        "Words within a window around the source phrase to capture the context.",
        "We choose adjacent words of the phrase boundary.",
        "The extracted phrase pairs after filtering are 47,227,789.",
        "The features that occur more than 10 times are 670,154.",
        "Sentence pair: f : f 1 f 2 1 f 3 f 4 f 5 2 f 6 3 .",
        "e : e 1 1 e 2 e 3 3 e 4 e 5 2 .",
        "Extracted phrase pairs ( ?",
        "f , e?)",
        ": ?",
        "f i ||| e?",
        "i ||| o i ||| alignment ||| context f 1 f 2 ||| e 1 ||| mono ||| 0-0 1-0 ||| f 3 f 3 f 4 f 5 ||| e 4 e 5 ||| swap ||| 0-1 2-0 ||| f 2 f 6 f 6 ||| e 2 e 3 ||| other ||| 0-0 0-1 ||| f 5 All linguistic features: 1. f 1 &e 1 2. f 2 &e 1 3. f 3 4. f 3 &e 5 5. f 5 &e 4 6. f 2 7. f 6 8. f 6 &e 2 9. f 6 &e 3 10. f 5 Bag-of-words representation: a phrase pair is represented as a vector where each feature is a discrete number (0=not exist).",
        "?",
        "( ?",
        "f i , e?",
        "i ) 1 2 3 4 5 6 7 8 9 10 ?",
        "( ?",
        "f 1 , e?",
        "1 ) = 1 1 1 0 0 0 0 0 0 0 ?",
        "( ?",
        "f 2 , e?",
        "2 ) = 0 0 0 1 1 1 1 0 0 0 ?",
        "( ?",
        "f 3 , e?",
        "3 ) = 0 0 0 0 0 0 1 1 1 1 Table 1: A generic example of the process of phrase pair extraction and representation.",
        "4.1 Classification We trained our reordering models by both primal and dual classifiers for 100 iterations.",
        "For the dual MLR, different shrinking levels have been tried by varying the parameter (\u000f) in Algorithm 1.",
        "Table 2 reports the training time and classification error rate of these models.",
        "Training the dual MLR with moderate shrinking level (i.e. \u000f = 0.1) is almost four times faster than training the primal one.",
        "Choosing larger value for (\u000f) leads to faster training but might harm the performance as shown below.",
        "Classifier Training Time Error Rate Primal MLR 1 hour 9 mins 17.81% Dual MLR \u000f:0.1 18 minutes 17.95% Dual MLR \u000f:1.0 13 minutes 21.13% Dual MLR \u000f:0.01 22 minutes 17.89% Table 2: Performance of the primal and dual MLR based on held-out data.",
        "Figure 1 shows the percentage of active set during training dual MLR with various shrinking levels.",
        "Interestingly, the dual MLR could disregard more than 99% of the data after a couple of iterations.",
        "For very large corpus, the data might not fit in memory and training primal MLR will take long time due to severe disk-swapping.",
        "In this sit-uation, using dual MLR is very beneficial.",
        "2 4 6 8 10 12 14 16 18 200 10 20 30 40 50 60 70 80 90 100 Training iteration Per cen tag e o f ac tive ph ras e p airs ?",
        "= 0.1 ?",
        "= 1.0 ?",
        "= 0.01 Figure 1: Percentage of active set in dual MLR.",
        "As the data size decreases, each iteration takes far less computation time (see Table 2 for total time).",
        "1761 4.2 Translation We used the Moses toolkit (Koehn et al., 2007) with its default settings to build three phrase-based translation systems.",
        "They differ in how their reordering models were estimated.",
        "The language model is a 5-gram with interpolation and Kneser-Ney smoothing (Kneser and Ney, 1995).",
        "We tuned the system by using MERT technique (Och, 2003).",
        "As commonly used in statistical machine trans-lation, we evaluated the translation performance by BLEU score (Papineni et al., 2002).",
        "The test sets are NIST MT06 and MT08 where the English sides are 35,481 words (1056 sentences) and 116,840 words (3252 sentences), respectively.",
        "Table 3 shows the BLEU scores for the translation systems.",
        "We also computed statistical significance for the models using the paired bootstrap resam-pling method (Koehn, 2004b).",
        "Translation System MT06 MT08 Baseline + Lexical.",
        "model 30.86 34.22 Baseline + Primal MLR 31.37* 34.85* Baseline + Dual MLR \u000f:0.1 31.36* 34.87* Table 3: BLEU scores for Arabic-English translation systems with different reordering models (*: better than the lexicalized model with at least 95% statistical significance).",
        "5 Conclusion In training such system with large data sizes and big dimensionality, computational complexity become a serious issue.",
        "In SMT, maximum entropy-based reordering model is often introduced as a better alternative to the commonly used lexicalized one.",
        "However, training this discriminative model using large-scale corpus might be computationally expensive due to the iterative learning.",
        "In this paper, we propose training the model using the dual MLR with shrinking method.",
        "It is almost four times faster than the primal MLR (also know as MaxEnt) and much more memory-efficient.",
        "For very large corpus, the data might not fit in memory and training primal MLR will take long time due to severe disk-swapping.",
        "In this sit-uation, using dual MLR is very beneficial.",
        "The proposed method is also useful for many classification problems in natural language processing that require large-scale data.",
        "References"
      ]
    }
  ]
}
