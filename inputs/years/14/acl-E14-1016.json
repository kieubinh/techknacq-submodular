{
  "info": {
    "authors": [
      "Eduardo Blanco",
      "Dan Moldovan"
    ],
    "book": "EACL",
    "id": "acl-E14-1016",
    "title": "Leveraging Verb-Argument Structures to Infer Semantic Relations",
    "url": "https://aclweb.org/anthology/E14-1016",
    "year": 2014
  },
  "references": [
    "acl-D08-1008",
    "acl-J02-3001",
    "acl-J05-1004",
    "acl-J08-2005",
    "acl-L08-1018",
    "acl-N10-1030",
    "acl-N10-1058",
    "acl-P10-1070",
    "acl-P10-1160",
    "acl-P11-1146",
    "acl-P86-1004",
    "acl-P91-1003",
    "acl-P98-1013",
    "acl-S10-1008",
    "acl-S10-1059",
    "acl-S10-1065",
    "acl-S12-1001",
    "acl-W04-2705",
    "acl-W05-0620",
    "acl-W05-0625",
    "acl-W07-2003",
    "acl-W09-2415",
    "acl-W09-2417"
  ],
  "sections": [
    {
      "text": [
        "Abstract",
        "This paper presents a methodology to infer implicit semantic relations from verb-argument structures.",
        "An annotation effort shows implicit relations boost the amount of meaning explicitly encoded for verbs.",
        "Experimental results with automatically obtained parse trees and verb-argument structures demonstrate that inferring implicit relations is a doable task."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Automatic extraction of semantic relations is an important step towards capturing the meaning of text.",
        "Semantic relations explicitly encode links between concepts.",
        "For example, in The accident left him a changed man, the ?accident?",
        "is the CAUSE of the man undergoing some ?change?.",
        "A question answering system would benefit from detecting this relation when answering Why did he change?",
        "Extracting all semantic relations from text is a monumental task and is at the core of language understanding.",
        "In recent years, approaches that aim at extracting a subset of all relations have achieved great success.",
        "In particular, previous research (Carreras and Ma`rquez, 2005; Punyakanok et al., 2008; Che et al., 2010; Zapirain et al., 2010) focused on verb-argument structures, i.e., relations between a verb and its syntactic arguments.",
        "PropBank (Palmer et al., 2005) is the corpus of reference for verb-argument relations.",
        "However, relations between a verb and its syntactic arguments are only a fraction of the relations present in texts.",
        "Consider the statement [Mr. Brown]NP 1 succeeds [Joseph W. Hibben, who retired last August]NP 2 and its parse tree (Figure 1).",
        "Verb-argument relations encode that NP 1 is the AGENT and NP 2 is the THEME of verb ?succeeds?",
        "(Prop- Bank uses labels ARG 0 and ARG 1 ).",
        "Any semantic relation between ?succeeds?",
        "and concepts dominated in the parse tree by one of its syntactic arguments NP 1 or NP 2 , e.g., ?succeeds?",
        "oc-S NP 1 VP Mr. Brown VBZ NP 2 succeeds AGENT THEME TIME-AFTER [Joseph W. Hibben, who]AGENT [retired]v [last August]TIME Figure 1: Example of parse tree and verb-argument structures (solid arrows).",
        "The relation between ?succeeds?",
        "and ?last August?",
        "is missing, but a TIME-AFTER holds (dashed arrow).",
        "curred after ?last August?, are missing.",
        "Note that in this example, verb-argument structures encode that ?retired ?",
        "has TIME ?last August?, and this knowledge could be exploited to infer the missing relation.",
        "The work presented here stems from two observations: (1) verbs are semantically connected with concepts that are not direct syntactic arguments (henceforth, implicit relations); and (2) verb-argument structures can be leveraged to infer implicit relations.",
        "This paper goes beyond verb-argument structures and targets implicit relations like the one depicted above.",
        "TIME, LOCATION, MANNER, PURPOSE and CAUSE are inferred without imposing syntactic restrictions between their argu-ments: systems trained over PropBank do not attempt to extract these relations.",
        "An annotation effort demonstrates implicit relations reveal as much as 30% of meaning on top of verb-argument structures.",
        "The main contributions are: (1) empirical study of verb-argument structures and implicit relations in PropBank; (2) annotations of implicit relations on top of PropBank; (3) novel features extracted from verb-argument structures; and (4) experimental results with features derived from gold and automatically obtained linguistic information, showing implicit relations can be extracted in a realistic environment.",
        "145 2 Related Work Several systems to extract verb-argument structures from plain text have been proposed (Johans- son and Nugues, 2008; Che et al., 2010).",
        "The work presented here complements them with additional semantic relations.",
        "The TimeBank corpus (Pustejovsky et al., 2003) and TempEval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE.",
        "Extracting missing relations is not a new problem.",
        "Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991).",
        "This section focuses on more recent work.",
        "Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments.",
        "Their supervised systems obtain F-measures of 42.3 and 50.3 (Ger- ber and Chai, 2012).",
        "Laparra and Rigau (2013a) present a deterministic algorithm and obtain an F-measure of 45.3.",
        "In contrast, our approach does not focus on a few selected predicates or core arguments.",
        "It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.",
        "), whose meaning is shared across verbs.",
        "The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cross-sentence missing core arguments in both PropBank and FrameNet (Baker et al., 1998).",
        "Ruppenhofer et al. (2013) detail the annotations and results.",
        "The task proved extremely difficult, participants (Chen et al., 2010; Tonelli and Delmonte, 2010) reported overall F-measures around 2 (out of 100).",
        "Posterior work (Silberer and Frank, 2012; Laparra and Rigau, 2013b) reported F-measures below 20 for the same task.",
        "The work presented here does not target missing core arguments but modifiers within the same sentence.",
        "Furthermore, results show our proposal is useful in a real environment.",
        "Finally, our previous work (Blanco and Moldovan, 2011; Blanco and Moldovan, 2014) proposed composing new relations out of chains of previously extracted relations.",
        "This approach is unsupervised and accurate (88% with gold an-notations), but inferences are made only between the ends of chains of existing relations.",
        "Our current proposal also leverages relations previously extracted, but productivity is higher and results with automatic annotations are presented.",
        "[But]MDIS [the surprisingly durable seven-year economic expansion]ARG 0 has [made]v [mincemeat]ARG 1 [of more than one forecast]ARG 2 .",
        "Also, financial planners advising on insurance say that to their knowledge there has not yet been [a tax ruling]ARG 0 [exempting]v [these advance payments]ARG 1 [from taxes]ARG 2 .",
        "Table 1: Examples of verb-argument structures from PropBank.",
        "3 Verb-Argument Structures and Implicit Relations Throughout this paper, R(x, y) denotes a semantic relation R holding between x and y. R(x, y) is interpreted ?x has R y?, e.g., AGENT(took, Bill ) could be read ?took has AGENT Bill?.",
        "Verb-argument structures, or semantic roles, account for semantic relations between a verb and its syntactic arguments.",
        "In other words, R(x, y) is a semantic role if ?x?",
        "is a verb and ?y?",
        "a syntactic argument of ?x?, and all semantic roles with ?x?",
        "as first argument form the verb-argument structure of verb ?x?.",
        "Implicit relations are relations R(x, y) where x is a verb and y is not a syntactic argument of x.",
        "The work presented in this paper aims at complementing verb-argument structures with implicit semantic relations.",
        "We follow a practical approach by inferring implicit relations from PropBank's verb-argument structures.",
        "We believe this is an advantage since PropBank is well-known in the field and several tools to predict PropBank annotations are documented and publicly available.",
        "The work presented here could be incorporated in any NLP pipeline after role labeling without modifications to other components.",
        "Furthermore, working on top of PropBank allows us to quantify the impact of features derived from gold and automatically extracted linguistic information when inferring implicit relations (Section 6).",
        "3.1 Verb-Argument structures in PropBank PropBank (Palmer et al., 2005) annotates verb-argument structures on top of the syntactic trees of the Penn TreeBank (Marcus et al., 1994).",
        "It uses a set of numbered arguments2 (ARG 0 , ARG 1 , ARG 2 , etc.)",
        "and modifiers (AM-TMP, AM-MNR, etc.).",
        "Numbered arguments do not share a common meaning across verbs, they are defined on a 1E.g., Illinois SRL, http://cogcomp.cs.illinois.edu/ page/software; SENNA, http://ml.nec-labs.com/senna/; SwiRL, http://www.surdeanu.info/mihai/swirl/ 2Numbered arguments are also referred to as core.",
        "146 SNP 1 VP 1 NP 2 VP 2 VBD VP 3 The first hybrid corn seeds VBN S-ADV were VBD PP produced TIME THEME MANNER using this mechanical approach introduced THEME TIME in the 1930s Figure 2: Verb-argument structures (solid arrows) and inferred implicit semantic relation (dashed arrow).",
        "AM-LOC: location AM-CAU: cause AM-EXT: extent AM-TMP: time AM-DIS: discourse connective AM-PNC: purpose AM-ADV: general-purpose AM-MNR: manner AM-NEG: negation marker AM-DIR: direction AM-MOD: modal verb Table 2: Argument modifiers in PropBank.",
        "Label # predicates % predicates ARG 0 79,334 70.26% ARG 1 106,331 94.17% ARG 2 24,560 21.75% AM-TMP 19,756 17.50% AM-MNR 7,833 6.94% AM-LOC 7,198 6.37% AM-PNC 2,784 2.47% AM-CAU 1,563 1.38% Table 3: Counts of selected PropBank semantic roles.",
        "Total number of predicates is 112,917. verb by verb basis in each frameset.",
        "For exam-ple, ARG 2 is used to indicate ?created-from, thing changed?",
        "with verb make and ?entity exempted from?",
        "with verb exempt (Table 1).",
        "Unlike numbered arguments, modifiers share a common meaning across verbs (Table 2).",
        "Some modifiers are arguably not a semantic relation and are not present in most relation inventories (Tratz and Hovy, 2010; Hendrickx et al., 2009).",
        "For example, AM-NEG and AM-MOD signal the presence of negation and modals, e.g., [wo]AM-MOD[n't]AM-NEG [go]v. For more information about PropBank annotations and examples, refer to the annotation guidelines.3 Inspecting PropBank annotations one can easily conclude that numbered arguments dominate the annotations and only a few modifiers are an-3http://verbs.colorado.edu/ ?",
        "mpalmer/projects/ace/ PBguidelines.pdf notated (Table 3).",
        "ARG 0 and ARG 1 are present in most verb-argument structures, other numbered arguments are often not defined in the corresponding frameset and are thus not annotated.",
        "Examining PropBank one can also conclude that information regarding TIME, LOCATION, MANNER, CAUSE and PURPOSE for a given verb is often present, yet not annotated because the text encoding this knowledge is not a direct syntactic argument of the verb (Section 4.3).",
        "Because of this fact, we decided to focus on these five relations.",
        "3.2 Implicit relations in PropBank Two scenarios are possible when inferring an implicit relation R(x, y): (1) a semantic role R?",
        "(x?, y) exists; or (2) such a semantic role does not exists.",
        "In (1), y is a syntactic argument of some verb x?, where x 6= x?",
        "and in (2) that is not the case.",
        "Inferences under scenario (1) can be further classified into (1a) when a semantic role R??",
        "(x, y?)",
        "such that y?",
        "contains y exists; or (1b) when such a semantic roles does not exist.",
        "The remainder of this section exemplifies the three scenarios.",
        "The example in Figure 1 falls under scenario (1a).",
        "Semantic roles encode, among others, ?re- tired ?",
        "has TIME ?last August?, and ?succeeds?",
        "has AGENT ?Mr.",
        "Brown?",
        "and THEME ?Joseph W. Hi-bben, who retired last August?.",
        "The second argument of implicit relation TIME-AFTER(succeeds, last August) is a semantic role of ?retired ?",
        "and is contained in the THEME of ?succeeds?.",
        "Figure 2 shows a statement in which implicit relation TIME(produced, in the 1930s) could be inferred under scenario (1b).",
        "Semantic roles of ?pro- duced ?",
        "only indicate that NP 2 is the THEME and S-ADV the MANNER; roles of ?introduced ?",
        "indicate that NP 1 is the THEME and ?",
        "[in the 1930s]PP?",
        "the TIME.",
        "In this case, there is no connection be-147 rs = {TIME, LOCATION, MANNER, CAUSE, PURPOSE}; foreach semantic role R?",
        "(x?, y) such that R?",
        "?",
        "rs do foreach verb x in the same sentence do generate potential implicit relation R(x, y); Algorithm 1.",
        "Procedure to generate all potential implicit relations in scenario (1) (Section 3.2).",
        "tween ?produced ?",
        "and ?",
        "[in the 1930s]PP?",
        "or any other node subsuming this PP in the parse tree.",
        "Scenario (2) occurs whenever the second argument of implicit relation R(x, y) is not a syntactic argument of a verb.",
        "If it were, a semantic role R?",
        "(x?, y) would exist and it would fall under scenario (1).",
        "For example, in [I]AGENT [gave]v [her]RECIPIENT [a book from 1945]THEME, we could infer the implicit semantic relation ?gave occurred after 1945?.",
        "4 Annotating Implicit Relations Inferring all implicit semantic relations is a challenging task.",
        "This paper targets implicit relations that can be inferred under scenarios (1a, 1b); scenario (2) is reserved for future work.",
        "All potential implicit relations under scenario (1) are generated using Algorithm 1.",
        "A manual annotation effort discards potential implicit relations that do not hold in order to create a gold standard.",
        "4.1 Annotation Guidelines Annotators are faced with the task of deciding whether a potential implicit relation R(x, y) holds.",
        "If it does, they mark it with YES, otherwise with NO.",
        "Annotators were initially trained with the original PropBank annotation guidelines4 as this task is very similar to annotating PropBank semantic roles.",
        "Indeed, the only difference is that ?y?",
        "is not a syntactic argument of ?x?.",
        "After some preliminary annotations, we found it useful to account for three subtypes of TIME.",
        "This way, richer semantic connections are inferred.",
        "When the task is to decide whether implicit relation TIME(x, y) holds, annotators have four labels to choose from: (1) TIME-BEFORE: x occurred before y; (2) TIME-AFTER: x occurred after y; (3) TIME-SAME x occurred at/during y; and (4) NO: y does not describe temporal information of x.",
        "If more than one label is valid, annotators choose the one encoding the temporal context y of x starting the earliest.",
        "Namely, TIME-BEFORE 4http://verbs.colorado.edu/ ?",
        "mpalmer/projects/ace/ PBguidelines.pdf has the highest priority, followed by TIME-SAME, TIME-AFTER and finally NO.",
        "Annotation examples are detailed in Section 4.2, the more complex annotations involving TIME are illustrated below.",
        "Consider the following statement and PropBank annotations: [The government's decision]ARG 2 , v 1 [reflects]v 1 [their true desires before [the next election]ARG 1 , v 2 , [expected]v 2 [in late 1991]TIME, v 2 ]ARG 1 , v 1 .",
        "When annotating potential implicit semantic relation R(reflects, in late 1991 ), annotators may select TIME-BEFORE, TIME-SAME and TIME-AFTER.",
        "However, they select TIME-BEFORE because it indicates the temporal context of ?reflects?",
        "that starts the earliest.",
        "4.2 Annotation Examples Several annotations examples are shown in Table 4.",
        "Semantic roles for statement (1) include TIME(remain, in 1990 ), MANNER(remain, at about 1,200 cars) and no other TIME or MANNER.",
        "Implicit relations reveal two extra semantic connections: TIME-BEFORE(said, in 1990 ) and TIME-BEFORE(expects, in 1990 ), i.e., ?said ?",
        "and ?expects?",
        "occurred before ?1990 ?.",
        "The potential implicit relations MANNER(said, at about 1,200 cars) and MANNER(expects, at about 1,200 cars) do not hold and are annotated N. Interpreting statement (2) one can see that ?this past summer?",
        "is not only indicating the TIME of ?proposed ?",
        "; events encoded by verbs ?make?",
        "and ?exempt?",
        "occurred after ?this past summer?.",
        "In this example, two implicit semantic relations are inferred from a single semantic role.",
        "Statement (3) shows that two potential implicit relations R(x, y) and R(x?, y) sharing the second argument ?y?",
        "may be assigned different labels.",
        "Regarding time, semantic roles only include TIME(report, in December).",
        "Implicit relations add TIME-BEFORE(proposed, in December) and TIME-SAME(allow, in December).",
        "Two implicit LOCATION relations are inferred in statement (4): ?discovered ?",
        "and ?preserving?",
        "occurred -- in the test-tube experiments?.",
        "The potential implicit relation LOCATION(said, in the test-tube experiments) is discarded (annotated N).",
        "Statement (5) shows two potential implicit MANNER that can be inferred.",
        "The ?program?",
        "was ?aired ?",
        "and ?seen by 12 million viewers?",
        "in the following manner: ?With Mr. Vila as host?.",
        "148 Statement TMP LOC MNR PRP CAU B A S N Y N Y N Y N Y N 1: Rolls-Royce said it expects [its U.S. sales]ARG 1 to [remain]v [steady]ARG 3 [at about 1,200 cars]MANNER [in 1990]TIME .",
        "?",
        "said, [in 1990]TIME X - - - ?",
        "expects, [in 1990]TIME X - - - ?",
        "said, [at about 1,200 cars]MANNER X ?",
        "expects, [at about 1,200 cars]MANNER X 2: They make the argument in letters to the agency about [rule changes]ARG 1 [proposed]v [this past summer]TIME that, among other things, exempt many middle-management executives from government supervision.",
        "?",
        "make, [this past summer]TIME X - - ?",
        "exempt, [this past summer]TIME X - - 3: The proposed changes also allow [executives]ARG 0 to [report]v [exercises of options]ARG 1 [in December]TIME .",
        "?",
        "proposed, [in December]TIME X - - - ?",
        "allow, [in December]TIME - X - 4: Two Japanese scientists said they discovered [an antibody that]ARG 0 , [in laboratory test-tube experiments]LOCATION , [kills]v [AIDS-infected cells]ARG 1 [while preserving healthy cells]TIME .",
        "?",
        "said, [in laboratory test-tube experiments]LOCATION X ?",
        "discovered, [in laboratory test-tube experiments]LOCATION X - ?",
        "preserving, [in laboratory test-tube experiments]LOCATION X - 5: [With Mr. Vila as host]MANNER , ?",
        "[This Old House]ARG 1 ?",
        "[became]v [one of the Public Broadcasting Service's top 10 programs]ARG 2 , [airing weekly on about 300 of the network 's stations and seen by an average of 12 million viewers]AM-ADV .",
        "?",
        "airing, [With Mr. Vila as host]MANNER X - ?",
        "seen, [With Mr. Vila as host]MANNER X - [6: It]ARG 0 [raised]v [financing of 300 billion lire]ARG 1 [for the purchase this summer by another Agnelli-related group of the food concern Galbani S.p.A.]PURPOSE , [by selling a chunk of its IFI shares to Mediobanca S.p.A.]MANNER ?",
        "selling, [for the purchase this summer by another .",
        ".",
        ".",
        "]PURPOSE X - 7: [Greece and Turkey]ARG 0 , for example, are suspected of [overstating]v [their arsenals]ARG 1 [in hopes that they can emerge from the arms-reduction treaty with large remaining forces to deter each other]PURPOSE .",
        "?",
        "suspected, [in hopes that they can emerge from the .",
        ".",
        ".",
        "]PURPOSE X 8: .",
        ".",
        ".",
        "the rationalization that [given the country's lack of natural resources]CAUSE , [they]ARG 0 [must]AM-MOD [work]v [hard]MANNER [to create value through exports]ARG 1 and buy food with the surplus.",
        "?",
        "create, [given the country's lack of natural resources]CAUSE X - ?",
        "buy, [given the country's lack of natural resources]CAUSE X - 9: Its third-quarter earnings were lower than analysts had forecast, and the company said [it]ARG 0 had [lowered]v [its projections for earnings growth through the end of 1990]ARG 1 [because of planned price cuts]CAUSE .",
        "?",
        "forecast, [because of planned price cuts]CAUSE X ?",
        "said, [because of planned price cuts]CAUSE X Table 4: Examples of potential implicit relations and their annotations.",
        "All of them but the ones annotated with N can be inferred.",
        "B stands for BEFORE, A for AFTER, S for SAME, N for NO and Y for YES.",
        "PropBank semantic roles from which implicit relations are generated are indicated between brackets.",
        "Statement (6, 7) exemplify potential implicit PURPOSE relations.",
        "While the ?selling?",
        "event in statement (6) has as its purpose ?the purchase [.",
        ".",
        ". ]",
        "?",
        "(label Y), the ?suspected ?",
        "event in statement (7) is clearly not done so that ?they (Greece and Turkey) can emerge from the [.",
        ".",
        ". ]",
        "?",
        "(label N).",
        "Finally, statements (8, 9) exemplify potential implicit CAUSE relations.",
        "In (8), both ?create?",
        "and ?buy?",
        "are done due to the ?country's lack of natural resources?.",
        "However, in (9), the analysts ?forecast- ing?",
        "and the company ?saying?",
        "do not have as their cause ?planned price cuts?.",
        "4.3 Annotation Analysis Table 5 shows counts for all potential implicit relations annotated.",
        "All labels except N indicate a valid implicit relation.",
        "94.1% of potential implicit relations generated from a TIME semantic role can be inferred.",
        "Other roles yield less inferences in relative terms, but substantial additional mean-ing: LOCATION 39.4%, MANNER 16.7%, PURPOSE 29.4%, and CAUSE 30.2%.",
        "Two annotators performed the annotations.",
        "A simple script generated all potential implicit relations and prompted for a label: BEFORE, AFTER, SAME or NO if the potential implicit relation was generated from a TIME semantic role; YES or NO otherwise.",
        "Annotators are not concerned with argument identification, as arguments of implicit relations are retrieved from the verb-argument structures in PropBank (Algorithm 1).",
        "This makes the annotation process easier and faster.",
        "Annotation quality was calculated with two agreement coefficients: observed agreement (raw percentage of equal annotations) and Cohen's ?",
        "(Artstein and Poesio, 2008).",
        "The actual num-149 Source No.",
        "Name Description ba sic x 1,2 word, POS tag x's surface form and part-of-speech tag 3 voice whether x is in active or passive voice y 4,5 first word, POS tag first word and part of speech tag in y 6,7 last word, POS tag last word and part-of-speech tag in y 8,9 head, POS tag head of y and its part-of-speech tag 10?12 node, left and right sibling syntactic nodes of y, and its left and right siblings 13 subcategory concatenation of y's children nodes x, y 14 direction whether x occurs before or after y 15 subsumer common syntactic node between x and y 16 path syntactic path between x and y pr ed st ru ct u re s x ps 17?31 verb semantic roles flags indicating presence of semantic roles in x ps y ps 32,33 verb, POS tag verb in y ps and its part-of-speech tag 34 arg label semantic role between verb in y ps and y 35?49 arg semantic roles flags indicating presence of semantic roles in y ps x ps, 50 overlapping semantic role role R??",
        "linking x and y?, where y?",
        "contains y y ps 51 overlapping head head of y?",
        "in semantic role detected in feature 50 52 overlapping direct whether feature 51 is the verb in y ps Table 6: Complete feature set to determine whether a potential implicit semantic relation R(x, y) should be inferred.",
        "Second column indicates the source: first or second argument (x, y), or their respective predicate structures (x ps, y ps).",
        "Features in bold are novel and specially designed for our task.",
        "Label # instances % instances TIME B 3,033 38.4% A 2,886 36.5% S 1,514 19.2% N 463 5.9% All 7,896 100.0% LOCATION Y 3,345 39.4% N 5,151 60.6% All 8,496 100.0% MANNER Y 1,600 16.7% N 7,987 83.3% All 9,587 100.0% PURPOSE Y 821 29.4% N 1,971 70.6% All 2,792 100.0% CAUSE Y 404 30.2% N 909 69.2% All 1,313 100.0% Table 5: Number of potential implicit relations (in- stances) annotated and counts for each label.",
        "Total number of instances is 30,084. bers are: 78.16% (observed) / 0.687 (?)",
        "for TIME, 86.63% / 0.733 for LOCATION, 93.02% / 0.782 for MANNER, 88.60% / 0.734 for PURPOSE, and 90.91% / 0.810 for CAUSE.",
        "These agreements are either comparable or superior to similar previous annotation efforts.",
        "Girju et al. (2007) reported observed agreements between 47.8% and 86.1% when annotating 7 semantic relations between nominals, and Bethard et al. (2008) observed agreements of 81.2% and 77.8% (Kappa: 0.715 and 0.556) when annotating temporal and causal relations between event pairs.",
        "5 Inferring Implicit Relations Inferring implicit relations is reduced to (1) generating potential implicit relations (Algorithm 1) and (2) labeling them.",
        "The second task determines if potential implicit relations should be discarded or inferred, all labels but N indicate potential implicit relations that should be inferred.",
        "We follow a standard supervised machine learning approach where each potential implicit relation is an instance.",
        "Instances were divided into training (70%) and test (30%).",
        "The feature set (Section 5.1) and model parameters were tuned using 10-fold stratified cross-validation over the training split, and results (Section 6) are reported using the test split.",
        "More features than the ones presented were tried and discarded because they did not improve per-formance, e.g., syntactic path between verbs in the verb-argument structures of x and y, depth of both structures, number of tokens in y.",
        "5.1 Feature Selection The full set of features to determine whether a potential implicit relation R(x, y) can be inferred is summarized in Table 6.",
        "Features are classified into basic and predicate structures.",
        "The former are commonly used by semantic role labelers.",
        "The latter exploit the output of role labelers, i.e., verb-argument structures, and, to our knowledge, are novel.",
        "Results show predicate structures features improve performance (Section 6.2).",
        "Basic features are derived from lexical and syntactic information.",
        "We do not elaborate more on 150 Feat No.",
        "Value 1,2 succeeds, VBZ 3 active 4,5 last, JJ 6,7 August, NNP 8,9 August, NNP 10?12 NP, VBD, nil 13 JJ-NNP 14 after 15 VP 16 VBZ+VP-NP-SBAR-S-VP-NP 17?31 ARG 0 and ARG 1 true, rest false 32,33 retired, VBD 34 AM-TMP 35-49 ARG 0 and AM-TMP true, rest false 50 ARG 1 51 Hibben 52 false Table 7: Feature values when deciding if R(succeeds, last summer) can be inferred from the verb-argument structures in Figure 1. these features, detailed descriptions and examples are provided by Gildea and Jurafsky (2002).",
        "Features (17?52) are derived from the predicate structures of x and y and specially defined to infer implicit semantic relations.",
        "Features (17?31, 35?",
        "49) are flags indicating the presence of semantic roles in the predicate structures of x and y.",
        "Features (32?34) characterize the semantic role R?",
        "(x?, y) from which the potential implicit relation was generated.",
        "They specify verb x?, its part-of-speech, and label R?.",
        "Note that x?",
        "is not present in the potential implicit relation R(x, y), but incorporating this information helps determining whether a relation actually holds as well as label R (TIME- BEFORE, TIME-AFTER, TIME-SAME, etc.).",
        "Finally, features 50?52 apply to inferences under scenario (1a) (Section 3.2).",
        "Feature (50) indicates the semantic role R??",
        "(x, y?",
        "), if any, such that y?",
        "contains y.",
        "Feature (51) indicates the head of argument y?",
        "found in feature (50).",
        "Feature (52) captures whether the head calculated in feature (51) is the verb in the predicate structure of y.",
        "Table 7 exemplifies all features when deciding whether TIME-AFTER(succeeds, last August) can be inferred from the verb-argument structures in Mr. Brown succeeds Joseph W. Hibben, who retired last August (Figure 1).",
        "Table 8 provides an additional example for features 50?52.",
        "6 Experiments and Results Experiments were carried out using Support Vector Machines with RBF kernel as implemented in Mr. Corr resigned to pursue other interests, the airline said.",
        "ARG 0 (resigned, Mr. Corr) AM-PNC(resigned, to pursue other interests) ARG 0 (pursue, Mr. Corr) ARG 1 (pursue, other interests) ARG 0 (said, the airline) ARG 1 (said, Mr. Corr resigned to pursue other interests) feature 50, overlapping sem rel ARG 1 feature 51, overlapping head resigned feature 52, overlapping direct true Table 8: PropBank roles and values for features (50?52) when predicting potential implicit relation R(said, to pursue other interests), labeled N. LIBSVM (Chang and Lin, 2011).",
        "Parameters ?",
        "and ?",
        "were tuned by grid search using 10-fold cross validation over training instances.",
        "Results are reported using features extracted from gold and automatic annotations.",
        "Gold annotations are taken directly from the Penn TreeBank and PropBank.",
        "Automatic annotations are obtained with Polaris (Moldovan and Blanco, 2012), a semantic parser that among others is trained with PropBank.",
        "Results using gold (automatic) annotations are obtained with a model trained with gold (automatic) annotations.",
        "6.1 Detailed Results Table 9 presents per-relation and overall results.",
        "In general terms, there is a decrease in performance when using automatic annotations.",
        "The difference is most noticeable in recall and it is due to missing semantic roles, which in turn are often due to syntactic parsing errors.",
        "This is not surprising as in order for an implicit relation R(x, y) to be generated as potential and fed to the learning algorithm for classification, a semantic role R?",
        "(x?, y) must be extracted first (Algorithm 1).",
        "However, using automatic annotations brings very little decrease in precision.",
        "This leads to the conclusion that as long as ?y?",
        "is identified as a semantic role of some verb, even if it is mislabeled, one can still infer the right implicit relations.",
        "Since results obtained with automatic parse trees and semantic roles are a realistic estimation of performance, the remainder of the discussion focuses on those.",
        "Results with gold annotations are provided for informational purposes.",
        "Overall results for inferring implicit semantic relations are encouraging: precision 0.66, recall 0.58 and F-measure 0.616.",
        "Direct comparison with previous work is not possible because the implicit relations we aim at inferring have not been considered before.",
        "However, we note the top 151 gold automatic basic basic + ps basic basic + ps P R F P R F P R F P R F TIME B .66 .72 .689 .72 .74 ?.730 .64 .65 .643 .68 .67 .677 A .63 .74 .681 .67 .75 .708 .61 .68 .642 .66 .72 .687 S .57 .41 .477 .54 .45 .491 .55 .36 .437 .55 .38 .450 LOCATION Y .71 .61 .656 .70 .64 .669 .71 .56 .624 .71 .58 .635 MANNER Y .65 .38 .480 .60 .45 .514 .54 .45 .489 .64 .41 .500 PURPOSE Y .65 .58 .613 .69 .60 .642 .56 .49 .525 .68 .49 .572 CAUSE Y .71 .60 .650 .74 .62 .675 .69 .65 .670 .71 .63 .669 All .66 .61 .625 .67 .64 ?.651 .63 .57 .591 .66 .58 ?.616 Table 9: Results obtained with the test split using features extracted from gold and automatic annotations, and using basic and predicate structures (ps) features.",
        "Statistical significance between F-measures using basic and basic + predicate structures features is indicated with ?",
        "(confidence 95%).",
        "performer (Koomen et al., 2005) at CoNLL-2005 Shared Task on role labeling obtained the following F-measures when extracting the same relations between a verb and its syntactic arguments: 0.774 (TIME), 0.6033 (LOCATION), 0.5922 (MANNER), 0.4541 (PURPOSE) and 0.5397 (CAUSE).",
        "The most difficult relations are TIME-SAME and MANNER, F-measures are 0.450 and 0.500 respectively.",
        "Even when using gold annotations these two relations are challenging: F-measures are 0.491 for TIME-SAME, an increase of 9.1%, and 0.514 for MANNER, an increase of 2.8%.",
        "Results show that other relations can be inferred with F-measures between 0.635 and 0.687, the only exception is PURPOSE with an F-measure of 0.572.",
        "6.2 Feature Ablation Results in Table 9 suggest that while implicit relations can be inferred using basic features, it is beneficial to complement them with the novel features derived from predicate structures.",
        "This is true for all relations except CAUSE when using automatic annotations with a negligible difference of 0.001.",
        "When considering all implicit relations, the difference in performance is 0.616 ?",
        "0.591 = 0.025, an increase of 4.2% that is statistically significant (Z-test, confidence 95%).",
        "The positive impact of features derived from predicate structures is most noticeable when inferring PURPOSE, with an increase of 8.9% (0.572 ?",
        "0.525 = 0.047).",
        "TIME-BEFORE and TIME-AFTER also benefit, with increases of 5.3% (0.677 ?",
        "0.643 = 0.034) and 7.0% (0.687?0.642 = 0.045) respectively.",
        "The improvement predicate structures features bring is statistically significant when taking into account all relations (confidence 95%).",
        "However, due to the lower number of instances, differences in performance when considering individual relations is not statistically significant.",
        "7 Conclusions Verb-argument structures, or semantic roles, comprise semantic relations between a verb and its syntactic arguments.",
        "The work presented in this paper leverages verb-argument structures to infer implicit semantic relations.",
        "A relation R(x, y) is implicit if x is a verb and y is not a syntactic argument of x.",
        "The method could be incorporated into any NLP pipeline after role labeling without modifications to other components.",
        "An analysis of verb-argument structures and implicit relations in PropBank has been presented.",
        "Out of all potential implicit relations R(x, y), this paper targets those that can be generated from a semantic role R?",
        "(x?, y), where x 6= x?.",
        "A manual annotation effort demonstrates implicit relations yield substantial additional meaning.",
        "Most of the time (94.1%) a semantic role TIME(x?",
        ", y) is present, we can infer temporal information for other verbs within the same sentence.",
        "Productivity is lower but substantial with other roles: 39.4% (LOCATION), 30.2% (CAUSE), 29.4% (PURPOSE) and 16.7% (MANNER).",
        "Experimental results show that implicit relations can be inferred using automatically obtained parse trees and verb-argument structures.",
        "Standard machine learning is used to decide whether a potential implicit relation should be inferred, and novel features characterizing the verb-argument structures we infer from have been proposed.",
        "152 References"
      ]
    }
  ]
}
