{
  "info": {
    "authors": [
      "Cyril Allauzen",
      "Bill Byrne",
      "AdriÃ  de Gispert",
      "Gonzalo Iglesias",
      "Michael Riley"
    ],
    "book": "CL",
    "id": "acl-J14-3008",
    "title": "Pushdown Automata in Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/J14-3008",
    "year": 2014
  },
  "references": [
    "acl-D07-1090",
    "acl-D08-1065",
    "acl-D10-1027",
    "acl-D10-1125",
    "acl-D11-1003",
    "acl-H05-1022",
    "acl-J10-3008",
    "acl-J95-2002",
    "acl-J97-3002",
    "acl-N03-1019",
    "acl-N04-1022",
    "acl-N06-1033",
    "acl-P07-1019",
    "acl-P10-2006",
    "acl-P11-2001"
  ],
  "sections": [
    {
      "text": [
        "Pushdown Automata in Statistical Machine Translation Cyril Allauzen?",
        "Google Research Bill Byrne??",
        "University of Cambridge Adria` de Gispert??",
        "University of Cambridge Gonzalo Iglesias??",
        "University of Cambridge Michael Riley?",
        "Google Research This article describes the use of pushdown automata (PDA) in the context of statistical machine translation and alignment under a synchronous context-free grammar.",
        "We use PDAs to compactly represent the space of candidate translations generated by the grammar when applied to an input sentence.",
        "General-purpose PDA algorithms for replacement, composition, shortest path, and expansion are presented.",
        "We describe HiPDT, a hierarchical phrase-based decoder using the PDA representation and these algorithms.",
        "We contrast the complexity of this decoder with a decoder based on a finite state automata representation, showing that PDAs provide a more suitable framework to achieve exact decoding for larger synchronous context-free grammars and smaller language models.",
        "We assess this experimentally on a large-scale Chinese-to-English alignment and translation task.",
        "In translation, we propose a two-pass decoding strategy involving a weaker language model in the first-pass to address the results of PDA complexity analysis.",
        "We study in depth the experimental conditions and tradeoffs in which HiPDT can achieve state-of-the-art performance for large-scale SMT.",
        "?",
        "Google Research, 76 Ninth Avenue, New York, NY 10011.",
        "??",
        "University of Cambridge, Department of Engineering.",
        "CB2 1PZ Cambridge, U.K. and SDL Research, Cambridge U.K. E-mail: {wjb31,ad465,gi212}@eng.cam.ac.uk.",
        "Submission received: 6 August 2012; revised version received: 20 February 2013; accepted for publication: 2 December 2013. doi:10.1162/COLI a 00197 ?",
        "2014 Association for Computational Linguistics  1.",
        "Introduction Synchronous context-free grammars (SCFGs) are nowwidely used in statistical machine translation, with Hiero as the preeminent example (Chiang 2007).",
        "Given an SCFG and an n-gram language model, the challenge is to decode with them, that is, to apply them to source text to generate a target translation.",
        "Decoding is complex in practice, but it can be described simply and exactly in terms of the formal languages and relations involved.",
        "We will use this description to introduce and analyze pushdown automata (PDAs) for machine translation.",
        "This formal description will allow close comparison of PDAs to existing decoders which are based on other forms of automata.",
        "Decoding can be described in terms of the following steps: 1.",
        "Translation: T = ?2({s}?G) The first step is to compose the finite language {s}, which represents the source sentence to be translated, with the algebraic relation G for the translation grammar G. The result of this composition projected on the output side is T , a weighted context-free grammar that contains all possible translations of s under G. Following the usual definition of Hiero grammars, we assume that G does not allow unbounded insertions so that T is a regular language.",
        "2.",
        "Language Model Application: L=T ?M The next step is to compose the result of Step 1 with the weighted regular grammarM defined by the n-gram language model, M. The result of this composition is L, whose paths are weighted by the combined language model and translation scores.",
        "3.",
        "Search: l?=argmaxl?LL The final step is to find the path through L that has the best combined translation and language model score.",
        "The composition {s} ?",
        "G in Step 1 that generates T can be performed by a modified CYK algorithm (Chiang 2007).",
        "Our interest is in the different types of automata that can be used to represent T as it is produced by this composition.",
        "We focus on three types of representations: hypergraphs (Chiang 2007), weighted finite state automata (Iglesias et al. 2009a; de Gispert et al. 2010), and PDAs.",
        "We will give a formal definition of PDAs in Section 2, but we will first illustrate and compare these different representations by a simple example.",
        "Consider translating a source sentence ?s1 s2 s3?",
        "with a simple Hiero grammar G : X?",
        "?s1, t2 t3?",
        "S?",
        "?X s2 s3, t1 t2 X t4 t7?",
        "S?",
        "?X s2 s3, t1 t3 X t6 t7?",
        "Step 1 yields the translations T = {?t1 t2 t2 t3 t4 t7?",
        ", ?t1 t3 t2 t3 t6 t7?",
        "}, and Figure 1 gives examples of the different representations of these translations.We summarize the salient features of these representations as they are used in decoding.",
        "Hypergraphs.",
        "As described by Chiang (2007), a Hiero decoder can generate translations in the form of a hypergraph, as in Figure 1a.",
        "As the figure shows, there is a 1:1 correspondence between each production in the CFG and each hyperedge in the hypergraph.",
        "688 Allauzen et al. Pushdown Automata in Statistical Machine Translation (a) Hypergraph 0 1t1 6 t1 2t2 7t3 3X 4t4 5 t7 8X 9t6 10 t7 0 1t2 2t3 S X (b) RTN 0 1t1 2 t1 3t2 4t3 5eps 6eps 7t2 8t2 9t3 10t3 11eps 12eps 13t4 14t6 15t7 16 t7 (c) FSA 0 1t1 6 t1 2t2 7t3 11 ( 12t2 3 4t4 5 t7 [ 8 9 t6 10t7 13t3 ) ] (d) PDA Figure 1 Alternative representations of the regular language of possible translation candidates.",
        "Valid paths through the PDA must have balanced parentheses.",
        "Decoding proceeds by intersecting the translation hypergraph with a language model, represented as a finite automaton, yielding L as a hypergraph.",
        "Step 3 yields a translation by finding the shortest path through the hypergraphL (Huang 2008).",
        "Weighted Finite State Automata (WFSAs).",
        "Because T is a regular language and M is represented by a finite automaton, it follows that T and L can themselves be represented as finite automata.",
        "Consequently, Steps 2 and 3 can be solved 689  using weighted finite-state intersection and single-source shortest path algo-rithms, respectively (Mohri 2009).",
        "This is the general approach adopted in the HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010), which first represents T as a Recursive Transition Network (RTN) and then performs expansion to generate a WFSA.",
        "Figure 1b shows the space of translations for this example represented as an RTN.",
        "Like the hypergraph, it also has a 1:1 correspondence between each production in the CFG and paths in the RTN components.",
        "The recursive RTN itself can be expanded into a single WFSA, as shown in Figure 1c.",
        "Intersection and shortest path algorithms are available for both of these WFSAs.",
        "Pushdown Automata.",
        "Like WFSAs, PDAs are easily generated from RTNs, as will be described later, and Figure 1d gives the PDA representation for this example.",
        "The PDA represents the same language as the FSA, but with fewer states.",
        "Procedures to carry out Steps 2 and 3 in decoding will be described in subsequent sections.",
        "We will show that PDAs provide a general framework to describe key aspects of several existing and novel translation algorithms.",
        "We note that PDAs have long been used to describe parsing algorithms (Aho and Ullman 1972; Lang 1974), and it is well known that pushdown transducers, the extended version of PDA with input and output labels in each transition, do not have the expressive power needed to generate synchronous context-free languages.",
        "For this reason, we do not use PDAs to implement Step 1 in decoding: throughout this article a CYK-like parsing algorithm is always used for Step 1.",
        "However, we do use PDAs to represent the regular languages produced in Step 1 and in the intersection and shortest distance operations needed for Steps 2 and 3.",
        "1.1 HiPDT: Hierarchical Phrase-Based Translation with PDAs We introduce HiPDT, a hierarchical phrase-based decoder that uses a PDA representation for the target language.",
        "The architecture of the system is shown in Figure 2, where CYK parse s with G Build RTN Expand RTN to FSA Intersect FSA with LM FSA Shortest Path FSA Pruning Lattice 1-Best Hypothesis RTN to PDA Replacement Intersect PDA with LM PDA (Pruned) Expansion PDA Shortest Path HiPDTHiFST Figure 2 HiPDT versus HiFST: General flow and high-level operations.",
        "690 Allauzen et al. Pushdown Automata in Statistical Machine Translation we contrast it with HiFST (de Gispert et al. 2010).",
        "Both decoders parse the sentence with a grammar G using a modified version of the CYK algorithm to generate the translation search space as an RTN.",
        "Each decoder then follows a different path: HiFST expands the RTN into an FSA, intersects it with the language model, and then prunes the result; HiPDT performs the following steps: 1.",
        "Convert the RTN into PDA using the replacement algorithm.",
        "The PDA representation for the example grammar in Section 1 is shown in Figure 1.",
        "The algorithm will be described in Section 3.2.",
        "2.",
        "Apply the language model scores to the PDA by composition.",
        "This operation is described in Section 3.3.",
        "3.",
        "Perform either one of the following operations: (a) Shortest path through the PDA to get the exact best translation under the model.",
        "Shortest distance/path algorithm is described in Section 3.4.",
        "(b) Pruned expansion to an FSA.",
        "This expansion uses admissible pruning and outputs a lattice.",
        "We do this for posterior rescoring steps.",
        "The algorithm will be presented in detail in Sections 3.5 and 3.5.2.",
        "The principal difference between the two decoders is the point at which finite-state expansion is performed.",
        "In HiFST, the RTN representation is immediately expanded to an FSA.",
        "In HiPDT, the PDA pruned expansion or shortest path computation is done after the language model is applied, so that all computation is done with respect to both the translation and language model scores.",
        "The use of RTNs as an initial translation representation is somewhat influenced by the development history of our FST and SMT systems.",
        "RTN algorithms were available in OpenFST at the time HiFST was developed.",
        "HiPDT was developed as an extension to HiFST using PDA algorithms, and these have subsequently been included in OpenFST.",
        "A possible alternative approach could be to produce a PDA directly by traversing the CYK grid.",
        "WFSAs could then be generated by PDA expansion, with a computational complexity in speed and memory usage similar to the RTN-based approach.We present RTNs as the initial translation representation because the generation of RTNs during parsing is straightforward and has been previously presented (de Gispert et al. 2010).",
        "We note, however, that RTN composition is algorithmically more complex than PDA (and FSA) composition, so that RTNs themselves are not ideal representations of T if a language model is to be applied.",
        "Composition of PDAs with FSAs will be discussed in Section 3.3.",
        "Figure 3 continues the simple translation example from earlier, showing how HiPDT andHiFST both benefit from the compactness offeredbyWFSA epsilon removal, determinization, andminimization operations.",
        "When applied to PDAs, these operations treat parentheses as regular symbols.",
        "Compact representations of RTNs are shared by both approaches.",
        "Figure 4 illustrates the PDA representation of the translation space under a slightly more complex grammar that includes rules with alternative orderings of nonterminals.",
        "The rule S?",
        "?X1 s2 X2, t1 X1 X2?",
        "produces the sequence ?t1 t3 t4 t5 t6?, and S?",
        "?X1 s2 X2, t2 X2 X1?",
        "produces ?t2 t5 t6 t3 t4?.",
        "The PDA efficiently represents the alternative orderings of the phrases ?t3 t4?",
        "and ?t5 t6?",
        "allowed under this grammar.",
        "In addition to translation, this architecture can also be used directly to carry out source-to-target alignment, or synchronous parsing, under the SCFG in a two-step composition rather than one synchronous parsing stage.",
        "For example, by using M as the automata that accepts ?t1 t2 t3 t6 t7?, Step 2 will yield all derivations that yield this string 691  0 1t1 2t2 3 t3 4X 5X 6 t4 t6 7 t7 0 1t2 2t3 S X (a) Optimized RTN 0 1t1 2t2 3 t3 4t2 5t2 6t3 7t3 8 t4 t6 9 t7 (b) Optimized FSA 0 1t1 2t2 3 t3 8 ( [ 9 t2 4 6 t4 7t7 5 t610t3 ) ] (c) Optimized PDA Figure 3 Optimized representations of the regular language of possible translation candidates.",
        "as a translation of the source string.",
        "This is the approach taken in Iglesias et al. (2009a) and de Gispert et al. (2010) for the RTN/FSA and in Dyer (2010b) for hypergraphs.",
        "In Section 4 we analyze how PDAs can be used for alignment.",
        "1.2 Goals We summarize here the aims of this article.",
        "We will show how PDAs can be used as compact representations of the space T of candidate translations generated by a hierarchical phrase-based SCFG when applied to an input sentence s and intersected with a language model M. We have described the architecture of HiPDT, a hierarchical phrase-based decoder based on PDAs, and have identified the general-purpose algorithms needed 0 1 t1 2t2 3( 4 [ 5 t3 6t57t4 8t69) 10] ) 11] ( [ X?",
        "?s1, t3 t4?",
        "X?",
        "?s3, t5 t6?",
        "S?",
        "?X1 s2 X2, t1 X1 X2?",
        "S?",
        "?X1 s2 X2, t2 X2 X1?",
        "Figure 4 Example of translation grammar with reordered nonterminals and the PDA representing the result of applying the grammar to input sentence s1 s2 s3.",
        "692 Allauzen et al. Pushdown Automata in Statistical Machine Translation to perform translation and alignment; in doing so we have highlighted the similarities and differences relative to translation with FSAs (Section 1.1).",
        "We will provide a formal description of PDAs (Section 2) and present in detail the associated PDA algorithms required to carry out Steps 2 and 3, including RTN replacement, composition, shortest path, expansion, and pruned expansion (Section 3).",
        "We will show both theoretically and experimentally that the PDA representation is well suited for exact decoding under a large SCFG and a small languagemodel.",
        "An analysis of decoder complexity in terms of the automata used in the representation is presented (Section 3).",
        "One important aspect of the translation task is whether the search for the best translation is admissible (or exact) under the translation and language models.",
        "Stated differently, we wish to know whether a decoder produces the actual shortest path found or whether some form of pruning might have introduced search errors.",
        "In our formulation, we can exclude inadmissible pruning from the shortest-path algorithms, and doing so makes it straightforward to compare the computational complexity of a full translation pipeline using different representations of T (Section 4).",
        "We empirically demonstrate that a PDA representation is superior to an FSA representation in the ability to perform exact decoding both in an inversion transduction grammar?style word alignment task and in a translation task with a small language model (Section 4).",
        "In these experiments we take HiFST as a contrastive system for HiPDT, but we do not present experimental results with hypergraph representations.",
        "Hypergraphs are widely used by the SMT community, and discussions and contrastive experiments between HiFST and cube pruning decoders are available in the literature (Iglesias et al. 2009a; de Gispert et al. 2010).",
        "We will propose a two-pass translation decoding strategy for HiPDT based on entropy-pruned first-pass language models.",
        "Our complexity analysis prompts us to investigate decoding strategies based on large translation grammars and small language models.",
        "We describe, implement, and evaluate a two-pass decoding strategy for a large-scale translation task using HiPDT (Section 5).",
        "We show that entropy-pruned languagemodels can be used in first-pass translation, followed by admissible beam pruning of the output lattice and subsequent rescoring with a full language model.",
        "We analyze the search errors that might be introduced by a two-pass translation approach and show that these can be negligible if pruning thresholds are set appropriately (Sec- tion 5.2).",
        "Finally, we detail the experimental conditions and speed/performance tradeoffs that allow HiPDT to achieve state-of-the-art performance for large-scale SMT under a large grammar (Section 5.3), including lattice rescoring steps under a vast 5-gram language model and lattice minimum Bayes risk decoding (Section 5.4).",
        "With this translation strategyHiPDT can yield very good translation performance.",
        "For comparison, the performance of this Chinese-to-English SMT described in Section 5.4 is equivalent to that of the University of Cambridge submission to the NIST OpenMT 2012 Evaluation.1 1 For details see http://www.nist.gov/itl/iad/mig/openmt12.cfm.",
        "693  2.",
        "Pushdown Automata Informally, pushdown transducers are finite-state transducers that have been augmented with a stack.",
        "Typically this is done by adding a stack alphabet and labeling each transition with a stack operation (a stack symbol to be pushed onto, popped, or read from the stack) in addition to the usual input and output labels (Aho and Ullman 1972; Berstel 1979) and weight (Kuich and Salomaa 1986; Petre and Salomaa 2009).",
        "Our equivalent representation allows a transition to be labeled by a stack operation or a regular input/output symbol, but not both.",
        "Stack operations are represented by pairs of open and close parentheses (pushing a symbol on and popping it from the stack).",
        "The advantage of this representation is that it is identical to the finite automaton representation except that certain symbols (the parentheses) have special semantics.",
        "As such, several finite-state algorithms either immediately generalize to this PDA representation or do so with minimal changes.",
        "In this section we formally define pushdown automata and transducers.",
        "2.1 Definitions A (restricted) Dyck language consist of ?well-formed?",
        "or ?balanced?",
        "strings over a finite number of pairs of parentheses.",
        "Thus the string ( [ ( ) ( ) ] { } [ ] ) ( ) is in the Dyck language over three pairs of parentheses (see Berstel 1979 for a more detailed presentation).",
        "More formally, let A and A be two finite alphabets such that there exists a bijection f from A to A.",
        "Intuitively, f maps an open parenthesis to its corresponding close parenthesis.",
        "Let a?",
        "denote f (a) if a?A and f?1(a) if a?A.",
        "The Dyck language DA over the alphabet A?=A ?",
        "A is then the language defined by the following context-free grammar: S?",
        "?, S?",
        "SS and S?",
        "aSa?",
        "for all a?A.",
        "We define the mapping cA : A??",
        "?",
        "A??",
        "as follows.",
        "cA(x) is the string obtained by iteratively deleting from x all factors of the form aa?",
        "with a ?",
        "A.",
        "Observe that DA=c?1A (?).",
        "Finally, for a subset B ?",
        "A, we define the mapping rB : A?",
        "?",
        "B?",
        "by rB(x1 .",
        ".",
        ".",
        "xn)=y1 .",
        ".",
        ".",
        "yn with yi=xi if xi?B and yi=?",
        "otherwise.",
        "A semiring (K,?,?, 0, 1) is a ring that may lack negation.",
        "It is specified by a set of values K, two binary operations ?",
        "and ?, and two designated values 0 and 1.",
        "The operation ?",
        "is associative, commutative, and has 0 as identity.",
        "The operation ?",
        "is associative, has identity 1, distributes with respect to ?, and has 0 as annihilator: for all a ?",
        "K, a?",
        "0 = 0?",
        "a = 0.",
        "If ?",
        "is also commutative, we say that the semiring is commutative.",
        "The probability semiring (R+,+,?, 0, 1) is used when the weights represent probabilities.",
        "The log semiring (R ?",
        "{?",
        "},?log,+,?, 0), isomorphic to the probability semiring via the negative-log mapping, is often used in practice for numerical stability.",
        "The tropical semiring (R ?",
        "{?",
        "}, min,+,?, 0) is derived from the log semiring using the Viterbi approximation.",
        "These three semirings are commutative.",
        "A weighted pushdown automaton (PDA) T over a semiring (K,?,?, 0, 1) is an 8-tuple (?,?,?,Q,E, I, F, ?)",
        "where ?",
        "is the finite input alphabet, ?",
        "and ?",
        "are the finite open and close parenthesis alphabets, Q is a finite set of states, I?Q the initial state, F ?",
        "Q the set of final states, E ?",
        "Q?",
        "(?",
        "?",
        "??",
        "?",
        "{?",
        "})?K?Q a finite set of transitions, and ?",
        ": F?",
        "K the final weight function.",
        "Let e= (p[e], i[e],w[e], n[e]) denote a transition in E; for simplicity, (p[e], i[e], n[e]) denotes an unweighted transition (i.e., a transition with weight 1?).",
        "694 Allauzen et al. Pushdown Automata in Statistical Machine Translation A path ?",
        "is a sequence of transitions ?=e1 .",
        ".",
        ".",
        "en such that n[ei]=p[ei+1] for 1 ?",
        "i < n. We then define p[?",
        "]=p[e1], n[?",
        "]=n[en], i[?",
        "]= i[e1] ?",
        "?",
        "?",
        "i[en], and w[?]=w[e1]?",
        ".",
        ".",
        ".?",
        "w[en].",
        "A path ?",
        "is accepting if p[?",
        "]= I and n[?]?F.",
        "A path ?",
        "is balanced if r??(i[?])?D?.",
        "A balanced path ?",
        "accepts the string x???",
        "if it is a balanced accepting path such that r?(i[?])=x.",
        "The weight associated by T to a string x???",
        "is T(x)= ?",
        "?",
        "?P(x) w[?]??(n[?])",
        "(1) where P(x) denotes the set of balanced paths accepting x.",
        "A weighted language is recognizable by a weighted pushdown automaton iff it is context-free.",
        "We define the size of T as |T|= |Q|+|E|.",
        "A PDA T has a bounded stack if there exists K ?",
        "N such that for any path ?",
        "from I such that c?(r??(i[?]))",
        "?",
        "??",
        ": |c?(r??(i[?",
        "]))| ?",
        "K (2) In other words, the number of open parentheses that are not closed along ?",
        "is bounded.",
        "If T has a bounded stack, then it represents a regular language.",
        "Figure 5 shows non-regular, regular, and bounded-stack PDAs.",
        "A weighted finite automaton (FSA) can be viewed as a PDA where the open and close parentheses alphabets are empty (see Mohri 2009 for a stand-alone definition).",
        "Finally, a weighted pushdown transducer (PDT) T over a semiring (K,?,?, 0, 1) is a 9-tuple (?,?,?,?,Q,E, I, F, ?)",
        "where ?",
        "is the finite input alphabet, ?",
        "is the finite output alphabet, ?",
        "and ?",
        "are the finite open and close parenthesis alphabets, Q is a finite set of states, I?Q the initial state, F ?",
        "Q the set of final states, E ?",
        "Q?",
        "(?",
        "?",
        "??",
        "?",
        "0 1a 2 ?",
        "( 3)b 0 1 a 2 ?",
        "( ?",
        "3 ) ?",
        "b 0 1 ( 3 ?",
        "2 a 4( ) 5 b ) (a) (b) (c) 0,?",
        "1,( ?",
        "3,?",
        "?",
        "2,(a 4,(?",
        "?",
        "5,( b ?",
        "0 1a:c/1 2 ?:?",
        "(:(/1 3):)b:c/1 2 0 ?:?",
        "1 a:c/1 3 S: /1?",
        "b:c/1 TS (d) (e) (f) Figure 5 PDA Examples: (a) Non-regular PDA accepting {anbn|n ?",
        "N}.",
        "(b) Regular (but not bounded-stack) PDA accepting a?b?.",
        "(c) Bounded-stack PDA accepting a?b?",
        "and (d) its expansion as an FSA.",
        "(e) Weighted PDT T1 over the tropical semiring representing the weighted transduction (anbn, c2n) 7?",
        "3n and (f) equivalent RTN ({S},{a, b}, {c}, {TS},S).",
        "695  {?",
        "})?K?Q a finite set of transitions, and ?",
        ": F?",
        "K the final weight function.",
        "Let e= (p[e], i[e], o[e],w[e], n[e]) denote a transition in E. Note that a PDA can be seen as a particular case of a PDT where i[e] = o[e] for all its transitions.",
        "For simplicity, our following presentation focuses on acceptors, rather than the more general case of transducers.",
        "This is adequate for the translation applications we describe, with the exception of the treatment of alignment in Section 4.3, for which the intersection algorithm for PDTs and FSTs is given in Appendix A.",
        "3.",
        "PDT Operations In this section we describe in detail the following PDA algorithms: Replacement, Com-position, Shortest Path, and (Pruned) Expansion.",
        "Although these are needed to implement HiPDT, these are general purpose algorithms, and suitable for many other applications outside the focus of this article.",
        "The algorithms described in this section have been implemented in the PDT extension (Allauzen and Riley 2011) of the OpenFst library (Allauzen et al. 2007).",
        "In this section, in order to simplify the presentation we will only consider machines over the tropical semiring (R+ ?",
        "{?",
        "}, min,+,?, 0).",
        "However, for each operation, we will specify in which semirings it can be applied.",
        "3.1 Recursive Transition Networks We briefly give formal definitions for RTNs that will be needed to present the RTN expansion operation.",
        "Examples are shown earlier in Figures 1(b) and 3(a).",
        "Informally, an RTN is an automaton where some labels, nonterminals, are recursively replaced by other automata.",
        "We give the formal definition for acceptors; the extension to RTN transducers is straightforward.",
        "An RTN R over the tropical semiring (R+ ?",
        "{?",
        "}, min,+,?, 0) is a 4-tuple (N,?, (T?)?",
        "?N, S) where N is the alphabet of nonterminals, ?",
        "is the input alpha-bet, (T?)?",
        "?N is a family of FSTs with input alphabet ?",
        "?N, and S ?",
        "N is the root nonterminal.",
        "A sequence x ?",
        "??",
        "is accepted by (R,?)",
        "if there exists an accepting path ?",
        "in T?",
        "such that ?",
        "= ?1e1 .",
        ".",
        ".",
        "?nen?n+1 with i[?k] ?",
        "?",
        "?, i[ek] ?",
        "N and such that there exists sequences xk such that xk is accepted by (R, i[ek]) and x = i[?1]x1 .",
        ".",
        ".",
        "i[?n]xni[?n+1].",
        "We say that x is accepted by R when it is accepted by (R, S).",
        "The weight associated by (R,?)",
        "(and by R) to x can be defined in the same recursive manner.",
        "As an example of testing whether an RTN accepts a sequence, consider the RTN R of Figure 6 and the sequence x = a a b. The path in the automata TS can be written as ?",
        "= ?1 e1 ?2, with i[?1] = a, i[e1] = X1, and i[?2] = b. In addition, the machine (R, i[e1]) accepts x1 = a.",
        "Because x = i[?1] x1 i[?2], it follows that x is accepted by (R, S).",
        "3.2 Replacement This algorithm converts an RTN into a PDA.",
        "As explained in Section 1.1, this PDT operation is applied by the HiPDT decoder in Step 1, and examples are given in earlier sections (e.g., in figures 1 and 3).",
        "Replacement acts on every transition of the RTN that is associated with a non-terminal.",
        "The source and destination states of these transitions are used to define the matched opening and closing parentheses, respectively, in the new PDA.",
        "Each RTN nonterminal transition is deleted and replaced by two new transitions that lead to and 696 Allauzen et al. Pushdown Automata in Statistical Machine Translation from the automaton indicated by the nonterminal.",
        "These new transitions have matched parentheses, taken from the source and destination states of the RTN transition they replace.",
        "Figure 6 gives a simple example.",
        "Formally, given an RTN R, defined as (N,?, (T?)?",
        "?N, S), its replacement is the PDA T equivalent to R defined by the 8-tuple (?,?,?,Q,E, I, F, ?)",
        "with Q = ?",
        "= ?",
        "?",
        "?N Q?, I = IS, F = FS, ?",
        "= ?S, and E = ?",
        "?",
        "?N ?",
        "e?E?",
        "E e where Ee = {e} if i[e] 6?",
        "N and Ee={(p[e], n[e], ?,w[e], I?",
        "), (f, n[e], ?, ??",
        "(f ), n[e])|f ?F?}",
        "(3) with ?",
        "= i[e] ?",
        "N otherwise.",
        "The complexity of the construction is in O(|T|).",
        "If |F?| = 1 for all ?",
        "?",
        "N, then |T| = O(??",
        "?N |T?|) = O(|R|).",
        "Creating a superfinal state for each T?",
        "would lead to a T whose size is always linear in the size of R. In this article, we assume this optimization is always performed.",
        "We note here that RTNs can be defined and the replacement operation can be applied in any semiring.",
        "3.3 Composition Once we have created the PDA with translation scores, Step 2 in Section 1.1 applies the language model scores to the translation space.",
        "This is done by composition with an FSA containing the relevant language model weights.",
        "The class of weighted pushdown transducers is closed under composition with weighted finite-state transducers (Bar-Hillel, Perles, and Shamir 1964; Nederhof and Satta 2003).",
        "OpenFST supports composition between automata T1 and T2, where T1 is a weighted pushdown transducer and T2 is a weighted finite-state transducer.",
        "If both T1 and T2 are acceptors, rather than transducers, the composition of a PDA and an FSA produces a PDA containing their intersection, and so no separate intersection algorithm is required for these automata.",
        "Given this, we describe only the simpler, special case of intersection between a PDA and an FSA, as this is sufficient for most of the translation applications described in this article.",
        "The alignment experiments of RTN R 1 2 3 4 a X1 b TS 5 6 X2 a 7 8 b TX1 TX2 R accepts a a b and a b b. PDT T 1 2 a 5 6 3 4 7 8 a b b 6 3 3?",
        "6?",
        "T accepts a 3 a 3?",
        "b and a 3 6 b 6?",
        "3?",
        "b. Figure 6 Conversion of an RTN R to a PDA T by the replacement operation of Section 3.2.",
        "Using the notation of Section 2.1, in this example ?",
        "= {3, 5} and ?",
        "= {3?, 5?",
        "}, with f (3) = 3?",
        "and f (5) = 5?.",
        "The unweighted transition (2,X1, 3) in R is deleted and replaced by two new transitions (2, 3, 5) and (6, 3?, 3); similarly, (5,X2, 6) is replaced by (5, 6, 7) and (8, 6?, 6).",
        "After application of the r?",
        "mapping, the strings accepted by R and by T are the same.",
        "697  0 1ab 2 a b 3 a b 4 a b T2 0 1a 2 ?",
        "( 3)b T1 0,0 1,1a 2,0 ?",
        "0,1( 3,0) 1,2a 2,1 ?",
        "b 0,2( 3,1) 1,3a 2,2 ?",
        "b 0,3( 3,2) 1,4a 2,3 ?",
        "b 0,4( 3,3) 2,4 ?",
        "b T Figure 7 Composition example: Composition of a PDA T1 accepting {an, bn} with an FSA T2 accepting {a, b}4 to produce a PDA T = T1 ?",
        "T2 .",
        "T has only one balanced path, and this path accepts a(a(?)b)b.",
        "Composition is performed by the PDA-FSA intersection described in Section 3.3.",
        "Section 4.3 do require composition of transducers; the algorithm for composition of transducers is given in Appendix A.",
        "An example of composition by intersection is given in Figure 7.",
        "The states of T are created as the product of all the states in T1 and T2.",
        "Transitions are added as illustrated in Figure 8.",
        "These correspond to all paths through T1 and T2 that can be taken by a synchronized reading of strings from {a, b}?.",
        "The algorithm is very similar to the composition algorithm for finite-state transducers, the difference being the handling of the parentheses.",
        "The parenthesis-labeled transitions are treated similarly to epsilon transitions, but the parenthesis labels are preserved in the result.",
        "This adds many unbalanced paths to T. In this example, T has five paths but only one balanced path, so that T accepts the string a a b b. Formally, given a PDA T1 = (?,?,?,Q1,E1, I1, F1, ?1) and an FSA T2 = (?,Q2,E2, I2, F2, ?2), intersection constructs a new PDA T = (?,?,?,Q,E, I, F, ?",
        "), where T = T1 ?",
        "T2 as follows: 1.",
        "The new state space is in the product of the input state spaces: Q ?",
        "Q1 ?Q2.",
        "2.",
        "The new initial and final states are I = (I1, I2), and F = {(q1, q2) : q1 ?",
        "F1, q2 ?",
        "F2}.",
        "3.",
        "Weights are assigned to final states (q1, q2) ?",
        "Q as ?",
        "(q1, q2) = ?",
        "(q1)+ ?(q2).",
        "4.",
        "For pairs of transitions (q1, a1,w1, q?1) ?",
        "E1 and (q2, a2,w2, q?2) ?",
        "E2, a transition is added between states (q1, q2) and (q?1, q?2) as specified in Figure 8.",
        "PDT T1 FSA T2 PDT T = T1 ?",
        "T2 Input Symbols q1 q?1 a1/w1 q2 q?2 a2/w2 q1, q2 q?1, q?2 a1/w1 + w2 a1 ?",
        "?",
        "and a1 = a2 q1, q2 q?1, q2 a1/w1 a1 ?",
        "?",
        "??",
        "or a1 = ?",
        "Transitions are added to T if and only if the conditions on the input symbols are satisfied.",
        "Figure 8 PDA?FSA intersection under the tropical semiring.",
        "The PDA T is created by the intersection of the PDA T1 and the FSA T2, i.e., T = T1 ?",
        "T2.",
        "698 Allauzen et al. Pushdown Automata in Statistical Machine Translation The intersection algorithm given here assumes that T2 has no input-?",
        "transitions.",
        "When T2 has input-?",
        "transitions, an epsilon filter (Mohri 2009; Allauzen, Riley, and Schalkwyk 2011) generalized to handle parentheses can be used.",
        "Note that Steps 1 and 2 do not require the construction of all possible pairs of states; only those states reachable from the initial state and needed in Step 4 are actually generated.",
        "The complexity of the algorithm is in O(|T1| |T2|) in the worst case, as will be discussed in Section 4.",
        "Composition requires the semiring to be commutative.",
        "3.4 Shortest Distance and Path Algorithms With a PDA including both translation and language model weights, HiPDT can extract the best translation (Step 3a in Section 1.1).",
        "To this end, a general PDA shortest distance/path algorithm is needed.",
        "A shortest path in a PDA T is a balanced accepting path with minimal weight and the shortest distance in T is the weight of such a path.",
        "We show that when T has a bounded stack, shortest distance and shortest path can be computed in O(|T|3 log |T|) time (assuming T has no negative weights) and O(|T|2) space.",
        "Figure 9 gives a pseudo-code description of the shortest-distance algorithm, which we now discuss.",
        "SHORTESTDISTANCE(T) 1 for each q ?",
        "Q and a ?",
        "?",
        "do 2 B[q, a]?",
        "?",
        "3 for each q ?",
        "Q do 4 d[q, q]??",
        "5 GETDISTANCE(T, I) ?",
        "I is the unique initial state 6 return d[I, f ] ?",
        "f is the unique final state RELAX(s,q,w,S ) 1 if d[s, q] > w then ?",
        "if w is a better estimate of the distance from s to q 2 d[s, q]?",
        "w ?",
        "update d[s, q] 3 if q 6?",
        "S then ?",
        "enqueue q in S if needed 4 ENQUEUE(S, q) GETDISTANCE(T,s) 1 for each q ?",
        "Q do 2 d[s, q]??",
        "3 d[s, s]?",
        "0 4 Ss ?",
        "{s} 5 while Ss 6=?",
        "do 6 q?",
        "HEAD(Ss ) 7 DEQUEUE(Ss ) 8 for each e ?",
        "E[q] do ?",
        "E(q) is the set of transitions leaving state q 9 if i[e] ?",
        "?",
        "?",
        "{?}",
        "then ?",
        "i[e] is a regular symbol 10 RELAX(s,n[e], d[s, q]+w[e],Ss ) 11 elseif i[e] ?",
        "?",
        "then ?",
        "i[e] is a close parenthesis 12 B[s, i[e]]?",
        "B[s, i[e]] ?",
        "{e} 13 elseif i[e] ?",
        "?",
        "then ?",
        "i[e] is an open parenthesis 14 if d[n[e], n[e]]=?",
        "then ?",
        "n[e] is the destination state of transition e 15 GETDISTANCE(T,n[e]) 16 for each e?",
        "?",
        "B[n[e], i[e]] do 17 w?",
        "d[s, q]+w[e]+ d[n[e], p[e?",
        "]]+ w[e?]",
        "18 RELAX(s,n[e?",
        "],w,Ss ) Figure 9 PDT shortest distance algorithm.",
        "699  Given a PDA T = (?,?,?,Q,E, I, F, ?",
        "), the GETDISTANCE(T) algorithm computes the shortest distance from the start state I to the final state2 f ?",
        "F. The algorithm recursively calculates d[q, q?]",
        "?",
        "K ?",
        "the shortest distance from state q to state q?",
        "along a balanced path At termination, the algorithm returns d[I, f ] as the cost of the shortest path through T. The core of the shortest distance algorithm is the procedure GETDISTANCE(T, s) which calculates the distances d[s, q] for all states q that can be reached from s. For an FSA, this procedure is called once, as GETDISTANCE(T, I), to calculate d[I, q] ?q.",
        "For a PDA, the situation is more complicated.",
        "Given a state s in T with at least one incoming open parenthesis transition, we denote by Cs the set of states that can be reached by a balanced path starting from s. If s has several incoming open parenthesis transitions, a naive implementation might lead to the states in Cs to be visited exponentially many times.",
        "This is avoided by memoizing the shortest distance from s to states in Cs.",
        "To do this, GETDISTANCE(T, s) calculates d[s, s?]",
        "for all s?",
        "?",
        "Cs, and it also constructs sets of transitions B[s, a] = {e ?",
        "E : p[e] ?",
        "Cs and i[e] = a} ?a ?",
        "?",
        "(4) These are the transitions with label a leaving states in Cs.",
        "Consider any incoming transition to s, (q, a,w, s), with a ?",
        "?.",
        "For every transition e?",
        "= (s?, a,w?, q?",
        "), e?",
        "?",
        "B[s, a] the following holds3 d[q, q?]",
        "= w+ d[s, s?",
        "]+ w?",
        "(5) If d[s, s?]",
        "is available, the shortest distance from q to q?",
        "along any balanced path through s can be computed trivially by Equation (5).",
        "For any state s with incoming open parenthesis transitions, only a single call to GETDISTANCE(T, s) is needed to precompute the necessary values.",
        "Figure 10 gives an example.",
        "When transition (2, (1, 0, 5) is processed, GETDISTANCE(T, 5) is called.",
        "The distance d[5, 7] is computed, and following transitions are logged: B[5, (1]?",
        "{(7, )1, 0, 8)} and B[5, (2]?",
        "{(7, )2, 0, 9)}.",
        "Later, when the transition (4, (2, 0, 5) is processed, its matching transition (7, )2, 0, 9) is extracted from B[5, (2].",
        "The distance d[4, 9] is then found by Equation (5) as d[5, 7].",
        "This avoids redundant re-calculation of distances along the shortest balanced path from state 4 to state 9.",
        "We now briefly discuss the shortest distance pseudo-code given in Figure 9.",
        "The description may be easier to follow after reading the worked example in Figure 10.",
        "Note that the sets Cs are not computed explicitly by the algorithm.",
        "The shortest distance calculation proceeds as follows.",
        "Self-distances, that is, d[q, q], are set initially to ?",
        "; when GETDISTANCE(T, q) is called it sets d[q, q] = 0 to note that q has been visited.",
        "GETDISTANCE(T, s) starts a new instance of the shortest-distance algorithm from s using the queue Ss, initially containing s. While the queue is not empty, a state is dequeued and its outgoing transitions examined (lines 7?11).",
        "Transitions labeled by non-parenthesis are treated as in Mohri (2009) (lines 7?8).",
        "When a transition e is labeled by a close parenthesis, e is added to B[s, i[e]] to indicate that this transition 2 For simplicity, we assume T has only one final state.",
        "3 This assumes all paths from q to q?",
        "pass through s. The RELAX operation (Figure 9) handles the general case.",
        "700 Allauzen et al. Pushdown Automata in Statistical Machine Translation 0 1 2 5 6 7 8 10 3 4 9t1/20 t3/200 (2 t1/10 t2/100 (1 t2/1 t3/1 )1 t4/1, 000 )2 t6/1, 000 GETDISTANCE(T) runs 1.",
        "Initialization: d[q, q]?",
        "?, ?q ?",
        "Q 2.",
        "GETDISTANCE(T, 0) is called GETDISTANCE(T, 0) runs 3.",
        "Distances are calculated from state 0: d[0, 0]?",
        "0; d[0, 1]?",
        "d[0, 0]+ w[0, 1]; d[0, 2]?",
        "d[0, 1]+ w[1, 2] 4.",
        "Transition e1 = (2, (1, 0, 5) is reached.",
        "e1 has symbol i[e1] = (1 and destination state n[e1] = 5 5. d[5, 5] =?",
        "so GETDISTANCE(T, 5) is called GETDISTANCE(T, 5) runs 6.",
        "Distances are calculated from state 5: d[5, 5]?",
        "0; d[5, 6]?",
        "d[5, 5]+ w[5, 6]; d[5, 7]?",
        "d[5, 6]+ w[6, 7] 7.",
        "The transitions (7, )1, 0, 8) and (7, )2, 0, 9) are reached and memoized B[5, (1]?",
        "{(7, )1, 0, 8)} B[5, (2]?",
        "{(7, )2, 0, 9)} GETDISTANCE(T, 5) ends GETDISTANCE(T, 0) resumes 8.",
        "Transition e1 = (2, (1, 0, 5) is still being processed, with p[e1] = 2, n[e1] = 5, and i[e1] = (1 9.",
        "Transition e2 = (7, )1, 0, 8) matching (1 is extracted from B[n[e1], i[e1]], with p[e2] = 7 and n[e2] = 8 10.",
        "Distance d[0, 8] is calculated as d[0, n[e2]] : d[0, n[e2]]?",
        "d[0, p[e1]]+ w[p[e1],n[e1]]+ d[n[e1], p[e2]]+ w[p[e2],n[e2]] 10.",
        "Processing of e1 finishes, and calculation of distances from 0 continues: d[0, 10]?",
        "d[0, 8]+ w[8, 10] 10 is a final state.",
        "Processing continues with transition (0, t1, 20, 3) d[0, 3]?",
        "d[0, 0]+ w[0, 3]; d[0, 4]?",
        "d[0, 3]+ w[3, 4] 13.",
        "Transition e3 = (4, (2, 0, 5) is reached e3 has symbol i[e3] = (2, source state p[e3] = 4, and destination state n[e3] = 5 14.",
        "GETDISTANCE(T, 5) is not called, since d[5, 5] = 0 indicates state 5 has been previously visited 15.",
        "Transition e4 = (7, )2, 0, 9) matching (2 is extracted from B[n[e3], i[e3]], with p[e4] = 7 and n[e4] = 9 16.",
        "Distance d[0, 9] is calculated as d[0, n[e4]], using cached values: d[0, n[e4]]?",
        "d[0, p[e3]]+ w[p[e3],n[e3]]+ d[n[e3], p[e4]]+ w[p[e4],n[e4]] 17. d[0, 10] is less than?",
        ": d[0, 10]?",
        "min(d[0, 10], d[0, 9]+ w[9, 10]) 18.",
        "GETDISTANCE(T, 0) ends and returns d[0, 10] GETDISTANCE(T) ends Figure 10 Step-by-step description of the shortest distance calculation for the given PDA by the algorithm of Figure 9.",
        "For simplicity, w[q, q?]",
        "indicates the weight of the transition connecting q and q?.",
        "balances all incoming open parentheses into s labeled by i[e] (lines 9?10).",
        "Finally, if e has an open parenthesis, and if its destination has not already been visited, a new instance of GETDISTANCE is started from n[e] (lines 12?13).",
        "The destination states of all transitions balancing e are then relaxed (lines 14?16).",
        "The space complexity of the algorithm is quadratic for two reasons.",
        "First, the number of non-infinity d[q, s] is |Q|2.",
        "Second, the space required for storing B is at most in O(|E|2) because for each open parenthesis transition e, the size of |B[n[e], i[e]]| 701  is O(|E|) in the worst case.",
        "This last observation also implies that the accumulated number of transitions examined at line 16 is in O(Z|Q| |E|2) in the worst case, where Z denotes the maximal number of times a state is inserted in the queue for a given call of GETDISTANCE.",
        "Assuming the cost of a queue operation is ?",
        "(n) for a queue containing n elements, the worst-case time complexity of the algorithm can then be expressed as O(Z|T|3 ?(|T|)).",
        "When T contains no negative weights, using a shortest-first queue discipline leads to a time complexity in O(|T|3 log |T|).",
        "When all the Cs's are acyclic, using a topological order queue discipline leads to a O(|T|3) time complexity.",
        "As was shown in Section 3.2, when T has been obtained by converting an RTN or a hypergraph into a PDA, the polynomial dependency in |T| becomes a linear dependency both for the time and space complexities.",
        "Indeed, for each q in T, there exists a unique s such that d[s, q] is non-infinity.",
        "Moreover, for each open parenthesis transition e, there exists a unique close parenthesis transition e?",
        "such that e?",
        "?B[n[e], i[e]].",
        "When each component of the RTN is acyclic, the complexity of the algorithm is O(|T|) in time and space.",
        "The algorithm can be modified (without changing the complexity) to compute the shortest path by keeping track of parent pointers.",
        "The notion of shortest path requires the semiring (K,?,?, 0, 1) to have the path property: for all a, b in K, a?",
        "b ?",
        "{a, b}.",
        "The shortest-distance operation as presented here and the shortest-path operation can be applied in any semiring having the path property by using the natural order defined by ?",
        ": a ?",
        "b iff a?",
        "b = a.",
        "However, the shortest distance algorithm given in Figure 9 can be extended to work for k-closed semirings using the same techniques that were used by Mohri (2002).",
        "The shortest distance in the intersection of a string s and a PDA T determines if T recognizes s. PDA recognition is closely related to CFG parsing; a CFG can be represented as a PDT whose input recognizes the CFG and whose output identifies the parse (Aho and Ullman 1972).",
        "Lang (1974) showed that the cubic tabular method of Earley can be naturally applied to PDAs; others give the weighted generalizations (Stolcke 1995; Nederhof and Satta 2006).",
        "Earley's algorithm has its analogs in the algorithm in Figure 9: the scan step corresponds to taking a non-parenthesis transition at line 10, the predict step to taking an open parenthesis at lines 14?15, and the complete step to taking the closed parentheses at lines 16?18.",
        "Specialization to Translation.",
        "Following the formalism of Section 1, we are interested in applying shortest distance and shortest path algorithms to automata created as L = Tp ?M, where Tp, the translation representation, is a PDA derived from an RTN (via replacement) and M, the language model, is a finite automaton.",
        "For this particular case, the time complexity is O(|Tp||M|3) and the space complexity is O(|Tp||M2|).",
        "The dependence on |Tp| is linear, rather than cubic or quadratic.",
        "The reasoning is as follows.",
        "Given a state q in Tp, there exists a unique sq such that q belongs to Csq.",
        "Given a state (q1, q2) in Tp?M, (q1, q2)?C(s1,s2 ) only if s1 = sq1 , and hence (q1, q2) belongs to at most |M| components.",
        "3.5 Expansion As explained in Section 1.1, HiPDT can apply Step 3b to generate translation lattices.",
        "This step is typically required for any posterior lattice rescoring strategies.",
        "We first 702 Allauzen et al. Pushdown Automata in Statistical Machine Translation describe the unpruned expansion.",
        "However, in practice a pruning strategy of some sort is required to avoid state explosion.",
        "Therefore, we also describe an implementation of the PDA expansion that includes admissible pruning under a likelihood beam, thus controlling on-the-fly the size of the output lattice.",
        "3.5.1 Full Expansion.",
        "Given a bounded-stack PDA T, the expansion of T is the FSA T?",
        "equivalent to T. A simple example is given in Figure 11.",
        "Expansion starts from the PDA initial state.",
        "States and transitions are added to the FSA as the expansion proceeds along paths through the PDA.",
        "In the new FSA, parentheses are replaced by epsilons, and as open parentheses are encountered on PDA transitions, they are ?pushed?",
        "into the FSA state labels; in this way the stack depth is maintained along different paths through the PDA.",
        "Conversely, when a closing parenthesis is encountered on a PDA path, a corresponding opening parenthesis is ?popped?",
        "from the FSA state label; if this is not possible, for example, as in state (5, ?)",
        "in Figure 11, expansion along that path halts.",
        "The resulting automata accept the same language.",
        "The FSA topology changes, typically with more states and transitions than the original PDA, and the number of added states is controlled only by the maximum stack depth of the PDA.",
        "Formally, suppose the PDA T = (?,?,?,Q,E, I, F, ?)",
        "has a maximum stack depth of K. The set of states in its FSA expansion T?",
        "are then Q?",
        "= {(q, z) : q ?",
        "Q , z ?",
        "??",
        "and |z| ?",
        "K} (6) and T?",
        "has initial state (I, ?)",
        "and final states F?",
        "= {(q, ?)",
        ": q ?",
        "F}.",
        "The condition that T has a bounded stack ensures that Q?",
        "is finite.",
        "Transitions are added to T?",
        "as described in Figure 12.",
        "The full expansion operation can be applied to PDA over any semiring.",
        "The complexity of the algorithm is linear in the size of T?.",
        "However, the size of T?",
        "can be exponential in the size of T, which motivates the development of pruned expansion, as discussed next.",
        "0 1 2 3 4 5 6[ [ [ a ] b ] c 0,?",
        "1, [ 2, [[ 3, [[ 4, [ 5, [ 6,??",
        "?",
        "a ?",
        "b ?",
        "2, [ 3, [ ?",
        "a c 4,?",
        "5,?",
        "?",
        "b Figure 11 Full expansion of a PDA to an equivalent FSA.",
        "The PDA maximum stack depth is 2; therefore the FSA states belong to {0, .., 6} ?",
        "{?, [, [[}.",
        "Expansion can create incomplete paths in the FSA (e.g., corresponding here to the unbalanced PDA path [ a ] b ]); however these are guaranteed to be unconnected, namely, not to lead to a final state.",
        "Any unconnected states are removed after expansion.",
        "703  Transition in PDA T New transition in FSA T?",
        "Conditions Explanation q, z q?, z a/w a ?",
        "?",
        "?",
        "{?}",
        "a is not a parenthesis; stackdepth is unchanged q q?",
        "a/w q, z q?, za ?",
        "a ?",
        "?",
        "a is an open parenthesis; an epsilon transition is added, and a is ?pushed?",
        "into the destination state, increasing the stack depth q, z?a q?, z?",
        "?",
        "a ?",
        "?",
        "a is a closing parenthe-sis; an epsilon transition is added, and the matching open parenthesis a is ?popped?",
        "from the destination state, decreasing the stack depth Figure 12 PDA Expansion.",
        "A states (q, z) and (q?, z? )",
        "in the FSA T?",
        "will be connected by a transition if and only if the above conditions hold on the corresponding transition between q and q?",
        "in the PDA T. 3.5.2 Pruned Expansion.",
        "Given a bounded-stack PDA T, the pruned expansion of T with threshold ?",
        "is an FST T??",
        "obtained by deleting from T?",
        "all states and transitions that do not belong to any accepting path ?",
        "in T?",
        "such that w[?]?",
        "?[?]",
        "?",
        "d+ ?, where d is the shortest distance in T. A naive implementation consisting of fully expanding T and then applying the FST pruning algorithm would lead to a complexity in O(|T?| log |T?|)=O(e|T||T|).",
        "Assuming that the reverse TR of T is also bounded-stack, an algorithm whose complexity is in O(|T| |T?",
        "?|+ |T|3 log |T|) can be obtained by first applying the shortest distance algorithm from the previous section to TR and then using this to prune the expansion as it is generated.",
        "To simplify the presentation, we assume that F={ f} and ?",
        "( f )=0.",
        "The motivation for using reversed automaton in pruning is easily seen by looking at FSAs.",
        "For an FSA, the cost of the shortest path through a transition (q, x,w, q?)",
        "can be stated as d[I, q]+ w+ d[q?, f ].",
        "Distances d[I, q] (i.e., distances from the start state) are computed by the shortest distance algorithm, as discussed in Section 3.4.",
        "However, distances of the form d[q?, f ] are not readily available.",
        "To compute these, a shortest distance algorithm is run over the reversed automaton.",
        "Reversal preserves states and transitions, but swaps the source and destination state (see Figure 13 for a PDA ex-ample).",
        "The start state in the reversed machine is f , so that distances are computed from f ; these are denoted dR[f, q] and correspond to d[q, f ] in the original FSA.",
        "The cost of the shortest path through an FSA transition (q, x,w, q?)",
        "can then be computed as d[I, q]+ w+ dR[f, q?].",
        "Calculation for PDAs is more complex.",
        "Transitions with parentheses must be handled such that distances through them are calculated over balanced paths.",
        "For example, if T in Figure 13 was an FSA, the shortest cost of any path through the transition e = (4, (2, 0, 5) could be calculated as d[0, 4]+ 0+ d[5, 10].",
        "However, this is not correct, because d[5, 10], the shortest distance from 5 to 10, is found via a path through the transition (7, )1, 0, 8).",
        "Correct calculation of the minimum cost of balanced paths through PDA transitions can be done using quantities computed by the PDA shortest distance algorithm.",
        "For a 704 Allauzen et al. Pushdown Automata in Statistical Machine Translation 0 1 2 5 6 7 8 10 3 4 9t1/20 t3/200 (2 t1/10 t2/100 (1 t2/1 t3/1 )1 t4/1, 000 )2 t6/1, 000 T 0 1 2 5 6 7 8 10 3 4 9t1/20 t3/200 (2 t1/10 t2/100 (1 t2/1 t3/1 )1 t4/1, 000 )2 t6/1, 000 TR Figure 13 PDA T and its reverse TR.",
        "TR has start state 10, final state 0, ?R = {)1, )2}, and ?",
        "R = {(1, (2}.",
        "PDA transition e = (q, a,w, q?",
        "), a ?",
        "?, the cost of the shortest balanced path through e can be found as4 c(e) = d[I, q]+ w[e]+ min e?",
        "?B[q?,a] d[q?, p[e?",
        "]]+ w[e?",
        "]+ dR[n[e?",
        "], f ] (7) where B[q?, a] and d[p[e?",
        "], q?]",
        "are computed by the PDA shortest distance algorithm over T, and dR[n[e?",
        "], f ] is computed by the PDA shortest distance algorithm over TR.",
        "In Figure 13, the shortest cost of paths through the transition e = (4, (2, 0, 5) is found as follows: the shortest distance algorithm over T calculates d[0, 4] = 220 , d[5, 7] = 2, and B[5, (2] = {7, )2, 0, 9}; the shortest distance algorithm over TR calculates dR[10, 9] = 1, 000 (trivially, here); the cost of the shortest path through e is d[0, 4]+ w[e]+ d[5, 7]+ w[e?",
        "]+ dR[10, 9] = 220+ 0+ 2+ 0+ 1, 000 Pruned expansion is therefore able to avoid expanding transitions that would not contribute to any path that would survive pruning.",
        "Prior to expansion of a PDA T to an FSA T?, the shortest distance d in T is calculated.",
        "Transitions e = (q, a,w, q?",
        "), a ?",
        "?, are expanded as transitions e = ((q, z), q,w, (q?, za)) in T?",
        "only if c(e) ?",
        "d+ ?, as calculated by Equation (7).",
        "The pruned expansion algorithm implemented in OpenFST is necessarily more complicated than the simple description given here.",
        "Pseudo-code describing the OpenFST implementation is given in Appendix B. The pruned expansion operation can be applied in any semiring having the path property.",
        "4 Note that d[p[e?",
        "], q?]",
        "could be replaced by dR[q?, p[e?]].",
        "705  4.",
        "HiPDT Analysis and Experiments: Computational Complexity We now address the following questions: r What are the differences between the FSA and PDA representations as observed in a translation/alignment task?",
        "r How do their respective decoding algorithms perform in relation to the complexity analysis described here?",
        "r How many times is exact decoding achievable in each case?",
        "We will discuss the complexity of both HiPDT and HiFST decoders as well as the hypergraph representation, with an emphasis on Hiero-style SCFGs.",
        "We assess our analysis for FSA and PDA representations by contrasting HiFST and HiPDT with large grammars for translation and alignment.",
        "For convenience, we refer to the hypergraph representation as Th, and to the FSA and PDA representations as Tf and Tp.",
        "We first analyze the complexity of each MT step described in the introduction: 1.",
        "SCFG Translation: Assuming that the parsing of the input is performed by a CYK parse, then the CFG, hypergraph, RTN, and PDA representations can be generated in O(|s|3|G|) time and space (Aho and Ullman 1972).",
        "The FSA representation can require an additional O(e|s|3|G|) time and space because the RTN expansion to FSA can be exponential.",
        "2.",
        "Intersection: The intersection of a CFG Th with a finite automaton M can be performed by the classical Bar-Hillel algorithm (Bar-Hillel, Perles, and Shamir 1964) with time and space complexity O(|Th||M|l+1), where l is the maximum number of symbols on the right-hand side of a grammar rule in Th.",
        "Dyer (2010a) presents a more practical intersection algorithm that avoids creating rules that are inaccessible from the start symbol.",
        "With deterministic M, the intersection complexity becomes O(|Th||M|lN+1), where lN is the rank of the SCFG (i.e., lN is the maximum number of nonterminals on the right-hand side of a grammar rule).",
        "With Hiero-styles rules, lN = 2 so the complexity is O(|Th||M|3) in that case.",
        "The PDA intersection algorithm from Section 3.3 has time and space complexity O(|Tp||M|).",
        "Finally, the FSA intersection algorithm has time and space complexity O(|Tf ||M|) (Mohri 2009).",
        "3.",
        "Shortest Path: The shortest path algorithm on the hypergraph, RTN, and FSA representations requires linear time and space (given the underlying acyclicity) (Huang 2008; Mohri 2009).",
        "As presented in Section 3.4, the PDA representation can require time cubic and space quadratic in |M|.",
        "Table 1 summarizes the complexity results for SCFGs of rank 2.",
        "The PDA representation is equivalent in time and superior in space complexity to the CFG/hypergraph representation, in general, and it can be superior in both space and time to the FSA representation depending on the relative SCFG and language model (LM) sizes.",
        "The FSA representation favors smaller target translation grammars and larger language models.",
        "5 The modified Bar-Hillel construction described by Chiang (2007) has time and space complexity O(|Th||M|4 ); the modifications were introduced presumably to benefit the subsequent pruning method employed (but see Huang, Zhong, & Gildea 2005).",
        "706 Allauzen et al. Pushdown Automata in Statistical Machine Translation Table 1 Translation complexity of target language representations for translation grammars of rank 2.",
        "Representation Time Complexity Space Complexity CFG/hypergraph O(|s|3 |G| |M|3) O(|s|3 |G| |M|3 ) PDA O(|s|3 |G| |M|3) O(|s|3 |G| |M|2 ) FSA O(e|s|3|G| |M|) O(e|s|3|G| |M|) In practice, the PDA and FSA representations benefit greatly from the optimizations mentioned previously (Figure 3 and accompanying discussion).",
        "For the FSA representation, these operations can offset the exponential dependencies in the worst-case complexity analysis.",
        "For example, in a translation of a 15-word sentence taken at random from the development sets described later, expansion of an RTN yields a WFSA with 174?",
        "106 states.",
        "By contrast, if the RTN is determinized and minimized prior to expansion, the resulting WFSA has only 34?",
        "103 states.",
        "Size reductions of this magnitude are typical.",
        "In general, the original RTN, hypergraph, or CFG representation can be exponentially larger than the RTN/PDT optimized as described.",
        "Although our interest is primarily in Hiero-style translation grammars, which have rank 2 and a relatively small number of nonterminals, this complexity analysis can be extended to other grammars.",
        "For SCFGs of arbitrary rank lN, translation complexity in time for hypergraphs becomes O(|G||s|lN+1|M|lN+1); with FSAs the time complexity becomes O(e|G||s|lN+1 |M|); and with PDAs the time complexity becomes O(|G||s|lN+1|M|3).",
        "For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and Langmead 2010).",
        "Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here.",
        "We will next describe the translation grammar and language model for our ex-periments, which will be used throughout the remainder of this article (except when stated otherwise).",
        "In the following sections we assess the complexity discussion with a contrast between HiFST (FSA representation) and HiPDT (PDA representation) under large grammars.",
        "4.1 Translation Grammars and Language Models Translation grammars are extracted from a subset of the GALE 2008 evaluation parallel text;6 this is 2.1M sentences and approximately 45M words per language.",
        "We report translation results on a development set tune-nw (1,755 sentences) and a test set test-nw (1,671 sentences).",
        "These contain translations produced by the GALE program and portions of the newswire sections of the NIST evaluation setsMT02 throughMT06.7 6 See http://projects.ldc.upenn.edu/gale/data/catalog.html.We excluded the UN material and the LDC2002E18, LDC2004T08, LDC2007E08, and CUDonga collections.",
        "7 See http://www.itl.nist.gov/iad/mig/tests/mt/.",
        "707  Table 2 Number of n-grams with explicit conditional probability estimates assigned by the 4-gram language models M?1 after entropy pruning of M1 at threshold values ?.",
        "Perplexities over the (concatenated) tune-nw reference translations are also reported.",
        "The Kneser-Ney and Katz 4-gram LM have 416,190 unigrams, which are not removed by pruning.",
        "?",
        "0 7.5?",
        "10?9 7.5?",
        "10?8 7.5?",
        "10?7 7.5?",
        "10?6 7.5?",
        "10?5 7.5?",
        "10?4 7.5?",
        "10?3 KN 2-grams 28M 10M 2.5M 442K 37K 1.3K 21 0 3-grams 61M 6M 969K 74K 2.7K 38 0 0 4-grams 117M 3M 219K 5K 44 0 0 0 perplexity 98.1 122.2 171.5 290.4 605.1 1270.2 1883.6 2200.0 KATZ 2-grams 28M 7M 2M 391K 52K 4K 117 1 3-grams 64M 10M 1.5M 148K 8.4K 197 1 0 4-grams 117M 4.6M 398K 19K 510 1 0 0 perplexity 106.7 120.4 146.9 210.5 336.6 596.5 905.0 1046.1 In tuning the systems, MERT (Och 2003) iterative parameter estimation under IBM BLEU8 is performed on the development set.",
        "The parallel corpus is aligned using MTTK (Deng and Byrne 2008) in both source-to-target and target-to-source directions.",
        "We then follow published procedures (Chiang 2007; Iglesias et al. 2009b) to extract hierarchical phrases from the union of the directional word alignments.",
        "We call a translation grammar (G) the set of rules extracted from this process.",
        "For reference, the number of rules in G that can apply to the tune-nw is 1.1M, of which 593K are standard non-hierarchical phrases and 511K are strictly hierarchical rules.",
        "We will use two English language models in these translation experiments.",
        "The first language model, denoted M1, is a 4-gram estimated over 1.3B words taken from the target side of the parallel text and the AFP and Xinhua portions of the English Gigaword Fourth Edition (LDC2009T13).",
        "We use both Kneser-Ney (Kneser and Ney 1995) and Katz (Katz 1987) smoothing in estimating M1.",
        "Where language model reduction is required, we apply Stolcke entropy pruning (Stolcke 1998) to M1 under the relative perplexity threshold ?.",
        "The resulting language model is labeled as M?1 .",
        "The reduction in size in terms of component n-grams is summarized in Table 2.",
        "For aggressive enough pruning, the original 4-gram model can be effectively reduced to a trigram, bigram, or unigram model.",
        "For both the Katz and the Kneser-Ney 4-gram language models: at ?",
        "= 7.5E?",
        "05 the number of 4-grams in the LM is effectively reduced to zero; at ?",
        "= 7.5E?",
        "4 the number of 3-grams is effectively 0; and at ?",
        "= 7.5E?",
        "3, only unigrams remain.",
        "Development set perplexities increase as entropy pruning becomes more aggressive, with the Katz smoothed model performing better under pruning (Chelba et al. 2010; Roark, Allauzen, and Riley 2013).",
        "We will also use a larger language model, denoted M2, obtained by interpolating M1 with a zero-cutoff stupid-backoff 5-gram model (Brants et al. 2007) estimated over 6.6B words of English newswire text; M2 is estimated as needed for the n-grams required for the test sets.",
        "8 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl.",
        "708 Allauzen et al. Pushdown Automata in Statistical Machine Translation Table 3 Success in finding the 1-best translation under G with various M?1 under a memory size limit of 10GB as measured over tune-nw (1,755 sentences).",
        "We note which operations in translation exceeded the memory limit: either Expansion and Intersection for HiFST, or Intersection and Shortest Path operation for HiPDT.",
        "Decoding with G + M?1 under a 10GB memory size limit # ?",
        "HiFST HiPDT Success Failure Success Failure Expansion Intersection Intersection Shortest Path 2 7.5?",
        "10?9 12% 51% 37% 40% 8% 52% 3 7.5?",
        "10?8 16% 53% 31% 76% 1% 23% 4 7.5?",
        "10?7 18% 53% 29% 99.8% 0% 0.2% 4.2 Exact Decoding with Large Grammars and Small LanguageModels We now compare HiFST and HiPDT in translation with our large grammar G. In this case we know that exact search is often not feasible for HiFST.",
        "We run both decoders over tune-nw with a restriction on memory use of 10 GB.",
        "If this limit is reached in decoding, the process is killed.9 Table 3 shows the number of times each decoder succeeds in finding a hypothesis under the memory limit when decoding with various entropy-pruned LMs M?1 .",
        "With ?=7.5?",
        "10?9 (row 2), HiFST can only decode 218 sentences, and HiPDT succeeds in 703 cases.",
        "The difference in success rates between the decoders is more pronounced as the language model is more aggressively pruned: for ?=7.5?",
        "10?7 HiPDT succeeds for all but three sentences.",
        "As Table 3 shows, HiFST fails most frequently in its initial expansion from RTN to FSA; this operation depends only on the translation grammar and does not benefit from any reduction in the language model size.",
        "Subsequent intersection of the FSA with the language model can still pose a challenge, although as the language model is reduced, this intersection fails less often.",
        "By contrast, HiPDT intersects the translation grammar with the language model prior to expansion and this operation nearly always finishes successfully.",
        "The subsequent shortest path (or pruned expansion) operation is prone to failure, but the risk of this can be greatly reduced by using smaller language models.",
        "In the next section we contrast both HiPDT and HiFST for alignment.",
        "4.3 Alignment with Inversion Transduction Grammars We continue to explore applications characterized by large translation grammars G and small language models M. As an extreme instance of a problem involving a large translation grammar and a simple target language model, we consider parallel text alignment under an Inversion Transduction Grammar (ITG) (Wu 1997).",
        "This task, or something like it, is often done in translation grammar induction.",
        "The process should yield the set of derivations, with scores, that generate the target sentence as a translation 9 We use the UNIX ulimit command.",
        "The experiment was carried out over machines with different configurations and loads, so these numbers should be considered as approximate values.",
        "709  of the source sentence.",
        "In alignment the target language model is extremely simple: It is simply an acceptor for the target language sentence so that |M| is linear in the length of the target sentence.",
        "In contrast, the search space needs now to be represented with pushdown transducers (instead of pushdown automata) keeping track of both translations and derivations, that is, indices of the rules in the grammar (Iglesias et al. 2009a; de Gispert et al. 2010; Dyer 2010b).",
        "We define a word-based translation grammar GITG for the alignment problem as follows.",
        "First, we obtain word-to-word translation rules of the form X?",
        "?s, t?",
        "based on probabilities from IBM Model 1 translation tables estimated over the parallel text, where s and t are one source and one target word, respectively (?16M rules).",
        "Then, we allow monotonic and inversion transduction of two adjacent nonterminals in the usual ITG style (i.e., add X?",
        "?X1 X2, X1 X2?",
        "and X?",
        "?X1 X2, X2 X1?).",
        "Additionally, we allow unrestricted source word deletions (X?",
        "?s, ??",
        "), and restricted target word insertions (X?",
        "?X1 X2, X1 t X2?).",
        "This restriction, which is solely motivated by efficiency reasons, disallows the insertion of two consecutive target words.",
        "We make no claims about the suitability or appropriateness of this specific grammar for either alignment or translation; we introduce this grammar only to define a challenging alignment task.",
        "A set of 2,500 sentence pairs of up to 50 source and 75 target words was chosen for alignment.",
        "These sentences come from the same Chinese-to-English parallel data described in Section 4.1.",
        "Hard limits on memory usage (10GB) and processing time (10 minutes) were imposed for processing each sentence pair.",
        "If HiPDT or HiFST exceeded either limit in aligning any sentence pair, alignment was stopped and a ?mem- ory/time failure?",
        "was noted.",
        "Even if the resource limits are not exceeded, alignment may fail due to limitations in the grammar.",
        "This happens when either a particular word pair rule that is not in our Model 1 table, or more than one consecutive target insertions are needed to reach alignment.",
        "In such cases, we record a ?grammar failure,?",
        "as opposed to a ?memory/time failure.?",
        "Results are reported in Table 4.",
        "Of the 2,500 sentence pairs, HiFST successfully aligns only 41% of the sentence pairs under these time and memory constraints.",
        "The reason for this low success rate is that HiFST must generate and expand all possible derivations under the ITG for a given sentence pair.",
        "Even if it is strictly enforced that the FSA in every CYK cell contains only partial derivations which produce sub-strings of the target sentence, expansion often exceeds the memory/time constraints.",
        "In contrast, HiPDT succeeds in aligning all sentence pairs that can be aligned under the grammar (89%), because it never fails due to memory or time constraints.",
        "In this experiment, if alignment is at all possible, HiPDT will find the best derivation.",
        "Alignment success rate (or coverage) could trivially be improved by modifying the ITG to allow more consecutive target insertions, or by increasing the number of word-to-word Table 4 Percentages of success and failure in aligning 2,500 sentence pairs under GITG with HiFST and HiPDT.",
        "HiPDT finds an alignment whenever it is possible under the translation grammar.",
        "HiFST HiPDT Success Failure Success Failure memory/time grammar memory/time grammar 41% 53% 6% 89% 0% 11% 710 Allauzen et al. Pushdown Automata in Statistical Machine Translation rules, but that would not change the conclusion in the contrast between HiFST and HiPDT.",
        "The computational analysis from the beginning of this section applies to alignment.",
        "The language model M is replaced by an acceptor for the target sentence, and if we assume that the target sentence length is proportional to the source sentence length, it follows that |M| ?",
        "|s| and the worst-case complexity for HiPDT in alignment mode is O(|s|6|G|).",
        "This is comparable to ITG alignment (Wu 1997) and the intersection algorithm of Dyer (2010b).",
        "Our experimental results support the complexity analysis summarized in Table 1.",
        "HiPDT is more efficient in ITG alignment and this is consistent with its linear dependence on the grammar size, whereas HiFST suffers from its exponential dependence.",
        "This use of PDAs in alignment does not rely on properties specific either to Hiero or to ITGs.",
        "We expect that the approach should be applicable with other types of SCFGs, although we note that alignment under SCFGs with an arbitrary number of nonterminals can be NP-hard (Satta and Peserico 2005).",
        "5.",
        "HiPDT Two-Pass Translation Architecture and Experiments The previous complexity analysis suggests that PDAs should excel when used with large translation grammars and relatively small n-gram language models.",
        "In hierarchical phrase-based translation, this is a somewhat unusual scenario: It is far more typical that translation tasks requiring a large translation grammar also require large language models.",
        "To accommodate these requirements we have developed a two-pass decoding strategy in which a weak version of a large language model is applied prior to the expansion of the PDA, after which the full language model is applied to the resulting WFSA in a rescoring pass.",
        "An effective way of generating weak language models is by means of entropy pruning under a threshold ?",
        "; these are the language models M?1 of Section 4.1.",
        "Such a two-pass strategy is widely used in automatic speech recognition (Ljolje, Pereira, and Riley 1999).",
        "The steps in two-pass translation using entropy-pruned language models are given here, and depicted in Figure 14.",
        "Step 1.",
        "We translate with M?1 and G using the same parameters obtained by MERT for the baseline system, with the exception that the word penalty parameter is adjusted to produce hypotheses of roughly the correct length.",
        "This produces translation lattices that contain hypotheses with exact scores under G and M?1 : ?2({s} ?",
        "G) ?M?1 .",
        "Step 2.",
        "These translation lattices are pruned at beamwidth ?",
        ": [?2({s} ?",
        "G) ?M?1 ]?.",
        "Step 3.",
        "We remove the M?1 scores from the pruned translation lattices, reapply the full language model M1, and restore the word penalty parameter to the baseline value obtained by MERT.",
        "This gives an approximation to ?2({s} ?",
        "G) ?M1: scores are correctly assigned underG andM1, but only hypotheses that survived pruning at Step 2 are included.",
        "We can rescore the lattices produced by the baseline system or by the two-pass system with the larger language model M2.",
        "If ?=?",
        "or if ?=0, the translation lattices obtained in Step 3 should be identical to lattices produced by the baseline system (i.e., the rescoring step is no longer needed).",
        "The aim is to increase ?",
        "to shrink the language model used at Step 1, but ?",
        "will then have to increase accordingly to avoid pruning away desirable hypotheses in Step 2.",
        "711  CYK parse s with G Build RTN RTN to PDA Replacement Intersect PDA with WFSA M1 ?",
        "PDA to FSA Pruned Expansion, threshold B Intersect FSA with LM M1 FSA Shortest Path FSA Pruning Lattice1-Best Hypothesis Remove LM scores Entropy Pruning, threshold ?",
        "LM M1 (as WFSA) further rescoring Figure 14 Two-pass HiPDT translation with an entropy pruned language model.",
        "5.1 Efficient Removal of First-Pass Language Model Scores Using Lexicographic Semirings The two-pass translation procedure requires removal of the weak language model scores used in the initial expansion of the translation search space; this is done so that only the translation scores under G remain after pruning.",
        "In the tropical semiring, the weak LM scores can be ?subtracted?",
        "at the path level from the lattice, but this involves a determinization of an unweighted translation lattice, which can be very inefficient.",
        "As an alternative we can define a lexicographic semiring (Shafran et al. 2011; Roark, Sproat, and Shafran 2011) ?w1,w2?",
        "over the tropical weights w1 and w2 with the operations ?",
        "and ?",
        ": ?w1,w2?",
        "?",
        "?w3,w4?",
        "= { ?w1,w2?",
        "if w1 < w3 or (w1 = w3 and w2 < w4) ?w3,w4?",
        "otherwise (8) ?w1,w2?",
        "?",
        "?w3,w4?",
        "= ?w1 + w3,w2 + w4?",
        "(9) The PDA algorithms described in Section 3 are valid under this new semiring because it is commutative and has the path property.",
        "In particular, the PDA representing {s} ?",
        "G is constructed so that the translation grammar score appears in both w1 and w2 (i.e., it is duplicated).",
        "In the first-pass language model, w1 has the n-gram language model scores and the w2 are 0.",
        "After composition, the resulting automata have the combined translation grammar score and language model score in the first dimension, and the second dimension contains the translation grammar scores alone.",
        "Pruning can be performed under the lexicographic semiring with a threshold set so that only the combined scores in the first dimension are considered.",
        "The resulting automata can easily be mapped back into the regular tropical semiring such that only the translation scores in the second 712 Allauzen et al. Pushdown Automata in Statistical Machine Translation dimension are retained (this is a linear operation done by the fstmap operation in the OpenFST library).",
        "5.2 Translation Quality and Modeling Errors in Two-Pass Decoding We wish to analyze the degree to which the two-pass decoding strategy introduces ?modeling errors?",
        "into translation.",
        "A modeling error occurs in two-pass decoding whenever the decoder produces a translation whose score is less than the best attainable under the grammar and language model (i.e., whenever the best possible translation is discarded by pruning at Step 2).",
        "We refer to these as modeling errors, rather than search errors, because they are due to differences in scores assigned by the models M1 and M?1 .",
        "Ideally, we would compare the two-pass translation system against a baseline system that performs exact translation, without pruning in search, under the grammar G and language model M1.",
        "This would allow us to address the following questions: r Is a two-pass decoding procedure that uses entropy-pruned language models adequate for translation?",
        "How many modeling errors are introduced?",
        "Does two-pass decoding impact on translation quality?",
        "r Which smoothing/discounting technique is best suited for the first-pass language model in two-pass translation, and which smoothing/ discounting technique is best at avoiding modeling errors?",
        "Our grammar G is not suitable for these experiments, as we do not have a system capable of exact decoding under both G and M1.",
        "To create a suitable baseline we therefore reduce G by excluding rules that have a forward translation probability p < 0.01, and refer to this reduced grammar as Gsmall.",
        "This process reduces the number of strictly hierarchical rules that apply to our tune-nw set from 511K to 189K, while the number of standard phrases is unchanged.",
        "Under Gsmall, both HiFST and HiPDT are able to exactly compose the entire space of possible candidate hypotheses with the language model and to extract the shortest path hypothesis.",
        "Because an exact decoding baseline is thus available, we can empirically evaluate the proposed two-pass strategy.",
        "Any degradation in translation quality can only be due to the modeling errors introduced by pruning under ?",
        "with respect to the entropy-pruned M?1 .",
        "Figure 15 shows translation performance under grammar Gsmall for different values of entropy pruning threshold ?.",
        "Performance is reported after first-pass decoding with M?1 (Step 1, Section 5), and after rescoring with M1 (Step 3, Section 5) the first-pass lattices pruned at alternative ?",
        "beams.",
        "The first column reports the baseline for either Kneser-Ney andKatz languagemodels, which are found by translation without entropy pruning, that is, performed with M1.",
        "Both yield 34.5 on test-nw.",
        "The first and main conclusion from this figure is that the two-pass strategy is adequate because we are always able to recover the baseline performance.",
        "As expected, the harsher the entropy-pruning ofM1 (as we lower ?)",
        "the greater?must be to recover from the significant degradation in first-pass decoding.",
        "But even at a harsh ?",
        "= 7.5?",
        "10?7, when first-pass performance drops over 7 BLEU points, a relatively-low value of ?",
        "= 15 can recover the baseline performance.",
        "Although this is true independently of the LM smoothing approach, a second conclusion from the figure is that the choice of LM smoothing does impact first-pass 713  Figure 15 Results (lower case IBM BLEU scores over test-nw) under Gsmall with various M?1 as obtained with several values of ?.",
        "Performance in subsequent rescoring with M1 after likelihood-based pruning of the resulting translation lattices for various ?",
        "is also reported.",
        "In the pipeline, M1 (and M?1 ) are estimated with either Katz or Kneser-Ney smoothing.",
        "translation performance.",
        "For entropy pruning at ?",
        "= 7.5?",
        "10?7, the Katz LMs perform better for smaller beamwidths ?.",
        "These results are consistent with the test set perplexities of the entropy pruned LMs (Table 2), and are also in line with other studies of Kneser-Ney smoothing and entropy pruning (Chelba et al. 2010; Roark, Allauzen, and Riley 2013).",
        "Modeling errors are reported in Table 5 at the entropy pruning threshold ?",
        "= 7.5?",
        "10?7.",
        "As expected, modeling errors decrease as the beamwidth ?",
        "increases, although we find that the language model with Katz smoothing has fewer modeling errors.",
        "However, modeling errors do not necessarily impact corpus level BLEU scores.",
        "For wide beamwidths (e.g., ?",
        "= 15 here), there are still some modeling errors, but these are either few enough or subtle enough that two-pass decoding under either smoothing method yields the same corpus level BLEU score as the exact decoding baseline.",
        "Table 5 Two-pass translation modeling errors as a function of RTN expansion pruning threshold ?.",
        "A modeling error occurs whenever the score of a hypothesis produced by the two-pass translation differs from the score found by the exact baseline system.",
        "Errors are tabulated over systems reported in Figure 15, at ?",
        "= 7.5?",
        "10?7.",
        "?",
        "Kneser-Ney Katz 8 814 619 12 343 212 15 240 110 714 Allauzen et al. Pushdown Automata in Statistical Machine Translation 5.3 HIPDT Two-Pass Decoding Speed and Translation Performance r What are the speed and quality tradeoffs for HiPDT as a function of first-pass LM size and translation grammar complexity?",
        "r How do these compare against the predicted computational complexity?",
        "In this section we turn back to the original large grammar, for which HiFST cannot perform exact decoding (see Table 3).",
        "In contrast, HiPDT is able to do exact decoding so we study tradeoffs in speed and translation performance.",
        "The speed of two-pass decoding can be increased by decreasing ?",
        "and/or increasing ?, but at the risk of degradation in translation performance.",
        "For grammar G and language model M1 we plot in Figure 16 the BLEU score against speed as a function of ?",
        "for a selection of ?",
        "values.",
        "BLEU score is measured over the entire test set test-nw but speed is calculated only on sentences of length up to 20 words (?500 sentences).",
        "In computing speed we measure not only the PDA operations, but the entire HiPDT decoding process described in Figure 14, including CYK parsing and the application of M1.",
        "We note in passing that these unusually slow decoding speeds are a consequence of the large grammars, language models, and broad pruning thresholds chosen for these experiments; in practice, translation with either HiPDT or HiFST is much faster.",
        "In these experiments we find that the language model entropy pruning threshold ?",
        "and the likelihood beamwidth ?",
        "work together to balance speed against translation quality.",
        "For every entropy pruning threshold ?",
        "value considered, there is a value of ?",
        "for which there is no degradation in translation quality.",
        "For example, suppose we want to attain a translation quality of 34.5 BLEU: then ?",
        "should be set to 12 or greater.",
        "If the goal is to find the fastest system at this level, then we choose ?",
        "= 7.5?",
        "10?5.",
        "The interaction between pruning in expansion and pruning of the language model is explained by Figure 17, where decoding and rescoring times are shown for various Figure 16 HiPDT translation quality versus speed (decoding with G, M?1 + rescoring with M1) under different entropy pruning thresholds ?",
        "and for likelihood beamwidths ?",
        "= 15, 12, 9, 8, 7.",
        "715  Figure 17 Accumulated decoding+rescoring times for HiPDT under different entropy pruning thresholds, reaching a performance of at least 34.5 BLEU, for which ?",
        "is set to 12. values of ?",
        "and ?",
        "that achieve at least the translation quality target of 34.5.",
        "As ?",
        "increases, decoding time decreases because a smaller language model is easier to apply; however, rescoring times increase, because the larger values of ?",
        "lead to larger WFSAs after expansion, and these are costly to rescore.",
        "The balance occurs at ?",
        "= 7.5?",
        "10?5 and a translation rate of 3.0 words/sec.",
        "In this case, entropy pruning yields a severely shrunken bigram language model, but this may vary depending on the translation grammar and the original, unpruned LM.",
        "5.4 Rescoring with 5-Gram Language Models and LMBR Decoding r Does the HiPDT two-pass decoding generate lattices that can be useful in rescoring?",
        "We now report on rescoring experiments using WFSAs produced by the two-pass HiPDT translation system under the large translation grammar G. We demonstrate that HiPDT can be used to generate large, compact representations of the translation space that are suitable for rescoring with large language models or by alternative decoding procedures.",
        "We investigate translation performance by applying versions of the language model M2 estimated with stupid backoff.",
        "We also investigate minimum Bayes risk (MBR) decoding (Kumar and Byrne 2004) as an alternative search strategy.",
        "We are particularly interested in lattice MBR (LMBR) (Tromble et al. 2008), which is well suited for the large WFSAs that the system can generate; we use the implementation described by Blackwood, de Gispert, & Byrne (2010).",
        "There are two parameters to be tuned: a scaling parameter to normalize the evidence scores and a word penalty applied to the hypotheses space; these are tuned jointly on the tune-nw set.",
        "Results are reported in Figure 18.",
        "We note first that rescoring with the large language model M2, which is effectively interpolated with M1, gives consistent gains over initial results obtained with M1 alone.",
        "After 5-gram rescoring there is already +0.5 BLEU improvement compared with Gsmall.",
        "With a richer translation grammar we have generated a richer lattice that allows gains to be gotten by our lattice rescoring techniques.",
        "716 Allauzen et al. Pushdown Automata in Statistical Machine Translation Figure 18 HiPDT decoding with G. Decoding language model M?1 and first pass rescoring language model M1 are Katz.",
        "Results on test-nw are given for ML-Decoding under the 5-gram stupid backoff language model (?5gML?)",
        "and for LMBR and for LMBR decoding.",
        "Parameter values are ?",
        "= 15, 12, 9, 8 and ?",
        "= 7.5?",
        "10?7 , 7.5?",
        "10?5, 7.5?",
        "10?3.",
        "We also find that BLEU scores degrade smoothly as ?",
        "decreases and the expansion pruning beamwidth narrows, and at all values of ?",
        "LMBR gives improvement over the MAP hypotheses.",
        "Because LMBR relies on posterior distributions over n-grams, we conclude that HiPDT is able to generate compact representations of large search spaces with posteriors that are robust to pruning conditions.",
        "Finally, we find that increasing ?",
        "degrades performance quite smoothly for ?",
        "?",
        "9.",
        "Again, with appropriate choices of ?",
        "and ?",
        "we can easily reach a compromise between decoding speed and final performance of our HiPDT system.",
        "For instance, with ?",
        "= 7.5?",
        "10?7 and?",
        "= 12, for whichwe decode at a rate of 3words/sec as seen in Figure 16, we are losing only 0.5 BLEU after LMBR compared to ?",
        "= 7.5?",
        "10?7 and ?",
        "= 15.",
        "6.",
        "RelatedWork There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase-based translation.",
        "The challenge is to find algorithms that can be made to work with large translation grammars and large language models.",
        "Following the original algorithms and analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits the left-to-right nature of n-gram language models.",
        "Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison.",
        "Using a relatively simple phrase-based translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning suffered significant search errors.",
        "For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010).",
        "The effect of search errors has also been 717  studied in phrase-based translation by Zens andNey (2008).",
        "Relaxation techniques have also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrase-based SMT (Chang and Collins 2011), and in tree-to-string translation under trigram language models (Rush and Collins 2011); this prior work involved much smaller grammars and languages models than have been considered here.",
        "Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions.",
        "We developed similar procedures for our HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010) via a different route, after noting that with the space of translations represented as WFSAs, alignment can be performed using operations over WFSTs (Kumar and Byrne 2005).",
        "Although entropy-pruned language models have been used to produce real-time translation systems (Prasad et al. 2007), we believe our use of entropy-pruned language models in two-pass translation to be novel.",
        "This is an approach that is widely used in automatic speech recognition (Ljolje, Pereira, and Riley 1999) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring, as is possible with FSAs and PDAs.",
        "7.",
        "Conclusion In this article, we have described a novel approach to hierarchical machine translation using pushdown automata.",
        "We have presented fundamental PDA algorithms including composition, shortest-path, (pruned) expansion, and replacement and have shown how these can be used in PDA-based machine translation decoding and how this relates to and compares with hypergraph and FSA-based decoding.",
        "On the basis of the experimental results presented in the previous sections, we can now address the questions laid out in Sections 4 and 5: r A two-pass translation decoding procedure in which translation is first performed with a weak entropy-pruned language model and followed by admissible likelihood-based pruning and rescoring with a full language model can yield good quality translations.",
        "Translation performance does not degrade significantly unless the first-pass language model is very heavily pruned.",
        "r As predicted by the analysis of algorithmic complexity, intersection and expansion algorithms based on the PDA representation are able to perform exact decoding with large translation and weak language models.",
        "By contrast, RTN to FSA expansion fails with large translation grammars, regardless of the size of the language model.",
        "With large translation grammars, language model composition prior to expansion may be more attractive than expansion prior to language model composition.",
        "r Our experimental results suggest that for a translation grammar and a language model of a particular size, and given a value of language model entropy pruning threshold ?, there is a value of the pruned expansion parameter ?",
        "for which there is no degradation in translation quality with HiPDT.",
        "This makes exact decoding under large translation grammars possible.",
        "The values of ?",
        "and ?",
        "will be grammar- and task-dependent.",
        "718 Allauzen et al. Pushdown Automata in Statistical Machine Translation r Although there is some interaction between parameter tuning, pruning thresholds, and language modeling strategies, the variation is not significant enough to indicate that a particular language model or smoothing technique is best.",
        "This is particularly true if minimum Bayes risk decoding is applied to the output translation lattices.",
        "Several questions naturally arise about the decoding strategies presented here.",
        "One is whether inadmissible pruning methods can be applied to the PDA-based systems that are analogous to those used in current hypergraph-based systems such as cube-pruning (Chiang 2007).",
        "Another is whether a hybrid PDA?FSA system, where some parts of the PDA are pre-expanded and some not, could provide benefits over full pre-expansion (FSA) or none (PDA).",
        "We leave these questions for future work.",
        "Appendix A.",
        "Composition of a Weighted PDT and a Weighted FST Given a pair (T1,T2) where T1 is a weighted pushdown transducer and the T2 is a weighted finite-state transducer, and such that T1 has input and output alphabets ?",
        "and ?",
        "and T2 has input and output alphabets ?",
        "and ?, then there exists a weighted pushdown transducer T1 ?",
        "T2, which is the composition of T1 and T2, such that for all (x, y) ?",
        "??",
        "?",
        "??",
        ": T = (T1 ?",
        "T2)(x, y) = minz???",
        "(T1(x, z)+ T2(z, y)) (A.10) We also assume that T2 has no input-?",
        "transitions, noting that for T2 with input-?",
        "tran-sitions, an epsilon filter (Mohri 2009; Allauzen, Riley, and Schalkwyk 2011) generalized to handle parentheses could be used.",
        "A state in T is a pair (q1, q2) where q1 is a state of T1 and q2 a state of T2.",
        "Given a transition e1 = (q1, a, b,w1, q?1) in T1, transitions out of (q1, q2) in T are obtained using the following rules.",
        "If b ?",
        "?, then e1 can be matched with a transition (q2, b, c,w2, q?2) in T2 resulting in a transition ((q1, q2), a, c,w1 + w2, (q?1, q?2)) in T. If b = ?, then e1 is matched with staying in q2 resulting in a transition ((q1, q2), a, ?,w1, (q?1, q2)).",
        "Finally, if b = a ?",
        "?",
        "?, e1 is also matched with staying in q2, resulting in a transition ((q1, q2), a, a,w1, (q?1, q2)) in T. The initial state is (I1, I2) and a state (q1, q2) in T is final when both q1 and q2 are both final.",
        "Weight values are assigned as ?",
        "((q1, q2)) = ?1(q1)+ ?2(q2).",
        "Appendix B. Pruned Expansion Let dR and BR be the data structures computed by the shortest-distance algorithm applied to TR.",
        "For a state q in T?",
        "(or equivalently T??",
        "), let d[q] denote the shortest distance from the initial state to q, d[q] denote the shortest distance from q to the final state, and s[q] denote the destination state of the last unbalanced open-parenthesis transition on a shortest path from the initial state to q.",
        "The algorithm is based on the following property: Letting e denote a transition in T?",
        "such that p[e] = (q, z) and z = z?a, the weight of a shortest path through e can be expressed as: d[(q, z)]+ w[e]+ min e?",
        "?BR[qs ,a] dR[n[e], p[e?",
        "]]+ w[e?",
        "]+ d[(n[e?",
        "], z?)]",
        "(B.11) 719  PRUNEDEXPANSION(T, ?)",
        "1 (dR,BR)?",
        "SHORTESTDISTANCE (TR ) 2 ??",
        "dR[I, f ]+?",
        "?",
        "Compute the pruning threshold 3 B?",
        "REVERSE (BR ) ?",
        "Compute the balance information in T from the one in TR 4 (I?, f ?",
        ")?",
        "((I,?",
        "), ( f,?))",
        "?",
        "I?",
        "and f ?",
        "are the initial and final states of the pruned expansion 5 (F?,??",
        "( f ?))?",
        "({ f ?",
        "}, 0) 6 (d[I?",
        "], s[I?])?",
        "(0, I? )",
        "7 (d[I?",
        "], d[ f ?])?",
        "(dR[I, f ], 0) 8 (zD,D[ f ])?",
        "(?, 0) 9 S?",
        "Q??",
        "{I?}",
        "10 while S 6=?",
        "do 11 (q, z)?",
        "HEAD(S) 12 DEQUEUE (S) 13 if s[(q, z)]= (q, z) then 14 if z 6= zD then ?",
        "If the stack has changed, D needs to be cleared and recomputed 15 CLEAR (D) 16 zD?",
        "z 17 for each e ?",
        "B[q, z|z|] do ?",
        "For each close paren.",
        "transition balancing the incoming z|z|-labeled open paren.",
        "transition in q 18 D[p[e]]?",
        "min(D[p[e]],w[e]+ d[(n[e], z1 ?",
        "?",
        "?",
        "z|z|?1 )]) 19 for each e ?",
        "E[q] do 20 if i[e] ?",
        "??",
        "{?}",
        "then ?",
        "If i[e] is a regular symbol 21 if RETAINPATH (q, z,w[e],n[e]) then 22 E??",
        "E?",
        "?",
        "{((q, z), i[e], o[e],w[e], (n[e], z))} 23 elseif i[e] ??",
        "then ?",
        "If i[e] is an open parenthesis 24 z??",
        "zi[e] 25 r?",
        "false 26 for each e?",
        "?",
        "B[n[e], i[e]] do ?",
        "For each close paren.",
        "transition e?",
        "that balances e 27 w?",
        "w[e]+ dR[n[e],p[e?]]+w[e?]",
        "?",
        "w: weight of the shortest bal.",
        "path beginning by e and ending by e?",
        "in T 28 r?",
        "r?",
        "RETAINPATH (q, z,w,n[e?])",
        "?",
        "Does the expansion of that path belong to an accepting path below threshold?",
        "29 wF?min(wF, dR[n[e], p[e?]]+w[e?",
        "]+ d[(n[e?",
        "], z)]) 30 if r then ?",
        "If any of the paths considered above are below threshold 31 E??",
        "E?",
        "?",
        "{((q, z),?,?,w[e], (n[e], z? ))}",
        "32 PROCESSSTATE ((n[e], z?))",
        "33 s[(n[e], z?)]?",
        "(n[e], z? )",
        "34 d[(n[e], z?",
        ")]?min(d[(n[e], z?",
        ")], d[(q, z)]+w[e]) 35 d[(n[e], z?",
        ")]?min(d[(n[e], z?",
        ")],wF ) 36 elseif i[e] ??",
        "and c?",
        "(zi[e]) ?",
        "??",
        "then ?",
        "If i[e] is the close parenthesis matching the top of the stack 37 z??",
        "c?",
        "(zi[e]) 38 if d[(q, z)]+w[e]+ d[(n[e], z? )]",
        "?",
        "?",
        "then 39 E??",
        "E?",
        "?",
        "{((q, z),?,?,w[e], (n[e], z? ))}",
        "40 return (?,?,?,?,Q?,E?, I?,F?,?? )",
        "RETAINPATH(q, z,w, q? )",
        "1 ?",
        "Returns true iff a path from (q, z) to (q?, z) with weight w belongs to an accepting path below threshold 2 wI?",
        "d[(q, z)]+w ?",
        "Shortest distance from I to (q?, z) when taking a path from (q, z) to (q?, z) of weight w 3 wF?",
        "min{dR[q?, t]+D[t]|D[t] 6=?}?",
        "Current estimate of s. d. from (q?, z) to f ?",
        "4 if wI < d[(q?",
        ", z)] then ?",
        "If wI is a better estimate of s.-d. from I?",
        "to (q?, z), update d[(q?, z)] and s[(q?, z)] 5 d[(q?",
        ", z)]?",
        "wI 6 s[(q?, z)]?",
        "s[(q, z)] 7 if wF < d[(q?, z)] then ?",
        "If wF is a better estimate of s. d. from (q?, z) to f ?",
        ", update d[(q?, z)] 8 d[(q?, z)]?",
        "wF 9 if ?",
        "< wI +wF then ?",
        "wI +wF: min.",
        "weight of an accepting path taking a path of weight w from (q, z) to (q?, z) 10 return false 11 PROCESSSTATE ((q?",
        ", z)) 12 return true PROCESSSTATE((q, z)) 1 if (q, z) 6?",
        "Q?",
        "then ?",
        "If state (q, z) does not exist yet, create it and add it to the queue 2 Q??",
        "Q?",
        "?",
        "{(q, z)} 3 ENQUEUE (S, (q, z)) Figure 19 PDT pruned expansion algorithm.",
        "We assume that F={ f} and ?",
        "( f )=0 to simplify the presentation.",
        "720 Allauzen et al. Pushdown Automata in Statistical Machine Translation where (qs, z) = s[(q, z)].",
        "This implies that assuming when (q, z) is visited, d[(n[e?",
        "], z?)]",
        "is known; we then have all the required information for deciding whether e should be pruned or retained.",
        "In order to ensure that each state is visited once, we need to ensure that d[(q, z)] is known when (q, z) is visited so we can apply an A?",
        "queue discipline among the states sharing the same stack.",
        "Both conditions can be achieved by using a queue discipline defined by a partial order?",
        "such that z is a prefix of z?",
        "?",
        "(q, z) ?",
        "(q?, z?)",
        "(B.12) d[(q, z)]+ d[(q, z)] < d[(q?, z)]+ d[(q?, z)]?",
        "(q, z) ?",
        "(q?, z) (B.13) We also assume that all states sharing the same stack will be dequeued consecutively (z 6= z?",
        "?",
        "for all (q, q?",
        "), (q, z) ?",
        "(q?, z?)",
        "or for all (q, q?",
        "), (q?, z?)",
        "?",
        "(q, z)).",
        "This allows us to cache some computations (the D data structure as described subsequently).",
        "The pseudo code of the algorithm is given in Figure 19.",
        "First, the shortest distance algorithm is applied to TR and the absolute pruning threshold is computed accordingly (lines 1?2).",
        "The resulting balanced data information is then reversed (line 3).",
        "The initial and final states are created (lines 4?5) and the d, d, and D data structures are initialized accordingly (lines 6?8).",
        "The default value in these data structures is assumed to be?.",
        "The queue is initialized containing the initial state (line 9).",
        "The state (q, z) at the head of the queue is dequeued (lines 10?12).",
        "If (q, z) admits an incoming open-parenthesis transition, B contains the balance information for that state and D can be updated accordingly (lines 13?18).",
        "If e is a regular transition, the resulting transition ((q, z), i[e], o[e],w[e], (n[e], z)) in T?",
        "can be pruned using the criterion derived from Equation (B.11).",
        "If it is retained, the transition is created as well as its destination state (n[e], z) if needed (lines 20?22).",
        "If e is an open-parenthesis transition, each balanced path starting by the resulting transition in T?",
        "and ending by a close-parenthesis transition is treated as a meta-transition and pruned using the same criterion as regular transitions (lines 23?29).",
        "If any of these meta-transitions is retained, the transition ((q, z), ?, ?,w[e], (n[e], zi[e])) resulting from e is created as well as its destination state (n[e], zi[e]) if needed (lines 30?35).",
        "If e is a closed-parenthesis transition, it is created if it belongs to a balanced path below the threshold (lines 36?39).",
        "Finally, the resulting transducer T??",
        "is returned (line 40).",
        "Acknowledgments The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7-ICT-2009-4) under grant agreement number 247762, and was supported in part by the GALE program of the Defense Advanced Research Projects Agency, contract no.",
        "HR0011-06-C-0022, and a May 2010 Google Faculty Research Award.",
        "References"
      ]
    }
  ]
}
