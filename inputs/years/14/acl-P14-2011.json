{
  "info": {
    "authors": [
      "Le Sun",
      "Xianpei Han"
    ],
    "book": "ACL",
    "id": "acl-P14-2011",
    "title": "A Feature-Enriched Tree Kernel for Relation Extraction",
    "url": "https://aclweb.org/anthology/P14-2011",
    "year": 2014
  },
  "references": [
    "acl-C08-1088",
    "acl-C10-1018",
    "acl-C96-1079",
    "acl-D07-1076",
    "acl-D11-1096",
    "acl-E09-1066",
    "acl-H05-1091",
    "acl-J93-2004",
    "acl-N06-1037",
    "acl-N07-1015",
    "acl-N10-1146",
    "acl-P01-1017",
    "acl-P04-1043",
    "acl-P04-1054",
    "acl-P04-3022",
    "acl-P05-1052",
    "acl-P05-1053",
    "acl-P06-1104",
    "acl-P09-1113",
    "acl-P11-1056",
    "acl-P12-1028",
    "acl-P13-1147",
    "acl-W02-1010"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 61?67, Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics A Feature-Enriched Tree Kernel for Relation Extraction Le Sun and Xianpei Han State Key Laboratory of Computer Science Institute of Software, Chinese Academy of Sciences HaiDian District, Beijing, China.",
        "{sunle, xianpei}@nfs.iscas.ac.cn",
        "Abstract",
        "Tree kernel is an effective technique for relation extraction.",
        "However, the traditional syntactic tree representation is often too coarse or ambiguous to accurately capture the semantic relation information between two entities.",
        "In this paper, we propose a new tree kernel, called feature-enriched tree kernel (FTK), which can enhance the traditional tree kernel by: 1) refining the syntactic tree representation by annotating each tree node with a set of discriminant features; and 2) proposing a new tree kernel which can better measure the syntactic tree similarity by taking all features into consideration.",
        "Experimental results show that our method can achieve a 5.4% F-measure improvement over the traditional convolution tree kernel."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Relation Extraction (RE) aims to identify a set of predefined relations between pairs of entities in text.",
        "In recent years, relation extraction has received considerable research attention.",
        "An effective technique is the tree kernel (Zelenko et al., 2003; Zhou et al., 2007; Zhang et al., 2006; Qian et al., 2008), which can exploit syntactic parse tree information for relation extraction.",
        "Given a pair of entities in a sentence, the tree kernel-based RE method first represents the relation information between them using a proper sub-tree (e.g., SPT ?",
        "the sub-tree enclosed by the shortest path linking the two involved entities).",
        "For example, the three syntactic tree representations in Figure 1.",
        "Then the similarity between two trees are computed using a tree kernel, e.g., the convolution tree kernel proposed by Collins and Duffy (2001).",
        "Finally, new relation instances are extracted using kernel based classifiers, e.g., the SVM classifier.",
        "Unfortunately, one main shortcoming of the traditional tree kernel is that the syntactic tree representation usually cannot accurately capture the Figure 1.",
        "The ambiguity of possessive structure relation information between two entities.",
        "This is mainly due to the following two reasons: 1) The syntactic tree focuses on representing syntactic relation/structure, which is often too coarse or ambiguous to capture the semantic relation information.",
        "In a syntactic tree, each node indicates a clause/phrase/word and is only labeled with a Treebank tag (Marcus et al., 1993).",
        "The Treebank tag, unfortunately, is usually too coarse or too general to capture semantic information.",
        "For example, all the three trees in Figure 1 share the same possessive syntactic structure, but express quite different semantic relations: where ?Mary's brothers?",
        "expresses PER-SOC Family relation, ?Mary's toys?",
        "expresses Possession rela-tion, and ?New York's airports?",
        "expresses PHYS-Located relation.",
        "2) Some critical information may lost during sub-tree representation extraction.",
        "For example, in Figure 2, when extracting SPT representation, all nodes outside the shortest-path will be pruned, such as the nodes [NN plants] and [POS ?s] in tree T1.",
        "In this pruning process, the critical information ?word town is the possessor of the possessive phrase the town's plants?",
        "will be lost, which in turn will lead to the misclassification of the DISC relation between one and town.",
        "This paper proposes a new tree kernel, referred as feature-enriched tree kernel (FTK), which can effectively resolve the above problems by enhancing the traditional tree kernel in following ways: 1) We refine the syntactic tree representation by annotating each tree node with a set of discriminant features.",
        "These features are utilized to NP NP NN NN POS Mary 's brothers (a) (b) (c) NP NP NN NN POS Mary 's toys NP NP NN NN POS NY 's airports 61 better capture the semantic relation information between two entities.",
        "For example, in order to differentiate the syntactic tree representations in Figure 1, FTK will annotate them with several features indicating ?brother is a male sibling?, ?toy is an artifact?, ?New York is a city?, ?airport is facility?, etc.",
        "2) Based on the refined syntactic tree repre-sentation, we propose a new tree kernel ?",
        "feature-enriched tree kernel, which can better measure the similarity between two trees by also taking all features into consideration.",
        "Figure 2.",
        "SPT representation extraction We have experimented our method on the ACE 2004 RDC corpus.",
        "Experimental results show that our method can achieve a 5.4% F-measure improvement over the traditional convolution tree kernel based method.",
        "This paper is organized as follows.",
        "Section 2 describes the feature-enriched tree kernel.",
        "Section 3 presents the features we used.",
        "Section 4 discusses the experiments.",
        "Section 5 briefly reviews the related work.",
        "Finally Section 6 concludes this paper.",
        "2 The Feature-Enriched Tree Kernel In this section, we describe the proposed feature-enriched tree kernel (FTK) for relation extraction.",
        "2.1 Refining Syntactic Tree Representation As described in above, syntactic tree is often too coarse or too ambiguous to represent the semantic relation information between two entities.",
        "To resolve this problem, we refine the syntactic tree representation by annotating each tree node with a set of discriminant features.",
        "Figure 3.",
        "Syntactic tree enriched with features Specifically, for each node in a syntactic tree , we represent it as a tuple: where is its phrase label (i.e., its Treebank tag), and is a feature vector which indicates the characteristics of node , which is represented as: where fi is a feature and is associated with a weight .",
        "The feature we used includes characteristics of relation instance, phrase properties and context information (See Section 3 for details).",
        "For demonstration, Figure 3 shows the feature-enriched version of tree T2 and tree T4 in Figure 2.",
        "We can see that, although T2 and T4 share the same syntactic structure, the annotated features can still differentiate them.",
        "For example, the NP5 node in tree T2 and the NP5 node in tree T4 are differentiated using their features PossessivePhrase and PPPhrase, which indicate that NP5 in T2 is a possessive phrase, meanwhile NP5 in T4 is a preposition phrase.",
        "2.2 Feature-Enriched Tree Kernel This section describes how to take into account the annotated features for a better tree similarity.",
        "In Collins and Duffy's convolution tree kernel (CTK), the similarity between two trees T1 and T2 is the number of their common sub-trees: Using this formula, CTK only considers whether two enumerated sub-trees have the identical syntactic structure (the indicator is 1 if the NP NP PP CD one IN E1 of NP NP NN town E2 DT the POS 's NN plants NP NP PP CD one IN E1 of NP NP NN town E2 DT the Prune NP NP PP CD one IN E1 of NP NP DT NN the teams E2 PP IN in USA NP NN Prune NP NP PP CD one IN E1 of NP NP NN teams E2 DT the (T1) (T2) (T3) (T4) NP NP PP CD one IN E1 of NP NP NN town E2 DT the (T2) PossessivePhrase, RootPath:NP-PP, Contain_Arg2_GPE, ... Possessor, Contain_Arg2_GPE, RootPath:NP-PP-NP, EndWithPOS, ... EntType:GPE, MentionType:NOM, RootPath:NP-PP-NP-NP, ... WN:town, WN:district, WN:region, WN:location, Match_Arg2_GPE ... PPPhrase, RootPath:NP-PP, Contain_Arg2_ORG, ... PP_Head, RootPath:NP-PP-NP, Contain_Arg2_ORG,... EntType:ORG, MentionType:NOM, RootPath:NP-PP-NP-NP, ... WN:team, WN:social_unit, WN:group, WN:organization, Match_Arg2_ORG ...",
        "Feature Vector 1 2 3 4 5 6 7 8 9 10 11 12 14 13 15 NP NP PP CD one IN E1 of NP NP NN team E2 DT the (T4) 1 2 3 4 5 6 7 8 9 10 12 14 11 13 15 62 two sub-trees and have the identical syntactic structure and 0 otherwise).",
        "Such an assumption makes CTK can only capture the syntactic structure similarity between two trees, while ignoring other useful information.",
        "To resolve the above problem, the feature-enriched tree kernel (FTK) compute the similarity between two trees as the sum of the similarities between their common sub-trees: where is the similarity between enumerated sub-trees and , which is computed as: where is the same indicator function as in CTK; is a pair of aligned nodes between and , where and are correspondingly in the same position of tree and ; is the set of all aligned node pairs; is the feature vector similarity between node and , computed as the dot product between their feature vectors and .",
        "Notice that, if all nodes are not annotated with features, will be equal to .",
        "In this perspective, we can view as a similarity adjusted version of , i.e., only considers whether two nodes are equal, in contrast further considers the feature similarity between two nodes.",
        "The Computation of FTK.",
        "As the same as CTK, FTK can be efficiently computed as: where is the set of nodes in tree , and evaluates the sum of the similarities of common sub-trees rooted at node and node , which is recursively computed as follows: 1) If the production rules of and are differ-ent, = 0; 2) If both and is pre-terminal nodes, ; Otherwise go to step 3; 3) Calculate recursively as: ?",
        "(n1;n2) = ??",
        "(1 + sim(n1; n2)) ?",
        "#ch(n1)X k=1 (1 + ?",
        "(ch(n1; k); ch(n2; k)) 3 Features for Relation Extraction This section presents the features we used to enrich the syntactic tree representation.",
        "3.1 Instance Feature Relation instances of the same type often share some common characteristics.",
        "In this paper, we add the following instance features to the root node of a sub-tree representation: 1) Syntactico-Semantic structure.",
        "A feature indicates whether a relation instance has the following four syntactico-semantic structures in (Chan & Roth, 2011) ?",
        "Premodifiers, Possessive, Preposition, Formulaic and Verbal.",
        "2) Entity-related information of arguments.",
        "Features about the entity information of arguments, including: a) #TP1-#TP2: the concat of the major entity types of arguments; b) #ST1- #ST2: the concat of the sub entity types of argu-ments; c) #MT1-#MT2: the concat of the mention types of arguments.",
        "3) Base phrase chunking features.",
        "Features about the phrase path between two arguments and the phrases?",
        "head before and after the arguments, which are the same as the phrase chunking features in (Zhou, et al., 2005).",
        "3.2 Phrase Feature As discussed in above, the Treebank tag is too coarse to capture the property of a phrase node.",
        "Therefore, we enrich each phrase node with features about its lexical pattern, its content infor-mation, and its lexical semantics: 1) Lexical Pattern.",
        "We capture the lexical pattern of a phrase node using the following fea-tures: a) LP_Poss: A feature indicates the node is a possessive phrase; b) LP_PP: A feature indicates the node is a preposition phrase; c) LP_CC: A feature indicates the node is a conjunction phrase; d) LP_EndWithPUNC: A feature indicates the node ends with a punctuation; e) LP_EndWith-POSS: A feature indicates the node ends with a possessive word.",
        "2) Content Information.",
        "We capture the property of a node's content using the following features: a) MB_#Num: The number of mentions contained in the phrase; b) MB_C_#Type: A feature indicates that the phrase contains a mention with major entity type #Type; c) MW_#Num: The number of words within the phrase.",
        "3) Lexical Semantics.",
        "If the node is a pre-terminal node, we capture its lexical semantic by adding features indicating its WordNet sense information.",
        "Specifically, the first WordNet sense of the terminal word, and all this sense's hyponym senses will be added as features.",
        "For example, WordNet senses {New York#1, city#1, district#1, 63 region#1, ?}",
        "will be added as features to the [NN New York] node in Figure 1.",
        "3.3 Context Information Feature The context information of a phrase node is critical for identifying the role and the importance of a sub-tree in the whole relation instance.",
        "This paper captures the following context information: 1) Contextual path from sub-tree root to the phrase node.",
        "As shown in Zhou et al. (2007), the context path from root to the phrase node is an effective context information feature.",
        "In this paper, we use the same settings in (Zhou et al., 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3.",
        "2) Relative position with arguments.",
        "We observed that a phrase's relative position with the relation's arguments is useful for identifying the role of the phrase node in the whole relation instance.",
        "To capture the relative position infor-mation, we define five possible relative positions between a phrase node and an argument, corresponding match, cover, within, overlap and other.",
        "Using these five relative positions, we capture the context information using the following features: a) #RP_Arg1Head_#Arg1Type: a feature indicates the relative position of a phrase node with argument 1's head phrase, where #RP is the relative position (one of match, cover, within, overlap, other), and #Arg1Type is the major entity type of argument 1.",
        "One example feature may be Match_Arg1Head_LOC.",
        "b) #RP_Arg2Head_#Arg2Type: The relative position with argument 2's head phrase; c) #RP_Arg1Extend_#Arg1Type: The relative position with argument 1's extended phrase; d) #PR_Arg2Extend_#Arg2Type: The relative position with argument 2's extended phrase.",
        "Feature weighting.",
        "Currently, we set all features with an uniform weight , which is used to control the relative importance of the feature in the final tree similarity: the larger the feature weight, the more important the feature in the final tree similarity.",
        "4 Experiments 4.1 Experimental Setting To assess the feature-enriched tree kernel, we evaluate our method on the ACE RDC 2004 corpus using the same experimental settings as (Qian et al., 2008).",
        "That is, we parse all sentences using the Charniak's parser (Charniak, 2001), relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence.",
        "In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joa- chims, 1998) with the proposed tree kernel function (Moschitti, 2004).",
        "We apply the one vs. others strategy for multiple classification using SVM.",
        "For SVM training, the parameter C is set to 2.4 for all experiments, and the tree kernel parameter ?",
        "is tuned to 0.2 for FTK and 0.4 (the optimal parameter setting used in Qian et al.(2008)) for CTK.",
        "4.2 Experimental Results 4.2.1 Overall performance We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al., 2007).",
        "We experiment our method with four different feature set-tings, correspondingly: 1) FTK with only instance features ?",
        "FTK(instance); 2) FTK with only phrase features ?",
        "FTK(phrase); 3) FTK with only context information features ?",
        "FTK(context); and 4) FTK with all features ?",
        "FTK.",
        "The overall performance of CTK and FTK is shown in Table 1, the F-measure improvements over CTK are also shown inside the parentheses.",
        "The detailed performance of FTK on the 7 major relation types of ACE 2004 is shown in Table 2.",
        "P(%) R(%) F CTK 77.1 61.3 68.3 (-------) FTK(instance) 78.5 64.6 70.9 (+2.6%) FTK(phrase) 78.3 64.2 70.5 (+2.2%) FTK(context) 80.1 67.5 73.2 (+4.9%) FTK 81.2 67.4 73.7 (+5.4%) Table 1.",
        "Overall Performance Relation Type P(%) R(%) F Impr EMP-ORG 84.7 82.4 83.5 5.8% PER-SOC 79.9 70.7 75.0 1.0% PHYS 73.3 64.4 68.6 7.0% ART 83.6 57.5 68.2 1.7% GPE-AFF 74.7 56.6 64.4 4.3% DISC 81.6 48.0 60.5 6.6% OTHER-AFF 74.2 36.8 49.2 1.0% Table 2.",
        "FTK on the 7 major relation types and their F-measure improvement over CTK From Table 1 and 2, we can see that: 1) By refining the syntactic tree with discriminant features and incorporating these features into the final tree similarity, FTK can significantly improve the relation extraction performance: compared with the convolution tree kernel baseline CTK, our method can achieve a 5.4% F-measure improvement.",
        "64 2) All types of features can improve the performance of relation extraction: FTK can correspondingly get 2.6%, 2.2% and 4.9% F-measure improvements using instance features, phrase features and context information features.",
        "3) Within the three types of features, context information feature can achieve the highest F-measure improvement.",
        "We believe this may be-cause: ?",
        "The context information is useful in providing clues for identifying the role and the importance of a sub-tree; and ?",
        "The context-free assumption of CTK is too strong, some critical information will lost in the CTK computation.",
        "4) The performance improvement of FTK varies significantly on different relation types: in Table 2, most performance improvement gains from the EMP-ORG, PHYS, GPE-AFF and DISC relation types.",
        "We believe this may because the discriminant features will better complement the syntactic tree for capturing EMP-ORG, PHYS, GPE-AFF and DISC relation.",
        "On contrast the features may be redundant to the syntactic information for other relation types.",
        "System P(%) R(%) F Qian et al., (2008): composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Ours: FTK with CSPT 81.2 67.4 73.7 Zhou et al., (2007): context sensitive CTK with CSPT 81.1 66.7 73.2 Ours: FTK with SPT 81.1 66.2 72.9 Jiang & Zhai (2007): MaxEnt classifier with features 74.6 71.3 72.9 Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao & Grishman, (2005): Composite kernel 69.2 70.5 70.4 Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7 Table 3.",
        "Comparison of different systems on the ACE RDC 2004 corpus 4.2.2 Comparison with other systems Finally, Table 3 compares the performance of our method with several other systems.",
        "From Table 3, we can see that FTK can achieve competitive per-formance: ?",
        "It achieves a 0.8% F-measure improvement over the feature-based system of Jiang & Zhai (2007); ?",
        "It achieves a 0.5% F-measure improvement over a state-of-the-art tree kernel: context sensitive CTK with CSPT of Zhou et al., (2007); ?",
        "The F-measure of our system is slightly lower than the current best performance on ACE 2004 (Qian et al., 2008) ?",
        "73.7 vs. 77.1, we believe this is because the system of (Qian et al., 2008) adopts two extra techniques: composing tree kernel with a state-of-the-art feature-based kernel and using a more proper sub-tree representation.",
        "We believe these two techniques can also be used to further improve the performance of our system.",
        "5 Related Work This section briefly reviews the related work.",
        "A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang & Zhai, 2007; Chan & Roth, 2010; Chan & Roth, 2011), and feature engineering is obviously the key for performance improvement.",
        "As an al-ternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhang et al., 2006).",
        "Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006).",
        "The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information.",
        "To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel.",
        "Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes.",
        "Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010).",
        "6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representa-tion; and 2) better measure the similarity between two trees.",
        "For future work, we want to develop a feature weighting algorithm which can accurately measure the relevance of a feature to a relation instance for better RE performance.",
        "Acknowledgments This work is supported by the National Natural Science Foundation of China under Grants no.",
        "61100152 and 61272324, and the Open Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Research under Grants no.",
        "ICDD201204.",
        "65 References"
      ]
    }
  ]
}
