{
  "info": {
    "authors": [
      "Min Yang",
      "Dingju Zhu",
      "Kam-Pui Chow"
    ],
    "book": "ACL",
    "id": "acl-P14-2069",
    "title": "A Topic Model for Building Fine-grained Domain-specific Emotion Lexicon",
    "url": "https://aclweb.org/anthology/P14-2069",
    "year": 2014
  },
  "references": [
    "acl-C10-2005",
    "acl-C10-3014",
    "acl-H05-1044",
    "acl-W07-2013"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 421?426, Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics A Topic Model for Building Fine-grained Domain-specific Emotion Lexicon Min Yang?",
        "Baolin Peng?",
        "Zheng Chen?",
        "Dingju Zhu*?,?",
        "Kam-Pui Chow?",
        "Abstract",
        "Emotion lexicons play a crucial role in sentiment analysis and opinion mining.",
        "In this paper, we propose a novel Emotion-aware LDA (EaLDA) model to build a domain-specific lexicon for predefined emotions that include anger, disgust, fear, joy, sad-ness, surprise.",
        "The model uses a minimal set of domain-independent seed words as prior knowledge to discover a domain-specific lexicon, learning a fine-grained emotion lexicon much richer and adaptive to a specific domain.",
        "By comprehensive experiments, we show that our model can generate a high-quality fine-grained domain-specific emotion lexicon."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Due to the popularity of opinion-rich resources (e.g., online review sites, forums, blogs and the microblogging websites), automatic extraction of opinions, emotions and sentiments in text is of great significance to obtain useful information for social and security studies.",
        "Various opinion mining applications have been proposed by different researchers, such as question answering, opinion mining, sentiment summarization, etc.",
        "As the fine-grained annotated data are expensive to get, the unsupervised approaches are preferred andmore used in reality.",
        "Usually, a high quality emotion lexicon play a significant role when apply the unsupervised approaches for fine-grained emotion classification.",
        "*Dingju Zhu is the corresponding author Thus far, most lexicon construction approaches focus on constructing general-purpose emotion lexicons (Stone et al., 1966; Hu and Liu, 2004; Wilson et al., 2005; Dong and Dong, 2006).",
        "How-ever, since a specific word can carry various emotions in different domains, a general-purpose emotion lexicon is less accurate and less informative than a domain-specific lexicon (Baccianella et al., 2010).",
        "In addition, in previous work, most of the lexicons label the words on coarse-grained dimensions (positive, negative and neutrality).",
        "Such lexicons cannot accurately reflect the complexity of human emotions and sentiments.",
        "Lastly, previous emotion lexicons are mostly annotated based on many manually constructed resources (e.g., emotion lexicon, parsers, etc.).",
        "This limits the applicability of these methods to a broader range of tasks and languages.",
        "To meet the challenges mentioned above, we propose a novel EaLDA model to construct a domain-specific emotion lexicon consisting of six primary emotions (i.e., anger, disgust, fear, joy, sadness and surprise).",
        "The proposed EaLDA model extends the standard Latent Dirichlet Allocation (LDA) (Blei et al., 2003) model by employing a small set of seeds to guide the model generating topics.",
        "Hence, the topics consequently group semantically related words into a same emotion category.",
        "The lexicon is thus able to best meet the user's specific needs.",
        "Our approach is a weakly supervised approach since only some seeds emotion sentiment words are needed to lanch the process of lexicon construction.",
        "In practical applications, asking users to provide some seeds is easy as they usually have a good knowledge what are important in their domains.",
        "421 Extensive experiments are carried out to evaluate our model both qualitatively and quantitatively using benchmark dataset.",
        "The results demonstrate that our EaLDA model improves the quality and the coverage of state-of-the-art fine-grained lexicon.",
        "2 Related Work Emotion lexicon plays an important role in opinion mining and sentiment analysis.",
        "In order to build such a lexicon, many researchers have investigated various kinds of approaches.",
        "However, these methods could roughly be classified into two categories in terms of the used information.",
        "The first kind of approaches is based on thesaurus that utilizes synonyms or glosses to determine the sentiment orientation of a word.",
        "The availability of the WordNet (Miller, 1995) database is an important starting point for many thesaurus-based approaches (Kamps et al., 2004; Hu and Liu, 2004; Esuli and Sebastiani, 2006).",
        "The second kind of approaches is based on an idea that emotion words co-occurring with each others are likely to convey the same polarity.",
        "There are numerous studies in this field (Turney and Littman, 2003; Wiebe and Riloff, 2005; Esuli and Sebastiani, 2006; Barbosa and Feng, 2010).",
        "Most of the previous studies for emotion lexicon construction are limited to positive and negative emotions.",
        "Recently, to enhance the increasingly emotional data, a few researches have been done to identity the fine-grained emotion of words (Strapparava andMihalcea, 2007; Gill et al., 2008; Rao et al., 2012).",
        "For example, Gill et al. (2008) utilize computational linguistic tools to identity the emotions of the words (such as, joy, sadness, ac-ceptance, disgust, fear, anger, surprise and antici-pation).",
        "While, this approach is mainly for public use in general domains.",
        "Rao et al. (2012) propose an method of automatically building the word-emotion mapping dictionary for social emotion detection.",
        "However, the emtion lexicon is not outputed explicitly in this paper, and the approach is fully unsupervised which may be difficult to be adjusted to fit the personalized data set.",
        "Our approach relates most closely to the method proposed by Xie and Li (2012) for the construction of lexicon annotated for polarity based on LDA model.",
        "Our approach differs from (Xie and Li, 2012) in two important ways: first, we do not address the task of polarity lexicon construction, but instead we focus on building fine-grained emotion lexicon.",
        "Second, we don't assume that every word in documents is subjective, which is impractical in real world corpus.",
        "3 Algorithm In this section, we rigorously define the emotion-aware LDA model and its learning algorithm.",
        "We descrige with the model description, a Gibbs sampling algorithm to infer the model parameters, and finally how to generate a emotion lexicon based on the model output.",
        "3.1 Model Description Like the standard LDA model, EaLDA is a generative model.",
        "To prevent conceptual confusion, we use a superscript ?(e)?",
        "to indicate variables related to emotion topics, and use a superscript ?(n)?",
        "to indicate variables of non-emotion topics.",
        "We assume that each document has two classes of topics: M emotion topics (corresponding to M different emotions) andK non-emotion topics (correspond- ing to topics that are not associated with any emo-tion).",
        "Each topic is represented by a multinomial distribution over words.",
        "In addition, we assume that the corpus vocabulary consists of V distinct words indexed by {1, .",
        ".",
        ".",
        ", V }.",
        "For emotion topics, the EaLDA model draws the word distribution from a biased Dirichlet prior Dir(?",
        "(e) k ).",
        "The vector ?",
        "(e) k ?",
        "R V is constructed with ?",
        "(e) k := ?",
        "(e) 0 (1 V ?",
        "?",
        "k ) + ?",
        "(e) 1 ?",
        "k , for k ?",
        "{1, .",
        ".",
        ".",
        ",M}.",
        "?",
        "k,w = 1 if and only if word w is a seed word for emotion k, otherwise?",
        "k,w = 0.",
        "The scalars ?",
        "(e) 0 and ?",
        "(e) 1 are hyperparameters of the model.",
        "Intuitively, when ?",
        "(e) 1 > ?",
        "(e) 0 , the biased prior ensures that the seed words are more probably drawn from the associated emotion topic.",
        "The generative process of word distributions for non-emotion topics follows the standard LDA definition with a scalar hyperparameter ?(n).",
        "For each word in the document, we decide whether its topic is an emotion topic or a non-emotion topic by flipping a coin with head-tail probability (p(e), p(n)), where (p(e), p(n)) ?",
        "Dir(?).",
        "The emotion (or non-emotion) topic is sampled according to a multinomial distribution Mult(?",
        "(e)) (or Mult(?(n))).",
        "Here, both ?",
        "(e) and ?",
        "(n) are document-level latent variables.",
        "They are generated from Dirichlet priors Dir(?",
        "(e)) and Dir(?",
        "(n)) with ?",
        "(s) and ?",
        "(n) being hyperparame-ters.",
        "422 We summarize the generative process of the EaLDA model as below: 1. for each emotion topic k ?",
        "{1, .",
        ".",
        ".",
        ",M}, draw ?",
        "(e) k ?",
        "Dir(?",
        "(e) k ) 2. for each non-emotion topic k ?",
        "{1, .",
        ".",
        ".",
        ",K}, draw ?",
        "(n) k ?",
        "Dir(?",
        "(n)) 3. for each document (a) draw ?",
        "(e) ?",
        "Dir(?",
        "(e)) (b) draw ?",
        "(n) ?",
        "Dir(?",
        "(n)) (c) draw (p(e), p(n)) ?",
        "Dir(?)",
        "(d) for each word in document i. draw topic class indicator s ?",
        "Bernoulli(p s ) ii.",
        "if s = ?emotion topic?",
        "A. draw z(e) ?",
        "Mult(?",
        "(e)) B. draw w ?",
        "Mult(?",
        "(e) z (e) ) , emit word w iii.",
        "otherwise A. draw z(n) ?",
        "Mult(?",
        "(n)) B. draw w ?",
        "Mult(?",
        "(n) z (n) ) , emit word w As an alternative representation, the graphical model of the the generative process is shown by Figure 1. .",
        "w w (e) w (n) z (e) z (n) s ?",
        "(e) ?",
        "(n) p ?",
        "(e) ?",
        "(n) ?",
        "?",
        "(e) ?",
        "(n) ?",
        "(e) ?",
        "(n) ?",
        "M K N d D Figure 1: The Emotion-aware LDA model.",
        "3.2 Inference Algorithm Assuming hyperparameters?, ?",
        "(e), ?",
        "(n), and ?",
        "(e), ?",
        "(n), we develop a collapsed Gibbs sampling algorithm to estimate the latent variables in the EaLDA model.",
        "The algorithm iteratively takes a word w from a document and sample the topic that this word belongs to.",
        "Let the whole corpus excluding the current word be denoted by D. Let n(e) i,w (or n(n) j,w ) indicate the number of occurrences of topic i(e) (or topic j (n)) with word w in the whole corpus.",
        "Let m(e) i (or m(n) j ) indicate the number of occurrence of topic i(e) (or topic j(n)) in the current document.",
        "All these counts are defined excluding the current word.",
        "Using the definition of the EaLDA model and the Bayes Rule, we find that the joint density of these random variables are equal to Pr ( p (e) , p (n) , ?",
        "(e) , ?",
        "(e) , ?",
        "(n) , ?",
        "(n) |D ) ?",
        "Pr ( p (e) , p (n) , ?",
        "(e) , ?",
        "(e) , ?",
        "(n) , ?",
        "(n) ) ?",
        "Pr ( D|p (e) , p (n) , ?",
        "(e) , ?",
        "(e) , ?",
        "(n) , ?",
        "(n) ) ?",
        "( p (e) ) ?+( ?",
        "M i=1 m (e) i ) ?",
        "( p (n) ) ?+( ?",
        "K j=1 m (n) j ) ?",
        "M ?",
        "i=1 ( ?",
        "(e) i ) ?",
        "(e) +m (e) i ?1 ?",
        "K ?",
        "j=1 ( ?",
        "(n) j ) ?",
        "(n) +m (n) j ?1 ?",
        "1 ?",
        "i=0 V ?",
        "w=1 ( ?",
        "(e) i,w ) ?",
        "(e) i,w +n (e) i,w ?1 ?",
        "K ?",
        "j=1 V ?",
        "w=1 ( ?",
        "(n) j,w ) ?",
        "(n) +n (n) j,w ?1 (1) According to equation (1), we see that {p (e) , p (n) }, {?",
        "(e) i , ?",
        "(n) j }, {?",
        "(e) i,w } and {?",
        "(n) j,w } are mutually independent sets of random variables.",
        "Each of these random variables satisfies Dirichlet distribution with a specific set of parameters.",
        "By the mutual independence, we decompose the probability of the topic z for the current word as Pr ( z = i (e) |D ) ?",
        "E[p (e) ] ?",
        "E[?",
        "(e) i ] ?",
        "E[?",
        "(e) i,w ] (2) Pr ( z = j (n) |D ) ?",
        "E[p (n) ] ?E[?",
        "(n) i ] ?E[?",
        "(n) j,w ] (3) Then, by examining the property of Dirichlet distribution, we can compute expectations on the right hand side of equation (2) and equation (3) by E[p (e) ] = ?",
        "+ ?",
        "1 i=0 m (e) i 2?",
        "+ ?",
        "M i=1 m (e) i + ?",
        "K j=1 m (n) j (4) E[p (n) ] = ?",
        "+ ?",
        "K j=1 m (n) j 2?",
        "+ ?",
        "M i=1 m (e) i + ?",
        "K j=1 m (n) j (5) 423 E[?",
        "(e) i ] = ?",
        "(e) + m (e) i M?",
        "(e) + ?",
        "M i ?",
        "=1 m (e) i ?",
        "(6) E[?",
        "(n) j ] = ?",
        "(e) + m (n) j K?",
        "(n) + ?",
        "K j ?",
        "=1 m (n) j ?",
        "(7) E[?",
        "(e) i,w ] = ?",
        "(e) i,w + n (e) i,w ?",
        "V w ?",
        "=1 ( ?",
        "(e) i,w ?",
        "+ n (e) i,w ? )",
        "(8) E[?",
        "(n) j,w ] = ?",
        "(n) j,w + n (n) j,w V ?",
        "(n) + ?",
        "V w ?",
        "=1 n (n) j,w ?",
        "(9) Using the above equations, we can sample the topic z for each word iteratively and estimate all latent random variables.",
        "3.3 Constructing Emotion Lexicon Our final step is to construct the domain-specific emotion lexicon from the estimates ?",
        "(e) and ?",
        "(n) that we obtained from the EaLDA model.",
        "For each word w in the vocabulary, we compare the M + 1 values {?",
        "(e) 1,w , .",
        ".",
        ".",
        ", ?",
        "(e) M,w } and 1 K ?",
        "K i=1 ?",
        "(n) i,w .",
        "If ?",
        "(e) i,w is the largest, then the word w is added to the emotion dictionary for the ith emotion.",
        "Otherwise, 1 K ?",
        "K i=1 ?",
        "(n) i,w is the largest among the M + 1 values, which suggests that the word w is more probably drawn from a non-emotion topic.",
        "Thus, the word is considered neutral and not included in the emotion dictionary.",
        "4 Experiments In this section, we report empirical evaluations of our proposed model.",
        "Since there is no metric explicitly measuring the quality of an emotion lexi-con, we demonstrate the performance of our algorithm in two ways: (1) we perform a case study for the lexicon generated by our algorithm, and (2) we compare the results of solving emotion classification task using our lexicon against different meth-ods, and demonstrate the advantage of our lexicon over other lexicons and other emotion classification systems.",
        "4.1 Datasets We conduct experiments to evaluate the effectiveness of our model on SemEval-2007 dataset.",
        "This is an gold-standard English dataset used in the 14th task of the SemEval-2007workshopwhich focuses on classification of emotions in the text.",
        "The attributes include the news headlines, the score of emotions of anger, disgust, fear, joy, sad and surprise normalizing from 0 to 100.",
        "Two data sets are available: a training data set consisting of 250 records, and a test data set with 1000 records.",
        "Following the strategy used in (Strapparava and Mi-halcea, 2007), the task was carried out in an unsupervised setting for experiments.",
        "In experiments, data preprocessing is performed on the data set.",
        "First, the texts are tokenized with a natural language toolkit NLTK1.",
        "Then, we remove non-alphabet characters, numbers, pronoun, punctuation and stop words from the texts.",
        "Finally, Snowball stemmer2 is applied so as to reduce the vocabulary size and settle the issue of data spareness.",
        "4.2 Emotion Lexicon Construction We first settle down the implementation details for the EaLDAmodel, specifying the hyperparameters that we choose for the experiment.",
        "We set topic numberM = 6,K = 4, and hyperparameters?",
        "= 0.75, ?",
        "(e) = ?",
        "(n) = 0.45, ?",
        "(n) = 0.5.",
        "The vector ?",
        "(e) is constructed from the seed dictionary using ?",
        "= (0.25, 0.95).",
        "As mentioned, we use a few domain-independent seed words as prior information for our model.",
        "To be specific, the seed words list contains 8 to 12 emotional words for each of the six emotion categories.3 However, it is important to note that the proposed models are flexible and do not need to have seeds for every topic.",
        "Example words for each emotion generated from the SemEval-2007 dataset are reported in Table 1.",
        "The judgment is to some extent subjective.",
        "What we reported here are based on our judgments what are appropriate and what are not for each emotion topic.",
        "From Table 1, we observe that the generated words are informative and coherent.",
        "For example, the words ?flu?",
        "and ?cancer?",
        "are seemingly neutral by its surface meaning, actually expressing fear emotion for SemEval dataset.",
        "These domain-specific words are mostly not included in any other existing general-purpose emotion lexicons.",
        "The experimental results show that our algorithm can successfully construct a fine-grained domain-specific emotion lexicon for this corpus that is able to understand the connotation of the words that may not be obvious without the context.",
        "1http://www.nltk.org 2http://snowball.tartarus.org/ 3http://minyang.me/acl2014/seed-words.html 424 Anger Disgust Fear Joy Sadness Surprise attack mar terror good kill surprise warn sex troop win die first gunman lebanon flu prize kidnap jump baghdad game dead victory lose marijuana immigration gaze die adopt confuse arrest hit cancer cancer madonna crach sweat kidnap amish kidnap celebrity leave find kill imigration force boost cancer attack alzheim sink iraq ship flu hiv iraqi force fear star kidnap discover Table 1: Part of Emotion example words Algorithm Anger Disgust Fear Joy Sadness Surprise WordNet-Affect 6.06% - - 22.81% 17.31% 9.92% SWAT 7.06% - 18.27% 14.91% 17.44% 11.78% UA 16.03% - 20.06% 4.21% 1.76% 15.00% UPAR7 3.02% - 4.72% 11.87% 17.44% 15.00% EaLDA 16.65% 10.52% 26.21% 25.57% 36.85% 20.17% Table 2: Experiment results for emotion classification in term of F1 score 4.3 Document-level Emotion Classification We compare the performance between a popular emotion lexiconWordNet-Affect (Strapparava and Valitutti, 2004) and our approach for emotion classification task.",
        "We also compare our results with those obtained by three systems participating in the SemEval-2007 emotion annotation task: SWAT, UPAR7 andUA.",
        "The emotion classification results is evaluated for each emotion category separately.",
        "For each emotion category, we evaluates it as a binary classification problem.",
        "In the evaluation of emotion lexicons, the binary classification is performed in a very simple way.",
        "For each emotion category and each text, we compare the number of words within this emotion category, and the average number of words within other emotion cate-gories, to output a binary prediction of 1 or 0.",
        "This simple approach is chosen to evaluate the robustness of our emotion lexicon.",
        "In the experiments, performance is evaluated in terms of F1-score.",
        "We summarize the results in Table 2.",
        "As an easy observation, the emotion lexicon generated by the EaLDA model consistently and significantly outperforms the WordNet-Affect emotion lexicon and other three emotion classification systems.",
        "In particular, we are able to obtain an overall F1-score of 10.52% for disgust classification task which is difficult to work out using previously proposed methods.",
        "The advantage of our model may come from its capability of exploring domain-specific emotions which include not only explicit emotion words, but also implicit ones.",
        "5 Conclusions and Future Work In this paper, we have presented a novel emotion-aware LDA model that is able to quickly build a fine-grained domain-specific emotion lexicon for languages without many manually constructed resources.",
        "The proposed EaLDA model extends the standard LDAmodel by accepting a set of domain-independent emotion words as prior knowledge, and guiding to group semantically related words into the same emotion category.",
        "Thus, it makes the emotion lexicon containing much richer and adaptive domain-specific emotion words.",
        "Experimental results showed that the emotional lexicons generated by our algorithm is of high quality, and can assist emotion classification task.",
        "For future works, we hope to extend the proposed EaLDA model by exploiting discourse structure knowledge, which has been shown significant in identifying the polarity of content-aware words.",
        "425 References"
      ]
    }
  ]
}
