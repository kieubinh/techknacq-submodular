{
  "info": {
    "authors": [
      "Yu-Yang Huang",
      "Rui Yan",
      "Tsung-Ting Kuo",
      "Shou-De Lin"
    ],
    "book": "ACL",
    "id": "acl-P14-2100",
    "title": "Enriching Cold Start Personalized Language Model Using Social Network Information",
    "url": "https://aclweb.org/anthology/P14-2100",
    "year": 2014
  },
  "references": [
    "acl-D11-1124",
    "acl-P12-1054",
    "acl-P96-1041"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 611?617, Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics Enriching Cold Start Personalized Language Model Using Social Network Information Yu-Yang Huang ?",
        ", Rui Yan*, Tsung-Ting Kuo ?",
        ", Shou-De Lin ??",
        "?",
        "Graduate Institute of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan ?",
        "Abstract",
        "We introduce a generalized framework to enrich the personalized language models for cold start users.",
        "The cold start problem is solved with content written by friends on social network services.",
        "Our framework consists of a mixture language model, whose mixture weights are estimated with a factor graph.",
        "The factor graph is used to incorporate prior knowledge and heuristics to identify the most appropriate weights.",
        "The intrinsic and extrinsic experiments show significant improvement on cold start users."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Personalized language models (PLM) on social network services are useful in many aspects (Xue et al., 2009; Wen et al., 2012; Clements, 2007), For instance, if the authorship of a document is in doubt, a PLM may be used as a generative model to identify it.",
        "In this sense, a PLM serves as a proxy of one's writing style.",
        "Furthermore, PLMs can improve the quality of information retrieval and content-based recommendation sys-tems, where documents or topics can be recommended based on the generative probabilities.",
        "However, it is challenging to build a PLM for users who just entered the system, and whose content is thus insufficient to characterize them.",
        "These are called ?cold start?",
        "users.",
        "Producing better recommendations is even more critical for cold start users to make them continue to use the system.",
        "Therefore, this paper focuses on how to overcome the cold start problem and obtain a better PLM for cold start users.",
        "The content written by friends on a social network service, such as Facebook or Twitter, is exploited.",
        "It can be either a reply to an original post or posts by friends.",
        "Here the hypothesis is that friends, who usually share common interests, tend to discuss similar topics and use similar words than non-friends.",
        "In other words, we believe that a cold start user's language model can be enriched and better personalized by incorporating content written by friends.",
        "Intuitively, a linear combination of document-level language models can be used to incorporate content written by friends.",
        "However, it should be noticed that some documents are more relevant than others, and should be weighted higher.",
        "To obtain better weights, some simple heuristics could be exploited.",
        "For example, we can measure the similarity or distance between a user language model and a document language model.",
        "In addition, documents that are shared frequently in a social network are usually considered to be more influential, and could contribute more to the language model.",
        "More complex heuristics can also be derived.",
        "For instance, if two documents are posted by the same person, their weights should be more similar.",
        "The main challenge lies in how such heuristics can be utilized in a systematic manner to infer the weights of each document-level language model.",
        "In this paper, we exploit the information on social network services in two ways.",
        "First, we impose the social dependency assumption via a finite mixture model.",
        "We model the true, albeit unknown, personalized language model as a combination of a biased user language model and a set of relevant document language models.",
        "Due to the noise inevitably contained in social media content, instead of using all available documents, we argue that by properly specifying the set of relevant documents, a better personalized language model can be learnt.",
        "In other words, each user language model is enriched by a personalized collection of background documents.",
        "Second, we propose a factor graph model (FGM) to incorporate prior knowledge (e.g. the heuristics described above) into our model.",
        "Each 611 mixture weight is represented by a random variable in the factor graph, and an efficient algorithm is proposed to optimize the model and infer the marginal distribution of these variables.",
        "Useful information about these variables is encoded by a set of potential functions.",
        "The main contributions of this work are summarized below: ?",
        "To solve the cold start problem encountered when estimating PLMs, a generalized framework based on FGM is proposed.",
        "We incorporate social network information into user language models through the use of FGM.",
        "An iterative optimization procedure utilizing perplexity is presented to learn the parameters.",
        "To our knowledge, this is the first proposal to use FGM to enrich language models.",
        "?",
        "Perplexity is selected as an intrinsic evalua-tion, and experiment on authorship attribution is used as an extrinsic evaluation.",
        "The results show that our model yields significant improvements for cold start users.",
        "2 Methodology 2.1 Social-Driven Personalized Language Model The language model of a collection of documents can be estimated by normalizing the counts of words in the entire collection (Zhai, 2008).",
        "To build a user language model, one naive way is to first normalize word frequency ?",
        "(?, ?)",
        "within each document, and then average over all the documents in a user's document collection.",
        "The resulting unigram user language model is: ??(?)",
        "= 1 |?",
        "?| ?",
        "?",
        "(?, ?)",
        "|?|????",
        "= 1 |?",
        "?| ?",
        "??(?)",
        "????",
        "(1) where ??(?)",
        "is the language model of a particular document, and ??",
        "is the user's document collection.",
        "This formulation is basically an equal-weighted finite mixture model.",
        "A simple yet effective way to smooth a language model is to linearly interpolate with a background language model (Chen and Good-man, 1996; Zhai and Lafferty, 2001).",
        "In the linear interpolation method, all background documents are treated equally.",
        "The entire document collection is added to the user language model ??(?)",
        "with the same interpolation coefficient.",
        "Our main idea is to specify a set of relevant documents for the target user using information embedded in a social network, and enrich the smoothing procedure with these documents.",
        "Let ????",
        "denote the content from relevant persons (e.g. social neighbors) of u1, our idea can be concisely expressed as: ?",
        "?1 ?",
        "(?)",
        "= ??1??1(?)",
        "+ ?",
        "??????(?)",
        "???????",
        "(2) where ???",
        "is the mixture weight of the language model of document di, and ?",
        "?1 + ????",
        "= 1 .",
        "Documents posted by irrelevant users are not included as we believe the user language model can be better personalized by exploiting the social relationship in a more structured way.",
        "In our experiment, we choose the first degree neighbor documents as ????.",
        "Also note that we have made no assumption about how the ?base?",
        "user language model ??1(?)",
        "is built.",
        "In practice, it need not be models following maximum likelihood estimation, but any language model can be integrated into our framework to achieve a better refined model.",
        "Furthermore, any smoothing method can be applied to the language model without degrading the effectiveness.",
        "2.2 Factor Graph Model (FGM) Now we discuss how the mixture weights can be estimated.",
        "We introduce a factor graph model (FGM) to make use of the diverse information on a social network.",
        "Factor graph (Kschischang et al., 2006) is a bipartite graph consisting of a set of random variables and a set of factors which signifies the relationships among the variables.",
        "It is best suited in situations where the data is clearly of a relational nature (Wang et al., 2012).",
        "The joint distribution of the variables is factored according to the graph structure.",
        "Using FGM, one can incorporate the knowledge into the potential function for optimization and perform joint inference over documents.",
        "As shown in Figure 1, the variables included in the model are described as follows: Candidate variables ??",
        "= ?",
        "?, ???",
        ".",
        "The random variables in the top layer stand for the degrees of belief that a document di should be included in the PLM of the target user ?.",
        "Figure 1: A two-layered factor graph (FGM) proposed to estimate the mixture weights.",
        "612 Attribute variables xi.",
        "Local information is stored as the random variables in the bottom layer.",
        "For example, x1 might represent the number of common friends between the author of a document di and our target user.",
        "The potential functions in the FGM are: Attribute-to-candidate function.",
        "This potential function captures the local dependencies of a candidate variable to the relevant attributes.",
        "Let the candidate variable yi correspond to a document di, the attribute-to-candidate function of yi is defined in a log-linear form: ?(??",
        ", ?)",
        "= 1 ??",
        "???{???(?",
        "?, ?)}",
        "(3) where A is the set of attributes of either the document di or target user u; f is a vector of feature functions which locally model the value of yi with attributes in A; ??",
        "is the local partition function and ?",
        "is the weight vector to be learnt.",
        "In our experiment, we define the vector of functions as ?",
        "= ????",
        "?, ????",
        ", ???",
        "?, ???",
        "?, ????",
        "?",
        "as: ?",
        "Similarity function ????",
        ".",
        "The similarity between language models of the target user and a document should play an important role.",
        "We use cosine similarity between two unigram models in our experiments.",
        "?",
        "Document quality function ????.",
        "The out-of-vocabulary (OOV) ratio is used to measure the quality of a document.",
        "It is defined as ????",
        "= 1 ?",
        "|{?:?",
        "?",
        "??",
        "?",
        "?",
        "?",
        "?",
        "}| |?",
        "?| (4) where ?",
        "is the vocabulary set of the entire corpus, with stop words excluded.",
        "?",
        "Document popularity function ????",
        ".",
        "This function is defined as the number of times di is shared to model the popularity of documents.",
        "?",
        "Common friend function ????.",
        "It is defined as the number of common friends between the target user u1 and the author of di.",
        "?",
        "Author friendship function ???",
        ".",
        "Assuming that documents posted by a user with more friends are more influential, this function is defined as the number of friends of di's author.",
        "Candidate-to-candidate function.",
        "This potential function defines the correlation of a candidate variable yi with another candidate variable yj in the factor graph.",
        "The function is defined as ?(??",
        ", ??)",
        "= 1 ???,?",
        "???{???(?",
        "?, ??)}",
        "(5) where g is a vector of feature functions indicating whether two variables are correlated.",
        "If we further denote the set of all related variables as ?(??)",
        ", then for any candidate variable yi, we have the following brief expression: ?(??",
        ", ?(??))",
        "= ?",
        "?(??",
        ", ??)",
        "????(??)",
        "(6) For two candidate variables, let the corresponding document be di and dj, respectively, we define the vector ?",
        "= ?????",
        ", ?????",
        "?",
        "as: ?",
        "User relationship function ????.",
        "We assume that two candidate variables have higher dependency if they represent documents of the same author or the two authors are friends.",
        "The dependency should be even greater if two documents are similar.",
        "Let ?(?)",
        "denote the author of a document d and ?[?]",
        "denote the closed neighborhood of a user u, we define ????",
        "= ?{?(??)",
        "?",
        "?[?(??)]}",
        "?",
        "???(??",
        ", ??)",
        "(7) ?",
        "Co-category function ????.",
        "For any two candidate variables, it is intuitive that the two variables would have a higher correlation if di and dj are of the same category.",
        "Let ?(?)",
        "denote the category of document d, we define ????",
        "= ?{?(??)",
        "= ?(??)}",
        "?",
        "???(??",
        ", ??)",
        "(8) 2.3 Model Inference and Optimization Let Y and X be the set of all candidate variables and attribute variables, respectively.",
        "The joint distribution encoded by the FGM is given by multiplying all potential functions.",
        "?",
        "(?, ?)",
        "=??(?",
        "?, ?)?(??",
        ", ?(??))",
        "?",
        "(9) The desired marginal distribution can be obtained by marginalizing all other variables.",
        "Since under most circumstances, however, the factor graph is densely connected, the exact inference is intractable and approximate inference is required.",
        "After obtaining the marginal probabilities, the mixture weights ???",
        "in Eq. 2 are estimated by normalizing the corresponding marginal probabilities ?(??)",
        "over all candidate variables, which can be written as ???",
        "= (1 ?",
        "?",
        "?1) ?(??)",
        "?",
        "?(??)?:???????",
        "(10) where the constraint ?",
        "?1 + ????",
        "= 1 leads to a valid probability distribution for our mixture model.",
        "A factor graph is normally optimized by gradient-based methods.",
        "Unfortunately, since the ground truth values of the mixture weights are not available, we are prohibited from using supervised approaches.",
        "Here we propose a two-step iterative procedure to optimize our model.",
        "At 613 first, all the model parameters (i.e.",
        "?, ?, ??)",
        "are randomly initialized.",
        "Then, we infer the marginal probabilities of candidate variables.",
        "Given these marginal probabilities, we can evaluate the perplexity of the user language model on a held-out dataset, and search for better parameters.",
        "This procedure is repeated until convergence.",
        "Also, notice that by using FGM, we reduce the number of parameters from 1 + |???",
        "?| to 1 + |?| + |?|, lowering the risk of overfitting.",
        "3 Experiments 3.1 Dataset and Experiment Setup We perform experiments on the Twitter dataset collected by Galuba et al. (2010).",
        "Twitter data have been used to verify models with different purposes (Lin et al., 2011; Tan et al., 2011).",
        "To emphasize on the cold start scenario, we randomly selected 15 users with about 35 tweets and 70 friends as candidates for an authorship attribution task.",
        "Our experiment corpus consists of 4322 tweets.",
        "All words with less than 5 occurrences are removed.",
        "Stop words and URLs are also removed and all tweets are stemmed.",
        "We identify the 100 most frequent terms as categories.",
        "The size of the vocabulary set is 1377.",
        "We randomly partitioned the tweets of each user into training, validation and testing sets.",
        "The reported result is the average of 10 random splits.",
        "In all experiments, we vary the size of training data from 1% to 15%, and hold out the same number of tweets from each user as validation and testing data.",
        "The statistics of our dataset, given 15% training data, are shown in Table 1.",
        "Loopy belief propagation (LBP) is used to obtain the marginal probabilities of the variables (Murphy et al., 1999).",
        "Parameters are searched with the pattern search algorithm (Audet and Dennis, 2002).",
        "To not lose generality, we use the default configuration in all experiments.",
        "# of Max.",
        "Min.",
        "Avg.",
        "Tweets 70 19 35.4 Friends 139 24 68.9 Variables 467 97 252.7 Edges 9216 231 3427.1 Table 1: Dataset statistics 3.2 Baseline Methods We compare our framework with two baseline methods.",
        "The first (?Cosine?)",
        "is a straightforward implementation that sets all mixture weights ???",
        "to the cosine similarity between the probability mass vectors of the document and user unigram language models.",
        "The second (?PS?)",
        "uses the pattern search algorithm to perform constrained optimization over the mixture weights.",
        "As mentioned in section 2.3, the main difference between this method and ours (?FGM?)",
        "is that we reduce the search space of the parameters by FGM.",
        "Furthermore, social network information is exploited in our frame-work, while the PS method performs a direct search over mixture weights, discarding valuable knowledge.",
        "Different from other smoothing methods that are usually mutually exclusive, any other smoothing methods can be easily merged into our framework.",
        "In Eq. 2, the base language model ??1(?)",
        "can be already smoothed by any techniques before being plugged into our framework.",
        "Our framework then enriches the user language model with social network information.",
        "We select four popular smoothing methods to demonstrate such effect, namely additive smoothing, absolute smoothing (Ney et al., 1995), Jelinek-Mercer smoothing (Jelinek and Mercer, 1980) and Dirichlet smoothing (MacKay and Peto, 1994).",
        "The results of using only the base model (i.e. set ???",
        "= 0 in Eq. 2) are denoted as ?Base?",
        "in the following tables.",
        "Train % Additive Absolute Base Cosine PS FGM Base Cosine PS FGM 1% 900.4 712.6 725.5 537.5** 895.3 703.1 722.1 544.5** 5% 814.5 623.4 690.5 506.8** 782.4 607.9 678.4 510.2** 10% 757.7 566.6 684.8 481.2** 708.4 552.7 661.0 485.8** 15% 693.8 521.0 635.2 474.8** 647.4 504.3 622.3 474.1** Train % Jelinek-Mercer Dirichlet Base Cosine PS FGM Base Cosine PS FGM 1% 637.8 571.4 643.1 541.0** 638.5 571.3 643.1 541.0** 5% 593.9 526.1 602.9 505.4** 595.0 526.6 616.5 507.2** 10% 559.2 494.1 573.8 483.6** 560.4 494.9 579.6 486.0** 15% 535.3 473.4 560.2 473.0 535.7 473.6 563.2 474.4 Table 2: Testing set perplexity.",
        "** indicates that the best score among all methods is significantly better than the next highest score, by t-test at a significance level of 0.05.",
        "614 3.3 Perplexity As an intrinsic evaluation, we first compute the perplexity of unseen sentences under each user language model.",
        "The result is shown in Table 2.",
        "Our method significantly outperforms all of the methods in almost all settings.",
        "We observe that the ?PS?",
        "method takes a long time to converge and is prone to overfitting, likely because it has to search about a few hundred parameters on average.",
        "As expected, the advantage of our model is more apparent when the data is sparse.",
        "3.4 Authorship Attribution (AA) The authorship attribution (AA) task is chosen as the extrinsic evaluation metric.",
        "Here the goal is not about comparing with the state-of-the-art approaches in AA, but showing that LM-based approaches can benefit from our framework.",
        "To apply PLM on this task, a naive Bayes classifier is implemented (Peng et al., 2004).",
        "The most probable author of a document d is the one whose PLM yields the highest probability, and is determined by ??",
        "= argmax?{?",
        "??(?)???",
        "}.",
        "The result is shown in Table 3.",
        "Our model improves personalization and outperforms the baselines under cold start settings.",
        "When data is sparse, the ?PS?",
        "method tends to overfit the noise, while the ?Cosine?",
        "method contains too few information and is severely biased.",
        "Our method strikes a balance between model complexity and the amount of information included, and hence performs better than the others.",
        "4 Related Work Personalization has long been studied in various textual related tasks.",
        "Personalized search is established by modeling user behavior when using search engines (Shen et al., 2005; Xue et al., 2009).",
        "Query language model could be also expanded based on personalized user modeling (Chirita et al., 2007).",
        "Personalization has also been modeled in many NLP tasks such as sum-marization (Yan et al., 2011) and recommendation (Yan et al., 2012).",
        "Different from our pur-pose, these models do not aim at exploiting social media content to enrich a language model.",
        "Wen et al. (2012) combines user-level language models from a social network, but instead of focusing on the cold start problem, they try to improve the speech recognition performance using a mass amount of texts on social network.",
        "On the other hand, our work explicitly models the more sophisticated document-level relationships using a probabilistic graphical model.",
        "5 Conclusion The advantage of our model is threefold.",
        "First, prior knowledge and heuristics about the social network can be adapted in a structured way through the use of FGM.",
        "Second, by exploiting a well-studied graphical model, mature inference techniques, such as LBP, can be applied in the optimization procedure, making it much more effective and efficient.",
        "Finally, different from most smoothing methods that are mutually ex-clusive, any other smoothing method can be incorporated into our framework to be further enhanced.",
        "Using only 1% of the training corpus, our model can improve the perplexity of base models by as much as 40% and the accuracy of authorship attribution by at most 15%.",
        "6 Acknowledgement This work was sponsored by AOARD grant number No.",
        "FA2386-13-1-4045 and National Science Council, National Taiwan University and Intel Corporation under Grants NSC102-2911-I-002-001 and NTU103R7501 and grant 102-2923-E-002-007-MY2, 102-2221-E-002-170, 101-2628-E-002-028-MY2.",
        "Train % Additive Absolute Base Cosine PS FGM Base Cosine PS FGM 1% 54.67 58.27 61.07 63.74 49.47 57.60 58.27 64.27** 5% 61.47 63.20 62.67 68.40** 59.60 62.40 61.33 66.53** 10% 61.47 65.73 66.27 69.20** 61.47 65.20 64.67 71.87** 15% 64.27 67.07 62.13 70.40** 64.67 68.27 63.33 71.60** Train % Jelinek-Mercer Dirichlet Base Cosine PS FGM Base Cosine PS FGM 1% 54.00 60.93 62.00 64.80** 52.80 60.40 61.87 64.67** 5% 62.67 65.47 64.00 68.00 60.80 65.33 62.40 66.93 10% 63.87 68.00 67.87 68.53 62.53 67.87 66.40 68.53 15% 65.87 70.40 64.14 69.87 65.47 70.27 64.53 68.40 Table 3: Accuracy (%) of authorship attribution.",
        "** indicates that the best score among all methods is significantly better than the next highest score, by t-test at a significance level of 0.05.",
        "615 Reference"
      ]
    }
  ]
}
