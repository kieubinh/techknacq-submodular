{
  "info": {
    "authors": [
      "Kai Zhao",
      "Liang Huang",
      "Haitao Mi",
      "Abe Ittycheriah"
    ],
    "book": "ACL",
    "id": "acl-P14-2127",
    "title": "Hierarchical MT Training using Max-Violation Perceptron",
    "url": "https://aclweb.org/anthology/P14-2127",
    "year": 2014
  },
  "references": [
    "acl-D07-1080",
    "acl-D08-1024",
    "acl-D11-1125",
    "acl-D13-1093",
    "acl-D13-1112",
    "acl-N12-1015",
    "acl-N13-1025",
    "acl-N13-1038",
    "acl-P03-1021",
    "acl-P05-1012",
    "acl-P05-1022",
    "acl-P05-1033",
    "acl-P06-1096",
    "acl-P07-1019",
    "acl-P07-2045",
    "acl-P08-1024",
    "acl-P08-1067",
    "acl-P09-1019",
    "acl-P10-4002",
    "acl-P13-1008",
    "acl-P13-1031",
    "acl-W02-1001"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 785?790, Baltimore, Maryland, USA, June 23-25 2014. c ?2014 Association for Computational Linguistics Hierarchical MT Training using Max-Violation Perceptron Kai Zhao ?",
        "Liang Huang ?",
        "?",
        "Abe Ittycheriah ?",
        "?",
        "Abstract",
        "Large-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus; for example the recent effort in phrase-based MT (Yu et al., 2013) significantly outperforms mainstream methods that only train on small tuning sets.",
        "However, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit.",
        "To address this problem, we extend Yu et al. (2013) to syntax-based MT by generalizing their latent variable ?violation-fixing?",
        "perceptron from graphs to hypergraphs.",
        "Experiments confirm that our method leads to up to +1.2 BLEU improvement over mainstream methods such as MERT and PRO."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms.",
        "However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets.",
        "What makes large-scale MT training so hard then?",
        "After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al. (2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning.",
        "To alleviate this prob-lem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming Collins (02) inexact ??",
        "search Huang et al. (12) latent ??",
        "variable Yu et al. (13) ?",
        "hypergraph ?",
        "Zhang et al. (13) ??",
        "variable this work Figure 1: Relationship with previous work.",
        "small-scale MERT/PRO by a large margin for the first time).",
        "However, the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion (about 1/3 in their Ch-En experiments) of the bitext in training.",
        "To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (HIERO) (Chiang, 2005), in order to exploit sentence pairs beyond the expressive capacity of phrase-based MT.",
        "The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al. (2013) to handle tree-structured derivations and translation hypergraphs.",
        "Luckily, Zhang et al. (2013) have recently generalized the underlying violation-fixing perceptron of Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding.",
        "We just need to further extend it to handle latent variables.",
        "We make the following contributions: 1.",
        "We generalize the latent variable violation-fixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and bottom-up parsing as special cases (see Fig. 1).",
        "2.",
        "We show that syntax-based MT, with its better handling of long-distance reordering, can exploit a larger portion of the training set, which facilitates sparse lexicalized features.",
        "3.",
        "Experiments show that our training algorithm outperforms mainstream tuning methods (which optimize on small devsets) by +1.2 BLEU over MERT and PRO on FBIS.",
        "785 id rule r 0 S?",
        "?X 1 ,X 1 ?",
        "r 1 S?",
        "?S 1 X 2 ,S 1 X 2 ?",
        "r 2 X?",
        "?B`ush??,Bush?",
        "r 3 X?",
        "?Sh?al?ong,Sharon?",
        "r 4 X?",
        "?hu`?t?an, talks?",
        "r 5 X?",
        "?y?u X 1 j?ux?",
        "?ng X 2 , held X 2 with X 1 ?",
        "r 6 X?",
        "?y?u Sh?al?ong, with Sharon?",
        "r 7 X?",
        "?X 1 j?ux?",
        "?ng X 2 , X 1 held X 2 ?",
        "S [0:5] X [1:5] X [4:5] hu`?t?an 5 j?ux?",
        "?ng 4 X [2:3] Sh?al?ong 3 | y?u 2 S [0:1] X [0:1] 0 B`ush??",
        "1 S X X Sharon 5 with 4 X talks 3 held 2 S X 0 Bush 1 S [0:5] X [1:5] X [4:5] hu`?t?an 5 j?ux?",
        "?ng 4 X [1:3] Sh?al?ong 3 y?u 2 S [0:1] X [0:1] 0 B`ush??",
        "1 S X X talks 5 held 4 X Sharon 3 with 2 S X 0 Bush 1 (a) HIERO rules (b) gold derivation (c) Viterbi derivation Figure 2: An example of HIERO translation.",
        "X[0:1] X[2:3] X[4:5] X[1:5] X[1:3] S[0:1] S[0:5] Figure 3: A ?LM hypergraph with two deriva-tions: the gold derivation (Fig. 2b) in solid lines, and the Viterbi derivation (Fig. 2c) in dashed lines.",
        "2 Review: Syntax-based MT Decoding For clarity reasons we will describe HIERO decoding as a two-pass process, first without a language model, and then integrating the LM.",
        "This section mostly follows Huang and Chiang (2007).",
        "In the first, ?LM phase, the decoder parses the source sentence using the source projection of the synchronous grammar (see Fig. 2 (a) for an ex-ample), producing a?LM hypergraph where each node has a signature N [i:j] , where N is the nonter-minal type (either X or S in HIERO) and [i : j] is the span, and each hyperedge e is an application of the translation rule r(e) (see Figure 3).",
        "To incorporate the language model, each node also needs to remember its target side boundary words.",
        "Thus a ?LM node N [i:j] is split into multiple +LM nodes of signature N a?b [i:j] , where a and b are the boundary words.",
        "For example, with a bi-gram LM, X held?Sharon [1:5] is a node whose translation starts with ?held?",
        "and ends with ?Sharon?.",
        "More formally, the whole decoding process can be cast as a deductive system.",
        "Take the partial translation of ?held talks with Sharon?",
        "in Figure 2 (b) for example, the deduction is X Sharon?Sharon [2:3] : s 1 X talks?talks [4:5] : s 2 X held?Sharon [1:5] : s 1 + s 2 + s(r 5 ) + ?",
        "r 5 , where s(r 5 ) is the score of rule r 5 , and the LM combo score ?",
        "is log P lm (talks | held)P lm (with | talks)P lm (Sharon | with).",
        "3 Violation-Fixing Perceptron for HIERO As mentioned in Section 1, the key to the success of Yu et al. (2013) is the adoption of violation-fixing perceptron of Huang et al. (2012) which is tailored for vastly inexact search.",
        "The general idea is to update somewhere in the middle of the search (where search error happens) rather than at the very end (standard update is often invalid).",
        "To adapt it to MT where many derivations can output the same translation (i.e., spurious ambiguity), Yu et al. (2013) extends it to handle latent variables which correspond to phrase-based derivations.",
        "On the other hand, Zhang et al. (2013) has generalized Huang et al. (2012) from graphs to hypergraphs for bottom-up parsing, which resembles HIERO decoding.",
        "So we just need to combine the two generalizing directions (latent variable and hyper-graph, see Fig. 1).",
        "3.1 Latent Variable Hypergraph Search The key difference between bottom-up parsing and MT decoding is that in parsing the gold tree for each input sentence is unique, while in MT many derivations can generate the same reference translation.",
        "In other words, the gold derivation to update towards is a latent variable.",
        "786 Here we formally define the latent variable ?max-violation?",
        "perceptron over a hypergraph for MT training.",
        "For a given sentence pair ?x, y?, we denote H(x) as the decoding hypergraph of HIERO without any pruning.",
        "We say D ?",
        "H(x) if D is a full derivation of decoding x, and D can be derived from the hypergraph.",
        "Let good(x, y) be the set of y-good derivations for ?x, y?",
        ": good(x, y) ?",
        "= {D ?",
        "H(x) | e(D) = y}, where e(D) is the translation from derivation D. We then define the set of y-good partial derivations that cover x [i:j] with root N [i:j] as good N [i:j] (x, y) ?",
        "= {d ?",
        "D | D ?",
        "good(x, y), root(d) = N [i:j] } We further denote the real decoding hypergraph with beam-pruning and cube-pruning as H ?",
        "(x).",
        "The set of y-bad derivations is defined as bad N [i:j] (x, y) ?",
        "= {d ?",
        "D | D ?",
        "H ?",
        "(x, y), root(d) = N [i:j] , d 6?",
        "good N [i:j] (x, y)}.",
        "Note that the y-good derivations are defined over the unpruned whole decoding hypergraph, while the y-bad derivations are defined over the real decoding hypergraph with pruning.",
        "The max-violation method performs the update where the model score difference between the incorrect Viterbi partial derivation and the best y-good partial derivation is maximal, by penalizing the incorrect Viterbi partial derivation and rewarding the y-good partial derivation.",
        "More formally, we first find the Viterbi partial derivation d ?",
        "and the best y-good partial derivation d + for each N [i:j] group in the pruned +LM hypergraph: d + N [i:j] (x, y) ?",
        "= argmax d?good N [i:j] (x,y) w ??",
        "(x, d), d ?",
        "N [i:j] (x, y) ?",
        "= argmax d?bad N [i:j] (x,y) w ??",
        "(x, d), where ?",
        "(x, d) is the feature vector for derivation d. Then it finds the group N ?",
        "[i ?",
        ":j ? ]",
        "with the maximal score difference between the Viterbi derivation and the best y-good derivation: N ?",
        "[i ?",
        ":j ? ]",
        "?",
        "= argmax N [i:j] w ???",
        "(x, d + N [i:j] (x, y), d ?",
        "N [i:j] (x, y)), and update as follows: w?",
        "w + ??",
        "(x, d + N ?",
        "[i ?",
        ":j ? ]",
        "(x, y), d ?",
        "N ?",
        "[i ?",
        ":j ? ]",
        "(x, y)), where ??",
        "(x, d, d ? )",
        "?",
        "= ?",
        "(x, d)??",
        "(x, d ?",
        ").",
        "3.2 Forced Decoding for HIERO We now describe how to find the gold derivations.",
        "1 Such derivations can be generated in way similar to Yu et al. (2013) by using a language model tailored for forced decoding: P forced (q | p) = { 1 if q = p+ 1 0 otherwise , where p and q are the indices of the boundary words in the reference translation.",
        "The +LM node now has signature N p?q [i:j] , where p and q are the indexes of the boundary words.",
        "If a boundary word does not occur in the reference, its index is set to ?",
        "so that its language model score will always be ??",
        "; if a boundary word occurs more than once in the reference, its ?LM node is split into multiple +LM nodes, one for each such index.",
        "2 We have a similar deductive system for forced decoding.",
        "For the previous example, rule r 5 in Figure 2 (a) is rewritten as X?",
        "?y?u X 1 j?ux?",
        "?ng X 2 , 1 X 2 4 X 1 ?, where 1 and 4 are the indexes for reference words ?held?",
        "and ?with?",
        "respectively.",
        "The deduction for X [1:5] in Figure 2 (b) is X 5?5 [2:3] : s 1 X 2?3 [4:5] : s 2 X 1?5 [1:5] : s(r 5 ) + ?+ s 1 + s 2 r 5 , where ?",
        "= log ?",
        "i?",
        "{1,3,4} P forced (i+ 1 | i) = 0.",
        "4 Experiments Following Yu et al. (2013), we call our max-violation method MAXFORCE.",
        "Our implementation is mostly in Python on top of the cdec system (Dyer et al., 2010) via the pycdec interface (Chahuneau et al., 2012).",
        "In addition, we use minibatch parallelization of (Zhao and Huang, 1 We only consider single reference in this paper.",
        "2 Our formulation of index-based language model fixes a bug in the word-based LM of Yu et al. (2013) when a sub-string appears more than once in the reference (e.g. ?the man...the man...?",
        "); thanks to Dan Gildea for pointing it out.",
        "787 2013) to speedup perceptron training.",
        "We evaluate MAXFORCE for HIERO over two CH-EN cor-pora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al., 2007), Hypergraph MERT (Kumar et al., 2009), and PRO (Hopkins and May, 2011) from cdec.",
        "4.1 Features Design We use all the 18 dense features from cdec, including language model, direct translation probability p(e|f), lexical translation probabilities p l (e|f) and p l (f |e), length penalty, counts for the source and target sides in the training corpus, and flags for the glue rules and pass-through rules.",
        "For sparse features we use Word-Edges features (Charniak and Johnson, 2005; Huang, 2008) which are shown to be extremely effective in both parsing and phrase-based MT (Yu et al., 2013).",
        "We find that even simple Word-Edges features boost the performance significantly, and adding complex Word-Edges features from Yu et al. (2013) brings limited improvement and slows down the decoding.",
        "So in the following experiments we only use Word-Edges features consisting of combinations of English and Chinese words, and Chinese characters, and do not use word clusters nor word types.",
        "For simplicity and efficiency reasons, we also exclude all non-local features.",
        "4.2 Datasets and Preprocessing Our first corpus, IWSLT09, contains ?30k short sentences collected from spoken language.",
        "IWSLT04 is used as development set in MAXFORCE training, and as tuning set for n-best MERT, Hypergraph MERT, and PRO.",
        "IWSLT05 is used as test set.",
        "Both IWSLT04 and IWSLT05 contain 16 references.We mainly use this corpus to investigate the properties of MAXFORCE.",
        "The second corpus, FBIS, contains ?240k sentences.",
        "NIST06 newswire is used as development set for MAXFORCE training, and as tuning set for all other tuning methods.",
        "NIST08 newswire is used as test set.",
        "Both NIST06 newswire and NIST08 newswire contain 4 references.",
        "We mainly use this corpus to demonstrate the performance of MAXFORCE in large-scale training.",
        "For both corpora, we do standard tokeniza-tion, alignment and rule extraction using the cdec tools.",
        "In rule extraction, we remove all 1-count rules but keep the rules mapping from one Chinese word to one English word to help balancing sent.",
        "words phrase-based MT 32% 12% HIERO 35% 30% HIERO (all rules) 65% 55% Table 1: Reachability comparison (on FBIS) between phrase-based MT reported in Yu et al. (2013) (without 1-count rules) and HIERO (with and without 1-count rules).",
        "0 0.2 0.4 0.6 0.8 1 20 40 60 80 100 fo rc ed d ec od ab le ra tio sentence length loose tight Figure 4: Reachability vs. sent.",
        "length on FBIS.",
        "See text below for ?loose?",
        "and ?tight?.",
        "between overfitting and coverage.",
        "We use a tri-gram language model trained from the target sides of the two corpora respectively.",
        "4.3 Forced Decoding Reachability We first report the forced decoding reachability for HIERO on FBIS in Table 1.",
        "With the full rule set, 65% sentences and 55% words of the whole corpus are forced decodable in HIERO.",
        "After pruning 1-count rules, our forced decoding covers significantly more words than phrase-based MT in Yu et al. (2013).",
        "Furthermore, in phrase-based MT, most decodable sentences are very short, while in HIERO the lengths of decodable sentences are more evenly distributed.",
        "However, in the following experiments, due to efficiency considerations, we use the ?tight?",
        "rule extraction in cdec that is more strict than the standard ?loose?",
        "rule extraction, which generates a reduced rule set and, thus, a reduced reachability.",
        "We show the reachability distributions of both tight and loose rule extraction in Figure 4.",
        "4.4 Evaluation on IWSLT For IWSLT, we first compare the performance from various update methods in Figure 5.",
        "The max-violation method is more than 15 BLEU 788 30 35 40 45 2 4 6 8 10 12 14 16 18 20 BL EU o n de v iteration Max-Violation local update skip standard update Figure 5: Comparison of various update methods.",
        "42 43 44 45 46 47 2 4 6 8 10 12 14 16 18 20 BL EU o n de v iteration sparse features dense features Hypergraph MERT PRO n-best MERT Figure 6: Sparse features (Word-Edges) contribute ?2 BLEU points, outperforming PRO and MERT.",
        "points better than the standard perceptron (also known as ?bold-update?",
        "in Liang et al. (2006)) which updates at the root of the derivation tree.",
        "3,4 This can be explained by the fact that in training ?58% of the standard updates are invalid (i.e., they do not fix any violation).",
        "We also use the ?skip?",
        "strategy of Zhang et al. (2013) which updates at the root of the derivation only when it fixes a search error, avoiding all invalid updates.",
        "This achieves ?10 BLEU better than the standard up-date, but is still more than ?5 BLEU worse than Max-Violation update.",
        "Finally we also try the ?local-update?",
        "method from Liang et al. (2006) which updates towards the derivation with the best Bleu +1 in the root group S [0:|x|] .",
        "This method is about 2 BLEU points worse than max-violation.",
        "We further investigate the contribution of sparse features in Figure 6.",
        "On the development set, max-violation update without Word-Edges features achieves BLEU similar to n-best MERT and 3 We find that while MAXFORCE generates translations of length ratio close to 1 during training, the length ratios on dev/test sets are significantly lower, due to OOVs.",
        "So we run a binary search for the length penalty weight after each training iteration to tune the length ratio to ?0.97 on dev set.",
        "4 We report BLEU with averaged reference lengths.",
        "algorithm # feats dev test n-best MERT 18 44.9 47.9 Hypergraph MERT 18 46.6 50.7 PRO 18 45.0 49.5 local update perc.",
        "443K 45.6 49.1 MAXFORCE 529K 47.4 51.5 Table 2: BLEU scores (with 16 references) of various training algorithms on IWSLT09.",
        "algorithm # feats dev test Hypergraph MERT 18 27.3 23.0 PRO 18 26.4 22.7 MAXFORCE 4.5M 27.7 23.9 Table 3: BLEU scores (with 4 references) of various training algorithms on FBIS.",
        "PRO, but lower than Hypergraph MERT.",
        "Adding simple Word-Edges features improves BLEU by ?2 points, outperforming the very strong Hypergraph MERT baseline by?1 point.",
        "See Table 2 for details.",
        "The results of n-best MERT, Hypergraph MERT, and PRO are averages from 3 runs.",
        "4.5 Evaluation on FBIS Table 3 shows BLEU scores of Hypergraph MERT, PRO, and MAXFORCE on FBIS.",
        "MAXFORCE actives 4.5M features, and achieves +1.2 BLEU over PRO and +0.9 BLEU over Hypergraph MERT.",
        "The training time (on 32 cores) for Hypergraph MERT and PRO is about 30 min.",
        "on the dev set, and is about 5 hours for MAXFORCE on the training set.",
        "5 Conclusions We have presented a latent-variable violation-fixing framework for general structured prediction problems with inexact search over hyper-graphs.",
        "Its application on HIERO brings significant improvement in BLEU, compared to algorithms that are specially designed for MT tuning such as MERT and PRO.",
        "Acknowledgment Part of this work was done during K.",
        "Z.'s intern-ship at IBM.",
        "We thank Martin ?",
        "Cmejrek and Lemao Liu for discussions, David Chiang for pointing us to pycdec, Dan Gildea for Footnote 2, and the anonymous reviewers for comments.",
        "This work is supported by DARPA FA8750-13-2-0041 (DEFT), DARPA HR0011-12-C-0015 (BOLT), and a Google Faculty Research Award.",
        "789 References"
      ]
    }
  ]
}
