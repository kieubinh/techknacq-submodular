{
  "info": {
    "authors": [
      "Hamidreza Chinaei",
      "Brahim Chaib-draa"
    ],
    "book": "Workshop on Speech and Language Processing for Assistive Technologies",
    "id": "acl-W14-1903",
    "title": "Dialogue Strategy Learning in Healthcare: A Systematic Approach for Learning Dialogue Models from Data",
    "url": "https://aclweb.org/anthology/W14-1903",
    "year": 2014
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 13?19, Baltimore, Maryland USA, August 26 2014. c?2014 Association for Computational Linguistics Dialogue Strategy Learning in Healthcare: A Systematic Approach for Learning Dialogue Models from Data Hamid R. Chinaei Hamidreza.Chinaei.1@ulaval.ca Brahim Chaib-draa Brahim.Chaib-Draa@ift.ulaval.ca",
        "Abstract",
        "We aim to build dialogue agents that optimize the dialogue strategy, specifically through learning the dialogue model components from dialogue data.",
        "In this paper, we describe our current research on automatically learning dialogue strategies in the healthcare domain.",
        "We go through our systematic approach of learning dialogue model components from data, specifically user intents and the user model, as well as the agent reward function.",
        "We demonstrate our experiments on healthcare data from which we learned the dialogue model components.",
        "We conclude by describing our current research for automatically learning dialogue features that can be used in representing dialogue states and learning the reward function."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Cognitive assistive technologies provide support systems for the elderly, possibly with cognitive or physical disabilities, for instance people with dementia (such as Alzheimer's disease) (Boger et al., 2005; Pineau et al., 2011; Rudzicz et al., 2012).",
        "Such support systems can significantly reduce the costs of performing several tasks, currently done by family members or employed caregivers.",
        "In this context, (Rudzicz et al., 2012) are working on a computerized caregiver that assist individuals with Alzheimer's disease (AD) to complete daily tasks (e.g., preparing meals) using verbal communication.",
        "Thus, an important component of such technologies is the dialogue agent.",
        "Table 1 (left) shows sample dialogues collected by SmartWheeler, an intelligent wheelchair for persons with disabilities (Pineau et al., 2011).",
        "In particular, SmartWheeler aims to minimize the physical and cognitive load required in steering it.",
        "SmartWheeler is equipped with a dialogue agent, thus the users can give their commands through the spoken language besides a joystick.",
        "The first line denoted by u1 shows the true user utterance, which is the one that has been extracted manually from user audio recordings.",
        "The following line denoted by u?1 is the recognized user utterances by automatic speech recognition (ASR).",
        "Fi-nally, the line denoted by a1 shows the performed action in response to the ASR output at the time of collecting the dialogues.",
        "First, the users may say a command in different ways.",
        "For instance for turning right, the user may say turn right a little please, turn right, right a little, or right.",
        "Furthermore, challenges can occur when ASR output is imperfect.",
        "That is, for each dialogue utterance recognized by ASR, the dialogue agent must estimate the human user intent and then to perform the best action that satisfies the user intent.",
        "The recognized utterance by ASR is not reliable for decision making.",
        "For instance, the first utterance, u1: ?turn right a little?, shows the true user utterance.",
        "The ASR output for this utterance is, u?1: [10 writer little].",
        "As such, the action performed by SmartWheeler at this dialogue turn is, the general query action u1: PLEASE REPEAT YOUR COMMAND.",
        "We are working on automatically learning the dialogue strategy by learning the dialogue model components such as user intents, dialogue fea-tures, and the agent's reward function from data.",
        "Our data includes dialogues between human users and expert agents (such as caregivers).",
        "2 Theoretical approach Sequential Decision Making: Figure 1 shows the cycle of interaction between an agent (such as a dialogue agent or a caregiver) and its environment (such as a human user).",
        "The agent performs an action and receives an observation in return.",
        "The observation can be used by the agent, for in-13 u1 : turn right u?1 : [turn right] a1 : TURN RIGHT u2 : turn right a little u?2 : [10 writer little] a2 : PLEASE REPEAT YOUR COMMAND u3 : turn right a little u?3 : [turn right to lead a] a3 : TURN RIGHT A LITTLE u4 : stop u?4 : [stop] a4 : STOP u1 : turn right u?1 : [turn right] s1 : turn-right-little, 60% u2 : turn right a little u?2 : [10 writer little] s2 : turn-right-little, 90% u3 : turn right a little u?3 : [turn right to lead a] s3 : turn-right-little, 90% u4 : stop u?4 : [stop] s4 : stop, 90% Table 1: Left: A sample from the SmartWheeler dialogues (Pineau et al., 2011).",
        "Right: results of learning human user intents from patients?",
        "noisy dialogues.",
        "stance to update its state and reward.",
        "The reward works as reinforcement from the environment that shows how well the agent performed.",
        "In sequential decision making, the agent is required to make decision for sequence of states rather than making a one-shot decision.",
        "Then, the sequential decision making is performed with the objective of maximizing the long term rewards.",
        "The sequence of actions is called a strategy, and the major question in sequential decision making is how to find a near optimal strategy.",
        "Reinforcement learning (RL): RL in (partially observable) Markov decision processes, so called the (PO)MDPs, is a learning approach in sequential decision making.",
        "In particular, (PO)MDPs have been successfully applied in dialogue agents (Roy et al., 2000; Zhang et al., 2001; Williams, 2006; Thomson and Young, 2010; Gas?ic?, 2011).",
        "The (PO)MDP framework is a formal framework to represent uncertainty explicitly while supporting automated strategy solving.",
        "Specifically, it is an optimization framework that supports automated strategy solving by maximizing a ?re- ward function?.",
        "3 Objective SDS (Spoken dialogue system) researchers have addressed several practical challenges of applying (PO)MDPs to SDS (Williams, 2006; Paek and Pieraccini, 2008).",
        "Specifically, estimating the user model and the reward function is a significant challenge since these model components have a direct impact on the optimized dialogue strategy.",
        "Furthermore, the reward function is perhaps the most hand-crafted aspect of the optimization frameworks such as (PO)MDPs (Paek and Pierac-cini, 2008).",
        "Using inverse reinforcement learning (IRL) techniques, a reward function can be determined from expert actions (such as caregiver ac-tions) (Ng and Russell, 2000).",
        "Fortunately, learning the reward function using IRL methods have already been proposed for the general (PO)MDP framework (Ng and Russell, 2000; Kim et al., 2011), paving the way for investigating its use for dialogue (PO)MDPs.",
        "In this context, the IRL algorithms require dialogue features (for instance ASR recognitions with their confidence scores) for representing the reward function.",
        "Extracting relevant dialogue features is important since the dialogue features and their representation highly affect the learned reward function and finally the optimized strategy.",
        "Thus, our goals include building (PO)MDP- based dialogue technologies that optimizes the dialogue strategy through learning user intents and the user model, and reward function from dialogue data, as follows: 1.",
        "Learning user intents and the user model from collected dialogues, i.e., ASR recogni-tions, or directly from acoustic data.",
        "2.",
        "Learning the reward function.",
        "(a) Learning useful dialogue features.",
        "(b) Representing features in IRL for learning the reward function.",
        "Recall Figure 1 that shows the cycle of interaction between an agent (such as a dialogue agent or a caregiver) and its environment (such as a human user).",
        "In this figure, circles represent the learned models.",
        "The model denoted by (PO)MDP includes the (PO)MDP model components, without14 (PO)MDP R IRL (PO)MDPsolverEnvironment Agent a/o trajectories learning acting Figure 1: The cycle of acting/learning between the agent and environment.",
        "The circles represent the models.",
        "The model denoted by (PO)MDP includes the (PO)MDP model components, without a reward function, learned from step 1 in the objective section.",
        "The learned (PO)MDP model together with expert action/observation trajectories are used in IRL to learn the reward function denoted by R, in step 2 in the objective section.",
        "The learned (PO)MDP and reward function are used in the (PO)MDP solver to learn/update the strategy.",
        "a reward function, which have been learned from step 1 above.",
        "The learned (PO)MDP together with action/observation trajectories are used in IRL to learn the reward function, denoted by R. Then, the learned (PO)MDP and the reward function are used in a (PO)MDP solver to learn/update the optimal strategy.",
        "4 SmartWheeler data The SmartWheeler project aims to build an intelligent wheelchair for persons with disabilities (Pineau et al., 2011).",
        "In particular, SmartWheeler aims to minimize the physical and cognitive load required in steering it.",
        "This project has been initiated in 2006, and a first prototype, shown in Figure 2, was built in-house at McGill's Center for Intelligent Machines.",
        "We used the dialogues collected by SmartWheeler to develop dialogue (PO)MDPs, learned primarily from data.",
        "The data includes eight dialogues with healthy users and nine dialogues with target users of SmartWheeler (Pineau et al., 2011).",
        "The dialogues with target users, who are the elderly, are somehow more noisy than the ones with healthy users.",
        "More specifically, the average word error rate (WER) equals 13.9% Figure 2: The SmartWheeler robot platform (Pineau et al., 2011).",
        "for the healthy user dialogues and 18.5% for the target user dialogues.",
        "In order to perform our experiments on a larger amount of data, we used all the healthy and target user dialogues.",
        "In total, there are 2853 user utterances and 422 distinct words in the SmartWheeler dialogues.",
        "5 Learning user intents from data We learned the (PO)MDP states by learning the user intents occurred in the dialogue set using a topic modeling approach, i.e., Hidden Topic15 Markov Model (HTMM) (Gruber et al., 2007).",
        "HTMM is a variation of Latent Dirichlet Allocation (LDA) which learns topics from text based on co-occurrence of words and using Dirichlet distribution for generating the topics of text documents (Blei et al., 2003).",
        "HTMM adds Markovian assumption to the LDA model in order to exploit the Markovian property between sentences in the documents.",
        "Thus, HTMM can be seen both as a variation of Hidden Markov Model (HMM) and a variation of LDA.",
        "Our experimental results showed that HTMM learns proper user intents that can be used as dialogue states, and is able to exploit the Markovian property between dialogue utterances, adequately.",
        "The learned states, using our proposed methods, from SmartWheeler data are as follows: s1 : move-forward-little, s2 : move-backward-little, s3 : turn-right-little, s4 : turn-left-little, s5 : follow-left-wall, s6 : follow-right-wall, s7 : turn-degree-right, s8 : go-door, s9 : set-speed, s10 : follow-person, s11 : stop.",
        "Table 3 shows the learned user intents, five of them, with their top-four words, i.e., the intent keywords.",
        "Table 1 (right) shows results of HTMM application on SmartWheeler for the example shown in Table 1 (left).",
        "For instance, the second utterance shows that the user actually uttered turn right a little, but it is recognized as 10 writer little by ASR.",
        "The most probable intent returned by HTMM for this utterance is s3 : turn-right-little with 90% probability.",
        "This is because HTMM considers Markovian property for deriving intents.",
        "As a result, in the second turn it estimates correctly the true user intent based on the user intent in the first turn.",
        "The list of all SmartWheeler actions are shown in Table 2.",
        "Each action is the right action of one state (the user intent for a specific com-mand).",
        "So, ideally, there should be 24 states for SmartWheeler dialogues (There are 24 actions other than the general query action: REPEAT).",
        "However, we only learned 11 of the states, mainly because of the number of dialogues.",
        "That is, not all of the states appeared in the data frequently enough.",
        "There are also states that do not appear in dialogues at all.",
        "6 Learning reward functions from data In this section, we experiment our implementation of the trajectory-based MDP-IRL algorithm pro-a1 DRIVE FORWARD A LITTLE a2 DRIVE BACKWARD A LITTLE a3 TURN RIGHT A LITTLE a4 TURN LEFT A LITTLE a5 FOLLOW THE LEFT WALL a6 FOLLOW THE RIGHT WALL a7 TURN RIGHT DEGREE a8 GO THROUGH THE DOOR a9 SET SPEED TO MEDIUM a10 FOLLOW THE WALL a11 STOP a12 TURN LEFT a13 DRIVE FORWARD a14 APPROACH THE DOOR a15 DRIVE BACKWARD a16 SET SPEED TO SLOW a17 MOVE ON SLOPE a18 TURN AROUND a19 PARK TO THE RIGHT a20 TURN RIGHT a21 DRIVE FORWARD METER a22 PARK TO THE LEFT a23 TURN LEFT DEGREE a24 PLEASE REPEAT YOUR COMMAND Table 2: The list of the possible actions, performed by SmartWheeler.",
        "posed by (Ng and Russell, 2000).",
        "The IRL experiments are designed to verify if the introduced IRL methods are able to learn a reward function for the expert strategy, where the expert strategy is represented as a (PO)MDP strategy.",
        "That is, the expert strategy is the strategy that the underlying (PO)MDP framework optimizes.",
        "The MDP expert strategy for each of the (PO)MDP state is represented in Table 4.",
        "This strategy suggests performing the right action of each state.",
        "6.1 MDP-IRL learned rewards We applied the MDP-IRL algorithm on SmartWheeler dialogue MDP described above using the introduced keyword features in Table 5.",
        "The algorithm was able to learn a reward function in which the strategy equals the expert strategy for all states, (the expert strategy shown in Table 4).",
        "Table 6 shows the learned reward function.",
        "Note that, for instance for state s3: turn-right-little, the reward of performing both actions a3: TURN RIGHT A LITTLE and a4: FOLLOW THE RIGHT WALL is close to 1.",
        "Nevertheless, the optimized strategy for this reward function suggest the correct action, i.e., TURN RIGHT A LITTLE for this state (turn-right-little).16 intent 1 forward 18.0% move 16.1% little 11.4% drive 08.1% .",
        ".",
        ".",
        ".",
        ".",
        ".",
        "intent 2 backward 38.0% drive 33.3% little 10.9% top 01.7% .",
        ".",
        ".",
        ".",
        ".",
        ".",
        "intent 3 right 20.9% turn 17.1% little 13.1% bit 07.4% .",
        ".",
        ".",
        ".",
        ".",
        ".",
        "intent 4 left 18.9% turn 17.1% little 13.8% right 09.0% .",
        ".",
        ".",
        ".",
        ".",
        ".",
        ".",
        ".",
        ".",
        ".",
        ".",
        ".",
        ".",
        ".",
        ".",
        "intent 11 stop 94.2% stopp 02.2% scott 00.7% but 00.2% .",
        ".",
        ".",
        ".",
        ".",
        ".",
        "Table 3: The learned user intents from the SmartWheeler dialogues and their top words.",
        "Each percentage shows the probability of each word given the intent.",
        "state state description expert action expert action description s1 move-forward-little a1 DRIVE FORWARD A LITTLE s2 move-backward-little a2 DRIVE BACKWARD A LITTLE s3 turn-right-little a3 TURN RIGHT A LITTLE s4 turn-left-little a4 TURN LEFT A LITTLE s5 follow-left-wall a5 FOLLOW THE LEFT WALL s6 follow-right-wall a6 FOLLOW THE RIGHT WALL s7 turn-degree-right a7 TURN RIGHT DEGREES s8 go-door a8 GO THROUGH THE DOOR s9 set-speed a9 SET SPEED TO MEDIUM s10 follow-wall a10 FOLLOW THE WALL s11 stop a11 STOP Table 4: The learned strategy using the learned dialogue MDP from SmartWheeler dialogues.",
        "forward backward right left turn go for top stop s1 1 0 0 0 0 0 0 0 0 s2 0 1 0 0 0 0 0 0 0 s3 0 0 1 0 0 0 0 0 0 s4 0 0 0 1 0 0 0 0 0 s5 0 0 0 1 0 0 0 0 0 s6 0 0 1 0 0 0 0 0 0 s7 0 0 0 0 1 0 0 0 0 s8 0 0 0 0 0 1 0 0 0 s9 0 0 0 0 0 0 1 0 0 s10 0 0 0 0 0 0 0 1 0 s11 0 0 0 0 0 0 0 0 1 Table 5: Keyword features for the SmartWheeler dialogues.",
        "a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 ... REPEAT s1 1.0 0 0 0 0 0 0 0 0 0 0 0 .",
        ".",
        ".",
        "0 s2 0 1.0 0 0 0 0 0 0 0 0 0 0 .",
        ".",
        ".",
        "0 s3 0 0 1.0 0 0 1.0 0 0 0 0 0 0 .",
        ".",
        ".",
        "0 s4 0 0 0 1.0 1.0 0 0 0 0 0 0 0 .",
        ".",
        ".",
        "0 s5 0 0 0 1.0 1.0 0 0 0 0 0 0 0 .",
        ".",
        ".",
        "0 s6 0 0 1.0 0 0 1.0 0 0 0 0 0 0 .",
        ".",
        ".",
        "0 s7 0 0 0 0 0 0 1.0 0 0 0 0 0 .",
        ".",
        ".",
        "0 s8 0 0 0 0 0 0 0 1.0 0 0 0 0 .",
        ".",
        ".",
        "0 s9 0 0 0 0 0 0 0 0 1.0 0 0 0 .",
        ".",
        ".",
        "0 s10 0 0 0 0 0 0 0 0 0 1.0 0 0 .",
        ".",
        ".",
        "0 s11 0 0 0 0 0 0 0 0 0 0 1.0 0 .",
        ".",
        ".",
        "0 Table 6: The learned reward function for the learned dialogue MDP from SmartWheeler dialogues using keyword features.",
        "17 6.2 Choice of features IRL needs features to represent the reward function.",
        "We propose keyword features for applying IRL on the learned dialogue MDP/POMDP from SmartWheeler.",
        "The keyword features are automatically learned as the top-one words for each user intent (see Table 3).",
        "There are nine learned key-words: forward, backward, right, left, turn, go, for, top, stop.",
        "The keyword features for each state of SmartWheeler dialogue POMDP are represented in a vector, as shown in Table 5.",
        "The figure shows that states s3, (turn-right-little) and s6 (follow-right-wall) share the same features, i.e., right.",
        "Moreover, states s4 (turn-left-little) and s5 (follow-left-wall) share the same feature, i.e., left.",
        "In our experiments, we used keyword-action-wise feature representation.",
        "Such features include an indicator function for each pair of state-keyword and action.",
        "Thus, the feature size for SmartWheeler equals 216 = 9 ?",
        "24 (9 keywords and 24 actions).",
        "Note that the choice of features is application dependent.",
        "The reason for using keywords as state features is that in the intent-based dialogue applications the states are the dialogue intents, where each intent is described as a vector of k-top words from the domain dialogues.",
        "Therefore, the keyword features are relevant features for the states.",
        "7 Conclusion In this paper, we described our our systematic approach for learning dialogue (PO)MDP model components from unannotated dialogues.",
        "In our approach, we start by learning the dialogue (PO)MDP states, i.e., the learned user intents from data.",
        "The learned states were then used for learning the user model.",
        "Building off these model com-ponents, we learned the agent's reward function by implementing a model-based IRL algorithm.",
        "We demonstrated our experiments on data collected in a healthcare domain to learn the dialogue model components solely from data.",
        "8 Ongoing work We are working on a variation of MDP-IRL algo-rithm, that is a model-free trajectory-based MDP-IRL algorithm.",
        "In the model-free MDPs, the states are usually presented using features (and thus there is no defined/learned transition model).",
        "Then, model-free MDP algorithms are used for estimating the optimal strategy of such MDPs.",
        "Model-free MDPs can be used in the place of POMDPs where state features are analogous to observations.",
        "In this context, data analysis for feature selection is highly important.",
        "Dialogue features can be used to represent dialogue situations (as well as the observations in the dialogue POMDPs).",
        "Moreover, the IRL algorithms require (dialogue) features for representing the reward function.",
        "As mentioned earlier, the reward function of (PO)MDPs highly affects the optimized strategy.",
        "A relevant reward function to the dialogue agent and users can only be learned by studying and extracting relevant features from the dialogue domain.",
        "We would like to learn the relevant and proper features that are suitable for both state features as well as the reward representation.",
        "In par-ticular, we are going to use the experts?",
        "(care- givers?)",
        "strategies in the place of a (PO)MDP strategy in order to learn a reward function that accounts for caregivers?",
        "strategies.",
        "9 Acknowledgment The authors thank Jason D. Williams and Suhrid Balakrishnan for helpful discussions in the early development of this work.",
        "The authors also thank Joelle Pineau for providing them with the Smartwheeler data.",
        "The dataset has been collected with contributions from researchers at McGill University, E?cole Polytechnique de Montre?al, Universite?",
        "de Montre?al, and the Centre de re?adaptation Lucie-Bruneau and Constance-Lethbridge.",
        "The authors thank Ethan Selfridge for his help in proofreading.",
        "Last but not least, many thank to FQRNT (Fonds Que?be?cois de la recherche sur la nature et les technologies) for partial financial support of this work.",
        "References"
      ]
    }
  ]
}
