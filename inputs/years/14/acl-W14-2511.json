{
  "info": {
    "authors": [
      "Miriam Boon"
    ],
    "book": "LTCSS",
    "id": "acl-W14-2511",
    "title": "Information density, Heapsâ€™ Law, and perception of factiness in news",
    "url": "https://aclweb.org/anthology/W14-2511",
    "year": 2014
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 33?37, Baltimore, Maryland, USA, June 26, 2014. c?2014 Association for Computational Linguistics Information density, Heaps?",
        "Abstract",
        "Seeking information online can be an exercise in time wasted wading through repeti-tive, verbose text with little actual content.",
        "Some documents are more densely populated with factoids (fact-like claims) than others.",
        "The densest documents are potentially the most efficient use of time, likely to include the most information.",
        "Thus some measure of ?factiness?",
        "might be useful to readers.",
        "Based on crowdsourced ratings of the factual content of 772 online news articles, we find that after controlling for widely varying document length using Heaps?",
        "Law, a significant positive correlation exists between perceived factual content and relative information entropy."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In today's information-based society, finding accurate information is of concern to everyone.",
        "There are many obstacles to this goal.",
        "Not all people are equally skilled at judging the veracity of a factoid (a term used here to indicate something that is stated as a fact, but that may or may not actually be true.).",
        "Nor is it always easy to find the single drop of content you need amidst the oceans of the Internet.",
        "Even for those equipped with both skill and access, time is always a limiting factor.",
        "It is this last problem with which this paper is concerned.",
        "How can we identify content that most efficiently conveys the most information, given that any information seeker's time is limited?",
        "1.1 The difficulty with factoids Imagine that we must select from a set of documents those that efficiently convey the most information in the fewest words possi-ble; that is, those with the highest factoid rate, count(factoids)/count(words).",
        "A human doing this by hand would count the factoids and words in each document.",
        "Automating this exact approach would require ?teaching?",
        "the computer to identify unique factoids in a document, which requires being able to recognize and discard redundant factoids, which requires at least a rudimentary understanding of each factoid's meaning.",
        "These are all difficult tasks for a computer.",
        "Luckily, to achieve our goal, we don't need to know which sentences are factoids.",
        "What we need is a good heuristic estimate of information density that computers can easily calculate.",
        "1.2 Linking vocabulary to factoids To insert new information into a text, an author must add words, making the document longer.",
        "While the new information can sometimes be conveyed using the same vocabulary as the rest of the text, if the information is sufficiently different from what is already present, it will also likely introduce new vocabulary words.",
        "The result is that the introduction of a new factoid into a text is likely to also introduce new vo-cabulary, unless it is redundant.",
        "Thus, the more non-redundant factoids a text contains, the more varied the vocabulary of the text is likely to be.",
        "1.3 From vocabulary to relative entropy Vocabulary is commonly used in connection with Shannon's information entropy to measure such things as surprisal, redundancy, perplexity, and, of course, information density (Shannon, 1949; Mc-Farlane et al., 2009).",
        "Entropy models text as being created via a Markov process.",
        "In its most basic form, it can be written as: H = ?K L ?",
        "i=0 p i log 2 p i (1) where K is a constant chosen based on the units, L is the length of the document, and p i is the probability of the i th word.",
        "This equation works 33 equally well whether it is used for unigrams, bi-grams, or trigrams.",
        "Consider for a moment the relationship between entropy and length, vocabulary, and the probability of each vocabulary word.",
        "Entropy increases as both document length and vocabulary increase.",
        "Words with lower probability increase entropy more than those with higher probabilities.",
        "For this study, probabilities were calculated based on corpus-wide frequencies.",
        "This means that, in the-ory, a large number of the words in a document could have very low probability.",
        "Given two documents of equal length on the same topic, only one of which is rich in infor-mation, we might wonder why the information-poor document is, relatively speaking, so long or the information-rich document is so short.",
        "This can be explained by noting that: 1.",
        "?translation?",
        "into simpler versions of a language often leads to a longer text, 2. simple versions of languages generally consist of the most common words in that language, and 3. words that are less common often have more specific, information-dense, complex meanings.",
        "Similarly, inefficient word choices typically make excessive use of highly probable function words, which do not increase the entropy as much as less common words.",
        "Thus, we can expect the entropy to be higher for the denser document.",
        "1.4 Controlling for document length with Heaps?",
        "Law While entropy may not rise as fast with the repetition or addition of highly probable words, how-ever, every word added does still increase the entropy.",
        "This follows naturally from the fact that for every word added, another term is added to the summation.",
        "We can try to compensate by dividing by document length.",
        "But dividing by document length doesn't remove this dependency.",
        "I propose that this is because, as Heap's Law tells us, the vocabulary used in a document has a positive relationship with document size (Heaps, 1978).",
        "To control for this effect, I fit a curve for unigrams, bigrams, and trigrams to create a model for these relationships; an example can be seen in Figure 1.",
        "I then used that model to calculate the expected document length, expected entropy, and relative entropy, as follows: L exp = 10 (log 10 v?b)/m (2) H exp = H L exp L (3) H rel = H H exp (4) Here the subscript ?exp?",
        "stands for ?expected?",
        "and the subscript ?rel?",
        "for ?relative.?",
        "This calculation eliminates the dependency on document length.",
        "Figure 1: Top: As you can see there is a strong relationship between document length and entropy.",
        "R 2 =0.992, p > F: < 0.0001.",
        "Bottom: Relative entropy, which controls for that relationship, no longer has a significant nor strong relationship with document length.",
        "R 2 =0.0017, p > F: 0.2425 2 Data and Analysis To further pursue the hypothesis that residual entropy could be used to identify news articles with lots of factoids, and thus, a sense of ?factiness,?",
        "a labeled data set is necessary.",
        "Lots of websites allow users to rate articles, but those ratings don't have anything to do with the presence of factoids.",
        "Labeling a data set of adequate size by hand would be tedious, time-consuming, and costly.",
        "34 Figure 2: Mousing over the question makes the text ?Is it based on facts or opinions??",
        "appear in pale grey text.",
        "Clicking on the question mark icon next to the question, ?Is this story factual??",
        "reveals an explanation of what the user should be rating.",
        "2.1 Crowdsourcing with NewsTrust Fortunately, a project called NewsTrust provided a feasible alternative.",
        "NewsTrust, founded in 2005 by Fabrice Florin, created four sets of specific review questions designed to inspire reviewers to think critically about the quality of articles they review.",
        "NewsTrust partnered with independent academic researchers Cliff Lampe and Kelly Garrett to validate the review questions.",
        "They jointly administered a survey in which respondents were asked to complete one of the review instruments regarding either the original version of an article or blog post, or a degraded version.",
        "The independent study found that even the less experienced, less knowledgeable readers were able to distinguish between the two versions of the story.",
        "The shortest review instrument, with only one question, had the most discriminating power, while the slightly longer normative review instrument (which added five more ques-tions) yielded responses from non-experts that most closely matched those of NewsTrust's expert journalists (Lampe and Garrett, 2007; Florin et al., 2006; Florin, 2009).",
        "Using their validated survey instrument, NewsTrust created a system that allowed users to read articles elsewhere, rate them using one of the four review instruments, and even rate other NewsTrust users?",
        "reviews of articles.",
        "Each user has a trustworthiness rating (which can be bolstered by becoming validated as a journalist expert), and each article has a composite rating, a certainty level for that rating, reviews, and ratings of reviews.",
        "One of the dimensions of journalistic quality for which NewsTrust users rate articles is called ?facts?.",
        "This can be taken as an aspect of ?facti- ness?",
        ": the extent to which people perceived the article as truthful and factual.",
        "It follows that, to the extent that the users are making a good-faith attempt to rate articles based on facts regardless of the soundness of their judgment about what is or is not true, articles with a high rating for ?facts?",
        "should have more factoids, and therefore a higher density of information.",
        "2.2 Data acquisition When this research project was launched, NewsTrust had recently been acquired by the Poynter Institute.",
        "Although they were open to making their data available for research purposes, they were not yet able to access the data in order to do so.",
        "In-stead, the review data for over 11000 stories from NewsTrust's political section were retrieved using Python, Requests, and Beautiful Soup.",
        "A combination of Alchemy API, digital library archives, and custom scrapers for 19 different publication websites were used to harvest the corresponding article texts.",
        "It quickly became clear, however, that it would not be possible to completely capture all 11,000 articles.",
        "Some of the independent blogs and websites no longer existed.",
        "Others had changed their link structure, making it difficult to find the correct article.",
        "A great deal of content was behind paywalls, or simply did not have a webpage structure that lent itself to clean extraction.",
        "As the text would be used for automated analysis, it was essential that the extracted text be as clean of detritus as possible.",
        "As a result, the dataset shrank from a potential 11,000 rated articles to only 3300 for which I could be confident of having clean text.",
        "Approximately 2600 of those articles have been rated by at least one NewsTrust user based on factiness, and after removing any with fewer than four facts ratings, the data set shrank further to only 805 1 articles.",
        "Unigrams, bigrams, and trigrams were extracted from these articles using the Natural Language Toolkit, NLTK; all text was lowercased, and only alphanumeric words were included.",
        "2.3 Analysis The relationship between length and vocabulary was modeled using the optimize toolkit from SciPy, and visualized with MatPlotLib.",
        "The result-1 One outlier document was removed.",
        "35 Figure 3: Top: Bivariate fit for bigrams.",
        "Bottom: Oneway ANOVA for bigrams.",
        "ing relationship was used to calculate the relative entropy for each document.",
        "For bivariate analysis, 772 of these documents could be used 2 .",
        "But for the oneway analysis, documents needed to be separated into two distinct clusters.",
        "We used Weka's K-Means clustering algorithm to find the location of three clusters.",
        "The 90% confidence interval for each article (calculated using the invididual user ratings for ?facts?)",
        "was used to determine cluster membership.",
        "That is, articles for which that confidence interval would overlap with both the upper and lower cluster were discarded (738 documents in total).",
        "This process was repeated for 80 and 85% confidence levels; they yielded more data points (198 and 458), a higher level of significance, and a lower R 2 .",
        "A 95% confidence level did not yield enough articles with a low facts rating to analyze.",
        "3 Results and Discussion The bivariate analysis showed a small but significant positive relationship between factual rating 2 33 documents with particularly low confidence levels for their rating were removed and relative entropy as calculated for unigrams, bi-grams, and to a lesser extent, trigrams.",
        "The results can be seen in Table 1 and Figure 3.",
        "These relationships strengthened according to the ANOVA for the more distinct high and low factiness classifications.",
        "If we accept the assumption that the articles rated by NewsTrust users as highly factual will contain a higher density of factoids, then this result supports the hypothesis that relative entropy is positively correlated with that characteristic.",
        "Con-versely, if we accept the assumption that entropy should be correlated with factoid density, then this result supports the claim that NewsTrust users effectively identify articles that are more information dense.",
        "Future work on the fact-rated sub-Unigram Bigram Trigram Bivar.",
        "R 2 0.033 0.032 0.014 p > F < 0.0001 < 0.0001 < 0.0008 Oneway R 2 0.086 0.084 0.082 p > |t| 0.0154 0.0163 0.0178 Table 1: Bivariate analysis (n = 772) and Oneway ANOVA (n = 68).",
        "corpus has two obvious directions.",
        "First, and most closely related to the work described in this pa-per, is the goal of proving either assumption in a more controlled experiment.",
        "If one of these assumptions can be supported, then it strengthens the claim about the other, which will be interesting from both a linguistic perspective, and a human-computer interaction perspective.",
        "The other avenue of inquiry that follows naturally from this work is to look for other textual features that might, in combination, enable the automatic prediction of fact ratings based on article text.",
        "Acknowledgments This work was partly supported by the Technology and Social Behavior program at Northwestern University, the National Science Foundation, the Knight Foundation, and Google.",
        "Many thanks to Dr. Darren Gergle for his insight on the larger NewsTrust data set, to Dr. Janet Pierrehumbert for her guidance on entropy and factiness, and to Dr. Larry Birnbaum for his intellectual guidance as well as his assistance on this paper.",
        "36 References"
      ]
    }
  ]
}
