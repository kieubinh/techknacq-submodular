{
  "info": {
    "authors": [
      "Gerard Lynch",
      "PÃ¡draig Cunningham"
    ],
    "book": "WASSA",
    "id": "acl-W14-2613",
    "title": "Linguistically Informed Tweet Categorization for Online Reputation Management",
    "url": "https://aclweb.org/anthology/W14-2613",
    "year": 2014
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 73?78, Baltimore, Maryland, USA.",
        "June 27, 2014. c?2014 Association for Computational Linguistics Linguistically Informed Tweet Categorization for Online Reputation Management Gerard Lynch and P ?",
        "adraig Cunningham Centre for Applied Data Analytics Research (CeADAR) University College Dublin Belfield Office Park Dublin 4, Ireland firstname.lastname@ucd.ie",
        "Abstract",
        "Determining relevant content automatically is a challenging task for any aggregation system.",
        "In the business intelligence domain, particularly in the application area of Online Reputation Manage-ment, it may be desirable to label tweets as either customer comments which deserve rapid attention or tweets from industry experts or sources regarding the higher-level operations of a particular entity.",
        "We present an approach using a combination of linguistic and Twitter-specific features to represent tweets and examine the efficacy of these in distinguishing between tweets which have been labelled using Amazon's Mechanical Turk crowd-sourcing platform.",
        "Features such as part-of-speech tags and function words prove highly effective at discriminating between the two categories of tweet related to several distinct entity types, with Twitter-related metrics such as the presence of hashtags, retweets and user mentions also adding to classification accuracy.",
        "Accuracy of 86% is reported using an SVM classifier and a mixed set of the aforementioned features on a corpus of tweets related to seven business entities.",
        "1 Motivation Online Reputation Management (ORM) is a growing field of interest in the domain of business intelligence.",
        "Companies and individuals alike are highly interested in monitoring the opinions of others across social and traditional media and this information can have considerable business value for corporate entities in particular.",
        "1.1 Challenges There are a number of challenges in creating an end-to-end software solution for such purposes, and several shared tasks have already been established to tackle these issues 1 .",
        "The most recent RepLab evaluation was concerned with four tasks related to ORM, filtering, polarity for reputation, topic detection and priority assignment.",
        "Based on these evaluations, it is clear that although the state of the art of topic-based filtering of tweets is relatively accomplished (Perez-Tellez et al., 2011; Yerva et al., 2011; Spina et al., 2013), other aspects of the task such as sentiment analysis and prioritisation of tweets based on content are less trivial and require further analysis.",
        "Whether Twitter mentions of entities are actual customer comments or in fact represent the views of traditional media or industry experts and sources is an important distinction for ORM systems.",
        "With this study we investigate the degree to which this task can be automated using supervised learning methods.",
        "2 Related Work 2.1 Studies on Twitter data While the majority of research in the computational sciences on Twitter data has focused on issues such as topic detection (Cataldi et al., 2010), event detection, (Weng and Lee, 2011; Sakaki et al., 2010), sentiment analysis, (Kouloumpis et al., 2011), and other tasks based primarily on the topical and/or semantic content of tweets, there is a growing body of work which investigates more subtle forms of information represented in tweets, such as reputation and trustworthiness, (O?Donovan et al., 2012), authorship attribution (Layton et al., 2010; Bhargava et al., 2013) and Twitter spam detection, (Benevenuto et al., 2010).",
        "1 See (Amig?o et al., 2012) and (Amig?o et al., 2013) for details of the RepLab series 73 These studies combine Twitter-specific and textual features such as retweet counts, tweet lengths and hashtag frequency, together with sentence-length, character n-grams and punctuation counts.",
        "2.2 Studies on non-Twitter data The textual features used in our work such as n-grams of words and parts-of-speech have been used for gender-based language classification (Koppel et al., 2002), social profiling and personality type detection (Mairesse et al., 2007), native language detection from L2 text, (Brooke and Hirst, 2012) translation source language detection, (van Halteren, 2008; Lynch and Vogel, 2012) and translation quality detection, (Vogel et al., 2013).",
        "3 Experimental setup and corpus Tweets were gathered between June 2013 and January 2014 using the twitter4j Java library.",
        "A language detector was used to filter only English-language tweets.",
        "2 The criteria for inclusion were that the entity name was present in the tweet.",
        "The entities focused on in this study had relatively un-ambigious business names, so no complex filtering was necessary.",
        "3.1 Pilot study A smaller pilot study was carried out before the main study in order to examine response quality and accuracy of instruction.",
        "Two hundred sample tweets concerning two airlines 3 were annotated using Amazon's Mechanical Turk system by fourteen Master annotators.",
        "After annotation, we selected the subset (72%) of tweets for which both annotators agreed on the category to train the classifier.",
        "During the pilot study, the tweets were pre-processed 4 to remove @ and # symbols and punctuation to treat account names and hashtags as words.",
        "Hyperlinks representations were maintained within the tweets.",
        "The Twitter-specific metrics were not employed in the pilot study.",
        "3.2 Full study In the full study, 2454 tweets concerning seven business entities 5 were tagged by forty annotators as to whether they corresponded to one of the 2 A small amount of non-English tweets were found in the dataset, these were assigned to the Other category.",
        "3 Aer Lingus and Ryanair 4 This was not done in the full study, these symbols were counted and used as features.",
        "5 Aer Lingus, Ryanair, Bank of Ireland, C & C Group, Permanent TSB, Glanbia, Greencore three categories described in Section 1.1.",
        "For 57% of the tweets, annotators agreed on the categories with disagreement in the remaining 43%.",
        "The disputed tweets were annotated again by two annotators.",
        "From this batch, a similar proportion were agreed on.",
        "For the non-agreed tweets in the second round, a majority category vote was reached by combining the four annotations over the first and second rounds.",
        "After this process, roughly two hundred tweets remained as ambiguous (each having two annotations for one of two particular categories) and these were removed from the corpus used in the experiments.",
        "3.3 Category breakdown Table 5 displays the number of tweets for which no majority category agreement was reached.",
        "The majority disagreement class across all entities are texts which have been labelled as both business operations and other.",
        "For the airline entities, a large proportion of tweets were annotated as both customer comment and other, this appeared to be a categorical issue which may have required clarification in the instructions.",
        "The smallest category for tied agreement is customer comment and business operations, it appears that the distinction between these categories was clearer based on the data provided to annotators.",
        "2078 tweets were used in the final experiments.",
        "The classes were somewhat imbalanced for the final corpus, the business operations category was the largest, with 1184 examples, customer comments contained 585 examples and the other category contained 309 examples.",
        "3.4 Feature types The features used for classification purposes can be divided into the following two categories: 1.",
        "Twitter-specific: ?",
        "Tweet is a retweet or not ?",
        "Tweet contains a mention ?",
        "Tweet contains a hashtag or a link ?",
        "Weight measure (See Fig 3) ?",
        "Retweet account for a tweet.",
        "2.",
        "Linguistic: The linguistic features are based on the textual content of the tweet represented as word unigrams, word bigrams and part-of-speech bigrams.",
        "74 We used TagHelperTools, (Ros?e et al., 2008) for textual feature creation which utilises the Stanford NLP toolkit for NLP annotation and returns formatted representations of textual features which can be employed in the Weka toolkit which implements various machine learning algorithms.",
        "All linguistic feature frequencies were binarised in our representations 6 .",
        "4 Results 4.1 Pilot study Using the Naive Bayes classifier in the Weka toolkit and a feature set consisting of 130 word tokens, 80% classification accuracy was obtained using ten-fold cross validation on the full set of tweets .",
        "Table 1 shows the top word features when ranked using 10-fold cross validation and the information gain metric for classification power over the three classes.",
        "Using the top 50 ranked POS-bigram features alone, 74% classification accuracy was obtained using the Naive Bayes classifier.",
        "Table 2 shows the top twenty features, again ranked by information gain.",
        "Combining the fifty POS-bigrams and the 130 word features, we obtained 84% classification accuracy using the Naive Bayes classifier.",
        "Accuracy was improved by removing all noun features from the dataset and using the top seventy five features from the remaining set ranked with information gain, resulting in 86.6% accuracy using the SVM classifier with a linear kernel.",
        "Table 3 displays the top twenty combined features.",
        "Rank Feature Rank Feature 1 http 11 investors 2 flight 12 would 3 talks 13 by 4 for 14 says 5 strike 15 profit 6 an 16 cabin 7 you 17 crew 8 I 18 via 9 that 19 at 10 action 20 since Table 1: Top 20 ranked word features for pilot study 6 1 if feature is present in a tweet, otherwise 0.",
        "Rank Feature Rank Feature 1 NNP EOL 11 VB PRP 2 VBD JJ 12 NN NNS 3 NNP VBD 13 IN PRP$ 4 NNP NN 14 BOL CD 5 BOL PRP 15 BOL JJS 6 VBD NNP 16 IN VBN 7 NNP CC 17 PRP$ JJ 8 TO NNP 18 PRP MD 9 NN RB 19 PRP$ VBG 10 RB JJ 20 CC VBP Table 2: Top 20 ranked POS bigram features for pilot study Rank Feature Rank Feature 1 http 11 TO NNP 2 NNP EOL 12 RB JJ 3 NNP VBD 13 that 4 VBD JJ 14 tells 5 NNP NN 15 way 6 BOL PRP 16 I 7 VBD NNP 17 would 8 NNP CC 18 you 9 for 19 NN RB 10 an 20 BOL JJS Table 3: Top 20 ranked combined features for pilot study 4.2 Full study 4.2.1 Results Using the SMO classifier, Weka's support vector machine implementation using a linear kernel, a hybrid feature set containing linguistic, custom and Twitter-specific features obtained 72% classification accuracy for the three categories.",
        "F-measures were highest for the business operations class, and lowest for the other class, which contained the most diversity.",
        "Examining Figure 2, it is clear that f-measures for the other class are almost zero.",
        "This indicates that tweets given this category may not be homogeneous enough to categorise using the features defined in Table 7.",
        "4.3 Two classes After the removal of the other class from the experiment, the same feature set obtained 86% classification accuracy between the two remaining classes.",
        "The distinguishing features consisted predominantly of pronouns (I, me, my), part-of-75 Entity BO CC Other Aer Lingus 174 138 44 Ryanair 58 212 52 AIB 69 29 43 BOI 208 85 40 C&C 45 14 15 Glanbia 276 39 46 Greencore 37 4 13 Kerry Group 158 10 36 Permanent TSB 160 54 20 Table 4: Tweets per entity by category: Majority agreement Entity CC+BO O-CC O-BO Aer Lingus 4 24 15 Ryanair 7 30 8 AIB 4 5 11 BOI 9 5 16 C&C 0 1 3 Glanbia 7 4 19 Greencore 0 0 2 Kerry Group 5 2 12 Permanent TSB 3 6 10 Table 5: Tweets per entity by category: Tied agreement speech bigrams including pairs of plural nouns, lines beginning with prepositions and function words (so, just, new, it).",
        "Business operations tweets were more likely to mention a user account or be a retweet, personal pronouns were more commonplace in customer comments and as observed in the pilot study, customer comments were more likely to begin with a preposition and business operations tweets were more likely to contain noun-noun compounds and pairs of coordinating conjunctions and nouns.",
        "4.4 Features Hashtags were slightly more common in business operations tweets, however the number of hashtags was not counted, simply whether at least one was present.",
        "Hashtags as a proportion of words might be a useful feature for further studies.",
        "Function words and POS tags were highly discrimina-tory, indicating that this classifier may be applicable to different topic areas.",
        "Weight (See Figure 3) was a distinguishing feature, with business operations tweets having higher weight scores, reflect-Figure 1: F-scores by category for pilot study Figure 2: F-scores by category for full study ing the tendency for these tweets to originate from Twitter accounts linked to news sources or influential industry experts.",
        "5 Results per sub-category To investigate whether the entity domain had a bearing on the results, we separated the data into three subsets, airlines, banks and food industry concerns.",
        "We performed the same feature selection as in previous experiments, calculating each feature type separately, removing proper nouns, hashtags and account names from the word n-grams, then combining and ranking the features using ten-fold cross validation and information gain.",
        "The SVM classifier reported similar results to the main study on the three class problem for each sub-domain, and for the two class problem results ranged between 86-87% accuracy, similar Number of followers Number following (retweets) Figure 3: Twitter weight metric 76 to the results on the mixed set 7 .",
        "Thus, we believe that the individual subdomains do not warrant different classifiers for the problem, indeed examining the top 20-ranked features for each sub-domain, there is a large degree of overlap, as seen in bold and italics in Table 6.",
        "Banks Airlines Food @ @ @ my NNP NNP PRP VBP i i i me BOL IN BOL IN PRP VBP PRP VBP VB PRP account DT NN BOL PRP NNP VBZ IN PRP HASHASH VB PRP the you IN PRP new me you PRP VBD know BOL RB NNP VBZ my RB JJ IN DT i know NNP NNP you PRP CC PRP VBD BOL PRP used my bank ISRT BOL CC DT NN it NNP CD NN PRP me NN NNP VBD PRP my CC PRP BOL IN RB RB ISRT i?m so CC NNP Table 6: Top twenty ranked features by Information Gain for three domains 6 Conclusions and future directions 6.1 Classification results We found that accurate categorization of our predefined tweet types was possible using shallow linguistic features.",
        "This was aided by Twitter specific metrics but these did not add significantly to the classification accuracy 8 .",
        "The lower score (72- 73%) in the three class categorization problem is due to the linguistic diversity of the other tweet category.",
        "6.2 Annotation and Mechanical Turk We found the definition of categorization criteria to be an important and challenging step when using Mechanical Turk for annotation.",
        "The high degree of annotator disagreement reflected this, however it is important to note that in many cases, tweets fit equally into two or more of our defined categories.",
        "The use of extra annotations 9 allowed for agreement to be reached in the majority of 7 The food subset was highly imbalanced however, containing only 43 customer comments and 313 business operations tweets, the other two subsets were relatively balanced.",
        "8 ca. 2% decrease in accuracy on removal.",
        "9 over the initial two annotators cases, however employing more evaluations could have also resulted in deadlock.",
        "Examples of ambiguous tweets included: Cheap marketing tactics.",
        "Well, if it ain't broke, why fix it!",
        "RT @Ryanair's summer ?14 schedule is now on sale!",
        "where a Twitter user has retweeted an official announcement and added their own comment.",
        "Another possible pitfall is that as Mechanical Turk is a US-based service and requires workers to have a US bank account in order to perform work, Turkers tend to be US-based, and therefore an annotation task concerning non-US business entities is perhaps more difficult without sufficient background awareness of the entities in question.",
        "Future experiments will apply the methodology developed here to a larger dataset of tweets, one candidate would be the dataset used in the RepLab 2013 evaluation series which contains 2,200 annotated tweets for 61 business entities in four domains.",
        "Acknowledgments The authors are grateful to Enterprise Ireland and the IDA for funding this research and CeADAR through their Technology Centre Programme.",
        "Rank Feature Rank Feature 1 @ 26 NNP PRP 2 i 27 NN PRP 3 PRP VBP 28 VBP PRP 4 my 29 when 5 BOL IN 30 if 6 me 31 don't 7 you 32 PRP MD 8 NNP NNP 33 they 9 IN PRP 34 like 10 VB PRP 35 PRP VB 11 PRP VBD 36 got 12 WEIGHT 37 CC NNP 13 so 38 but 14 NNP VBZ 39 RB IN 15 BOL PRP 40 RT 16 RB JJ 41 with 17 DT NN 42 PRP IN 18 BOL RB 43 a 19 it 44 NNS RB 20 PRP RB 45 CC PRP 21 RB RB 46 VBD PRP 22 IN DT 47 VBD DT 23 i?m 48 no 24 just 49 the 25 get 50 PRP$ NN Table 7: Top 50 ranked mixed features for main study 77 References"
      ]
    }
  ]
}
