{
  "info": {
    "authors": [
      "Rachel Rudinger",
      "Benjamin Van Durme"
    ],
    "book": "Events: Definition, Detection, Coreference, and Representation",
    "id": "acl-W14-2908",
    "title": "Is the Stanford Dependency Representation Semantic?",
    "url": "https://aclweb.org/anthology/W14-2908",
    "year": 2014
  },
  "references": [
    "acl-H05-1049",
    "acl-P03-1054",
    "acl-W07-1427"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 54?58, Baltimore, Maryland, USA, June 22-27, 2014. c?2014 Association for Computational Linguistics Is the Stanford Dependency Representation Semantic?",
        "Abstract",
        "The Stanford Dependencies are a deep syntactic representation that are widely used for semantic tasks, like Recognizing Textual Entailment.",
        "But do they capture all of the semantic information a meaning representation ought to convey?",
        "This paper explores this question by investigating the feasibility of mapping Stanford dependency parses to Hobbsian Logical Form, a practical, event-theoretic semantic rep-resentation, using only a set of deterministic rules.",
        "Although we find that such a mapping is possible in a large number of cases, we also find cases for which such a mapping seems to require information beyond what the Stanford Dependencies encode.",
        "These cases shed light on the kinds of semantic information that are and are not present in the Stanford Dependencies."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The Stanford dependency parser (De Marneffe et al., 2006) provides ?deep?",
        "syntactic analysis of natural language by layering a set of hand-written post-processing rules on top of Stanford's statistical constituency parser (Klein and Manning, 2003).",
        "Stanford dependency parses are commonly used as a semantic representation in natural language understanding and inference systems.",
        "1 For example, they have been used as a basic meaning representation for the Recognizing Textual Entailment task proposed by Dagan et al. (2005), such as by Haghighi et al. (2005) or MacCartney (2009) and in other inference systems (Chambers et al., 2007; MacCartney, 2009).",
        "Because of their popular use as a semantic rep-resentation, it is important to ask whether the Stanford Dependencies do, in fact, encode the kind of 1 Statement presented by Chris Manning at the *SEM 2013 Panel on Language Understanding http://nlpers.blogspot.com/2013/07/the-sem-2013-panel-on-language.html.",
        "information that ought to be present in a versatile semantic form.",
        "This paper explores this question by attempting to map the Stanford Dependencies into Hobbsian Logical Form (henceforth, HLF), a neo-Davidsonian semantic representation designed for practical use (Hobbs, 1985).",
        "Our approach is to layer a set of hand-written rules on top of the Stanford Dependencies to further transform the representation into HLFs.",
        "This approach is a natural extension of the Stanford Dependencies which are, themselves, derived from manually engineered post-processing routines.",
        "The aim of this paper is neither to demonstrate the semantic completeness of the Stanford Depen-dencies, nor to exhaustively enumerate their semantic deficiencies.",
        "Indeed, to do so would be to presuppose HLF as an entirely complete semantic representation, or, a perfect semantic standard against which to compare the Stanford Dependencies.",
        "We make no such claim.",
        "Rather, our intent is to provide a qualitative discussion of the Stanford Dependencies as a semantic resource through the lens of this HLF mapping task.",
        "It is only necessary that HLF capture some subset of important semantic phenomena to make this exercise meaningful.",
        "Our results indicate that in a number of cases, it is, in fact, possible to directly derive HLFs from Stanford dependency parses.",
        "At the same time, however, we also find difficult-to-map phenomena that reveal inherent limitations of the dependencies as a meaning representation.",
        "2 Background This section provides a brief overview of the HLF and Stanford dependency formalisms.",
        "2.1 Hobbsian Logical Form The key insight of event-theoretic semantic representations is the reification of events (Davidson, 1967), or, treating events as entities in the world.",
        "As a logical, first-order representation, Hobbsian 54 Logical Form (Hobbs, 1985) employs this approach by allowing for the reification of any predicate into an event variable.",
        "Specifically, for any predicate p(x 1 , ?",
        "?",
        "?",
        ", x n ), there is a corresponding predicate, p ?",
        "(E, x 1 , ?",
        "?",
        "?",
        ", x n ), where E refers to the predicate (or event) p(x 1 , ?",
        "?",
        "?",
        ", x n ).",
        "The reified predicates are related to their non-reified forms with the following axiom schema: (?x 1 ?",
        "?",
        "?x n )p(x 1 ?",
        "?",
        "?x n ) ?",
        "(?e)Exist(e) ?",
        "p ?",
        "(e, x 1 ?",
        "?",
        "?x n ) In HLF, ?A boy runs?",
        "would be represented as: (?e, x)Exist(e) ?",
        "run ?",
        "(e, x) ?",
        "boy(x) and the sentence ?A boy wants to build a boat quickly?",
        "(Hobbs, 1985) would be represented as: (?e 1 , e 2 , e 3 , x, y)Exist(e 1 ) ?",
        "want ?",
        "(e 1 , x, e 2 ) ?",
        "quick ?",
        "(e 2 , e 3 )?build ?",
        "(e 3 , x, y)?boy(x)?boat(y) 2.2 Stanford Dependencies A Stanford dependency parse is a set of triples consisting of two tokens (a governor and a depen-dent), and a labeled syntactic or semantic relation between the two tokens.",
        "Parses can be rendered as labeled, directed graphs, as in Figure 1.",
        "Note that this paper assumes the collapsed version of the Stanford Dependencies.",
        "2 Figure 1: Dependency parse of ?A boy wants to build a boat quickly.?",
        "3 Mapping to HLF We describe in this section our deterministic algorithm for mapping Stanford dependency parses to HLF.",
        "The algorithm proceeds in four stages: event 2 The collapsed version is more convenient for our pur-poses, but using the uncollapsed version would not significantly affect our results.",
        "extraction, argument identification, predicate-argument assignment, and formula construction.",
        "We demonstrate these steps on the above example sentence ?A boy wants to build a boat quickly.?",
        "3 The rule-based algorithm operates on the sentence level and is purely a function of the dependency parse or other trivially extractible informa-tion, such as capitalization.",
        "3.1 Event Extraction The first step is to identify the set of event predicates that will appear in the final HLF and assign an event variable to each.",
        "Most predicates are generated by a single token in the sentence (e.g., the main verb).",
        "For each token t in the sentence, an event (e i , p t ) (where e i is the event variable and p t is the predicate) is added to the set of events if any of the following conditions are met: 1. t is the dependent of the relation root, ccomp, xcomp, advcl, advmod, or partmod.",
        "2. t is the governor of the relation nsubj, dobj, ccomp, xcomp, xsubj, advcl, nsubjpass, or agent.",
        "Furthermore, an event (e i , p r ) is added for any triple (rel, gov, dep) where rel is prefixed with ?prep ?",
        "(e.g., prep to, prep from, prep by, etc.).",
        "Applying this step to our example sentence ?A boy wants to build a boat quickly.?",
        "yields the following set: (e 1 , wants), (e 2 , quickly), (e 3 , build) 3.2 Argument Identification Next, the set of entities that will serve as predicate arguments are identified.",
        "Crucially, this set will include some event variables generated in the previous step.",
        "For each token, t, an argument (x i , t) is added to the set of arguments if one of the following conditions is met: 1. t is the dependent of the relation nsubj, xsubj, dobj, ccomp, xcomp, nsubjpass, agent, or iobj.",
        "2. t is the governor of the relation advcl, advmod, or partmod.",
        "3 Hobbs (1985) uses the example sentence ?A boy wanted to build a boat quickly.?",
        "55 Applying this step to our example sentence, we get the following argument set: (x 1 , boat), (x 2 , build), (x 3 , boy) Notice that the token build has generated both an event predicate and an argument.",
        "This is because in our final HLF, build will be both an event predicate that takes the arguments boy and boat, as well as an argument to the intensional predicate want.",
        "3.3 Predicate-Argument Assignment In this stage, arguments are assigned to each predicate.",
        "p t .arg i denotes the i th argument of predicate p t and arg(t) denotes the argument associated with token t. For example, arg(boy) = x 2 and arg(quickly) = e 3 .",
        "We also say that if the token t 1 governs t 2 by some relation, e.g. nsubj, then t 1 nsubj-governs t 2 , or t 2 nsubj-depends on t 1 .",
        "Note that arg i refers to any slot past arg 2 .",
        "Arguments are assigned as follows.",
        "For each predicate p t (corresponding to token t): 1.",
        "If there is a token t ?",
        "such that t nsubj-, xsubj-, or agent-governs t ?",
        ", then p t .arg 1 = arg(t ?",
        ").",
        "2.",
        "If there is a token t ?",
        "such that t dobj-governs t ?",
        ", then p t .arg 2 = arg(t ?",
        ").",
        "3.",
        "If there is a token t ?",
        "such that t nsubjpass-governs t ?",
        ", then p t .arg i = arg(t ?",
        ").",
        "4.",
        "If there is a token t ?",
        "such that t partmod-depends on t ?",
        ", then p t .arg 2 = arg(t ?",
        ").",
        "5.",
        "If there is a token t ?",
        "such that t iobj-governs t ?",
        ", then p t .arg i = arg(t ?",
        ").",
        "6.",
        "If there is a token t ?",
        "such that t ccomp-or xcomp-governs t ?",
        ", then p t .arg i = arg(t ? )",
        "(a) UNLESS there is a token t ??",
        "such that t ?",
        "advmod-governs t ??",
        ", in which case p t .arg i = arg(t ??",
        ").",
        "7.",
        "If there is a token t ?",
        "such that t advmod-or advcl-depends on t ?",
        ", then p t .arg i = arg(t ?",
        ").",
        "And for each p r generated from relation (rel, gov, dep) (i.e. all of the ?prep ?",
        "relations): 1. p r .arg 1 = arg(gov) 2. p r .arg i = arg(dep) After running this stage on our example sen-tence, the predicate-argument assignments are as follows: wants(x 3 , e 2 ), build(x 3 , x 1 ), quickly(e 3 ) Each predicate can be directly replaced with its reified forms (i.e., p ?",
        "): wants ?",
        "(e 1 , x 3 , e 2 ),build ?",
        "(e 3 , x 3 , x 1 ), quickly ?",
        "(e 2 , e 3 ) Two kinds of non-eventive predicates still need to be formed.",
        "First, every entity (x i , t) that is neither a reified event nor a proper noun, e.g., (x 3 , boy), generates a predicate of the form t(x i ).",
        "Second, we generate Hobbs's Exist predicate, which identifies which event actually occurs in the ?real world.?",
        "This is simply the event generated by the dependent of the root relation.",
        "3.4 Formula Construction In this stage, the final HLF is pieced together.",
        "We join all of the predicates formed above with the and conjunction, and existentially quantify over every variable found therein.",
        "For our example sen-tence, the resulting HLF is: A boy wants to build a boat quickly.",
        "(?e 1 , e 2 , e 3 , x 1 , x 3 )[Exist(e 1 ) ?",
        "boat(x 1 ) ?",
        "boy(x 3 ) ?",
        "wants ?",
        "(e 1 , x 3 , e 2 ) ?",
        "build ?",
        "(e 3 , x 3 , x 1 ) ?",
        "quickly ?",
        "(e 2 , e 3 )] 4 Analysis of Results This section discusses semantic phenomena that our mapping does and does not capture, providing a lens for assessing the usefulness of the Stanford Dependencies as a semantic resource.",
        "4.1 Successes Formulas 1-7 are correct HLFs that our mapping rules successfully generate.",
        "They illustrate the diversity of semantic information that is easily recoverable from Stanford dependency parses.",
        "Formulas 1-2 show successful parses in simple transitive sentences with active/passive alter-nations, and Formula 3 demonstrates success in parsing ditransitives.",
        "Also easily recovered from the dependency structures are semantic parses of sentences with adverbs (Formula 4) and reporting verbs (Formula 5).",
        "Lest it appear that these phenomena may only be handled in isolation, Equations 6-7 show successful parses for sentences 56 with arbitrary combinations of the above phenomena.",
        "A boy builds a boat.",
        "(?e 1 , x 1 , x 2 )[Exist(e 1 ) ?",
        "boy(x 2 ) ?",
        "boat(x 1 ) ?",
        "builds ?",
        "(e 1 , x 2 , x 1 )] (1) A boat was built by a boy.",
        "(?e 1 , x 1 , x 2 )[Exist(e 1 ) ?",
        "boat(x 2 ) ?",
        "boy(x 1 ) ?",
        "built ?",
        "(e 1 , x 1 , x 2 )] (2) John gave Mary a boat.",
        "(?e 1 , x 1 )[Exist(e 1 ) ?",
        "boat(x 1 ) ?",
        "gave ?",
        "(e 1 , John, x 1 ,Mary)] (3) John built a boat quickly.",
        "OR John quickly built a boat.",
        "(?e 1 , e 2 , x 1 )[Exist(e 1 ) ?",
        "boat(x 1 ) ?",
        "quickly(e 2 , e 1 ) ?",
        "built ?",
        "(e 1 , John, x 1 )] (4) John told Mary that a boy built a boat.",
        "(?e 1 , e 2 , x 1 , x 4 )[Exist(e 1 )?boy(x 1 )?boat(x 4 )?",
        "built ?",
        "(e 2 , x 1 , x 4 ) ?",
        "told ?",
        "(e 1 , John,Mary, e 2 )] (5) John told Mary that Sue told Joe that Adam loves Eve.",
        "(?e 1 , e 2 , e 3 )[Exist(e 1 )?told ?",
        "(e 2 , Sue, Joe, e 3 )?",
        "loves ?",
        "(e 3 , Adam,Eve) ?",
        "told ?",
        "(e 1 , John,Mary, e 2 )] (6) John was told by Mary that Sue wants Joe to build a boat quickly.",
        "(?e 1 , e 2 , e 3 , e 4 , x 7 )[Exist(e 1 ) ?",
        "boat(x 7 ) ?",
        "build ?",
        "(e 2 , Joe, x 7 )?told ?",
        "(e 1 ,Mary, John, e 4 )?",
        "wants ?",
        "(e 4 , Sue, e 3 ) ?",
        "quickly ?",
        "(e 3 , e 2 )] (7) 4.2 Limitations Though our mapping rules enable us to directly extract deep semantic information directly from the Stanford dependency parses in the above cases, there are a number of difficulties with this approach that shed light on inherent limitations of the Stanford Dependencies as a semantic resource.",
        "A major such limitation arises in cases of event nominalizations.",
        "Because dependency parses are syntax-based, their structures do not distinguish between eventive noun phrases like ?the bombing of the city?",
        "and non-eventive ones like ?the mother of the child?",
        "; such a distinction, however, would be found in the corresponding HLFs.",
        "Certain syntactic alternations also prove problematic.",
        "For example, the dependency structure does not recognize that ?window?",
        "takes the same semantic role in the sentences ?John broke the mir-ror.?",
        "and ?The mirror broke.?",
        "The use of additional semantic resources, like PropBank (Palmer et al., 2005), would be necessary to determine this.",
        "Prepositional phrases present another problem for our mapping task, as the Stanford dependencies will typically not distinguish between PPs indicating arguments and adjuncts.",
        "For exam-ple, ?Mary stuffed envelopes with coupons?",
        "and ?Mary stuffed envelopes with John?",
        "have identical dependency structures, yet ?coupons?",
        "and ?John?",
        "are (hopefully for John) taking on different semantic roles.",
        "This is, in fact, a prime example of how Stanford dependency parses may resolve syntactic ambiguity without resolving semantic ambiguity.",
        "Of course, one might manage more HLF coverage by adding more rules to our system, but the limitations discussed here are fundamental.",
        "If two sentences have different semantic interpretations but identical dependency structures, then there can be no deterministic mapping rule (based on dependency structure alone) that yields this distinction.",
        "5 Conclusion We have presented here our attempt to map the Stanford Dependencies to HLF via a second layer of hand-written rules.",
        "That our mapping rules, which are purely a function of dependency struc-ture, succeed in producing correct HLFs in some cases is good evidence that the Stanford Dependencies do contain some practical level of semantic information.",
        "Nevertheless, we were also able to quickly identify aspects of meaning that the Stanford Dependencies did not capture.",
        "Our argument does not require that HLF be an optimal representation, only that it capture worthwhile aspects of semantics and that it not be readily derived from the Stanford representation.",
        "This is enough to conclude that the Stanford Dependencies are not complete as a meaning representation.",
        "While not surprising (as they are intended as a syntactic representation), we hope this short study will help further discussion on what the community wants or needs in a meaning representation: what gaps are acceptable, if any, and whether a more ?complete?",
        "representation is needed.",
        "Acknowledgments This material is partially based on research sponsored by the NSF under grant IIS-1249516 and DARPA under agreement number FA8750-13-2-0017 (the DEFT program).",
        "57 References"
      ]
    }
  ]
}
