{
  "info": {
    "authors": [
      "Markus Freitag",
      "Stephan Peitz",
      "Joern Wuebker",
      "Hermann Ney",
      "Matthias Huck",
      "Rico Sennrich",
      "Nadir Durrani",
      "Maria Nadejde",
      "Philip Williams",
      "Philipp Koehn",
      "Teresa Herrmann",
      "Eunah Cho",
      "Alex Waibel"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W14-3310",
    "title": "EU-BRIDGE MT: Combined Machine Translation",
    "url": "https://aclweb.org/anthology/W14-3310",
    "year": 2014
  },
  "references": [
    "acl-C08-1098",
    "acl-D07-1091",
    "acl-D08-1089",
    "acl-D09-1022",
    "acl-J03-1002",
    "acl-N04-1022",
    "acl-N07-1051",
    "acl-P03-1021",
    "acl-P06-1055",
    "acl-P07-1019",
    "acl-P07-2045",
    "acl-P10-2041",
    "acl-P12-1031",
    "acl-W05-0909",
    "acl-W10-1738",
    "acl-W11-2123",
    "acl-W11-2124"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105?113, Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics EU-BRIDGE MT: Combined Machine Translation ?",
        "Markus Freitag, ?",
        "Stephan Peitz, ?",
        "Joern Wuebker, ?",
        "Hermann Ney, ?",
        "Matthias Huck, ?",
        "Rico Sennrich, ?",
        "Nadir Durrani, ?",
        "Maria Nadejde, ?",
        "Philip Williams, ?",
        "Philipp Koehn, ?",
        "Teresa Herrmann, ?",
        "Eunah Cho, ?",
        "Alex Waibel ?",
        "RWTH Aachen University, Aachen, Germany ?",
        "University of Edinburgh, Edinburgh, Scotland ?",
        "Karlsruhe Institute of Technology, Karlsruhe, Germany ?",
        "{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de ?",
        "{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk ?",
        "v1rsennr@staffmail.ed.ac.uk ?",
        "Abstract",
        "This paper describes one of the collaborative efforts within EU-BRIDGE to further advance the state of the art in machine translation between two European language pairs, German?English and English?German.",
        "Three research institutes involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014).",
        "We combined up to nine different machine translation engines via system combination.",
        "RWTH Aachen University, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input.",
        "We devoted special attention to building syntax-based systems and combining them with the phrase-based ones.",
        "The joint setups yield empirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT newstest2013 test set compared to the best single systems."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "EU-BRIDGE 1 is a European research project which is aimed at developing innovative speech translation technology.",
        "This paper describes a 1 http://www.eu-bridge.eu joint WMT submission of three EU-BRIDGE project partners.",
        "RWTH Aachen University (RWTH), the University of Edinburgh (UEDIN) and Karlsruhe Institute of Technology (KIT) all provided several individual systems which were combined by means of the RWTH Aachen system combination approach (Freitag et al., 2014).",
        "As distinguished from our EU-BRIDGE joint submission to the IWSLT 2013 evaluation campaign (Fre- itag et al., 2013), we particularly focused on translation of news text (instead of talks) for WMT.",
        "Be-sides, we put an emphasis on engineering syntax-based systems in order to combine them with our more established phrase-based engines.",
        "We built combined system setups for translation from German to English as well as from English to German.",
        "This paper gives some insight into the technology behind the system combination framework and the combined engines which have been used to produce the joint EU-BRIDGE submission to the WMT 2014 translation task.",
        "The remainder of the paper is structured as fol-lows: We first describe the individual systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), and Karlsruhe Institute of Technology (Section 4).",
        "We then present the techniques for machine translation system combination in Section 5.",
        "Experimental results are given in Section 6.",
        "We finally conclude the paper with Section 7.",
        "2 RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH's publicly available translation toolkit Jane (Vilar 105 et al., 2010; Wuebker et al., 2012).",
        "The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets.",
        "RWTH used BLEU as optimization objective.",
        "Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used.",
        "All RWTH systems include the standard set of models provided by Jane.",
        "Both systems have been augmented with a hierarchical orientation model (Galley and Man-ning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013).",
        "The phrase-based system (RWTH scss) has been further improved by maximum expected BLEU training similar to (He and Deng, 2012).",
        "The latter has been performed on a selection from the News Commen-tary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011).",
        "3 University of Edinburgh UEDIN contributed phrase-based and syntax-based systems to both the German?English and the English?German joint submission.",
        "3.1 Phrase-based Systems UEDIN's phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b).",
        "The features include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA ++ align-ments, an interpolated Kneser-Ney smoothed 5- gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reorder-ing, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation op-tions, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic.",
        "UEDIN uses POS and morphological target sequence models built on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007).",
        "UEDIN has furthermore built OSM models over POS and morph sequences following Durrani et al. (2013c).",
        "The English?German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013).",
        "UEDIN has applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German?English system.",
        "The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences.",
        "UEDIN used newstest2013 as held-out test set.",
        "On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs.",
        "Furthermore, it learns OSM models over POS, morph and word classes.",
        "3.2 Syntax-based Systems UEDIN's syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004).",
        "The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012).",
        "Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five.",
        "Singleton hierarchical rules are dropped.",
        "The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model.",
        "UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model scoring during decoding.",
        "Model weights are optimized to maximize BLEU.",
        "2000 sentences from the newstest2008-2012 sets have been selected as a development set.",
        "The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrase-based system, and each contain less than 30 words for more rapid tuning.",
        "Decoding for the syntax-based systems is carried out with cube pruning using Moses?",
        "hierarchical decoder (Hoang et al., 2009).",
        "UEDIN's German?English syntax-based setup is a string-to-tree system with compound splitting 106 on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side.",
        "For English?German, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation.",
        "A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well.",
        "The English?German UEDIN GHKM system names in Table 3 denote: UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013).",
        "It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints.",
        "UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004).",
        "UEDIN GHKM S2T (Stanford): A string-to-tree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a).",
        "UEDIN GHKM S2T (Berkeley): A string-to-tree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008).",
        "UEDIN GHKM T2S (Berkeley): A tree-to-string system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006).",
        "UEDIN GHKM S2S (Berkeley): A string-to-string system.",
        "The extraction is GHKM-based with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels.",
        "The final grammar contains rules with a single generic non-terminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014).",
        "4 Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003).",
        "The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation model.",
        "The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German?English are used as monolingual training data for the different language models.",
        "Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively.",
        "Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German?English translation before training.",
        "In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011).",
        "The word alignment for German?English is generated using the GIZA ++ toolkit (Och and Ney, 2003).",
        "For English?German, KIT uses discriminative word alignment (Niehues and Vogel, 2008).",
        "Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007).",
        "Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in (Foster et al., 2006).",
        "In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order.",
        "The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model.",
        "In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied.",
        "Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified Kneser-Ney smoothing.",
        "Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999).",
        "The English?German system comprises language models based on fine-grained part-of-speech tags (Schmid and Laws, 2008).",
        "In addi-tion, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence.",
        "107 In total, the English?German system uses the following language models: two 4-gram word-based language models trained on the parallel data and the filtered Common Crawl data separately, two 5-gram POS-based language models trained on the same data as the word-based language mod-els, and a 4-gram cluster-based language model trained on 1,000 MKCLS word classes.",
        "The German?English system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010).",
        "Again, a 4-gram cluster-based language model trained on 1000 MKCLS word classes is applied.",
        "5 System Combination System combination is used to produce consensus translations from multiple hypotheses which are outputs of different translation engines.",
        "The consensus translations can be better in terms of translation quality than any of the individual hypotheses.",
        "To combine the engines of the project partners for the EU-BRIDGE joint setups, we apply a system combination implementation that has been developed at RWTH Aachen University.",
        "The implementation of RWTH's approach to machine translation system combination is described in (Freitag et al., 2014).",
        "This approach includes an enhanced alignment and reordering framework.",
        "Alignments between the system outputs are learned using METEOR (Banerjee and Lavie, 2005).",
        "A confusion network is then built using one of the hypotheses as ?primary?",
        "hypothesis.",
        "We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible confusion networks into a single lattice.",
        "Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models, e.g. a special n-gram language model which is learned on the input hypotheses.",
        "Scaling factors of the models are optimized using the Minimum Error Rate Training algorithm.",
        "The translation with the best total score within the lattice is selected as consensus translation.",
        "6 Results In this section, we present our experimental results on the two translation tasks, German?English and English?German.",
        "The weights of the individual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012.",
        "System combination weights are either optimized on newstest2011 or newstest2012.",
        "We kept newstest2013 as an unseen test set which has not been used for tuning the system combination or any of the individual systems.",
        "6.1 German?English The automatic scores of all individual systems as well as of our final system combination submission are given in Table 1.",
        "KIT, UEDIN and RWTH are each providing one individual phrase-based system output.",
        "RWTH (hiero) and UEDIN (GHKM) are providing additional systems based on the hierarchical translation model and a string-to-tree syntax model.",
        "The pairwise difference of the single system performances is up to 1.3 points in BLEU and 2.5 points in TER.",
        "For German?English, our system combination parameters are optimized on newstest2012.",
        "System combination gives us a gain of 1.6 points in BLEU and 1.0 points in TER for newstest2013 compared to the best single system.",
        "In Table 2 the pairwise BLEU scores for all individual systems as well as for the system combination output are given.",
        "The pairwise BLEU score of both RWTH systems (taking one as hypothesis and the other one as reference) is the highest for all pairs of individual system outputs.",
        "A high BLEU score means similar hypotheses.",
        "The syntax-based system of UEDIN and RWTH scss differ mostly, which can be observed from the fact of the lowest pairwise BLEU score.",
        "Furthermore, we can see that better performing individual systems have higher BLEU scores when evaluating against the system combination output.",
        "In Figure 1 system combination output is compared to the best single system KIT.",
        "We distribute the sentence-level BLEU scores of all sentences of newstest2013.",
        "To allow for sentence-wise evalu-ation, all bi-, tri-, and four-gram counts are initialized with 1 instead of 0.",
        "Many sentences have been improved by system combination.",
        "Neverthe-less, some sentences fall off in quality compared to the individual system output of KIT.",
        "6.2 English?German The results of all English?German system setups are given in Table 3.",
        "For the English?German translation task, only UEDIN and KIT are con-108 system newstest2011 newstest2012 newstest2013 BLEU TER BLEU TER BLEU TER KIT 25.0 57.6 25.2 57.4 27.5 54.4 UEDIN 23.9 59.2 24.7 58.3 27.4 55.0 RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0 RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9 UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9 syscom 25.6 57.1 26.4 56.5 29.1 53.4 Table 1: Results for the German?English translation task.",
        "The system combination is tuned on news-test2012, newstest2013 is used as held-out test set for all individual systems and system combination.",
        "Bold font indicates system combination results that are significantly better than the best single system with p < 0.05.",
        "KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscom KIT 59.07 57.60 57.91 55.62 77.68 UEDIN 59.17 56.96 57.84 59.89 72.89 RWTH scss 57.64 56.90 64.94 53.10 71.16 RWTH hiero 57.98 57.80 64.97 55.73 70.87 UEDIN S2T 55.75 59.95 53.19 55.82 65.35 syscom 77.76 72.83 71.17 70.85 65.24 Table 2: Cross BLEU scores for the German?English newstest2013 test set.",
        "(Pairwise BLEU scores: each entry is taking the horizontal system as hypothesis and the other one as reference.)",
        "system newstest2011 newstest2012 newstest2013 BLEU TER BLEU TER BLEU TER UEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7 UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3 UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8 UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9 UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2 UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8 UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8 UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3 KIT 17.1 67.0 17.8 64.8 20.2 62.2 syscom 18.4 65.0 18.7 63.4 21.3 60.6 Table 3: Results for the English?German translation task.",
        "The system combination is tuned on news-test2011, newstest2013 is used as held-out test set for all individual systems and system combination.",
        "Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than the best single system with p< 0.05.",
        "Italic font indicates system combination results that are significantly better than the best single system with p < 0.1. tributing individual systems.",
        "KIT is providing a phrase-based system output, UEDIN is providing two phrase-based system outputs and six syntax-based ones (GHKM).",
        "For English?German, our system combination parameters are optimized on newstest2011.",
        "Combining all nine different system outputs yields an improvement of 0.5 points in BLEU and 1.7 points in TER over the best single system performance.",
        "In Table 4 the cross BLEU scores for all English?German systems are given.",
        "The individual system of KIT and the syntax-based ParZu system of UEDIN have the lowest BLEU score when scored against each other.",
        "Both approaches are quite different and both are coming from different institutes.",
        "In contrast, both phrase-based systems pbt 1 and pbt 2 from UEDIN are very similar and hence have a high pairwise BLEU score.",
        "109 pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscom pbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12 pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75 ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39 BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08 Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51 S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81 T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13 S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46 KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33 syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27 Table 4: Cross BLEU scores for the German?English newstest2013 test set.",
        "(Pairwise BLEU scores: each entry is taking the horizontal system as reference and the other one as hypothesis.)",
        "0 50 100 150 200 250 300 350 400 0 20 40 60 80 100 amo unt sent ence s sBLEU bettersameworse Figure 1: Sentence distribution for the German?English newstest2013 test set comparing system combination output against the best individual system.",
        "As for the German?English translation direction, the best performing individual system outputs are also having the highest BLEU scores when evaluated against the final system combination output.",
        "In Figure 2 system combination output is compared to the best single system pbt 2.",
        "We distribute the sentence-level BLEU scores of all sentences of newstest2013.",
        "Many sentences have been improved by system combination.",
        "But there is still room for improvement as some sentences are still better in terms of sentence-level BLEU in the individual best system pbt 2.",
        "7 Conclusion We achieved significantly better translation performance with gains of up to +1.6 points in BLEU and -1.0 points in TER by combining up to nine different machine translation systems.",
        "Three different research institutes (RWTH Aachen Univer-sity, University of Edinburgh, Karlsruhe Institute of Technology) provided machine translation engines based on different approaches like phrase-0 50 100 150 200 250 300 350 400 0 20 40 60 80 100 amo unt sent ence s sBLEU bettersameworse Figure 2: Sentence distribution for the English?German newstest2013 test set comparing system combination output against the best individual system.",
        "based, hierarchical phrase-based, and syntax-based.",
        "For English?German, we included six different syntax-based systems, which were combined to our final combined translation.",
        "The automatic scores of all submitted system outputs for the actual 2014 evaluation set are presented on the WMT submission page.",
        "2 Our joint submission is the best submission in terms of BLEU and TER for both translation directions German?English and English?German without adding any new data.",
        "Acknowledgements The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement n o 287658.",
        "Rico Sennrich has received funding from the Swiss National Science Foundation under grant P2ZHP1 148717.",
        "2 http://matrix.statmt.org/ 110 References"
      ]
    }
  ]
}
