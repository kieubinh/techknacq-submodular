{
  "info": {
    "authors": [
      "Michael Denkowski",
      "Alon Lavie"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W14-3348",
    "title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
    "url": "https://aclweb.org/anthology/W14-3348",
    "year": 2014
  },
  "references": [
    "acl-P02-1040",
    "acl-P05-1074",
    "acl-P07-2045",
    "acl-W09-0441",
    "acl-W11-2103",
    "acl-W11-2106",
    "acl-W11-2107"
  ],
  "sections": [
    {
      "text": [
        "Abstract",
        "This paper describes Meteor Universal, released for the 2014 ACL Workshop on Statistical Machine Translation.",
        "Meteor Universal brings language specific evaluation to previously unsupported target languages by (1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions.",
        "Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14)."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach?a?cek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia).",
        "While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world's 7,000+ languages lack the prerequisites for building advanced metrics.",
        "Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality.",
        "Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a universal metric parameter set that generalizes across languages.",
        "Given only the bitext used to build a standard phrase-based translation system, Meteor Universal learns a paraphrase table and function word list, two of the most consistently beneficial language specific resources employed in versions of Meteor.",
        "Whereas previous versions of Meteor require human ranking judgments in the target language to learn parameters, Meteor Universal uses a single parameter set learned from pooling judgments from several languages.",
        "This universal parameter set captures general preferences shown by human evaluators across languages.",
        "We show this approach to significantly outperform baseline BLEU for two new languages, Russian and Hindi.",
        "The following sections review Meteor's scoring function (?2), describe the automatic extraction of language specific resources (?3), discuss training of the universal parameter set (?4), report experimental results (?5), describe released software (?6), and conclude (?7).",
        "2 Meteor Scoring Meteor evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores.",
        "For a hypothesis-reference pair, the space of possible alignments is constructed by exhaustively identifying all possible matches between the sentences according to the following matchers: Exact: Match words if their surface forms are identical.",
        "Stem: Stem words using a language appropriate Snowball Stemmer (Porter, 2001) and match if the stems are identical.",
        "Synonym: Match words if they share membership in any synonym set according to the WordNet database (Miller and Fellbaum, 2007).",
        "Paraphrase: Match phrases if they are listed as 376 paraphrases in a language appropriate paraphrase table (described in ?3.2).",
        "All matches are generalized to phrase matches with a span in each sentence.",
        "Any word occurring within the span is considered covered by the match.",
        "The final alignment is then resolved as the largest subset of all matches meeting the following criteria in order of importance: 1.",
        "Require each word in each sentence to be covered by zero or one matches.",
        "2.",
        "Maximize the number of covered words across both sentences.",
        "3.",
        "Minimize the number of chunks, where a chunk is defined as a series of matches that is contiguous and identically ordered in both sentences.",
        "4.",
        "Minimize the sum of absolute distances between match start indices in the two sentences.",
        "(Break ties by preferring to align phrases that occur at similar positions in both sentences.)",
        "Alignment resolution is conducted as a beam search using a heuristic based on the above criteria.",
        "The Meteor score for an aligned sentence pair is calculated as follows.",
        "Content and function words are identified in the hypothesis (h c , h f ) and reference (r c , r f ) according to a function word list (described in ?3.1).",
        "For each of the matchers (m i ), count the number of content and function words covered by matches of this type in the hypothesis (m i (h c ), m i (h f )) and reference (m i (r c ), m i (r f )).",
        "Calculate weighted precision and recall using matcher weights (w i ...w n ) and content-function word weight (?",
        "): P = ?",
        "i w i ?",
        "(?",
        "?m i (h c ) + (1?",
        "?)",
        "?m i (h f )) ?",
        "?",
        "|h c |+ (1?",
        "?)",
        "?",
        "|h f | R = ?",
        "i w i ?",
        "(?",
        "?m i (r c ) + (1?",
        "?)",
        "?m i (r f )) ?",
        "?",
        "|r c |+ (1?",
        "?)",
        "?",
        "|r f | The parameterized harmonic mean of P and R (van Rijsbergen, 1979) is then calculated: F mean = P ?R ?",
        "?",
        "P + (1?",
        "?)",
        "?R To account for gaps and differences in word order, a fragmentation penalty is calculated using the total number of matched words (m, averaged over hypothesis and reference) and number of chunks (ch): Pen = ?",
        "?",
        "( ch m ) ?",
        "The Meteor score is then calculated: Score = (1?",
        "Pen) ?",
        "F mean The parameters?, ?, ?, ?, andw i ...w n are tuned to maximize correlation with human judgments.",
        "3 Language Specific Resources Meteor uses language specific resources to dramatically improve evaluation accuracy.",
        "While some resources such as WordNet and the Snowball stemmers are limited to one or a few languages, other resources can be learned from data for any language.",
        "Meteor Universal uses the same bitext used to build statistical translation systems to learn function words and paraphrases.",
        "Used in conjunction with the universal parameter set, these resources bring language specific evaluation to new target languages.",
        "3.1 Function Word Lists The function word list is used to discriminate between content and function words in the target language.",
        "Meteor Universal counts words in the target side of the training bitext and considers any word with relative frequency above 10 ?3 to be a function word.",
        "This list is used only during the scoring stage of evaluation, where the tunable ?",
        "parameter controls the relative weight of content versus function words.",
        "When tuned to match human judgments, this parameter usually reflects a greater importance for content words.",
        "3.2 Paraphrase Tables Paraphrase tables allow many-to-many matches that can encapsulate any local language phenom-ena, including morphology, synonymy, and true paraphrasing.",
        "Identifying these matches allows far more sophisticated evaluation than is possible with simple surface form matches.",
        "In Meteor Uni-versal, paraphrases act as the catch-all for non-exact matches.",
        "Paraphrases are automatically extracted from the training bitext using the translation pivot approach (Bannard and Callison-Burch, 2005).",
        "First, a standard phrase table is learned from the bitext (Koehn et al., 2003).",
        "Paraphrase extraction then proceeds as follows.",
        "For each target language phrase (e 1 ) in the table, find each 377 source phrase f that e 1 translates.",
        "Each alternate phrase (e 2 6= e 1 ) that translates f is considered a paraphrase with probability P (f |e 1 ) ?",
        "P (e 2 |f).",
        "The total probability of e 2 being a paraphrase of e 1 is the sum over all possible pivot phrases f : P (e 2 |e 1 ) = ?",
        "f P (f |e 1 ) ?",
        "P (e 2 |f) To improve paraphrase precision, we apply several language independent pruning techniques.",
        "The following are applied to each paraphrase instance (e 1 , f , e 2 ): ?",
        "Discard instances with very low probability (P (f |e 1 ) ?",
        "P (e 2 |f) < 0.001).",
        "?",
        "Discard instances where e 1 , f , or e 2 contain punctuation characters.",
        "?",
        "Discard instances where e 1 , f , or e 2 contain only function words (relative frequency above 10 ?3 in the bitext).",
        "The following are applied to each final paraphrase (e 1 , e 2 ) after summing over all instances: ?",
        "Discard paraphrases with very low probability (P (e 2 |e 1 ) < 0.01).",
        "?",
        "Discard paraphrases where e 2 is a sub-phrase of e 1 .",
        "This constitutes the full Meteor paraphrasing pipeline that has been used to build tables for fully supported languages (Denkowski and Lavie, 2011).",
        "Paraphrases for new languages have the added advantage of being extracted from the same bitext that MT systems use for phrase extraction, resulting in ideal paraphrase coverage for evaluated systems.",
        "4 Universal Parameter Set Traditionally, building a version of Meteor for a new target language has required a set of human-scored machine translations, most frequently in the form of WMT rankings.",
        "The general lack of availability of these judgments has severely limited the number of languages for which Meteor versions could be trained.",
        "Meteor Universal addresses this problem with the introduction of a ?universal?",
        "parameter set that captures general human preferences that apply to all languages for Direction Judgments cs-en 11,021 de-en 11,934 es-en 9,796 fr-en 11,594 en-cs 18,805 en-de 14,553 en-es 11,834 en-fr 11,562 Total 101,099 Table 1: Binary ranking judgments per language direction used to learn parameters for Meteor Universal which judgment data does exist.",
        "We learn this parameter set by pooling over 100,000 binary ranking judgments from WMT12 (Callison-Burch et al., 2012) that cover 8 language directions (de- tails in Table 1).",
        "Data for each language is scored using the same resources (function word list and paraphrase table only) and scoring parameters are tuned to maximize agreement (Kendall's ? )",
        "over all judgments from all languages, leading to a single parameter set.",
        "The universal parameter set encodes the following general human preferences: ?",
        "Prefer recall over precision.",
        "?",
        "Prefer word choice over word order.",
        "?",
        "Prefer correct translations of content words over function words.",
        "?",
        "Prefer exact matches over paraphrase matches, while still giving significant credit to paraphrases.",
        "Table 2 compares the universal parameters to those learned for specific languages in previous versions of Meteor.",
        "Notably, the universal parameter set is more balanced, showing a normalizing effect from generalizing across several language directions.",
        "5 Experiments We evaluate the Universal version of Meteor against full language dedicated versions of Meteor and baseline BLEU on the WMT13 rankings.",
        "Results for English, Czech, German, Spanish, and French are biased in favor of Meteor Universal since rankings for these target languages are included in the training data while Russian constitutes a true held out test.",
        "We also report the results of the WMT14 Hindi evaluation task.",
        "Shown 378 Language ?",
        "?",
        "?",
        "?",
        "w exact w stem w syn w par English 0.85 0.20 0.60 0.75 1.00 0.60 0.80 0.60 Czech 0.95 0.20 0.60 0.80 1.00 ?",
        "?",
        "0.40 German 0.95 1.00 0.55 0.55 1.00 0.80 ?",
        "0.20 Spanish 0.65 1.30 0.50 0.80 1.00 0.80 ?",
        "0.60 French 0.90 1.40 0.60 0.65 1.00 0.20 ?",
        "0.40 Universal 0.70 1.40 0.30 0.70 1.00 ?",
        "?",
        "0.60 Table 2: Comparison of parameters for language specific and universal versions of Meteor.",
        "WMT13 ?",
        "M-Full M-Universal BLEU English 0.214 0.206 0.124 Czech 0.092 0.085 0.044 German 0.163 0.157 0.097 Spanish 0.106 0.101 0.068 French 0.150 0.137 0.099 Russian ?",
        "0.128 0.068 WMT14 ?",
        "M-Full M-Universal BLEU Hindi ?",
        "0.264 0.227 Table 3: Sentence-level correlation with human rankings (Kendall's ? )",
        "for Meteor (language specific versions), Meteor Universal, and BLEU in Table 3, Meteor Universal significantly outperforms baseline BLEU in all cases while suffering only slight degradation compared to versions of Meteor tuned for individual languages.",
        "For Russian, correlation is nearly double that of BLEU.",
        "This provides substantial evidence that Meteor Universal will further generalize, bringing improved evaluation accuracy to new target languages currently limited to baseline language independent metrics.",
        "For the WMT14 evaluation, we use the traditional language specific versions of Meteor for all language directions except Hindi.",
        "This includes Russian, for which additional language specific resources (a Snowball word stemmer) help significantly.",
        "For Hindi, we use the release version of Meteor Universal to extract linguistic resources from the constrained training bitext provided for the shared translation task.",
        "These resources are used with the universal parameter set to score all system outputs for the English?Hindi direction.",
        "6 Software Meteor Universal is included in Meteor version 1.5 which is publicly released for WMT14.",
        "Meteor 1.5 can be downloaded from the official webpage 1 and a full tutorial for Meteor Universal is available online.",
        "2 Building a version of Meteor for a new language requires a training bitext (corpus.f, corpus.e) and a standard Moses format phrase table (phrase-table.gz) (Koehn et al., 2007).",
        "To extract linguistic resources for Meteor, run the new language script: $ python scripts/new_language.py out \\ corpus.f corpus.e phrase-table.gz To use the resulting files to score translations with Meteor, use the new language option: $ java -jar meteor-* .jar test ref -new \\ out/meteor-files Meteor 1.5, including Meteor Universal, is free software released under the terms of the GNU Lesser General Public License.",
        "7 Conclusion This paper describes Meteor Universal, a version of the Meteor metric that brings language specific evaluation to any target language using the same resources used to build statistical translation systems.",
        "Held out tests show Meteor Universal to significantly outperform baseline BLEU on WMT13 Russian and WMT14 Hindi.",
        "Meteor version 1.5 is freely available open source software.",
        "Acknowledgements This work is supported in part by the National Science Foundation under grant IIS-0915327, by the Qatar National Research Fund (a member of the Qatar Foundation) under grant NPRP 09-1140-1- 177, and by the NSF-sponsored XSEDE program under grant TG-CCR110017.",
        "1 http://www.cs.cmu.edu/~alavie/METEOR/ 2 http://www.cs.cmu.edu/~mdenkows/meteor-universal.html 379 References"
      ]
    }
  ]
}
