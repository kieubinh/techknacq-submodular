{
  "info": {
    "authors": [
      "Nadi Tomeh",
      "Nizar Habash",
      "Ramy Eskander",
      "Joseph Le Roux"
    ],
    "book": "ANLP",
    "id": "acl-W14-3614",
    "title": "A Pipeline Approach to Supervised Error Correction for the QALB-2014 Shared Task",
    "url": "https://aclweb.org/anthology/W14-3614",
    "year": 2014
  },
  "references": [
    "acl-N03-1017",
    "acl-P06-1032",
    "acl-P07-2045",
    "acl-P08-2015",
    "acl-P11-1088",
    "acl-P14-2027"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 114?120, October 25, 2014, Doha, Qatar.",
        "c?2014 Association for Computational Linguistics A Pipeline Approach to Supervised Error Correction for the QALB-2014 Shared Task Nadi Tomeh ?",
        "Nizar Habash ?",
        "Ramy Eskander ?",
        "Joseph Le Roux ?",
        "{nadi.tomeh,leroux}@lipn.univ-paris13.fr ?",
        "?",
        "Universit?e Paris 13, Sorbonne Paris Cit?e, LIPN, Villetaneuse, France ?",
        "Computer Science Department, New York University Abu Dhabi ?",
        "Center for Computational Learning Systems, Columbia University",
        "Abstract",
        "This paper describes our submission to the ANLP-2014 shared task on automatic Arabic error correction.",
        "We present a pipeline approach integrating an error detection model, a combination of character- and word-level translation mod-els, a reranking model and a punctuation insertion model.",
        "We achieve an F 1 score of 62.8% on the development set of the QALB corpus, and 58.6% on the official test set."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Devising algorithms for automatic error correction generated considerable interest in the community since the early 1960s (Kukich, 1992) for at least two reasons.",
        "First, typical NLP tools lack in robustness against errors in their input.",
        "This sensitivity jeopardizes their usefulness especially for unedited text, which is prevalent on the web.",
        "Sec-ond, automated spell and grammar checkers facilitate text editing and can be of great help to non-native speakers of a language.",
        "Several resources and shared tasks appeared recently, including the HOO task (Dale and Kilgarriff, 2010) and the CoNLL task on grammatical error correction (Ng et al., 2013b).",
        "In this paper we describe our participation to the first shared task on automatic error correction for Arabic (Mohit et al., 2014).",
        "While non-word errors are relatively easy to handle, the task is more challenging for grammatical and semantic errors.",
        "Detecting and correcting such errors require context-sensitive approaches in order to capture the dependencies between the words of a text at various lexical and semantic levels.",
        "All the more so for Arabic which brings dependence down to the morphological level (Habash, 2010).",
        "A particularity interesting approach to error correction relies on statistical machine translation (SMT) (Brockett et al., 2006), due to its context-sensitivity and data-driven aspect.",
        "Therefore, the pipeline system which we describe in Section 2 has as its core a phrase-based SMT component (PBSMT) (Section 2.3).",
        "Nevertheless, several factors may hinder the success of this approach, such as data sparsity, discrepancies between translation and error correction tasks, and the difficulty of incorporating context-sensitive features into the SMT decoder.",
        "We address all these issues in our system which achieves a better correction quality than a simple word-level PBSMT baseline on the QALB corpus (Zaghouani et al., 2014) as we show in our experiments in Section 3.",
        "2 Pipeline Approach to Error Correction The PBSMT system accounts for context by learn-ing, from a parallel corpus of annotated errors, mappings from erroneous multi-word segments of text to their corrections, and using a language model to help select the suitable corrections in context when multiple alternatives are present.",
        "Furthermore, since the SMT approach is data-driven, it is possible to address multiple types of errors at once, as long as examples of them appear in the training corpus.",
        "These errors may include non-word errors, wrong lexical choices and grammatical errors, and can also handle normalization issues (Yvon, 2010).",
        "One major issue is data sparsity, since large amount of labeled training data is necessary to provide reliable statistics of all error types.",
        "We ad-114 dress this issue by backing-off the word-level PBSMT model with a character-level correction com-ponent, for which richer statistics can be obtained.",
        "Another issue may stem from the inherent difference in nature between error correction and translation.",
        "Unlike translation, the input and output vocabularies in the correction task overlap sig-nificantly, and the majority of input words are typically correct and are copied unmodified to the output.",
        "The SMT system should handle correct words by selecting their identities from all possible op-tions, which may fail resulting in over-correction.",
        "To help the SMT decoder decide, we augment our pipeline with a problem zone detection compo-nent, which supplies prior information on which input words need to be corrected.",
        "The final issue concerns the difficulty of incorporating features that require context across phrase boundaries into the SMT decoder.",
        "A straightforward alternative is to use such features to rerank the hypotheses in the SMT n-best hypotheses lists.",
        "Since punctuation is particularity noisy in Arabic data, we add a specialized punctuation insertion component to our pipeline, depicted in Figure 1.",
        "2.1 Error Detection We formalize the error detection problem as a sequence labeling problem (Habash and Roth, 2011).",
        "Errors are classified into substitution, insertion and deletion errors.",
        "Substitutions involve an incorrect word form that should be replaced by another correct form.",
        "Insertions are words that are incorrectly added into the text and should be deleted.",
        "Deletions are simply missing words that should be added.",
        "We group all error classes into a simple binary problem tag: a word from the input text is tagged as ?PROB?",
        "if it is the result of an insertion or a substitution of a word.",
        "Deleted words, which cannot be tagged themselves, cause their adjacent words to be marked as PROB instead.",
        "In this way, the subsequent components in the pipeline can be alerted to the possibility of a missing word via its surroundings.",
        "Any words not marked as PROB are given an ?OK?",
        "tag.",
        "Gold tags, necessary for training, can be generated by comparing the text to its correction using some sequence alignment technique, for which we use SCLITE (Fiscus, 1998).",
        "For this task, we use Yamcha (Kudo and Mat-sumoto, 2003) to train an SVM classifier using morphological and lexical features.",
        "We employ a quadratic polynomial kernel.",
        "The static feature window context size is set to +/- 2 words; the previous two (dynamic) predicted tags are also used as features.",
        "The feature set includes the surface forms and their normalization after ?Alef?, ?Ya?",
        "and digit normalization, the POS tags and the lemmas of the words.",
        "These morphological features are obtained using MADA 3.0 (Habash et al., 2009).",
        "1 We also use a set of word, POS and lemma 3-gram language models scores as features.",
        "These LMs are built using SRILM (Stolcke, 2002).",
        "The error detection component is integrated into the pipeline by concatenating the predicted tags with the words of the input text.",
        "The SMT model uses this additional information to learn distinct mappings conditional on the predicted correctness of words.",
        "2.2 Character-level Back-off Correction Each word that is labeled as error (PROB) in the output of the error detection component is mapped to multiple possible corrections using a weighted finite-state transducer similar to the transducers used in speech recognition (Mohri et al., 2002).",
        "The WFST, for which we used OpenFST (Al- lauzen et al., 2007), operates on the character level, and the character mapping is many-to-many (similar to the phrase-based SMT framework).",
        "The score of each proposed correction is a combination of the scores of character mappings used to build it.",
        "The list is filtered using WFST scores and an additional character-level LM score.",
        "The result is a list of error-tagged words and their correction suggestions, which constitutes a small on-the-fly phrase table used to back-off primary PBSMT table.",
        "During training, the mapping dictionary is learned from the training after aligning it at the character level using SCLITE.",
        "Mapping weights are computed as their normalized frequencies in the aligned training corpus.",
        "2.3 Word-level PBSMT Correction We formalize the correction process as a phrase-based statistical machine translation problem (Koehn et al., 2003), at the word-level, and solve 1 We did not use MADAMIRA (the newest version of MADA) since it was not available when this component was built.",
        "115 Character-level Correction Error Detection Word-level PBSMT Correction N-best Reranking Punctuation Insertion , ?",
        ".",
        "?",
        "Input Error-tagged text N-best hypotheses Reranked best hypothesis Output Back-off Phrase tables Primary Figure 1: Input text is run through the error detection component which labels the problematic words.",
        "The labeled text is then fed to the character-level correction components which constructs a back-off phrase table.",
        "The PBSMT component then uses two phrase tables to generate n-best correction hypotheses.",
        "The reranking component selects the best hypothesis, and pass it to the punctuation insertion component in order to produce the final output.",
        "it using Moses, a well-known PBSMT tool (Koehn et al., 2007).",
        "The decoder constructs a correction hypothesis by first segmenting the input text into phrases, and mapping each phrase into its best correction using a combination of scores including a context-sensitive LM score.",
        "Unlike translation, error correction is mainly monotonic, therefore we set disallow reordering by setting the distortion limit in Moses to 0.",
        "2 When no mapping can be found for a given phrase in the primary phrase table, the decoder looks it up in the back-off model.",
        "The decoder searches the space of all possible correction hy-potheses, resulting from alternative segmentations and mappings, and returns the list of n-best scoring hypotheses.",
        "2.4 N-best List Reranking In this step, we combine LM information with linguistically and semantically motivated features using learning to rank methods (Tomeh et al., 2013).",
        "Discriminative reranking (Liu, 2009) allows each hypothesis to be represented as an arbitrary set of features without the need to explicitly model their interactions.",
        "Therefore, the system benefits from global and potentially complex features which are not available to the baseline decoder.",
        "Each hypothesis in an n-best list is represented by a d-dimensional feature vector.",
        "Word error rate (WER) is computed for each hypotheses by comparing it to the reference correction.",
        "The resulting 2 Only 0.14% of edits in the QALB corpus are actually reordering.",
        "scored n-best list is used for supervised training of a reranking model.",
        "We employ a pairwise approach to ranking which takes pairs of hypotheses as instances in learning, and formalizes the ranking problem as pairwise classification.",
        "For this task we use RankSVM (Joachims, 2002) which is a method based on Support Vector Machines (SVMs).",
        "We use only linear kernels to keep complexity low.",
        "We use a rich set of features including LM scores on surface forms, POS tags and lemmas.",
        "We also use a feature based on a global model of the semantic coherence of the hypotheses (Tomeh et al., 2013).",
        "The new top ranked hypothesis is the output of this step which is then fed to the next component.",
        "2.5 Punctuation Insertion We developed a model that predicts the occurrence of periods and commas in a given Arabic text.",
        "The core model is a decision tree classifier trained on the QALB parallel training data using WEKA (Hall et al., 2009).",
        "For each space between two words, the classifier decides whether or not to insert a punctuation mark, using a window size of three words surrounding the underlying space.",
        "The model uses the following features: ?",
        "A class punctuation feature, that is whether to insert a period, a comma or none at the current space location; ?",
        "The part-of-speech of the previous word; ?",
        "The existence of a conjunctive or connective proclitic in the following word; that is a ?wa?",
        "116 Precision?Recall Curve Recall Prec ision 0.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 ?",
        "8.33 ?",
        "5.02 ?",
        "1.7 1.61 4.93 AUC= 0.715PRBE= 0.483, Cutoff= ?0.349Prec@rec(0.800)= 0.345, Cutoff= ?1.045 Figure 2: Evaluation of the error detection component.",
        "AUC: Area Under the Curve, PRBE: precision-recall break-even point.",
        "Classifier thresholds are displayed on the right vertical axis.",
        "or ?fa?",
        "proclitic that is either a conjunction, a sub-conjunction or a connective particle.",
        "We obtain POS and proclitic information using MADAMIRA (Pasha et al., 2014).",
        "The output of this component is the final output of the system.",
        "3 Experiments All the models we use in our pipeline are trained in a supervised way using the training part of the QALB corpus (Zaghouani et al., 2014), while we reserve the development part of the corpus for testing.",
        "3.1 Error detection We evaluate the error detection binary classifier in terms of standard classification measures as shown in Figure 2.",
        "Each point on the curve is computed by selecting a threshold on the classifier score.",
        "The threshold we use correspond to recall equal to 80%, at which the precision is very low which leaves much room for improvement in the performance of the error detection component.",
        "3.2 Character-level correction We evaluate the character-level correction model by measuring the percentage of erroneous phrases that have been mapped to their in-context reference corrections.",
        "We found this percentage to be 41% on QALB dev data.",
        "We limit the size of such phrases to one in order to focus on out-of-vocabulary words.",
        "3.3 Punctuation insertion To evaluate the punctuation insertion independently from the pipeline, we first remove the periods and commas from input text.",
        "Considering only the locations where periods and commas ex-ist, our model gives a recall of 49% and a precision of 53%, giving an F 1 -score of 51%.",
        "When we apply our punctuation model in the correction pipeline, we find that it is always better to keep the already existing periods and commas in the input text instead of overwriting them by the model prediction.",
        "While developing the model, we ran experiments where we train the complete list of features produced by MADAMIRA; that is part-of-speech, gender, number, person, aspect, voice, case, mood, state, proclitics and enclitics.",
        "This was done for two preceding words and two following words.",
        "However, the results were significantly outperformed by our final set-up.",
        "3.4 The pipeline The performance of the pipeline is evaluated in terms of precision, recall and F 1 as computed by the M 2 Scorer (Dahlmeier and Ng, 2012b).",
        "The results presented in Table 1 show that a simple PBSMT baseline achieves relatively good performance compared to more sophisticated models.",
        "The character-level back-off model helps by improving recall at the expense of decreased precision.",
        "The error detection component hurts the performance which could be explained by its intrinsic bad performance.",
        "Since more investigation is needed to clarify on this point, we drop this component from our submission.",
        "Both reranking and punctuation insertion improve the performance.",
        "Our system submission to the shared task (back- off+PBSMT+Rank+PI) resulted in an F 1 score of 58.6% on the official test set, with a precision of 76.9% and a recall of 47.3%.",
        "4 Related Work Both rule-based and data-driven approaches to error correction can be found in the literature (Sidorov et al., 2013; Berend et al., 2013; Yi et al., 2013) as well as hybridization of them (Putra and Szabo, 2013).",
        "Unlike our approach, most of 117 System PR RC F 1 PBSMT 75.5 49.5 59.8 backoff+PBSMT 74.1 51.8 60.9 ED+backoff+PBSMT 61.3 45.4 52.2 backoff+PBSMT+Rank 75.7 52.1 61.7 backoff+PBSMT+Rank+PI 74.9 54.2 62.8 Table 1: Pipeline precision, recall and F 1 scores.",
        "ED: error detection, PI: punctuation insertion.",
        "the proposed systems build distinct models to address individual types of errors (see the CoNLL-2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Ro- zovskaya et al., 2013).",
        "This approach is relatively time-consuming when the number of error types increases.",
        "Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizu-moto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a).",
        "Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture.",
        "Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012).",
        "A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008).",
        "5 Conclusion and Future Work We described a pipeline approach based on phrase-based SMT with n-best list reranking.",
        "We showed that backing-off word-level model with a character-level model improves the performance by ameliorating the recall of the system.",
        "The main focus of our future work will be on better integration of the error detection model, and on exploring alternative methods for combining the character and the word models.",
        "Acknowledgments This material is partially based on research funded by grant NPRP-4-1058-1-168 from the Qatar National Research Fund (a member of the Qatar Foundation).",
        "The statements made herein are solely the responsibility of the authors.",
        "Nizar Habash performed most of his contribution to this paper while he was at the Center for Computational Learning Systems at Columbia University.",
        "References"
      ]
    }
  ]
}
