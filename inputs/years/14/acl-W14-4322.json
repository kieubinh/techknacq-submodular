{
  "info": {
    "authors": [
      "Mehwish Riaz",
      "Roxana Girju"
    ],
    "book": "SIGDIAL",
    "id": "acl-W14-4322",
    "title": "In-depth Exploitation of Noun and Verb Semantics to Identify Causation in Verb-Noun Pairs",
    "url": "https://aclweb.org/anthology/W14-4322",
    "year": 2014
  },
  "references": [
    "acl-D12-1091",
    "acl-N03-1033",
    "acl-P05-1045",
    "acl-P09-2004",
    "acl-W04-3205"
  ],
  "sections": [
    {
      "text": [
        "Abstract",
        "Recognition of causality is important to achieve natural language discourse understanding.",
        "Previous approaches rely on shallow linguistic features.",
        "In this work, we propose to identify causality in verb-noun pairs by exploiting deeper semantics of nouns and verbs.",
        "Particularly, we acquire and employ three novel types of knowledge: (1) semantic classes of nouns with a high and low tendency to encode causality along with information regarding metonymies, (2) data-driven semantic classes of verbal events with the least tendency to encode causality, and (3) tendencies of verb frames to encode causality.",
        "Using these knowledge sources, we achieve around 15% improvement in F-score over a supervised classifier trained using linguistic features."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The identification of cause-effect relations is critical to achieve natural language discourse understanding.",
        "Causal relations are encoded in text using various linguistic constructions e.g., between two verbs, a verb and a noun, two discourse seg-ments, etc.",
        "In this research, we focus on identifying causality encoded between a verb and a noun (or noun phrase).",
        "For example, consider the following example: 1.",
        "At least 1,833 people died in the hurricane.",
        "In example (1), the verb-noun phrase pair ?died?-?the hurricane?",
        "encodes causality where event ?died?",
        "is the effect of ?hurricane?",
        "event.",
        "Previously several approaches have been proposed to identify causality between two verbs (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009).",
        "However, the problem of identifying causality in verb-noun pairs has not received a considerable attention.",
        "For example, Do et al. (2011) have studied this task but they worked only with a list of predefined nouns representing events.",
        "In this work, we focus on the linguistic construction of verb-noun (or noun phrase) pairs where noun can be of any semantic type.",
        "Traditional approaches for identifying causality mainly employ linguistic features (e.g., lexical items, part-of-speech tags of words, etc.)",
        "in the framework of supervised learning (Girju, 2003; Sporleder and Lascarides, 2008; Bethard and Mar-tin, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009) and do not involve deeper semantics of language.",
        "Analysis of such approaches by Sporleder and Lascarides (2008) have revealed that the linguistic features are not always sufficient to achieve a good performance on the task of identifying semantic relations including causality.",
        "In this work, we propose a model that deeply processes and acquires the specific semantic information about the participants of a verb-noun phrase (v-np) pair (i.e., noun and verb semantics) to identify causality with a better performance over the baseline model depending merely on shallow linguistic features.",
        "The work in this paper builds on our recent work reported in Riaz and Girju (2014).",
        "In that previous model, we identified the semantic classes of nouns and verbs with a high and low tendency to encode causation.",
        "For example, a named entity such as LOCATION may have the least tendency to encode causation.",
        "We leveraged such information about nouns to filter false positives.",
        "Sim-ilarly, we utilized the TimeBank's (Pustejovsky et al., 2006) classification of verbal events (i.e., Occurrence, Perception, Aspectual, State, I State, I Action and Reporting) and their definitions to claim that the reporting events (e.g., say, tell, etc.)",
        "161 just describe and narrate other events instead of encoding causality with them.",
        "We proposed an Integer Linear Programming (ILP) model (Roth and Yih, 2004; Do et al., 2011) to combine noun and verb semantics with the decisions of a supervised classifier which only relies on linguistic features.",
        "In this paper, we extend our previous model by acquiring and exploiting the following three novel types of knowledge: 1.",
        "We learn the information about tendencies of various verb frames to encode causation.",
        "For example, our model identifies if the subject of verb ?destroy?",
        "(?occur?)",
        "has a high (low) tendency to encode causation.",
        "Such information helps gain performance by exploiting causal semantics of each verb frame separately.",
        "We also learn and incorporate information about the verb frames in general e.g., how likely it is for the subject of any verb to encode causation with its verb.",
        "2.",
        "In Riaz and Girju (2014), we utilized the Time-Bank's definition of reporting events to argue that such events have the least tendency to encode causation.",
        "Instead of relying on human judgment we now introduce a data intensive approach to identify the TimeBank's classes of events with the least tendency to encode causation.",
        "3.",
        "Although, information about the nouns with the least tendency to encode causation helps to filter false positives it can lead to false negatives when metonymic readings are associated with such nouns.",
        "Therefore, we introduce a metonymy resolver on top of our current model to avoid false negatives.",
        "We provide details of our previously proposed model in section 3.",
        "We introduce new model and discuss its performance in sections 4 and 5.",
        "Section 6 concludes the current research.",
        "2 Relevant Work In Natural Language Processing (NLP), researchers are showing lots of interest in the task of identifying causality due to its various applications e.g., question answering (Girju, 2003), sum-marization (Chklovski and Pantel, 2004), future prediction (Radinsky and Horvitz, 2013), etc.",
        "Several approaches have been proposed to identify causality in pairs of verbal events (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009).",
        "However causality a pervasive relation of language can be encoded via various linguistic constructions.",
        "For example, verbs and nouns are the key components of language to represent events.",
        "Therefore in this work we focus on identifying causality in verb-noun pairs.",
        "Previously researchers have followed the path of utilizing linguistic features in the framework of supervised learning (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009).",
        "Though linguistic features are important but other sources of knowledge are also critically required to achieve progress on the current task.",
        "In recent years, researchers have proposed unsupervised metrics to identify causality between events (Riaz and Girju, 2010; Do et al., 2011).",
        "For example, Riaz and Girju (2010) and Do et al. (2011) introduced unsupervised metrics to learn causal dependencies between events.",
        "These metrics mainly depend on probabilities of co-occurrences of events and do not distinguish well causality from any other types of correlation (Riaz and Girju, 2013).",
        "In order to overcome this problem Riaz and Girju (2013) proposed some advanced metrics which combine probabilities of co-occurrences of events with the supervised estimates of cause and non-cause relations.",
        "Considering the importance of employing rich sources of knowledge other than linguistic features for the current task, we have recently proposed a model that incorporates semantic classes of nouns and verbs with a high and low tendency to encode causation (Riaz and Girju, 2014).",
        "In this work, we exploit information about verb frames, data-driven verb semantics and metonymies to achieve more progress on our recent work.",
        "3 Model for Recognizing Causality In this section we provide an overview of our previous model (Riaz and Girju, 2014) for identifying causality in v-np pairs where v (np) stands for verb (noun phrase).",
        "This model works in the following two stages: (1) A supervised classifier is used to make binary predictions (i.e., the label cause (C) or non-cause (?C)) employing linguistic features, and (2) noun and verb semantics are then combined with the predictions of supervised classifier in the ILP framework to identify causality.",
        "162 3.1 Supervised Classifier To the best of our knowledge, there is no data set of v-np pairs with the labels C and ?C available to us.",
        "For the current task we employ some heuristics to extract a training corpus of v-np pairs using FrameNet (Baker et al., 1998).",
        "FrameNet provides frame elements for the verbs and hand annotated examples (aka annotations) of these frame elements.",
        "Consider the following annotation from FrameNet ?They died [ Cause from shotgun wounds]?",
        "where the frame element ?Cause?",
        "is given for the verb ?died?.",
        "We remove the preposition ?from?",
        "from the above annotation of frame element to acquire an instance of v-np (i.e., died-shotgun wounds) pair.",
        "We extract all annotations for verbs from FrameNet in which a frame element must contain at least one noun and no verb in it.",
        "We found such annotations for 729 distinct frame elements.",
        "We manually assigned the labels C and ?C to these frame elements.",
        "Cause, Purpose, Rea-son, Result, Explanation are some examples of the frame elements to which we assigned the label C. Using the above mentioned assignments of labels C and ?C to frame elements, we have acquired a training corpus of 4, 141 (77, 119) C (?C) instances from FrameNet.",
        "In order to avoid class imbalance while training we employ an equal number of instances of both labels.",
        "Due to space constraints, we refer the reader to Appendix A for the details of linguistic features to build the supervised classifier.",
        "We employ both Naive Bayes (NB) and Maximum Entropy (Max- Ent) algorithms to acquire predictions and probabilities of assignments of labels.",
        "We set up the following ILP using these probabilities: Z 1 = max ?",
        "v-np?I ?",
        "l?L 1 x 1 (v-np, l)P (v-np, l) (1) ?",
        "l?L 1 x 1 (v-np, l) = 1 ?",
        "v-np ?",
        "I (2) x 1 (v-np, l) ?",
        "{0, 1} ?",
        "v-np ?",
        "I ?l ?",
        "L 1 (3) Here, L 1 = {C,?C}, I is the set of all v-np pairs.",
        "x 1 (v-np, l) is a binary decision variable set to 1 only if the label l ?",
        "L 1 is assigned to a v-np pair and only one label out of |L 1 | choices can be assigned to a v-np pair (see constraints 2 and 3).",
        "In particular, we maximize the objective function Z 1 (1) assigning the labels l ?",
        "{L 1 } to v-np pairs depending on the probabilities of assignments (i.e., P (v-np, l)) obtained through the supervised classifier.",
        "3.2 Noun and Verb Semantics We automatically acquire and employ semantic classes of nouns and verbs with a high and low tendency to encode causation.",
        "Such information helps to reduce errors in predictions of the supervised classifier.",
        "We derive two semantic classes of nouns for our purpose i.e., C np and ?C np where the class C np (?C np ) represents the noun phrases with a high (low) tendency to encode causation.",
        "For exam-ple, a noun phrase expression for a location has the least tendency to encode causation unless a metonymic reading is associated with it.",
        "In order to acquire these classes, we extract annotations of 936 distinct frame elements from FrameNet in which a frame element must contain at least one noun and no verb in it.",
        "These annotations of frame elements roughly represent instances of noun phrases (np).",
        "We manually assigned the labels C np and ?C np to the frame elements.",
        "For example, we assign the label ?C np to the frame element ?Place?",
        "which represents a location (see Appendix B for some examples of the frame elements with labels C np and ?C np ).",
        "We also follow the approach similar to Girju and Moldovan (2002) to employ WordNet senses of nouns to acquire more instances of the classes C np and ?C np (see Appendix B for the details).",
        "We have acquired a total of 280, 212 instances of np (50% for each of the two classes i.e., C np and ?C np ) using both FrameNet and WordNet.",
        "Using these instances, we build a supervised classifier to identify the semantic class of np (see Appendix B for the details of features to build the classifier).",
        "We incorporate the knowledge of semantic classes of nouns by making the following additions to ILP: Z 2 = Z 1 + ?",
        "v-np?I?M ?",
        "l?L 2 x 2 (f np (v-np), l)P (f np (v-np), l) (4) ?",
        "l?L 2 x 2 (f np (v-np), l) = 1 ?",
        "v-np ?",
        "I ?M (5) x 2 (f np (v-np), l) ?",
        "{0, 1} ?",
        "v-np ?",
        "I ?M (6) ?l ?",
        "L 2 x 1 (v-np,?C)?",
        "x 2 (f np (v-np),?C np ) ?",
        "0 (7) ?",
        "v-np ?",
        "I ?M Here L 2 = {C np ,?C np }.",
        "f np (v-np) is a function which returns np of a v-np pair.",
        "M is the set of v-np pairs with metonymic readings associated with np.",
        "Currently, this set is empty and in section 4.3 we introduce a metonymy resolver to pop-163 ulate this set.",
        "x 2 (f np (v-np), l) is a binary decision variable set to 1 only if the label l ?",
        "L 2 is assigned to np and only one label out of |L 2 | choices can be assigned to np (see constraints 5 and 6).",
        "Constraint 7 enforces that if an np belongs to the class ?C np then its corresponding v-np pair is assigned the label ?C.",
        "In particular, we maximize the objective function Z 2 (4) subject to the constraints introduced till now.",
        "For each v-np pair, we predict the semantic class of np using our supervised classifier for the labels l ?",
        "L 2 and set the probabilities ?",
        "i.e., P (f np (v-np), l) = 1, P (f np (v-np), {L 2 } ?",
        "{l}) = 0 if the label l ?",
        "L 2 is assigned to np.",
        "Also before running our supervised classifier, we run a named entity recognizer (Finkel et al., 2005) and assign the label ?C np to all noun phrases identified as named entities.",
        "We also determine association of metonymies with the noun phrases identified as named entities.",
        "For the current task we also acquire two semantic classes of verbs i.e., C e v and ?C e v where the class C e v (?C e v ) contains the verbal events with a high (low) tendency to encode causation.",
        "In order to derive these two classes we exploit the TimeBank corpus (Pustejovsky et al., 2003) which provides seven semantic classes of verbal events ?",
        "i.e., Occurrence, Perception, Aspectual, State, I State, I Action and Reporting.",
        "According to the definitions of these classes, we claim that the reporting events (e.g., say, tell, etc.)",
        "just describe and narrate other events instead of encoding causality with them.",
        "Using this claim, we consider that all instances of reporting verbal events of TimeBank belong to the class ?C e v and the rest of instances of verbal events lie in the class C e v .",
        "After acquiring instances of the classes C e v and ?C e v , we build a supervised classifier for these two classes.",
        "We use the features introduced by Bethard and Martin (2006) to build this classifier (see Bethard and Martin (2006) for the details).",
        "Employing predictions and probabilities of assignments of the labels C e v and ?C e v we add the following two constraints to ILP: (1) if the event represented by v belongs to ?C e v then the corresponding v-np pair must be labeled with ?C and (2) if a v-np pair is a causal pair then the event represented by v must be labeled with C e v .",
        "4 Enriched Verb and Noun Semantics This section describes the novel contributions of this work i.e., identification of semantics of verb frames, semantic classes of verbal events via a data intensive approach and association of metonymic readings with noun phrases to identify causality with a better performance.",
        "4.1 Verb Frames We introduce a method to acquire tendencies of various verb frames to encode causation.",
        "Consider the following two examples to understand the tendencies of verb frames of form {v, gr} to encode causation where v is the verb and gr is the grammatical relation of np with the verb v. 1.",
        "The Great Storm of October 1987 almost totally destroyed the eighty year old pinetum at Nymans Garden in Sussex.",
        "(Cause (C)) 2.",
        "The explosion occurred in the city's main business area.",
        "(Non-Cause (?C)) In above two examples the nps ?The Great Storm of October 1987?",
        "and ?The explosion?",
        "have the grammatical relations of subject with the verbs ?destroyed?",
        "and ?died?.",
        "In examples (1) and (2) the verb frames {destroy, subject} and {occur, subject} encode cause and non-cause relations.",
        "These examples reveal that each verb frame has its own tendency to encode causation.",
        "This type of knowledge helps gain performance by exploiting the semantics of each verb frame separately.",
        "We leverage FrameNet annotations to acquire such type of knowledge.",
        "We collect all annotations of verbs from FrameNet and assign the labels C and ?C to the frame elements as discussed in section 3.1.",
        "In FrameNet, example (1) is given as follows: 3.",
        "[ Cause The Great Storm of October 1987] [ Degree almost totally] destroyed [ Undergoer the eighty year old pinetum at Nymans Garden in Sussex].",
        "According to our assignments of labels C and ?C to the frame elements, example (1) is given as ?",
        "[ C The Great Storm of October 1987] [ ?C almost totally] destroyed [ ?C the eighty year old pinetum at Nymans Garden in Sussex].?.",
        "After acquiring instances of the labels C and ?C from example (1), we populate the fields of a knowledge base of verb frames (see Table 1).",
        "Fields of this knowledge base are {v, gr}, count({v, gr},C) and count({v, gr},?C).",
        "gr is the dependency relation of the frame element with the verb v. We use Stan-ford's dependency parser (Marneffe et al., 2006) to collect dependency relations.",
        "count({v, gr},C) (count({v, gr},?C)) is the count of the label C (?C) of the frame {v, gr}.",
        "As shown in Table 1, 164 for the frame element ?The Great Storm of October 1987?, the word ?Storm?",
        "has the dependency relation of ?nsubj?",
        "with the verb ?destroy?.",
        "If there exists more than one dependency relations between the frame element and its verb then we choose the very first relation in the text order.",
        "According to the counts given in Table 1, {destroy, nsubj} has more tendency to encode a cause relation than the non-cause one.",
        "We have acquired 7,156 and 114,898 instances of the labels C and ?C from FrameNet for populating the knowledge base of verb frames.",
        "We compute tendencies of verb frames to encode causality using the following scores: S({v, gr}, l) = S 1 ({v, gr}, l)?",
        "S 2 ({*, gr}, l) (8) S 1 ({v, gr}, l) = count({v,gr},l) count({v,gr},l)+count({v,gr},L 1 ?",
        "{l}) S 2 ({*, gr}, l) = count({*,gr},l) count({*,gr},l)+count({*,gr},L 1 ?",
        "{l}) Counts of first component (S 1 ) can be taken from the knowledge base of verb frames of form {v, gr}.",
        "The second component (S 2 ) with counts count({*, gr}, l) and count({*, gr}, L 1 ?",
        "{l}) captures tendencies of verb frames in general.",
        "For example, what is the tendency of any subject to encode causality with its verb i.e., the score S 2 ({?, nsubj},C).",
        "We populate the knowledge base of Table 1 with equal number of C and ?C instances to calculate counts for S 2 .",
        "We make the following additions to ILP to incorporate information about verb frames: Z 3 = Z 2 + ?",
        "v-np?I?",
        "g(v-np)?KB?",
        "f np (v-np)?C np ?",
        "l?L 1 x 3 (g(v-np), l)S(g(v-np), l) (9) ?",
        "l?L 1 x 3 (g(v-np), l) = 1 ?",
        "v-np?I?",
        "g(v-np)?KB?",
        "f np (v-np)?C np (10) x 3 (g(v-np), l) ?",
        "{0, 1} ?l ?",
        "L 1 ,?",
        "v-np?I?",
        "g(v-np)?KB?",
        "f np (v-np)?C np (11) x 3 (g(v-np), l) ?",
        "x 1 (v-np, l) ?l ?",
        "L 1 , (12) ?",
        "v-np?I ?g(v-np)?KB ?",
        "f np (v-np)?C np x 1 (v-np, l) ?",
        "x 3 (g(v-np), l) ?l ?",
        "L 1 , (13) ?",
        "v-np?I?",
        "g(v-np)?KB?",
        "f np (v-np)?C np Here, KB is the knowledge base of verb frames and g(v-np) is the function which returns the verb frame i.e., {v, gr}.",
        "This function returns NULL value if there is no grammatical relation between v and np in an instance.",
        "The above changes in ILP are only applicable for the v-np pairs with {v, gr} count({v, gr},C) count({v, gr},?C) {destroy,nsubj} 1 0 {destroy,advmod} 0 1 {destroy,dobj} 0 1 [ C The Great Storm of October 1987] [ ?C almost totally] destroyed [ ?C the eighty year old pinetum at Nymans Garden in Sussex].",
        "Table 1: A knowledge base of verb frames.",
        "This knowledge base is populated using the instances of C and ?C labels given in this table.",
        "g(v-np) ?",
        "KB and np identified as of class C np because we have already filtered the cases of np ?",
        "?C np in section 3.2. x 3 (g(v-np), l) is a binary decision variable set to 1 only if the label l ?",
        "L 1 is assigned to g(v-np) and only one label out of |L 1 | choices can be assigned to g(v-np) (see constraints 10 and 11).",
        "We add information about verb frames using constraints 12 and 13.",
        "These constraints enforce the predictions of the supervised classifier of causality (section 3.1) to be consistent with the predictions using tendencies of verb frames (i.e., score S({v, gr}, l)).",
        "We maximize objective function (9) subject to the above constraints.",
        "We remove those {v, gr} from KB which have count({v, gr},C)+count({v, gr},?C) < 5 to avoid wrong predictions based on the small counts of verb frames.",
        "4.2 Data-driven Verb Semantics In section 3.2 we considered that reporting events belong to the class ?C e v with the least tendency to encode causation using the definition of these events in the TimeBank corpus.",
        "Instead of relying on definitions of events we now introduce a data intensive approach to automatically identify the class ?C e v of verbal events.",
        "In order to identify this class we extract training instances of verbal events encoding C and ?C relations.",
        "Verbal events encode cause-effect relations using verb-verb (e.g., Five shoppers were killed when a car blew up.)",
        "and verb-noun linguistic constructions.",
        "Therefore for the current purpose we use the following two types of training instances: (A) a training corpus of 240K instances of verb-verb (v i -v j ) pairs encoding C and ?C relations (named as Training v i -v j ) (we refer the reader to Riaz and Girju (2013) for the details of this training corpus) and (B) the training corpus v-np instances introduced in section 3.1 (named as Training v-np ).",
        "Following is the procedure to derive V ?C ?",
        "V where V={Occurrence, Perception, Aspectual, State, I State, I Action, Reporting} and the set V ?C contains the TimeBank's semantic classes 165 with the least tendency to encode a cause relation.",
        "1.",
        "Input: Training corpus, V 2.",
        "Output: Set V ?C 3.",
        "For each training instance k employ the supervised classifier of Bethard and Martin (2006) to do the following: (a) if k ?",
        "Training v i -v j then identify the semantic class (sc) of both events represented by both verbs v i and v j and add this information to a set i.e., T = T ?",
        "(k v i , sc v i , l) ?",
        "(k v j , sc v j , l) where sc v i is the semantic class of event of the verb v i of instance k and l ?",
        "{C,?C}.",
        "(b) Else if k ?",
        "Training v-np then identify the semantic class (sc) of event represented by the verb v and set T = T ?",
        "(k v , sc v , l).",
        "4.",
        "Using results of step 3, calculate tendency of each semantic class sc ?",
        "V to encode non-causality (i.e., score(sc,?C)) as follows: score(sc,?C) = score 1 (sc,?C)?",
        "score 2 (sc,?C) score 1 (sc,?C) = ( count(sc,?C) count(sc) ?",
        "count(sc,C) count(sc) ) score 2 (sc,?C) = ( count(sc,?C) count(?C) ?",
        "count(sc,C) count(C) ) where count(m, n) is the number of instances of verbal events with the labels m and n and count(m) is the number of instances of verbal events with the label m. 5.",
        "Acquire a ranked list of semantic classes list sc = [sc 1 , sc 2 , .",
        ".",
        ".",
        "sc m ] s.t.",
        "score(sc i ,?C) ?",
        "score(sc i+1 ,?C).",
        "From this list we remove the class sc i if either score 1 (sc i , ?c) < 0 or score 2 (sc i , ?c) < 0. .",
        "The following steps are used to determine the cutoff class sc i ?",
        "list sc s.t.",
        "the semantic classes {sc 1 , sc 2 , .",
        ".",
        "., sc i-1 } have the least tendency to encode causation.",
        "6. result sc ?1 = 0 and result sc 0 = 0.",
        "7.",
        "Remove sc i from the front of list sc and do the following: (c) Predict the label (l) ?C for all tuples of form (m, sc, l) ?",
        "T if sc ?",
        "{sc 1 , sc 2 , .",
        ".",
        "., sc i } and predict C for the rest of the tuples.",
        "(d) Using predictions from step (c), calculate the result sc i = F1-score ?",
        "accuracy for the label l ?",
        "{C,?C}.",
        "(e) If result sc i ?result sc i-1 < result sc i-1 ?result sc i-2 then output {sc 1 , sc 2 , .",
        ".",
        "., sc i?1 } (f) Else go to step 7.",
        "Using the above procedure, we obtain the sets {Aspectual} and {Reporting, I State} with Training v i -v j and Training v-np corpora.",
        "We consider that the Aspectual, Reporting and I State events of the TimeBank corpus belong to the class ?C e v and rest of the events lie in C e v .",
        "Using these semantic classes we apply the constraints introduced in section 3.2.",
        "4.3 Metonymy Resolution: Metonymy resolution is the task to determine if a literal or non-literal reading is associated with a {v, gr} count({v, gr},C np ) count({v, gr},?C np ) {kill,nsubj} 1 0 {kill,dobj} 0 1 [ C np Pissed off Angelus] just kills [ ?C np me] Table 2: A knowledge base of verb frames.",
        "This knowledge base is populated using the instances of C np and ?C np labels given in this table.",
        "natural language expression (Markert and Nissim, 2009).",
        "Consider the following example: 4.",
        "The United States has killed Osama bin Laden and has custody of his body.",
        "(Cause (C)) In example (4) ?The United States?",
        "refers to a non-literal reading i.e., the event of ?raid in Ab-bottabad on May 2, 2011 by the United States?",
        "rather than merely referring to a literal sense i.e., a country.",
        "The association of non-literal reading with ?The United States?",
        "results in killing event.",
        "Previously, researchers have worked with hand-annotated selectional restrictions violation for this task (Markert and Nissim, 2009).",
        "In the example (4) a country cannot ?kill?",
        "someone and thus a metonymic reading is associated with it.",
        "In this work we identify association of metonymies with noun phrases via verb frames and prepositions as explained below in this section.",
        "In the first part of our approach we employ violations of tendencies of verb frames to identify if a non-literal reading is associated with a noun phrases.",
        "Particularly, we build a knowledge base of verb frames using C np and ?C np classes as discussed in section 4.1.",
        "Consider the knowledge base given in Table 2 populated using the following FrameNet annotations ?",
        "[ Stimulus Pissed off Angelus] just kills [ Experiencer me].?",
        "with assignments of labels C np and ?C np to the frame elements.",
        "We populate the knowledge base using only those FrameNet annotations in which a frame element does not contain a verb.",
        "Now we introduce our method to identify the association of non-literal reading with the ?The United States?",
        "in example (4).",
        "The supervised classifier predicts the class ?C np for the np ?The United States?.",
        "However, in the current state of knowledge base (Table 2) P({destroy, nsubj}, C np ) > P({destroy, nsubj}, ?C np ) where P is the probability.",
        "The prediction of ?C np for ?The United States?",
        "violates the above probabilities.",
        "Considering this violation, we predict the association of metonymy with np.",
        "In the second part of our approach we identify tendencies of prepositions to encode causation 166 and use violation of these tendencies to identify metonymies.",
        "For this purpose, we use the training corpus of v-np pairs with 4, 141 C and 77, 119 ?C training instances (see section 3.1).",
        "We employ only those training instances in which a preposition appears between v and np and there appears no verb between them.",
        "From these instances, we acquire a set of prepositions that appear between v and np.",
        "Using this set of prepositions (PR) as input to the following procedure, we acquire a set of prepositions (PR C ) with the highest tendency to encode causation: 1.",
        "Input: Training Corpus of v-np pairs, PR 2.",
        "Output: PR C 3.",
        "Calculate tendency of each preposition pr ?",
        "PR to encode causality (i.e., score(pr,C)) as follows: score(pr,C) = score 1 (pr,C)?",
        "score 2 (pr,C) score 1 (pr,C) = ( count(pr,C) count(pr) ?",
        "count(pr,?C) count(pr) ) score 2 (pr,C) = ( count(pr,C) count(C) ?",
        "count(pr,?C) count(?C) ) 4.",
        "Acquire a ranked list of prepositions list pr = [pr 1 , pr 2 , .",
        ".",
        ".",
        "pr m ] s.t.",
        "score(pr i ,C) ?",
        "score(pr i+1 ,C).",
        "From this list we remove pr i if either score 1 (pr i , C) or score 2 (pr i , C) < 0.",
        "5. result pr ?1 = 0, result pr 0 = 0 6.",
        "Remove pr i from the front of the list pr and do the follow-ing: (a) Predict the label C for all v-np training instances with pr ?",
        "{pr 1 , pr 2 , .",
        ".",
        "., pr i } and assign the label ?C to the rest of the instances.",
        "(b) Using predictions from step (a) calculate the result pr i = F1-score ?",
        "accuracy.",
        "(c) If result pr i -result pr i-1 < result pr i-1 -result pr i-2 then output {pr 1 , pr 2 , .",
        ".",
        "., pr i?1 }.",
        "(d) Else go to step 6.",
        "The above procedure outputs the set PR C = {for, by}.",
        "Now we introduce method to identify association of non-literal reading for the example ?All weapon sites in Iraq were destroyed by the United States?",
        "where ?the United States?",
        "?",
        "?C np as identified by the supervised classifier.",
        "However, the preposition ?by?",
        "has a high tendency to encode causation and thus ?the United States?",
        "may encode causation.",
        "Therefore, there is a possibility that this noun phrase has a non-literal sense attached to it which results in encoding causality.",
        "Using this method, we predict metonymies only for the v-np instances where preposition appears between v and np and there appears no verb between them.",
        "If any of two methods of metonymy resolution predicts the association of metonymy with np then we add v-np to the set M used in ILP (see section 3.2).",
        "5 Evaluation and Discussion In this section we present experiments and discussion on the performance achieved for the current task.",
        "In order to evaluate our model, we generated a test set of instances of v-np pairs.",
        "For this pur-pose, we collected three wiki articles on the topics of Hurricane Katrina, Iraq War and Egyptian Revolution of 2011.",
        "We apply a part-of-speech tagger and a dependency parser on all sentences of these three articles (Toutanova et al., 2003; Marneffe et al., 2006).",
        "We extracted all v-np pairs from each sentence of these articles.",
        "For each of the these three articles, we selected first 500 instances of v-np pairs.",
        "Two annotators were asked to provide the labels C and ?C to the instances of v-np pairs using the annotation guidelines from Riaz and Girju (2010).",
        "We have achieved a 0.64 kappa score for the human inter-annotator agreement on a total of 1,500 v-np instances.",
        "This results in a total of 1,365 instances of v-np pairs with 11.86% C pairs.",
        "In this section, we present performance of the following models (see Table 3): 1.",
        "Baseline: NB and MaxEnt (McCallum, 2002) supervised classifiers using only the shallow linguistic features (see section 3.1).",
        "2.",
        "Basic noun and verb semantics: ILP with the addition of semantic classes of nouns without metonymy (denoted by +N !M ) and the addition of semantic classes of verbs where ?C e v ={(R)eporting events} (denoted by +N !M +V {R} ).",
        "These models represent the work proposed in Riaz and Girju (2014) (sec- tion 3).",
        "3.",
        "Noun semantics with metonymies: ILP with the addition of noun semantics involving metonymies resolved via verb frames (de- noted by +N M 1 ), metonymies resolved via verb frames {v, gr} where gr ?",
        "GR = {csubj, csub-jpass, nsubj, nsubjpass, xsubj, dobj, iobj, pobj, agent} a set of core dependency relations of subjects and objects (denoted by +N M 1 GR ) and metonymies resolved via both verb frames and prepositions (denoted by +N M 1 GR +M 2 ).",
        "4.",
        "Verb frames and data-driven verb seman-tics: ILP with the addition of information about verb frames (denoted by +N M +VF where M = M 1 GR +M 2 ), data-driven verb semantics i.e., ?C e v ={(A)spectual, (R)eporting, (I) (S)tate events} (denoted by +N M +V {A,R,IS} ) and both verb frames and data-driven verb semantics (denoted by +N M +VF+V {A,R,IS} ) 167 S B +N !M +N !M +V {R} +N M 1 +N M 1 GR +N M 1 GR +M 2 +N M +VF +N M +V {A,R,IS} +N M +VF +V {A,R,IS} A 28.86 71.86 73.40 71.35 71.42 71.64 72.96 75.16 76.19 P 13.52 26.18 27.21 26.29 26.34 27.54 28.39 29.93 30.82 R 92.59 75.30 74.07 78.39 78.39 85.18 83.95 81.48 80.86 F 23.60 38.85 39.80 39.37 39.44 41.62 42.43 43.78 44.63 A 61.46 80.73 81.17 80.65 80.73 81.02 81.39 81.75 82.05 P 19.46 32.02 32.72 32.41 32.52 34.09 34.66 35.25 35.64 R 71.60 55.55 55.55 58.02 58.24 64.19 64.19 64.19 63.58 F 30.60 40.63 41.18 41.59 41.68 44.53 45.02 45.51 45.67 Table 3: Performance of (B)aseline, +N !M , +N !M +V {R} , +N M 1 , +N M 1 GR , +N M 1 GR +M 2 , +N M +VF, +N M +V {A,R,IS} and +N M +VF+V {A,R,IS} (see text for details) in terms of (S)cores of (A)ccuracy, (P)recision, (R)ecall, (F)-score.",
        "The row 1 (2) of this table presents results over NB (MaxEnt) baseline supervised classifier, respectively.",
        "Table 3 shows that MaxEnt gives a very high accuracy and F-score as compared with NB.",
        "Model +N !M +V {R} with basic noun and verb semantics introduced in section 3.2 results in more than 10% improvement in F-score over NB and MaxEnt classifiers relying only on shallow linguistic features.",
        "Model +N M +VF+V {A,R,IS} with enriched verb and noun semantics brings more than 4% improvement in F-score over +N !M +V {R} with MaxEnt as baseline.",
        "We perform statistical significance test using bootstrap sampling method given in Berg-Kirkpatrick et al. (2012) (see Berg-Kirkpatrick et al. (2012) for the de-tails).",
        "+N M +VF+V {A,R,IS} brings significant improvement in F-score over +N !M +V {R} with p-value 0.0.",
        "Though +N !M gives significantly better F-score over baseline, it drops recall by more than 16%.",
        "Metonymy resolution helps perform quite better by recovering more than 8% recall with +N M 1 GR +M 2 over +N !M .",
        "+N M 1 GR +M 2 also results in 3.9% improvement in F-score over +N !M with MaxEnt as baseline model (significant improvement with p-value 0.0).",
        "Metonymies resolved via verb frames with all and core grammatical relations (i.e., set GR) recover more than 2% recall and slightly improve F-score.",
        "Model with the addition of information of verb frames (i.e.,+N M +VF) brings 0.49% improvement in F-score over +N M 1 GR +M 2 using MaxEnt as baseline model (significant improvement with p-value 0.027).",
        "Model with the addition of data-driven verb semantics (i.e., +N M +V {A,R,IS} ) results in 0.98% improvement in F-score over +N M 1 GR +M 2 using MaxEnt as baseline model (significant improvement with p-value 0.0021).",
        "Overall the model +N M +VF+V{A,R, IS} yields more than 16% (20%) F-score (accuracy) over the baseline models build via NB and MaxEnt.",
        "5.1 Error Analysis We performed error analysis for the model +N M +VF+V {A,R,IS} by randomly selecting 50 False Positives (FP) and 50 False Negatives (FN).",
        "For 32% FP instances information about verb frames is not available in the knowledge base of verb frames.",
        "To avoid this problem researchers should exploit some abstractions e.g., {semantic sense of v, gr} frames.",
        "Our model fails to identify the class ?C np for noun phrases of 29% FP instances due to the lack of enough training data for the semantic classes of nouns.",
        "In 21% FP instances v and np are not even relevant to each other.",
        "Our model first needs to determine relevance between v and np before identifying causality.",
        "Remaining 18% instances have v and np in temporal only sense, comparison relation or both represent parts of same event.",
        "There is need to extract more knowledge sources to better distinguish causality from any other type of relation.",
        "77% FN instances are classified as non-causal due to the lack of enough v-np training data and require more sources of knowledge e.g., background knowledge.",
        "On remaining 23% FN instances our model fails to identify C np class due to the lack of enough training data for the semantic classes of nouns.",
        "6 Conclusion This work has revealed that enriched semantics of nouns and verbs help gain significant improvement in performance over a baseline relying only on shallow linguistic features.",
        "Through empirical evaluation and error analysis of our model we have highlighted strengths and weaknesses of our model for the current task.",
        "Our work has provided a novel direction to exploit semantics of participants of causal relations to solve the challenge of identifying causality.",
        "168 References"
      ]
    }
  ]
}
