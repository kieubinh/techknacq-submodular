{
  "info": {
    "authors": [
      "Michael White"
    ],
    "book": "INLG",
    "id": "acl-W14-4424",
    "title": "Towards Surface Realization with CCGs Induced from Dependencies",
    "url": "https://aclweb.org/anthology/W14-4424",
    "year": 2014
  },
  "references": [
    "acl-D10-1119",
    "acl-D12-1023",
    "acl-P02-1043",
    "acl-P08-1022",
    "acl-P13-2107",
    "acl-W04-3250"
  ],
  "sections": [
    {
      "text": [
        "Abstract",
        "We present a novel algorithm for inducing Combinatory Categorial Grammars from dependency treebanks, along with initial experiments showing that it can be used to achieve competitive realization results using an enhanced version of the surface realization shared task data."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In the first surface realization shared task (Belz et al., 2011), no grammar-based systems achieved competitive results, as input conversion turned out to be more difficult than anticipated.",
        "Since then, Narayan & Gardent (2012) have shown that grammar-based systems can be substantially improved with error mining techniques.",
        "In this pa-per, inspired by recent work on converting dependency treebanks (Ambati et al., 2013) and semantic parsing (Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013) with Combinatory Categorial Grammar (CCG), we pursue the alternative strategy of inducing a CCG from an enhanced version of the shared task dependencies, with initial experiments showing even better results.",
        "A silver lining of the failure of grammar-based systems in the shared task is that it revealed some problems with the data.",
        "In particular, it became evident that in cases where a constituent is annotated with multiple roles in the Penn Treebank (PTB), the partial nature of Propbank annotation and the restriction to syntactic dependency trees meant that information was lost between the surface and deep representations, leading grammar-based systems to fail for good reason.",
        "For ex-ample, Figure 1 shows that with free object rel-atives, only one of the two roles played by how much manufacturing strength is captured in the deep representation, making it difficult to linearize this phrase correctly.",
        "By contrast, Figure 2 (top) \u0001\u0002\u0003\u0004\u0003\u0005\u0006\u0007\b \u0001\b \u0004\u0006 \u0004 \u0005 \u000e \u000f\u0010\u0011\u0011 \u0005\u0003\u0003 \u0002 \b\u0001\u0004\u0007\u0001 \u000e\u0012\u0001\u0007 \u0010\u0012\u0013\u0014 \u0006\u0015\u0006 \u0001\u0016\u0014\u0017 \u000e\u0018\u0012\b\u0006\u0002 \u000e\u0018\u0007\u0007 \u0010\u0012\u0013\u0019 \u0010\u0012\u0013\u0019 \u0018\u0007 \u001a\u0003 \b\u0003 \u0010\u0012\u0013\u0019 \u001b\u0003 \u0010\u0012\u0013\u0019 \u0005 \u0002\u001b \u0010\u0012\u0013\u0014 \u0007\b\u0012\u0001\u0004\u0013\b\u001b \u0004 \u0005 \u0007\u0013 \u0010\u0012\u0013\u0019 \u0001 \u000e\u0001\u0002\b\u0016\u0014\u0019 \b\u0001\u0004\u0007\u0001 \u000e\u0012\u0001\u0007 \u0010\u0012\u0013\u0017 \u0005\u0018\u0004 \u0018\u0002\b \u0012\u0001\u0016\u0014\u0019 \u000e\u0018\u0012\b\u0006\u0002 \u000e\u0012\u0001\u0007 \u0010\u0012\u0013\u001a \b\u001b\u0001 \u0010\u0012\u0013\u0014 \u0007\u0001\u0001\u0016\u0014\u0019 \u0010\u0012\u0013\u0019\u0010\u0012\u0013\u0014 \u0010\u0012\u0013\u0014 \u0006\u0004 \u001a\u0003 \u0012\u0001\u000e\u0003\u0012\b \u0001\b \u0004\u0006 \u0004 \u0005 \u000e \u0010\u0012\u0013\u0019 \u0011\u0001\u000e\b\u0001\u0005 \u0001\u0012 \u0004 \u0005 \u0007\u0013 \u001a\u0003 \u0003\u0004 \u001a\u0003 \u0001 \u0010\u000e\u000e\u0003\u0007!\u0001 \u0018\u0004 \u0010\u0012\u0013\u0019 \u0010\u0012\u0013\u0014 \u0018 \u0007\u0003 \u001a\u0003 \b\u0003\u0005\u0003\u0012\u0012\u0003 \u0001\b \u0004\u0006 \u0004 \u0005 \u0007\u0013 \u0010\u0012\u0013\u0019 \u000e\u0012\u0003 \u0002\b\u0006\u0003\u0004 \u0001\b \u0004\u0006 \u0004 \u0005 \u0007\u0013 \"\u0006\u0012\u0007\b \b\u0006 \u0006#\u0018\b\u0006\u0003\u0004 \u0001\b \u0004\u0006 \u0004 \u0005 \u0007\u0013 $\u0001 \b \u0006\u0004 \u0007\b\u0012\u0006\u0018 \u001a\u0003 \u0002\u0018\u000e\u0018\u0002\u0006\b \u0004 \u0005 \u0007\u0013 \u001a\u0003 \u0001\u0002\u0003\u0004\u0003\u0005\u0006\u0007\b \u0001 \u0006\u000e\u0006 \u0001 \u000f\u0010 \u0011 \u0012 \u0013\u0014 \u0015\u0007 \u0013\u0016\u0017\u0010\u0013\u0018 \b\u0003 \u0013\u0014 \u0001\u0019\u001a\u0001\u0002\b \u0013\u0014 \b\u001b\u0001 \u0013 \u0007\u0001\u0001 \u0013\u0014 \u0013 \u0007\b \u0001\u0004 \b\u001b \u0013\u0014 \u0006\u0004 \u0013\u0016\u0017 !\u0010 \u001b\u0003\" \u0013# \u0005\u0015\u0004$%\u0015\u0002\b$ \u0006\u0004 \u0013\u0014 \u0001\u001a\u0003 \b \u0013\u0014 \u0005$\u0002\u001b \u0013&'\u000f \u0007\u0001\u001a\b\u0001\u0005 \u0001 \u0013\u0016\u0017(\u0016\u0012 \u0003\u0004 \u0013\u0014 \u0015)\u0007\u0003 \u0013\u0012\u0012!",
        "\u001a \u0003 $\u0002\b\u0006\u0003\u0004 \u0013\u0014 $\u0001 \u0013&'\u000f \b\u0003\u0005\u0003 \u0003\" (\u0016\u0012 \u0006\u0004 $\u0007\b \u0006\u0015) \u0013\u0016\u0017 !\u0010 \u0015\u0004 \u0010!!",
        "*+ $\b\u0006)\u0006,\u0015\b\u0006\u0003\u0004 \u0010!'",
        "\u0002\u0015\u001a\u0015\u0002\u0006\b \u0013\u0014 !",
        "\"#$%&'(#)*'+,-./' 0#12%'+,-./' 3%-%,&%,45'6$78'!",
        "\"# /7'$\"%'89))9,:'6$78' )\"#$%&'/#)*'9,-./;'' <=,&'6$78'&'''/7'$\"%' 9,',#12%'9,-./>?'",
        "Figure 1: Shared Task Input for Economists are divided as to [how much manufacturing strength]i they expect to see ti in September reports on industrial production and capacity utilization , also due tomorrow (wsj 2400.6, ?deep?",
        "representation) shows an experimental version of the shallow representation intended to capture all the syntactic dependencies in the PTB, including the additional object role played by this phrase here.1 Including all PTB syntactic dependencies in the shallow representation makes it feasible to define a compatible CCG; at the bottom of the figure, a corresponding CCG derivation for these dependencies is shown.",
        "In the next section, we present an algorithm for inducing such derivations.",
        "In contrast to Ambati et al.'s (2013) approach, the algorithm integrates the proposal of candidate lexical categories with the derivational process, making it possible to derive categories involving unsaturated arguments, such as se,dcl\\npx/(se?,to\\npx ); it also makes greater use of unary type-changing rules, as with Artzi & Zettlemoyer's (2013) approach.",
        "1Kudos to Richard Johansson for making these enhancements available.",
        "147 Unlike their approach though, it works in a broad coverage setting, and makes use of all the combinators standardly used with CCG, including ones for type-raising.",
        "2 Inducing CCGs from Dependencies Pseudocode for the induction algorithm is given in Figure 3.",
        "The algorithm takes as input a set of training sentences with their gold standard dependencies.",
        "We pre-processed the dependencies to make coordinating conjunctions the head, and to include features for zero-determiners.",
        "The algorithm also makes use of a seed lexicon that specifies category projection by part of speech as well as a handful of categories for function words.",
        "For example, (1) shows how a tensed verb projects to a finite clause category, while (2) shows the usual CCG category for a determiner, which here introduces a ?NMOD?",
        "dependency.2 (1) expect ` se,dcl : @e(expect ?",
        "?TENSE?pres) (2) the ` npx/nx : @x(?NMOD?",
        "(d ?",
        "the)) The algorithm begins by instantiating the lexical categories and type-changing rules that match the input dependency graph, tracking the categories in a map (edges) from nodes to edges (i.e., signs with a coverage vector).",
        "It then recursively visits each node in the primary dependency tree bottom up (combineEdges), using a local chart (do- Combos) at each step to combine categories for adjacent phrases in all possible ways.",
        "Along the way, it creates new categories (extendCats and co-ordCats) and unary rules (applyNewUnary).",
        "For example, when processing the node for expect in Figure 2, the nodes for they and to are recursively processed first, deriving the categories npw9 and sw11 ,to\\npw9 /npw8 for they and to see .",
        ".",
        ".",
        ", respectively.",
        "The initial category for expect is then extended as shown in (3), which allows for composition with to see .",
        ".",
        ".",
        "(as well as with a category for simple application).",
        "When there are coordination relations for a coordinating conjunction (or coordinating punctuation mark), the appropriate category for combining like types is instead constructed, as in (4).",
        "Additionally, for modifiers, unary rules are instantiated and applied, e.g. the rule for noun-noun compounds in (5).",
        "2In the experiments reported here, we made use of only six (non-trivial) hand-specified categories and two type-changing rules; though we anticipate adding more initial categories to handle some currently problematic cases, the vast majority of the categories in the resulting grammar can be induced automatically.",
        "Inputs Training set of sentences with dependencies.",
        "Initial lexicon and rules.",
        "Argument and modifier relations.",
        "Derivation scoring metric.",
        "Maximum agenda size.",
        "Definitions edges is a map from dependency graph nodes to their edges, where an edge is a CCG sign together with a coverage bitset; agenda is a priority queue of edges sorted by the scoring metric; chart manages equivalence classes of edges; see text for descriptions of auxiliary functions such as extendCats and coordCats below.",
        "Algorithm bestDerivs, lexcats, unaryRules?",
        "?",
        "For each item in training set: 1. edges[node] ?",
        "instCats(node), ruleInsts[node] ?",
        "in-stRules(node), for node in input graph 2. combineEdges(root), with root of input graph 3. bestEdge ?",
        "unpack (edges[root]); bestDerivs +?",
        "bestEdge.sign; lexcats +?",
        "abstractedCats(bestEdge), unaryRules +?",
        "abstractedRules(bestEdge), if bestEdge complete def combineEdges(node): 1. combineEdges(child) for child in node.kids 2. edges[node] +?",
        "coordCats(node) if node has co-ord relations, otherwise edges[node] ?",
        "extend-Cats(node,rels) for argument rels 3. agenda ?",
        "edges[node]; agenda +?",
        "edges[child] for child in node.kids; chart?",
        "?",
        "4.",
        "While agenda not empty: (a) next?",
        "agenda.pop (b) chart +?",
        "next (c) doCombos(next), unless next packed into an existing chart item 5. edges[node]?",
        "chart edges for node filtered for maximal input coverage def doCombos(next): 1. agenda +?",
        "applyUnary(next), if next is for node 2.",
        "For item in chart: (a) agenda +?",
        "applyBinary(next,item), if next is adjacent to item (b) agenda +?",
        "applyNewUnary(next,item), if next connected to item by a modifier relation Outputs bestDerivs, lexcats, unaryRules Figure 3: CCG Induction Algorithm 148 economists are divided as to how much manufacturing strength they expect to see in .",
        ".",
        ".",
        ".",
        "root subj vc obj adv pmod pmodnmod amod nmod sub obj subj sbj oprd im loc pmod p .",
        ".",
        ".",
        "September reports on industrial production and capacity utilization , also due tomorrow pmod nmod nmod pmod nmod coord1 coord2 nmod p appo amod tmp .",
        ".",
        ".",
        "to how much manuf.",
        "strength they expect to see .",
        ".",
        ".",
        "pp/np wh adj sng n np sdcl\\np/(sto\\np) sto\\np/(sb\\np) sb\\np/np >T >Bwh\\wh n/n s/(s\\np) sto\\np/np < > >Bwh n sdcl\\np/np >Bnp/(s/np)/n sdcl/np >np/(s/np) >np >pp .",
        ".",
        ".",
        "Sept. reports on industrial production and capacity utilization .",
        ".",
        ".",
        "np n pp/np adj n np\\?np/?np n n n/n n/n n/n > > >n n n np np >np\\?np <np >pp n\\n <n np Figure 1: exampleFigure 2: Augmented Syntactic Dependencies with Corresponding CCG Derivation (dashed dependencies indicate relations from additional parents beyond those in the primary tree structure) 149 (3) expect ` sw10 ,dcl\\npw9 /(sw11 ,to\\npw9 ) : @w10(expect ?",
        "?TENSE?pres ?",
        "?SUBJ?w9 ?",
        "?OPRED?w11) (4) and ` npw19 \\?npw18 /?npw21 : @w19(and ?",
        "?COORD1?w18 ?",
        "?COORD2?w21) (5) nw20 ?",
        "nw21 /nw21 : @w21(?NMOD?w20) At the end of the recursion, the lexical categories and type-changing rules are extracted from the highest-scoring derivation and added to the output sets, after first replacing indices such as w10 with variables.",
        "3 Experiments and Future Work We ran the induction algorithm over the standard PTB training sections (02?21), recovering complete derivations more than 90% of the time for most sections.",
        "Robust treatment of coordina-tion, including argument cluster coordination and gapping, remains a known issue; other causes of derivation failures remain to be investigated.",
        "To select preferred derivations, we used a complexity metric that simply counts the number of steps and the number of slashes in the categories.",
        "We then trained a generative syntactic model (Hock- enmaier and Steedman, 2002) and used it along with a composite language model to generate n-best realizations for reranking (White and Rajku-mar, 2012), additionally using a large-scale (giga- word) language model.",
        "Development and test results appear in Table 1.",
        "Perhaps because of the expanded use of type-changing rules with simple lexical categories, the generative model and hypertagger (Espinosa et al., 2008) performed worse than expected.",
        "Combining the generative syntactic model and composite language model (GEN) with equal weight yielded a devtest BLEU score of only 0.4513, while discriminatively training the generative component models (GLOBAL) increased the score to 0.7679.",
        "Using all features increased the score to 0.8083, while doubling the beam size (ALL+) pushed the score to 0.8210, indicating that search errors may be an issue.",
        "Ablation results show that leaving out the large-scale language model (NO-BIGLM) and dependency-ordering features (NO-DEPORD) substantially drops the score.3 Focusing only on the 80.5% of the sentences for which a complete derivation was found (COMPLETE) yielded a score of 0.8668.",
        "By comparison, realization with the 3All differences were statistically significant at p < 0.01 with paired bootstrap resampling (Koehn, 2004).",
        "Model Exact Complete BLEU Sect 00 GEN 2.4 79.5 0.4513 GLOBAL 29.7 79.0 0.7679 NO-BIGLM 29.1 78.2 0.7757 NO-DEPORD 34.3 77.9 0.7956 ALL 35.8 78.4 0.8083 ALL+ 36.4 80.5 0.8210 COMPLETE 44.4 0.8668 NATIVE 48.0 88.7 0.8793 Sect 23 GEN 2.8 80.3 0.4560 GLOBAL 31.3 78.5 0.7675 ALL 37.6 77.2 0.8083 ALL+ 38.1 80.4 0.8260 COMPLETE 47.0 0.8743 NATIVE 46.4 86.4 0.8694 Table 1: Development set (Section 00) & test set (Section 23) results, including exact match and complete derivation percentages and BLEU scores native OpenCCG inputs (and the large-scale LM) on all sentences (NATIVE) yields a score more than five BLEU points higher, despite using inputs with more semantically-oriented relations and leaving out many function words, indicating that there is likely substantial room for improvement in the pre-processing and grammar induction process.",
        "Towards that end, we tried selecting the best derivations using several rounds of Viterbi EM with the generative syntactic model, but doing so did not improve realization quality.",
        "A similar pattern is seen in the Section 23 re-sults, with a competitive BLEU score of 0.8260 with the expanded beam, much higher than Narayan & Gardent's (2012) score of 0.675 with 38.8% coverage, the best previous score with a grammar-based system.",
        "This score still trails the shared task scores of the top statistical dependency realizers by several points (STUMABA-S at 0.8911 and DCU at 0.8575), though it exceeds the score of a purpose-built system using no external resources (ATT at 0.6701).",
        "In future work, we hope to close the gap with the top systems by integrating an improved ranking model into the induction process and resolving the remaining representational issues with problematic constructions.",
        "Acknowledgments Thanks to the anonymous reviewers, Richard Johansson and the University of Sydney Schwa Lab for helpful comments and discussion.",
        "This work was supported in part by NSF grants IIS-1143635 and IIS-1319318.",
        "150 References"
      ]
    }
  ]
}
