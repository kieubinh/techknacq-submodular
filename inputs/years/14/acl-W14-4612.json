{
  "info": {
    "authors": [
      "Akshay Minocha",
      "Francis Tyers"
    ],
    "book": "CLTW",
    "id": "acl-W14-4612",
    "title": "Subsegmental language detection in Celtic language text",
    "url": "https://aclweb.org/anthology/W14-4612",
    "year": 2014
  },
  "references": [
    "acl-W03-0419"
  ],
  "sections": [
    {
      "text": [
        "Subsegmental language detection in Celtic language text Akshay Minocha IIIT Hyderabad Hyderabad (India) akshay.minocha@students.iiit.ac.in Francis M. Tyers Giellatekno UiT Norgga ?arktala's universitehta 9017 Romsa (Norway) francis.tyers@uit.no",
        "Abstract",
        "This paper describes an experiment to perform language identification on a sub-sentence basis.",
        "The typical case of language identification is to detect the language of documents or sentences.",
        "However, it may be the case that a single sentence or segment contains more than one language.",
        "This is especially the case in texts where code switching occurs."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Determining the language of a piece of text is one of the first steps that must be taken before proceeding with further computational processing.",
        "This task has received a substantial amount of attention in recent years (Cavnar and Trenkle, 1994; Lui and Baldwin, 2012).",
        "However, previous research has on the whole assumed that a given text will be in a single language.",
        "When dealing with text from formal domains, this may be the case ?",
        "although there are exceptions ?",
        "such as quotations embedded in the text in another language.",
        "But when dealing with informal text, particularly in languages where the speech community is predominantly bi-or multi-lingual, this assumption may not hold.",
        "The work presented in this paper was motivated by the problems in normalising non-standard input for the Celtic languages as a precursor to machine translation.",
        "When applying a normalisation strategy to a piece of text, it is necessary to first know the language of the piece of text you are applying it to.",
        "The remainder of the paper is laid out as follows.",
        "In Section 3 we describe the problem in more detail and look at relevant prior work before proposing a novel method of sub-sentential language detection.",
        "Section 4 describes the evaluation methodology.",
        "Then in Section 5 we present the results of our method and compare it against several other possible methods.",
        "Finally, Section 6 presents future work and conclusions.",
        "2 Related Work Code-switching and segment detection problems have been the subject of previous research.",
        "A good deal of work has been done on detecting code-switched segments in speech data (Chan et al., 2004; Lyu et al., 2006).",
        "It is seen that language modelling techniques have shown promise earlier, such as in Yu et al. (2013), the experiment on Mandarin-Taiwanese sentences show a high accuracy in terms of detecting code-switched sentences.",
        "In Chan et al. (2004) the authors have made use of the bi-phone probabilities and calculated them to measure a confidence metric, to (Lyu et al., 2006) which has made use of named syllable-based duration classification, which uses the tonal syllable properties along with the speech signals to help predict the code switch points.",
        "In Yeong and Tan (2010) the authors use syllable structure information to identify words in code-switched text in Malay-English, however they did not recognise segments in running text, only identifying individual words.",
        "76 Pair Language Statistics (%) Tokens Segments Irish?English Irish 332 40 English 379 42 Welsh?English Welsh 419 64 English 378 66 Breton?French Breton 388 54 French 379 53 Table 1: Document statistics of the annotated data used.",
        "[en You?re a] [ga Meirice?anach, c?en f?ath] [en are you] [ga foghlaim Gaeilge?!]",
        "@afaltomkins [cy gorfod cael bach o tan] [en though init] [en omg] [cy mar cwn bach yn] [en black and tan] [cy a popeth,] [en even cuter!!]",
        "Figure 1: Example of text from a microblogging site chunked manually.",
        "3 Methodology As the number of possible languages for each segment is in theory the set of all the world's written languages, we take a decision to simplify the task by only looking at texts in the Celtic language and the corresponding majority language spoken where the Celtic language is spoken.",
        "That is, we looked at detecting between Irish and English, Welsh and English and Breton and French.",
        "3.1 Corpus We hand-annotated a small evaluation set from a selection of posts to a popular microblogging site.",
        "1 The tweets (microblog posts) were filtered into three sets which had been identified as Irish, Welsh and Breton using the langid tool (Lui and Baldwin, 2012).",
        "From these, we manually selected between 40 and 50 tweets for each language pair.",
        "Statistics on the number of segments and tokens is presented in Table 1.",
        "Certain tokens were escaped from the data, such as the ?mentioned?",
        "character (@ symbol), subject tags ?hashtags?",
        "which are preceded by a # symbol, hyperlinks and the sequence rt which stands for ?re-tweet?.",
        "An example of the content of our corpus after hand annotation is given in Figure 1.",
        "All of the tweets had at least one instance of code-switching.",
        "3.2 Alphabet n-gram approach We use the character n-gram approach along with some heuristics which are relevant to our problem domain of identifying segments for subsequent processing.",
        "We would like to both predict the code switched points but looking at the surrounding structure also decide the inclusion of them into the current or the next segment.",
        "We first built character language models using IRSTLM (Federico et al., 2008) for the five languages in question.",
        "For English and French a model was trained using the EuroParl corpus (Koehn, 2005).",
        "For Breton, Welsh and Irish we used corpora of text crawled from the web.",
        "To ensure no bias and also since our dataset for Breton was around 1.5 million, we sampled the same size of data for all the five languages.",
        "In order to build a character language model we replaced spaces with the underscore symbol ?",
        "?, and then placed a space character between each character.",
        "Punctuation and non-letter characters are also part of this language model.",
        "For example, the word ?sl?ainte!?",
        "would be broken down into a sequence of {?",
        "s?, 's l?, ?l ?a?, ?",
        "?a i?, ?i n?, ?n t?, ?t e?, ?e !",
        "?, ?!",
        "?}.",
        "3.3 Sequence chunking This section describes the way we apply heuristics to segment and label the input text.",
        "In Figure 2, ?chunks?",
        "represent the list of evaluated tuples of segments and their labelled language, ?buffer?",
        "is the expanding segment.",
        "LANGPREDICT corresponds to any function which is used to determine the language 1 http://indigenoustweets.blogspot.in/2013/12/mapping-celtic-twittersphere.html 77 of the token.",
        "The flag variable helps in implementing the heuristic of minimum segment size while labelling chunks.",
        "Require: s : sentence to chunk 1: buffer = [ ] /*Undecided expanding window of chunk*/ 2: chunks = [ ] /*Decided labelled segment*/ 3: buffer language?",
        "LANGPREDICT(s[0]) /* Language of first word */ 4: flag?",
        "0 5: for all w ?",
        "s do 6: if LANGPREDICT(w)=buffer language then 7: if flag = 1 then 8: buffer?",
        "buffer + [word buffer,w] 9: flag?",
        "0 10: else 11: buffer?",
        "buffer + [ w] 12: if LANGPREDICT(w) ?",
        "=buffer language then 13: if flag= 0 then 14: flag?",
        "1 15: word buffer?",
        "w 16: continue 17: else 18: chunks?",
        "chunks + [(buffer,buffer language)] 19: buffer?",
        "[word buffer,w] 20: buffer language?",
        "LANGPREDICT(w) /* Language of new expanding chunk */ 21: flag?",
        "0 22: if length(buffer) ?",
        "=0 then 23: chunks?",
        "chunks + [(buffer,buffer language)] Figure 2: Chunking Algorithm 3.4 Word-based prediction Designed keeping in mind importance of the most common words, this procedure included checking each word against both of the word lists in question, 2 it is associated to one language or another.",
        "In case of a conflict, for example, when the word exists in both wordlists, or in the case that it is unknown to both, the option of continuing with the previous span was taken and the previous selected tag was labelled, thus increasing the chunk.",
        "3.5 Word-based prediction with character backoff In case of the word being present in only one of the two monolingual word lists the classification is simple, but in case of a conflict, a character bigram backoff was introduced to help us disambiguate the language label.",
        "4 Evaluation For the Evaluation procedure, we follow the footsteps of the CoNLL-2000 shared task on language-independent named entity recognition: dividing text into syntactically related non-overlapping groups of words.",
        "This chunking mechanism (Tjong Kim Sang and De Meulder, 2003) is very similar to ours, in terms of words which only belong to one category (here, language), and also evaluation based on the segment structure present in the data.",
        "The chunks here are such that they belong to only one language.",
        "The evaluation statistics shown in Tables 2 and 3 mention two values for each of the experiment conducted on the three bilingual language datasets.",
        "The first, is the percentage of correctly detected phrases, which is the overall precision and the second is the number of phrases in the data that were found by the chunker, which is the overall recall.",
        "Apart from the techniques discussed in Section 3, some baselines are also used to give a comparative view of how well all the mechanisms perform.",
        "4.1 Baseline We used the language identification tool langid.py (Lui and Baldwin, 2012) on the whole dataset and labelled all the individual lines according to this majority classification.",
        "As no chunking is performed we 2 For the wordlists we used the aspell wordlists widely available on Unix systems.",
        "78 System Irish?English Welsh?English Breton?French Irish English Welsh English Breton French baseline p 2.50 0.0 0.0 0.0 0.0 0.0 r 2.56 0.0 0.0 0.0 0.0 0.0 langid-3character p 5.00 14.29 0.0 21.21 1.85 20.75 r 5.41 8.45 0.0 14.58 1.92 12.36 wordlist p 32.50 28.57 26.69 40.91 57.41 33.96 r 23.64 26.09 26.03 33.75 47.69 33.33 character bigram p 32.50 35.71 23.44 19.70 57.41 52.83 r 22.41 26.79 15.31 16.67 41.33 37.84 wordlist+character bigram p 52.50 50.00 32.81 31.82 70.37 67.92 r 38.18 43.75 24.14 25.61 57.58 57.14 Table 2: Precision, p and recall, r for the systems by language.",
        "System Accuracy (%) Irish?English Welsh?English Breton?French baseline 42.76 42.16 44.07 langid-3character 57.24 45.92 43.16 wordlist 79.75 74.28 83.96 character bigram 81.29 65.62 76.79 wordlist+character bigram 85.79 72.40 88.79 Table 3: Accuracy of the systems over the three language pairs.",
        "The accuracy measures how often a token was assigned to the right language, independent of span.",
        "can expect that the precision and recall will be very low.",
        "However it does provide a reasonable baseline for the per-word accuracy.",
        "4.2 langid character trigram prediction For this system we used the character trigram probabilities to predict the detected language for each token.",
        "Trigrams were chosen after experimenting with 1?5 grams.",
        "The heuristics in Section 3 were followed for the text processing and chunking part of the method.",
        "5 Results As described in Section 3 the data collected from Twitter for the three language pairs, was processed using the techniques mentioned.",
        "The statistics of the same are given in Table 1.",
        "While the precision and recall are low for the remaining models, we see that we are able to improve The performance by combining the wordlist based model with a character bigram model.",
        "And what is more, we are able to begin to not only identify particular words in a language, but also segments.",
        "6 Conclusions This paper has presented a very preliminary investigation into subsegment language identification in Celtic language texts.",
        "We have proposed a model that chunks input text into segments and performs language identification on these segments at the same time.",
        "Precision and recall are low, leaving a lot of room for further work.",
        "Although King and Abney (2013) label on a per word level, yet we would like to include supervised methods and features talked about in this research to improve our efficiency while dealing with segments.",
        "We would also like to attempt our method using higher order character n-gram models for backoff, and n-gram word language models for detection and on more annotated data.",
        "Acknowledgements We thank Kevin Scannell for help with providing data from Twitter and for useful comments and suggestions.",
        "This work was undertaken as part of the Apertium project in the 2014 Google Summer of Code.",
        "79 References"
      ]
    }
  ]
}
