{
  "info": {
    "authors": [
      "Anders Björkelund",
      "Özlem Çetinoğlu",
      "Agnieszka Faleńska",
      "Richárd Farkas",
      "Thomas Mueller",
      "Wolfgang Seeker",
      "Zsolt Szántó"
    ],
    "book": "Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages",
    "id": "acl-W14-6110",
    "title": "Introducing the IMS-Wrocław-Szeged-CIS entry at the SPMRL 2014 Shared Task: Reranking and Morpho-syntax meet Unlabeled Data",
    "url": "https://aclweb.org/anthology/W14-6110",
    "year": 2014
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We summarize our approach taken in the SPMRL 2014 Shared Task on parsing morphologically rich languages.",
        "Our approach builds upon our contribution from last year, with a number of modifications and extensions.",
        "Though this paper summarizes our contribution, a more detailed description and evaluation will be presented in the accompanying volume containing notes from the SPMRL 2014 Shared Task."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "This paper summarizes the approach of IMS-Wroc?aw-Szeged-CIS taken for the SPMRL 2014 Shared Task on parsing morphologically rich languages (Seddah et al., 2014).",
        "Since this paper is a rough summary that is written before submission of test runs we refer the reader to the full description paper which will be published after the shared task (Bjo?rkelund et al., 2014).1 The SPMRL 2014 Shared Task is a direct extension of the SPMRL 2013 Shared Task (Seddah et al., 2013) which targeted parsing morphologically rich languages.",
        "The task involves parsing both dependency and phrase-structure representations of 9 languages: Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish, and Swedish.",
        "The only difference between the two tasks is that large amounts of unlabeled data are additionally available to participants for the 2014 task.",
        "Our contribution builds upon our system from last year (Bjo?rkelund et al., 2013), with additional features and components that try to exploit the unlabeled data.",
        "Given the limited window of time to participate in this year's shared task, we only contribute to the setting with predicted preprocessing, using the largest available training data set for each language.2 We also do not participate in the Arabic track since the shared task organizers did not provide any unlabeled data at a reasonable time.",
        "2 Review of Last Year's System Our current system is based on the system we participated with in the SPMRL 2013 Shared Task.",
        "We summarize the architecture of this system as three different components.",
        "?Authors in alphabetical order 1Due to logistical constraints this paper had to be written before the deadlines for the actual shared task and do thus not contain a full description of the system, nor the experimental evaluation of the same.",
        "2 In other words, no gold preprocessing or smaller training sets.",
        "2.1 Preprocessing As the initial step of preprocessing we converted the Shared Task data from the CoNLL06 format to CoNLL09, which required a decision on using coarse or fine grained POS tags.",
        "After a set of preliminary experiments we picked fine POS tags where possible, except Basque and Korean.",
        "We used MarMoT3 (Mu?ller et al., 2013) to predict POS tags and morphological features jointly.",
        "We integrated the output from external morphological analyzers as features to MarMoT.",
        "We also experimented with the integration of predicted tags provided by the organizers and observed that these stacked models help improve Basque, Polish, and Swedish preprocessing.",
        "The stacked models provided additional information to our tagger since the provided predictions were coming from models trained on larger training sets than the shared task training sets.",
        "2.2 Dependency Parsing The dependency parsing architecture of our SPMRL 2013 Shared Task contribution is summarized in Figure 1.",
        "The first step combines the n-best trees of two parsers, namely the mate parser4 (Bohnet, 2010) and a variant of the EasyFirst parser (Goldberg and Elhadad, 2010), which we call best-first parser.",
        "We merged the 50-best analyses from these parsers into one n-best list of 50 to 100 trees.",
        "We then added parsing scores to the n-best trees from the two parsers, and additionally from the turboparser5 (Martins et al., 2010).",
        "mate parser best-first parser turboparser merged list of 50-100 best trees/sentence merged list scored by all parsers ranker ptb trees Parsing Ranking IN OUT scores scores scores features Figure 1: Architecture of the dependency ranking system from (Bjo?rkelund et al., 2013).",
        "The scored trees are fed into the ranking system.",
        "The ranker utilizes the parsing scores and features coming from both constituency and dependency parses.",
        "We specified a default feature set and experimented with additional features for each language for optimal results.",
        "We achieved over 1% LAS improvement on all languages except a 0.3% improvement on Hungarian.",
        "2.3 Constituency Parsing The constituency parsing architecture advances in three steps.",
        "For all setups we removed the morphological annotation of POS tags and the function labels of non-terminals and apply the Berkeley Parser (Petrov et al., 2006) as our baseline.",
        "As the first setup, we replaced words with a frequency < 20 with their predicted part-of-speech and morphology tags and improved the PARSEVAL scores across languages.",
        "The second setup employed a product grammar (Petrov, 2010), where we combined 8 different grammars trained on the same data but with different initialization setups.",
        "As a result, the scores substantially improved on all languages.",
        "Finally, we conducted ranking experiments on the 50-best outputs of the product grammars.",
        "We used a slightly modified version of the Mallet toolkit (McCallum, 2002), where the reranker is trained for the 3https://code.google.com/p/cistern/ 4https://code.google.com/p/mate-tools 5http://www.ark.cs.cmu.edu/TurboParser/ 98 maximum entropy objective function of Charniak and Johnson (2005) and uses the standard feature set from Charniak and Johnson (2005) and Collins (2000).",
        "Hebrew and Polish scores remained almost the same, whereas Basque, French, and Hungarian highly benefited from reranking.",
        "3 Planned Additions to Last Year's System This year we extend our systems for both the constituency and dependency tracks to add additional information and try to profit from unlabeled data.",
        "3.1 Preprocessing We use the mate-tools?",
        "lemmatizer and MarMoT to preprocess all labeled and unlabeled data.",
        "From the SPMRL 2013 Shared Task, we learned that getting as good preprocessing as possible is an important part of the overall improvements.",
        "Preprocessing consists of predicting lemmas, part-of-speech, and morphological features.",
        "Preprocessing for the training data is done via 5-fold jackknifing to produce realistic input features for the parsers.",
        "This year we do not do stacking on top of provided morphological analyses since the annotations on the labeled and unlabeled data were inconsistent for some languages.6 3.2 Dependency Parsing We pursue two different ways of integrating additional information into our system from the SPMRL 2013 Shared Task (Bjo?rkelund et al., 2013): supertags and co-training.",
        "Supertags (Bangalore and Joshi, 1999) are tags that encode more syntactic information than standard part-of-speech tags.",
        "Supertags have been used in deep grammar formalisms like CCG or HPSG to prune the search space for the parser.",
        "The idea has been applied to dependency parsing by Foth et al. (2006) and recently to statistical dependency parsing (Ouchi et al., 2014; Ambati et al., 2014), where supertags are used as features rather than to prune the search space.",
        "Since the supertag set is dynamically derived from the gold-standard syntactic structures, we can encode different kinds of information into a supertag, in particular also morphological information.",
        "Supertags are predicted before parsing using MarMoT and are then used as features in the mate parser and the turboparser.",
        "We will use a variant of co-training (Blum and Mitchell, 1998) by applying two different parsers to select additional training material from unlabeled data.",
        "We use the mate parser and the turboparser to parse the unlabeled data provided by the organizers.",
        "We then select sentences where both parsers agree on the structure as additional training examples following Sagae and Tsujii (2007).",
        "We then train two more models: one on the labeled training data and the unlabeled data selected by the two parsers, and one only on the unlabeled data.",
        "These two models are then integrated into our parsing system from 2013 as additional scorers to score the n-best list.",
        "Their scores are used as features in the ranker.",
        "Before we parse the unlabeled data to obtain the training sentences, we filter it in order to arrive at a cleaner corpus.",
        "Most importantly, we only keep sentences up to length 50, and which contain at maximum two unknown words (compared to the labeled training data).",
        "3.3 Constituency Parsing We experiment with two approaches for improving constituency parsing: Preterminal labelsets play an important role in constituency parsing of morphologically rich languages (Dehdari et al., 2011).",
        "Instead of removing the morphological annotation of POS tags, we use a preterminal set which carries more linguistic information while still keeping it compact.",
        "We follow the merge procedure for morphological feature values of Sza?nto?",
        "and Farkas (2014).",
        "This procedure outputs a clustering of full morphological descriptions and we use the cluster IDs as preterminal labels for training the Berkeley Parser.",
        "Reranking at the constituency parsing side is enriched by novel features.",
        "We define feature templates exploiting co-occurrence statistics from the unlabeled datasets; automatic dependency parses of the sentence in question (Farkas and Bohnet, 2012); Brown clusters (Brown et al., 1992); and atomic morphological feature values (Sza?nto?",
        "and Farkas, 2014).",
        "6The organizers later resolved this issue by patching the data, although time constraints prevented us from using the patched data.",
        "99 4 Conclusion This paper describes our plans for the SPMRL 2014 Shared Task, most of which are yet to be implemented.",
        "For the actual system description and our results, we refer the interested reader to (Bjo?rkelund et al., 2014) and (Seddah et al., 2014).",
        "Acknowledgements Agnieszka Falen?ska is funded through the Project International computer science and applied mathematics for business study programme at the University of Wroc?aw co-financed with European Union funds within the European Social Fund No.",
        "POKL.04.01.01-00-005/13.",
        "Richa?rd Farkas and Zsolt Sza?nto?",
        "are funded by the European Union and the European Social Fund through the project FuturICT.hu (grant no.",
        ": TA?MOP-4.2.2.C-11/1/KONV-2012-0013).",
        "Thomas Mu?ller is supported by a Google Europe Fellowship in Natural Language Processing.",
        "The remaining authors are funded by the Deutsche Forschungsgemein-schaft (DFG) via the SFB 732, projects D2 and D8 (PI: Jonas Kuhn).",
        "We also express our gratitude to the treebank providers for each language: Arabic (Maamouri et al., 2004; Habash and Roth, 2009; Habash et al., 2009; Green and Manning, 2010), Basque (Aduriz et al., 2003), French (Abeille?",
        "et al., 2003), Hebrew (Sima?an et al., 2001; Tsarfaty, 2010; Goldberg, 2011; Tsarfaty, 2013), German (Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian (Csendes et al., 2005; Vincze et al., 2010), Korean (Choi et al., 1994; Choi, 2013), Polish (S?widzin?ski and Wolin?ski, 2010), and Swedish (Nivre et al., 2006).",
        "References"
      ]
    }
  ]
}
