{
  "info": {
    "authors": [
      "Benny Brodda"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C67-1036",
    "title": "The Entropy of Recursive Markov Processes",
    "url": "https://aclweb.org/anthology/C67-1036",
    "year": 1967
  },
  "references": [],
  "sections": [
    {
      "text": [
        "The Entropy of Recursive Markov Processes By"
      ]
    },
    {
      "heading": "BENNY BRODDA",
      "text": [
        "The work reported in this paper has been sponsored by Humanistiska forskningsr&det, Tekniska forskningsr&det and Riksbankens Jubileums-fond, Stockholm, Sweden."
      ]
    },
    {
      "heading": "Summary",
      "text": [
        "The aim of this communication is to obtain an explicit formula for calculating the entropy of a source which behaves in accordance with the rules of an arbitrary Phrase Structure Grammar, in which relative probabilities are attached to the rules in the grammar.",
        "With this aim in mind we introduce an altetnative definition of the concept of a PSG as a set of self-embedded (recursive) Finite State Grammars; when the probabilities are taken into account in such a grammar we call it a Recursive Markov Process.",
        "1.",
        "In the first section we give a more detailed definition of what kind of Markov Processes we are going to generalize later on (in sec. 3), and we also outline the concept of entropy in an ordinary Markov source.",
        "More details of information may be found, e.g., in Khinchins \"Mathematical Foundations of Information Theory\", N. Y., 1957, or \"Information Theory\" by R. Ash, N. Y., 1965.",
        "A Markov Grammar is defined as a Markov Source with the following properties: Assume that there are n + 1 states, say So, SI, Sn, in the source.",
        "So is defined as the initial state and Sn is defined as the final state and the other states are called intermediate states.",
        "We shall, of course, also have a transition matrix, M = (pd), containing the transition probabilities of the source.",
        "a) A transition from state S1 state Skis always accompanied by a production of a.",
        "(non-zero) letterai .k from a given finite alphabet.",
        "Transition to different states from one given state always produce different letters.",
        "b) From the initial state, So, direct or indirect transitions should be possible to any other state in the source.",
        "From no state is a transition to So allowed.",
        "c) From any state, direct or indirect transitions to the final state Sn should be possible.",
        "From Sn no transition is allowed to any other state (Sn is an \"absorbing state\").",
        "A (grammatical) sentence should now be defined as the (left-to-right) concatenation of the letters produced by the source, when passing from the initial state to the final state.",
        "The length of a sentence is defined as the number of letters in the sentence.",
        "To simplify matters without dropping much of generality we also require that d) The greatest common divisor for all the possible lengths of sentences is = 1 (i.e., the source becomes an aperiodic source, if it is short-circuited by identifying the final and initial states)., With the properties a - d above, the source obtained by identifying the final and initial states is an indecomposable, ergodic Markov process (cf. Feller, \"Probability Theory and Its Applications\", ch.",
        "15, N. Y., 1950).",
        "In the transition matrix M for a Markov grammar of our type all elements in the first column are zero, and in the last row all elements are zero except the last one which is = 1.",
        "For a given Markov grammar we define the uncertainty or entropy, Hi, for each state Si, i 0, 1, n, as: 1H.",
        "= p.. log p..; i = 1, 2, n. j=0 We also define the entropy, H or H(M), for the grammar as n= (1).",
        "H x.H.",
        "11 where x = (x0, xz, xn_1) is defined as the stationary distribution?or the source obtained when S 0 and Sn are identified; thus x is defined as the (unique) solution to the set of simultaneous equations (2) xM,1 = x xo +x1 + + xn-1 =1 where M1 is formed by shifting the last and first columns and then omitting the last row and column.",
        "The mean sentence length, p,, of the set of grammatical sentences can now be easily calculated as (3) = 1/x0 (cf. Feller, op.",
        "cit.)",
        "2.",
        "Embedded Grammars We now assume that we have two Markov grammars, M and MI, with states So, Si, Sn, and To, T/, Tm, respectively, where So and Sn, To and T are the corresponding initial and final states.",
        "Now consider two states S. and Skin the grammar M; assume that the corresponding transition probability is = Pik• We now transform the grammar, MI, into a new one, M, by embedding the grammar M2 in M1 between the states Si and Sk, an operation which is performed by identifying the states To and T with the 'states S. and Sk respectively.",
        "Or, to be more precise, assume that in the g jrammar M1 the transitions to the states T., has the probabilities• • u.",
        "Then, ll J take place with the probability =.pikqoi.",
        "A return to the state Skin the \"main\" grammar from an intermediate state T. in M1 takes place with the probability q• • M With the conditions above fulfilled, we propose that the entropy for the composed grammar be Calculated according to the formula: (4) H _ H (M) xiPik 1.1,1 H(M 1) 1 + xipik (p1 - 1) where H(M) is the entropy of the grammar M when there is an ordinary connection (with probability pik) between the states Si and Sk, and where x. is the inherent probability of being in the state Si under the same conditions.",
        "ill is the mean sentence length of the sentences produced by the grammar MI alone.",
        "(It is quite natural that this number appears as a weight in the formula, since if one is producing a sentence according to the grammar M and arrives at the state S. and from there \"dives\" into the grammar MI, then p, is the expected waiting time for emerging again in the main grammar M.) The factor x.p.",
        "may be interpreted as the combined probability of ever arriving at Si and there choosing the path over to M1 (you may, of course, choose quite another path from S.).",
        "3 The proof of formula (4) is very straightforward, once the premises according to the above have been given, and we omit it here, as it does not give much extra insight to the theory.",
        "The formula may be extended to the case when there are more than one sub-grammar embedded in the grammar M', by adding similar terms as the one standing to the right in the numerator and the denominator.",
        "The important thing here is that the factors of the type x.p.",
        "depend only on the probability matrix for the grammar M and are dependent of the sub-grammars involved.",
        "3.",
        "Recursive or Self-embedded Sources It is now quite natural to allow a grammar to have itself as a sub-grammar or to allow a grammar Mi to contain a grammar M2 which, in its turn, contains MI, and so on.",
        "The grammars thus obtained cannot, however, be rewritten as an ordinary Markov grammar.",
        "The relation between an ordinary Markov grammar and a recursive one is exactly similar to the relation between Finite State Languages and Phrase Structure Languages.",
        "To be more precise, assume that we have a set of Markov grammars MIS, M'„, where Mlo is called the main grammar and in the sense that the process always starts at the initial state in M; and ceases when it reaches the final state in M0' Each of the grammars may contain any number of the others (and itself) as sub-grammars.",
        "The only restriction is that from any state in any one of the grammars there should exist a path which ends up at the final state of Mo.",
        "Remark If we interpret a source of our kind as a Phrase Structure Language, the rewriting rules are all of the following kind: (5) S. A.",
        "+ S or Sn 0 #; where the S' s are all non-terminal symbols.",
        "(They stand for the names of the states in the sources - INA6a • • • a ' and where So is assumed to be the initial symbol /the Chomskyan S/ and Sn is the terminating state which produces the sentence delimiter #.",
        "The symbols Aik are either terminal symbols /letters from a finite ailphabet/ or non-terminal symbols equal to the name of the initial state in one of the grammars M;, M, ..., MI\\T /one may 4 also say that A. stands as an abbreviation for an arbitrary Sentence of that grammar/.)",
        "We associate each grammar M!",
        "with the grammar M., j = 0, 1, N, by just considering it as a non-recursive one, that is, we consider all the symbols Aik as terminal symbols (even if they are not).",
        "The grammars thus obtained are ordinarily Markov grammars according to our definition, and the e Hntropies H. = (M.) are easily computed according to formula (1), as are the stationary distributions /formula (2)/.",
        "The follwoing theorem shows how the entropies H!",
        "for the fully recursive grammars M!",
        "are connected with the numbers Hj.",
        "Theorem The entropy Hj for a set of recursive Markov grammar IV, j = 0, 1, N, can be calculated according to the formula (6) H!",
        "(1+1y.",
        "(p. - 1)3 1 y. LI 1-11 = H. J jk k jk -lc k j k k j= 0, 1, ..., N. Here the factors yjk are dependent only of the probability matrix of the grammar and the numbers Ilk defined as the mean sentence length of the - sentences of the grammar M, k = 0, 1, N, and computable according to lemma below.",
        "H is the entropy for the grammar.",
        "0 The theorem above is a direct application for the grammar of formula (4), sec. 2.",
        "The coefficients yjk in formula (6) can, more precisely, be calculated as a sum of terms of the type xipim with the indices (i, m) are where the grammar /v0 appears in the grammar M!• x. and p. are the components the stationary distribution and probability matrix for the grammar M. Assume now that we have a Markov grammar of our type, but for which each transition will take a certain amount of time.",
        "A very natural question is then:_ \"What is the expected time to produce a sentence in that language?\"",
        "The answer is in the following lemma.",
        "Lemma Let M be a Markov grammar with states Si, i = 0, 1, n, where So and Sn are the initial and final states respectively.",
        "Assume that each transition S. Sk will take y, time units.",
        "1 Denote the expected time for arrival at Sn given that the grammar is in state S. by t., i = 0, 1, n, (thus t0 is the expected time for producing a sentence).",
        "The times t1 will then fulfill the following set of simultaneously linear equations: (7) pik (tik + tk) Formula (7) is itself a proof of the lemma.",
        "With more convenient notations we can write (7) as (E - P) t = pt where E is the unit matrix, P is the probability matrix (with P = 0) and pt is the vector with components Pi (t) =P• t. , 11M 1M = 0, 1, .",
        "n. The application of he lemma for computing the numbers p,k in formula (6) is now the following.",
        "The transition times of the lemma are, of course, the expected time (or \"lengths\" as we have called it earlier) for passing via a sub-grammar of the grammar under consideration.",
        "Thus the number tik ii-litself the unknown entities w,k.",
        "6 For each of the sub-grammars M!",
        "j = 0, 1, N, we get a set of linear equations of type (n for determining the vectors t of lemma.",
        "The first component of this vector, i.e., the number to, is then equal to the expected length, of the sentences of that grammar.",
        "(Unfortunately, we have to compute extra the expected time for going from any state of the sub-grammars to the corresponding final state.)",
        "• The total number of unknowns involved when computing the entropy of our grammar (i.e., the entropy Hb) is equal to (the total number of states in all our sub-grammars) plus (the number of sub-grammars).",
        "This is also the number of equations for we have n + 1 equations from formula (6) and then (n + 1) sets of equations of the type (7).",
        "We assert that all these simultaneous equations are solvable, if the grammar fulfills the conditions we earlier stated for the grammar, i.e., that from each state in any sub-grammar exists at least one path to the final state of that grammar.",
        "7",
        "1H.",
        "= p.. log p..; i = 1, 2, n. j=0 We also define the entropy, H or H(M), for the grammar as n= (1).",
        "H x.H.",
        "11 where x = (x0, xz, xn_1) is defined as the stationary distribution?or the source obtained when S 0 and Sn are identified; thus x is defined as the (unique) solution to the set of simultaneous equations (2) xM,1 = x xo +x1 + + xn-1 =1 where M1 is formed by shifting the last and first columns and then omitting the last row and column.",
        "The mean sentence length, p,, of the set of grammatical sentences can now be easily calculated as (3) = 1/x0 (cf. Feller, op.",
        "cit.)",
        "2.",
        "Embedded Grammars We now assume that we have two Markov grammars, M and MI, with states So, Si, Sn, and To, T/, Tm, respectively, where So and Sn, To and T are the corresponding initial and final states.",
        "Now consider two states S. and Skin the grammar M; assume that the corresponding transition probability is = Pik• We now transform the grammar, MI, into a new one, M, by embedding the grammar M2 in M1 between the states Si and Sk, an operation which is performed by identifying the states To and T with the 'states S. and Sk respectively.",
        "Or, to be more precise, assume that in the g jrammar M1 the transitions to the states T., has the probabilities• • u.",
        "Then, ll J take place with the probability =.pikqoi.",
        "A return to the state Skin the \"main\" grammar from an intermediate state T. in M1 takes place with the probability q• • M With the conditions above fulfilled, we propose that the entropy for the composed grammar be Calculated according to the formula: (4) H _ H (M) xiPik 1.1,1 H(M 1) 1 + xipik (p1 - 1) where H(M) is the entropy of the grammar M when there is an ordinary connection (with probability pik) between the states Si and Sk, and where x. is the inherent probability of being in the state Si under the same conditions.",
        "ill is the mean sentence length of the sentences produced by the grammar MI alone.",
        "(It is quite natural that this number appears as a weight in the formula, since if one is producing a sentence according to the grammar M and arrives at the state S. and from there \"dives\" into the grammar MI, then p, is the expected waiting time for emerging again in the main grammar M.) The factor x.p.",
        "may be interpreted as the combined probability of ever arriving at Si and there choosing the path over to M1 (you may, of course, choose quite another path from S.).",
        "3 The proof of formula (4) is very straightforward, once the premises according to the above have been given, and we omit it here, as it does not give much extra insight to the theory.",
        "The formula may be extended to the case when there are more than one sub-grammar embedded in the grammar M', by adding similar terms as the one standing to the right in the numerator and the denominator.",
        "The important thing here is that the factors of the type x.p.",
        "depend only on the probability matrix for the grammar M and are dependent of the sub-grammars involved.",
        "3.",
        "Recursive or Self-embedded Sources It is now quite natural to allow a grammar to have itself as a sub-grammar or to allow a grammar Mi to contain a grammar M2 which, in its turn, contains MI, and so on.",
        "The grammars thus obtained cannot, however, be rewritten as an ordinary Markov grammar.",
        "The relation between an ordinary Markov grammar and a recursive one is exactly similar to the relation between Finite State Languages and Phrase Structure Languages.",
        "To be more precise, assume that we have a set of Markov grammars MIS, M'„, where Mlo is called the main grammar and in the sense that the process always starts at the initial state in M; and ceases when it reaches the final state in M0' Each of the grammars may contain any number of the others (and itself) as sub-grammars.",
        "The only restriction is that from any state in any one of the grammars there should exist a path which ends up at the final state of Mo.",
        "Remark If we interpret a source of our kind as a Phrase Structure Language, the rewriting rules are all of the following kind: (5) S. A.",
        "+ S or Sn 0 #; where the S' s are all non-terminal symbols.",
        "(They stand for the names of the states in the sources - INA6a • • • a ' and where So is assumed to be the initial symbol /the Chomskyan S/ and Sn is the terminating state which produces the sentence delimiter #.",
        "The symbols Aik are either terminal symbols /letters from a finite ailphabet/ or non-terminal symbols equal to the name of the initial state in one of the grammars M;, M, ..., MI\\T /one may 4 also say that A. stands as an abbreviation for an arbitrary Sentence of that grammar/.)",
        "We associate each grammar M!",
        "with the grammar M., j = 0, 1, N, by just considering it as a non-recursive one, that is, we consider all the symbols Aik as terminal symbols (even if they are not).",
        "The grammars thus obtained are ordinarily Markov grammars according to our definition, and the e Hntropies H. = (M.) are easily computed according to formula (1), as are the stationary distributions /formula (2)/.",
        "The follwoing theorem shows how the entropies H!",
        "for the fully recursive grammars M!",
        "are connected with the numbers Hj.",
        "Theorem The entropy Hj for a set of recursive Markov grammar IV, j = 0, 1, N, can be calculated according to the formula (6) H!",
        "(1+1y.",
        "(p. - 1)3 1 y. LI 1-11 = H. J jk k jk -lc k j k k j= 0, 1, ..., N. Here the factors yjk are dependent only of the probability matrix of the grammar and the numbers Ilk defined as the mean sentence length of the - sentences of the grammar M, k = 0, 1, N, and computable according to lemma below.",
        "H is the entropy for the grammar.",
        "0 The theorem above is a direct application for the grammar of formula (4), sec. 2.",
        "The coefficients yjk in formula (6) can, more precisely, be calculated as a sum of terms of the type xipim with the indices (i, m) are where the grammar /v0 appears in the grammar M!• x. and p. are the components the stationary distribution and probability matrix for the grammar M. Assume now that we have a Markov grammar of our type, but for which each transition will take a certain amount of time.",
        "A very natural question is then:_ \"What is the expected time to produce a sentence in that language?\"",
        "The answer is in the following lemma.",
        "Lemma Let M be a Markov grammar with states Si, i = 0, 1, n, where So and Sn are the initial and final states respectively.",
        "Assume that each transition S. Sk will take y, time units.",
        "1 Denote the expected time for arrival at Sn given that the grammar is in state S. by t., i = 0, 1, n, (thus t0 is the expected time for producing a sentence).",
        "The times t1 will then fulfill the following set of simultaneously linear equations: (7) pik (tik + tk) Formula (7) is itself a proof of the lemma.",
        "With more convenient notations we can write (7) as (E - P) t = pt where E is the unit matrix, P is the probability matrix (with P = 0) and pt is the vector with components Pi (t) =P• t. , 11M 1M = 0, 1, .",
        "n. The application of he lemma for computing the numbers p,k in formula (6) is now the following.",
        "The transition times of the lemma are, of course, the expected time (or \"lengths\" as we have called it earlier) for passing via a sub-grammar of the grammar under consideration.",
        "Thus the number tik ii-litself the unknown entities w,k.",
        "6 For each of the sub-grammars M!",
        "j = 0, 1, N, we get a set of linear equations of type (n for determining the vectors t of lemma.",
        "The first component of this vector, i.e., the number to, is then equal to the expected length, of the sentences of that grammar.",
        "(Unfortunately, we have to compute extra the expected time for going from any state of the sub-grammars to the corresponding final state.)",
        "• The total number of unknowns involved when computing the entropy of our grammar (i.e., the entropy Hb) is equal to (the total number of states in all our sub-grammars) plus (the number of sub-grammars).",
        "This is also the number of equations for we have n + 1 equations from formula (6) and then (n + 1) sets of equations of the type (7).",
        "We assert that all these simultaneous equations are solvable, if the grammar fulfills the conditions we earlier stated for the grammar, i.e., that from each state in any sub-grammar exists at least one path to the final state of that grammar.",
        "7",
        "We also define the entropy, H or H(M), for the grammar as",
        "where x = (x0, xz, xn_1) is defined as the stationary distribution?or the source obtained when S 0 and Sn are identified; thus x is defined as the (unique) solution to the set of simultaneous equations",
        "where M1 is formed by shifting the last and first columns and then omitting the last row and column.",
        "The mean sentence length, p,, of the set of grammatical sentences can now be easily calculated as"
      ]
    },
    {
      "heading": "2. Embedded Grammars",
      "text": [
        "We now assume that we have two Markov grammars, M and MI, with states So, Si, Sn, and To, T/, Tm, respectively, where So and Sn, To and T are the corresponding initial and final states.",
        "Now consider two states S. and Skin the grammar M; assume that the corresponding transition probability is = Pik• We now transform the grammar, MI, into a new one, M, by embedding the grammar M2 in M1 between the states Si and Sk, an operation which is performed by identifying the states To and T with the 'states S. and Sk respectively.",
        "Or, to be more precise, assume that in the",
        "take place with the probability =.pikqoi.",
        "A return to the state Skin the \"main\" grammar from an intermediate state T. in M1 takes place with the probability",
        "With the conditions above fulfilled, we propose that the entropy for the composed grammar be Calculated according to the formula:",
        "where H(M) is the entropy of the grammar M when there is an ordinary connection (with probability pik) between the states Si and Sk, and where x. is the inherent probability of being in the state Si under the same conditions.",
        "ill is the mean sentence length of the sentences produced by the grammar MI alone.",
        "(It is quite natural that this number appears as a weight in the formula, since if one is producing a sentence according to the grammar M and arrives at the state S. and from there \"dives\" into the grammar MI, then p, is the expected waiting time for emerging again in the main grammar M.) The factor x.p.",
        "may be interpreted as the combined probability of ever arriving at Si and there choosing the path over to M1 (you may, of course, choose quite another path from S.).",
        "The proof of formula (4) is very straightforward, once the premises according to the above have been given, and we omit it here, as it does not give much extra insight to the theory.",
        "The formula may be extended to the case when there are more than one sub-grammar embedded in the grammar M', by adding similar terms as the one standing to the right in the numerator and the denominator.",
        "The important thing here is that the factors of the type x.p.",
        "depend only on the probability matrix for the grammar M and are dependent of the sub-grammars involved."
      ]
    },
    {
      "heading": "3. Recursive or Self-embedded Sources",
      "text": [
        "It is now quite natural to allow a grammar to have itself as a sub-grammar or to allow a grammar Mi to contain a grammar M2 which, in its turn, contains MI, and so on.",
        "The grammars thus obtained cannot, however, be rewritten as an ordinary Markov grammar.",
        "The relation between an ordinary Markov grammar and a recursive one is exactly similar to the relation between Finite State Languages and Phrase Structure Languages.",
        "To be more precise, assume that we have a set of Markov grammars MIS, M'„, where Mlo is called the main grammar and in the sense that the process always starts at the initial state in M; and ceases when it reaches the final state in M0' Each of the grammars may contain any number of the others (and itself) as sub-grammars.",
        "The only restriction is that from any state in any one of the grammars there should exist a path which ends up at the final state of Mo."
      ]
    },
    {
      "heading": "Remark",
      "text": [
        "If we interpret a source of our kind as a Phrase Structure Language, the rewriting rules are all of the following kind:",
        "(5) S. A.",
        "+ S or Sn 0 #;",
        "where the S' s are all non-terminal symbols.",
        "(They stand for the names of the states in the sources - INA6a • • • a ' and where So is assumed to be the initial symbol /the Chomskyan S/ and Sn is the terminating state which produces the sentence delimiter #.",
        "The symbols Aik are either terminal symbols /letters from a finite ailphabet/ or non-terminal symbols equal to the name of the initial state in one of the grammars M;, M, ..., MI\\T /one may",
        "also say that A. stands as an abbreviation for an arbitrary Sentence of that grammar/.)",
        "We associate each grammar M!",
        "with the grammar M., j = 0, 1, N, by just considering it as a non-recursive one, that is, we consider all the symbols Aik as terminal symbols (even if they are not).",
        "The grammars thus obtained are ordinarily Markov grammars according to our definition, and the e Hntropies H. = (M.) are easily computed according to formula (1), as are the stationary distributions /formula (2)/.",
        "The follwoing theorem shows how the entropies H!",
        "for the fully recursive grammars M!",
        "are connected with the numbers Hj."
      ]
    },
    {
      "heading": "Theorem",
      "text": [
        "The entropy Hj for a set of recursive Markov grammar IV, j = 0, 1, N, can be calculated according to the formula",
        "Here the factors yjk are dependent only of the probability matrix of the grammar and the numbers Ilk defined as the mean sentence length of the - sentences of the grammar M, k = 0, 1, N, and computable according to lemma below.",
        "The theorem above is a direct application for the grammar of formula (4), sec. 2.",
        "The coefficients yjk in formula (6) can, more precisely, be calculated as a sum of terms of the type xipim with the indices (i, m) are where the grammar /v0 appears in the grammar M!• x. and p. are the components the stationary distribution and probability matrix for the grammar M. Assume now that we have a Markov grammar of our type, but for which each transition will take a certain amount of time.",
        "A very natural question is then:_ \"What is the expected time to produce a sentence in that language?\"",
        "The answer is in the following lemma.",
        "Lemma Let M be a Markov grammar with states Si, i = 0, 1, n, where So and Sn are the initial and final states respectively.",
        "Assume that each transition S. Sk will take y, time units.",
        "1 Denote the expected time for arrival at Sn given that the grammar is in state S. by t., i = 0, 1, n, (thus t0 is the expected time for producing a sentence).",
        "The times t1 will then fulfill the following set of simultaneously linear equations:",
        "With more convenient notations we can write (7) as",
        "where E is the unit matrix, P is the probability matrix (with P = 0) and pt is the vector with components",
        "The application of he lemma for computing the numbers p,k in formula (6) is now the following.",
        "The transition times of the lemma are, of course, the expected time (or \"lengths\" as we have called it earlier) for passing via a sub-grammar of the grammar under consideration.",
        "Thus the number tik ii-litself the unknown entities w,k.",
        "6 For each of the sub-grammars M!",
        "j = 0, 1, N, we get a set of linear equations of type (n for determining the vectors t of lemma.",
        "The first component of this vector, i.e., the number to, is then equal to the expected length, of the sentences of that grammar.",
        "(Unfortunately, we have to compute extra the expected time for going from any state of the sub-grammars to the corresponding final state.)",
        "• The total number of unknowns involved when computing the entropy of our grammar (i.e., the entropy Hb) is equal to (the total number of states in all our sub-grammars) plus (the number of sub-grammars).",
        "This is also the number of equations for we have n + 1 equations from formula (6) and then (n + 1) sets of equations of the type (7).",
        "We assert that all these simultaneous equations are solvable, if the grammar fulfills the conditions we earlier stated for the grammar, i.e., that from each state in any sub-grammar exists at least one path to the final state of that grammar."
      ]
    }
  ]
}
