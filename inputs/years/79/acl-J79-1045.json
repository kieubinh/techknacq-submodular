{
  "info": {
    "authors": [
      "Madeleine Bates"
    ],
    "book": "ACL Microfiche Series 1-83, Including Computational Linguistics",
    "id": "acl-J79-1045",
    "title": "Syntax in Automatic Speech Understanding",
    "url": "https://aclweb.org/anthology/J79-1045",
    "year": 1979
  },
  "references": [],
  "sections": [
    {
      "heading": "SYNTAX IN AUTOMATIC SPEECH UNDERSTANDLNG MADELEINE BATES",
      "text": [
        "Boston University and Bolt Beranek and Newman Inc."
      ]
    },
    {
      "heading": "5 *ore Details of the Parsing Process 10",
      "text": [
        "Depth vs.",
        "Breadth 10"
      ]
    },
    {
      "heading": "Scoring Paths 42 Storing Predictions 46",
      "text": [
        "Examples and Results Example 1 Examole 2 Example 3"
      ]
    },
    {
      "heading": "7 Conclusions 78",
      "text": []
    },
    {
      "heading": "Strengths and Weaknesses of SPARSER 78 Prosodies 80 Extensions and Further Research 83",
      "text": []
    },
    {
      "heading": "Conclusion 8q Appendix I MINIGRAMMAR 37 Appendix II The Vocabulary and Syntax Classes 89 Bibliography 93",
      "text": [
        "Page 3 Section 1 Introduction Understanding speech is an extremely complex process which requires the use of many types of knowledge, one of which is syntax.",
        "This report presents a system called SPARSER which is designed to provide and use the syntactic knowledge necessary to support an artificial speech understanding system.",
        "(We will assume for the remainder of this paper that unless explicitly stated otherwise \"speech\" means grammatical speech spoken at a mogerate rate with natural inflections and pauses, spontaneously produced but similar to the type of speech produced by reading text.)",
        "We will make the following assumptions about the characteristics of speech and a speech processor:",
        "1.",
        "There is not enough information in the spech signal to uniquelSr identify the phonemes or words in a normally spoken utterance.",
        "2.",
        "The acoustic processing component of any artificial speech understanding system will introduce additional errors and ambiguity as it attempts to identify the phonemes.",
        "or words in the utterance.",
        "3.",
        "As a consequence of 1 and 2, when an utterance is scanned to try to identify the words, it is reasonable to suppose that a",
        "number of (perhaps overlapping) candidates will be found.",
        "Page 4 This is illustrate in Figure 1.1 by a structure called a word lattice which shows schematically that many Vords may initially appear to be present.",
        "In this representation, the numbers along the horizontal Scale are segment boundary points in the utterance which roughly correspond to points in time This word lattice was produced by the lexical retrieval component of the BBN speech understanding system from an utterance which had been segmented and labeled by hand under conditions designed to simulate the performance of an automatic segmenter and labeler."
      ]
    },
    {
      "heading": "A Word Lattice Sentence: Give me all glass samples with magnetite.",
      "text": [
        "In the system described here, such a word lattice can be represented by a collection of word matches, each of which is composed of a word, the boundary points at the left and right ends of the portion of the utterance where it appears to match well, and a score indicating how well it matches' the ideal phonemic representation of the word.",
        "Page 5 We also make a number of assumptiOns about the nature of the speech understanding process and the characteristics of a system to carry out that process:",
        "1.",
        "People can understand 'a speaker even when the speech •is",
        "fairly ungramfttical, so a syntax-driven system which would accept only input me4ing rigid syntactic requirements would not be adequate for natural, converstional speech.",
        "2.",
        "Since a number o word candidates are likely to be found throughout the utterlane, it may be fruitful to be able to select a subset of them on semantic, pragmatic, or prosodic grounds as well as syntactic, depending on which cues seem most robust.",
        "3.",
        "Syntax must interact with semantics in order to cut down the combinatorial explosion of syntactically correct but meaningless",
        "subsets of the utterance.",
        "Even in the small word lattice of Figure 1.1 lt can be seen that there are numerous short sequences which are syntactically but not semantically valid (e.g. \"Ten people are glass samples with magnetite\", \"glaes samples give magnetite\", \"lunar samples give magnetite\", \"samples give lead\", \"people are percent\", etc.).",
        "4.",
        "The input to a speech parser will be similar to the word lattice described above, thus the parser will have to face not only the problem that one or more words in its input might be incorrect, but that gaps may ppear in the input as well.",
        "5.",
        "The parser will have to have the ability to predict words and syntactic classes which are consistent with partial hypotheses about the content of the sentence in order to help fill gaps in the lattice.",
        "6.",
        "Because of the combinatorial explosion of syntactic",
        "Page 6 alternatives which occurs when all syntactic possibilities are explored for small sections of an utterance, the syntactic component must limit the number of such alternatives which are actually generated, or at least factor them or treat them implicitly rather than explicitly.",
        "One way of partially solving this problem is to order the alternatives in such a way that only the best alternatives al-se extended.",
        "Section 2 The BBN Speech Understanding System In the past few years there has been a flurry of activity in the field of automatic speech understanding, resulting in a number of different systems.",
        "For surveys of a number of these systems the reader is recommended to Woif (31], Bates[4], and Hyde (10].",
        "For more specific details on some of the individual systems, see [1, 2, 7, 8, 16, 19, 20, 21, 22, 28, 29, 33, 353.",
        "Since SPARSER was implemented as part of a speech understanding system called SPEECHLIS which is under development at Bolt Beranek and Newman Inc., that system is briefly described here and is further documented in [31 4, 5, 6, 15, 23, 24, 26, 33, 35].",
        "SPEECHLIS has used two task domains; that of the LUNAR text question-answering system [36] which deals with chemical analyses of Apollo 11 moon rocks and one dealing with travel budget management."
      ]
    },
    {
      "heading": "Page 7",
      "text": [
        "The overall design of the systein is illustrated in Figure 2.4.",
        "The acoustics component analyzes the acoustic signal to extract features and segment the utterance into a lattice of alternative possible sequences of phonemes (Schwartz and Makhoul [26]), phonological rules augment the output of the acoustic component to include sequences of phonemes which could have resulted in the observed phonemes; the lexical retrieval component retrieves words from the lexicon on the basis of this information (Rovner, et.al.",
        "[24)); the word matcher determines the degree to which the ideal phonetic spelling of a given word matches the acoustic analysis at a particular location [24].",
        "All of these components structure their output in such a way ds to represent the ambiguity whi.",
        ";11 is inherent in their analyses.",
        "For example, they can be used to produce word lattices such as that which was shown in Figure 1.1.",
        "The syntactic component is SPARSER, the system comprising the body of this paper (see also Bates [3, 4]).",
        "Acceptable utterances are not restricted to context-free syntax, since the grammar which SPARSER uses is a molified ATN grammar, capable of handling a large, natural subset of English.",
        "The remaining sections of this thesis detail the structure and operation of SPARSER.",
        "The semantic component uses a semantic network to associate semantically related words and to judge the meaningfulness of a hypothesized interpretation (See Nash-Webber [15]).",
        "This semantic formalism is very-general although a new network must be constructed for each new task domain.",
        "The pragmatics component is cUrrenrn being implemented, but is projected to contain information about the past dialogue, a model of the user, and other pragmatic 'data (see Bruce [6]).",
        "control component contains an overall strategy for employing the other components in order to obtain an interpretation of an utterance (see Rovner, et al. [23]).",
        "It decides which component is to be called, what input it is to be given, and what is to be done with the output.",
        "It sets thresholds on word match quality.",
        "It combines the scores produced by the other components in order to rank competing hypothesies, and is the primary interface to all other components.",
        "*Page 9 Section 3 The Grammar We have chosen the Augmented Transition Network formalism [32] for the grammar which drives SPARSER because it is a representation which allows merging of common portions of the analysis, it is amenable to both bottom up and top down parsing techniques, it fairly clearly separates the use of local information from infoemation which was obtained from a distant portion of the utterance and, the author s previous experience with a large ATN 'grammar for parsing text laid the groundwprk for the development of a similar grammar for speech.",
        "We have tried as much as possible to keep the formalism which was developed by Woods intact, but some changes have been necessary or desirable to make the grammar more amenable to the speech parser.",
        "We call the formalism a Modified AgramgnIgd Transition.",
        "Network (MATN), and assert that it has the same power as the original ATM formalism.",
        "The changes, are briefly indicated here.",
        "For a fuller discussion, see Bates [4].",
        "Every arc of an ordinary ATN has a test component, which may bp any predicate.",
        "It is usually a boolean combination of tests on the current input word (its features, etc.)",
        "and the contents of registers which have been set by actions on previous arcs.",
        "In the MATN formalism, the test component of each arc is, on all but the PUSH arc, a list of two tests.",
        "The first is a test on the Page 10 current word and its features, i.e. a local, context-free test.",
        "The second is a test on the register contents, i.e. a context-sensitive test.",
        "Both tests must succeed for the arc to be taken.",
        "The reason for splitting up the tests in this way is that register checking tests cannot be made unless the registers are set, and in many situations in the speech environment there may not be enough left context to guarantee that the proper registers would be set.",
        "Thus it is useful to be able to evaluate the context-free test on an arc at a different time in the parsing process from the context-sensitive one.",
        "On PUSH arcs, there are three types of tests which are used.",
        "It is useful and efficient to test the next word of input before actually doing the PUSH, to see, for example, if the next word can begin a constituent of the type being PUSHed for.",
        "This test is called a lookahead test, and takes the place of the normal context-free test in the test component of the arc.",
        "There is also the usual context-sensitive test on registers which were set before the PUSH arc was encountered.",
        "And finally, when the PUSH arc returns with a constituent, another context-free test may be done on the structure of the entire constituent.",
        "Therefore, the test component of a PUSH arc is a list of the three tests lust described.",
        "SENDR s were an efficient mechanism for text parsing because they allowed tests to be made on a lower level which involved information obtaine0 somewhere (possibly fAr) to the left in the input string -- information vihich would normally be inaccessible Page 11 beoause it would be hidden on the stack during the parsing of sub-constituents.",
        "There are svveral reasons for not allowing this mechanism in the speech parser Suppose, in the input that looks like \"... the person who.",
        "travels ...\", the word \"person\" is not the word which was really uttered.",
        "If it were allowed to be passed down it would become an integral part of the analysis at the lower level, and if another word were to be hypothesized in its place, the kower level the analysis would have to be redone even if none of the words in the relative clause had been cnanged.",
        "Thls is a process which would be extremely wasteful, especially in the speech environment where one wants to be able to take as much advantage as possible of information which was gained at one point and slightly altered at another.",
        "In particular, it is advantageous to consider as constituents such constructions as relative clauses so that they can be placed in a well-formed-substring table for use by other professes.",
        "Another reason is that some types of verifications (semantic, prosodic, and pragmatic, at least) can be done most conveniently on portions of an utterance which have been assigned a syntactic structure, i.e. on constituents.",
        "If a portion of an utterance is parsed (e.g1 \"that I gave you\" from the complete utterance \"The book that I gave you\") but doas not form a comDlete constituent because it is missing a piece of information from a higher constituent to the left which would have been sent down had it been available, then these verifications may not be made until the missing word or words are identified.",
        "Yet it may",
        "be important to build and verify the constituent in order to predict the missing word to the left.",
        "Therefore, it is better to allow constituents to be built without information which would normally have been passed down When parsing possibly incorrect fragments with little or no left context, it is heat to keep constituents as small and as independent as possible.",
        "The conversion process from an AIIN grammar to a MATN r!rammar with regard to SENDR s is straightforwardand infolves the use of a dummy symbol which is used in the construction of the lower level constituent.",
        "When the structure is popped, the PUSH arc examines it for agreement and may replace the dummy node by the appropriate item which would have been sent down.",
        "The structure returned by the PUSH for a relative clause on the fragment \"that I gave you\" might look like Figure 3.1 (where the structure is shown in both the usual tree diagram form and a corresponding form more amenable to computer output)."
      ]
    },
    {
      "heading": "PRO FEIATS TNIS I NU PAST SG",
      "text": [
        "The fourth element of every arc in a MATN is a small integer which is called the weight of the arc.",
        "This weight was originally conceived of as a rough measure of either (a) how likely the arc is to be taken when the parser is in that state or (b) how much information is likely to be gained from taking this arc, i.e. whether the parse path will block quickly if the arc is wrong.",
        "That these two schemes are not equivalent can be seen by the following example.",
        "In a given state, say just after the main verb of the sentence has been found, the arc which accepts a particle may be much less likely than the arc which jumps to another state to look for complements.",
        "However if a particle which agrees with the verb is found in the input stream at this point, then the particle arc is more likely to be correct.",
        "Since it is not at all clear how to measure or even intuit how much information is likely to be gained from taking an arc, it was decided that the weights would reflect relative likelihoods.",
        "The actual weights winch have been used in the speech grammar reflect an intuitive, though experienced guess as to how likely the arc is to be correct if it is taken, assuming the state itself is on the correct path.",
        "Two grammars which will figure predominantly in the remainder of this paper have been written in the MATN formalism.",
        "One is an extensive grammar which can handle many questions, declaratives, noun phrase utterances, imperatives, active and passive forms, relative clauses (reduced and unreduced), complements, simple quantifiers, noun-noun modifiers, verb-particle constructions, numbers, and dates (but not conjunctions).",
        "It began as a modification of the grammar for the",
        "For some illustrative purposes, SPEECHORAMMAR is too big nnd complex, so we have produced a AINIGRAMMAR which wilt be usell to show the basic operation of the speech parser.",
        "A detailed listing is given in Appendix 1, but the diagram in Figure 3.: probably shows the structure more clearly.",
        "The serious render is encouraged to sketch a copy of this grammar for reference later on."
      ]
    },
    {
      "heading": "CAT ADJ CAT N PUSH PP/",
      "text": []
    },
    {
      "heading": "MINIGRAMMAR",
      "text": [
        "Since the work reported here was finished, the author has written another grammar, called SMALLGRAM which uses the MATN formalism but which embodies a great deal of semantic and pragmatic information specific to the domain of discourse currently being used by the BBN speech understanding project.",
        "In or for the parser der to move from right to left (to predict what could precede that first given word), it must be able to determine for any state which arcs can enter it, and for any arc which state it comes from.",
        "Since the grammar is organized for normal parsing in just the opposite fashion, i.e. for any state one can determine what arcs leave it and for any arc (except POP) one can determine which state it terminates on, it was necessary to build an index into the grammar.",
        "This index consists of a number of tables cttntaining precomputed informationwhich in effect inverts the grammar."
      ]
    },
    {
      "heading": "Section It Overview of SPARSER",
      "text": [
        "The input to SPARSER is assumed to be a set of words together with their boundary points (which may or may not be related to points in time).",
        "A word together with its boundaries is termed a word match.",
        "A word match also includes a score which indicates how well the ideal phonemic representation of the word matched the acoustic analysis of the utterance (but as we Shall see the parser has little need of this information).",
        "Since the same word may match at several sets of boundary points or may match in several ways between the same boundary points, each word match is also given a unique number to help identify it.",
        "Thus Page 16 the structure for a basic word match is: (number word leffboundary rightboundary lexicalscore) e.g. (4 TRAVEL 5 11 94), or (4 TRAVEL 9 11 (q4 110)) where the score is given as a pair of numbers representing the actual and maximum scores, or (4 TRAVEL 5 11) where the score is omitted.",
        "How is the input to the parser to be constructed?",
        "We assume that acoustic processing and lexical scanning components can operate on a digitized waveform to produce a number of word matches such as prevdously shown in the word lattice of 1.1.",
        "(That this is possible has been demonstrated by [33]).",
        "Allowing the parser to operate unrestricted on the word lattice would probably not be fruitful because of the number of locally syntactically correct combinations of Figure Woods entire large words, but one possibility for input to the parser would be to take a set of the best-matching, non-overlapping word matches in the lattice, such as those in Figure 4.1.",
        "A set of non-overlapping word matches is a hypothesis about the content of the utterance.",
        "In order to avoid creating large numbers of such sets which are put together combinatorially with no basis except local acoustic match, semantic or pragmatic processes can be used to group word matches based on what is meaningful or likely to be heard.",
        "For example, if a dialogue has been about various nickel compounds, the combination \"nickel analyses\" may be more likely than \"chemical analyses\" even though the word match for °chemical\" has a higher score than that for \"nickel\".",
        "We will not attempt to detail here how this semantic grouping could.",
        "be done and how the sets could be scored, since it",
        "Using more terminology from the BBN speech system, the word theory to denotes a set of word matches such as we have just described together with (possibly empty) slots for information from each of the possible knowledge sources in the system.",
        "From the point of view, of SPARSER, usually only the word match portion of a theory is of interest, hence we shall fall into the habit of using the word \"theory\" to refer to the word match set it contains.",
        "When speaking of the syntactic component of a theory, however, we are refering to the information slot for syntax whicn accompanies each word match set.",
        "Theories have the fallowing characteristics: 1) They contain a set of basic, non.4overlapping word matches.",
        "2) They tend at first to contain long content words and not many shOrt function words.",
        "This is because long words are more reliably acoustically verified and content words are easier to Page 18 relate semantically and pragmatically.",
        "Since small words such as \"am\", \"do\", \"the\", \"one\", \"have\", \"of\", \"in\", etc.",
        "may be represented by very little acotstic information, they would tend to match at many places in the utterance where they do not really occur.",
        "Consequently they are not searched for by the initial word match scan, nor are they proposed in the semantic stages of hypothesis formation.",
        "3) They need not (and generally do not) completely span the utterance, but have numerous gaps of varying sizes (e.g. for the function words).",
        "4) They tend to contain some sequences of contiguous word matches.",
        "Such a sequence is called an island.",
        "That such a set of theories can be created has been demonstrated by the BBN SPEECHLIS system.",
        "The syntactic component, SPARSER, is expected to process these theories one at a time.",
        "In certain circumstances which will be detailed later, the input to SPARSER will be a theory together with one or more word matches which are to be added in order to create a new larger theory which is then to be syntactically analyzed.",
        "We will assume that there exists a control component which presents SPARSER with theories to process and to which SPARSER can communicate predictions and results.",
        "Preliminaries Given a theory, what is to be done with it?",
        "We begin by considering a subset of the question: Given an island of word matches, what is to be done with it?",
        "The answer is to create one Page 19 or more parse paths through the island and to predict what words or syntactic classes could surround the island.",
        "A parse path is the sequence of arcs in the grammar which would be used by a conventional ATM parser to process the words in the island, if the island were embedded in a complete sentence.",
        "For example, consider the way a parser might process an island of word matches such as (1 CHEMICAL 14 22) (2 ANALYSES 22 30) using the MINIGRAMMAR of the previous section.",
        "Beginning in state NP/ of the grammar (omitting for the moment the problem of how it is known that NP/ is the right place to begin) the sequence or arcs which would IA taken to parse \"chemical analyses\" as a noun phrase is that shown below in Figure 4.2."
      ]
    },
    {
      "heading": "CAT ADJ",
      "text": [
        "Figure 4.2"
      ]
    },
    {
      "heading": "Portion of MINIGRAMMAR needed to parse \"chemical analyses\"",
      "text": [
        "Let us define a configaration to be a representation of the parser being in a given state (say NP/QUANT) at a given point in the utterance (say 14).",
        "We will write conifiKurations as STATE:POSITION in text (e.g. NP/QUANT:14) and schematically as a",
        "•",
        "box within which are written the state and the position.",
        "If a configuration represents a state which is either the initial state of the grammar or a state which can be PUSHed to (i.e a JUMP JUMP CAT.",
        "N POP state which can begin the parsing of a constituent), it is called Page 20 ad initial configuration, and is indicated schematically by a filled-in semicircle attached to the left edge of the box.",
        "Note that a configuration NP/QUANT:14 is quite distinct from a configuration NP/QUANT:22 since they are at different positions in the input.",
        "In SPARSER, each configuration is also assigned a unique number which is a convenient internal pointer.",
        "The process of traversing an arc of the grammar using a particular word is represented by a transition from one configuration to another.",
        "A transition can be made only if the arc type is compatible with the current item of input and if the context-free test on the arc is satisfied.",
        "(The context-sensitive tests are evaluated later.)",
        "A transition carries with it information about the arc which it represents and the item of input it uses.",
        "The item of input is usually the word match which the arc uses, but it it NIL in cases such as JUMP arcs which do not use input, and it is a complete constituent for PUSH arcs.",
        "A unique identifying number and the list of features, if any, which is associated with the input word or constituent are also recorded on the transition in SPARSER, but they are not shown schematically.",
        "A transition is represented schematically by an arrow from one configuration to another with an abbreviated form of the arc written above the arrow and the item of input under it.",
        "The syntactic part of any theory which SPARSER processes contains, among other things, lists of the transitions and configurations which are created or used by the theory.",
        "Thus wheh we talk about creating a configuration or transition it is",
        "implicitly understood that SPARSER also adds it to the appropriate list, and when we talk of adding an existing configuration or transition to a theory we mean adding it to the appropriate list.",
        "Therefore, removing a configuration or transition from a theory means removing it from the syntactic part of the thebryt not removing it entirely from SPARSER s data base.",
        "Like configurations, transitions are unique, so only one transition is ever constructed from point A to point B for arc X and input Y.",
        "We will frequently speak of creating a transition or a configuration, but the reader must bear in mind that if such a configuration or transition already exists, this fact will be recognized and the preexisting configuration or transition will be used.",
        "(Timing, measurements indicate that it takes about .052 seconds to create a configuration and only .01 seconds to test if a particular configuration already exists.",
        "For transitions, creation takes about .54 second'S and recognition .012 seconds.",
        "The seqUence of configurations and transitions which would parse the above example is displayed in Figure 4.3.",
        "JUMP NP/ART JUMP NP/QUANT JUMP NP/ADJ CAT N NP/ADJ CAT N IT17' NIL 14 NIL 14 CHEMICAL 22 ANALYSES Figure 4.3 Path for parsing \"chemical analyses\" A connected sequence of transitions and configurations is called a oath.",
        "If the sequence begins with an initial configuration and ends with a transition representing a POP arc,"
      ]
    },
    {
      "heading": "Page 22",
      "text": [
        "it is a complete path, otherwise it is a partial path.",
        "Paths are assumed to be partial unless otherwise specified.",
        "agglnning to Parse an Igignd SPARSER processes an island of words by beginning with the leftmost word and determining its possible parts of speech.",
        "Then the arcs of the grammar which can process the word are fpund (by looking in the previonsly constructed grammar index).",
        "For each arc, two configurations are constructed one for the state at the tail of the arc and one for the state at the head, using the left and right boundary positions of the word match, respectively, and a transition for that arc using the current word match is also built.",
        "Schematically, we have for our example a situation which looks like that of Figure 4.4 (such a display of all or some of the transitions and configupations which the parser has constructed is called a map).",
        "Notice that a configuration may have any number of transitions entering or leaving it."
      ]
    },
    {
      "heading": "Figure 4.4",
      "text": [
        "Initial map for parsing \"chemical analyses\" Page 23 The idea of this process is to begin to5 set up paths which may be used to parse the island.",
        "However it is not necessarily the case that the only donfigurations which could start paths through the island are those which have just been obtained, since it may be possible to create transitions which enter them via JUMP arcs or TST arcs.",
        "For each state, the sequence of arcs which can reach it without using the previous word of input have been be pre-calculated by the grammar indexing package so the appropriate configurations and transitions may be constructed.",
        "These transitions are cqlled lead-in transitions.",
        "Thus the map becomes that in Figure 4.5",
        "Lead-in transitions for parsing \"chemical analyses\" Note that any of the configurations (except for NP/ADJ:22 and NP/N:22) could actually be the correct leftmost configuration for this island, depending upon what the (currently unknown) left context of the island is.",
        "By looking in the grammar index, SPARSER can determine, for each configuration which could start the island, just what sort of left context could be appropriate.",
        "For example, the CAT ADJ arc in MINIGRAMMAR which enters state NP/QUANT implies that an"
      ]
    },
    {
      "heading": "Page 24",
      "text": [
        "adjective could precede the island and, if it did, the transition which would process it would terminate on configuration NP/ADJ:14."
      ]
    },
    {
      "heading": "Because the initial configuration NP/:14 could start the",
      "text": [
        "island, anything which could precede a noun phrase could occur to the left; again the grammar index provides the information that the CAT PREP arc could lead to a configuration which could accept a noun phrase (via the PUSH NP/ arc), so a preposition could also prefix the island.",
        "If the index functions indicate that a constituent could be picked up by a PUSH arc which could terminate on the configuration under consideration, an indication is made in the WFST so that any time a constituent of the desired type is built which ends at the proper location, it may be tried here."
      ]
    },
    {
      "heading": "Because of the highly recursive nature of ATN grammars, it",
      "text": [
        "is very likely that as we chain back through the possible sequences of PUSHes which could lead to the beginning of the current constituent (or the seouence of POPs which could be initiated by the completion of the current constituent) a large number of predictions will be made.",
        "Rather than make all these predictions automatically, before we are even sure that there is in fact a constituent at the current level, the possible configurations which could make predictions on other levels are saved to be activated later if the predictions from the current set of active configurations are not sufficient.",
        "Page 25 The predictions which are made (not saved) are not acted upon at this time, but art?",
        "kept internally by SPARSER until all the islands of the theory have been processed.",
        "We shall see belaw what then becomes of the predictions."
      ]
    },
    {
      "heading": "Parsing Through an Island",
      "text": [
        "Once processing has proceeded this far, we can go back and consider the set of configurations which represent states the parser could be in just after processing the first word of the island.",
        "In our examine, these are configurations NP/ADJ:22 and NP/N:22.",
        "Configurations such as these which are waiting to be extended to the right are called active configurations.",
        "SPARSER selects a subset of the set of active cdnfigurations (how this subset is selected will be discussed in the next section) and for each configuration tries to extend it by trying to parse the rest of the Island beginning in that configuration.",
        "When the parser is considering a configuration at some position, the input pointer is set to the word match of the island, if any, which begins at the same position in the input.",
        "The grammar associates with the state of the configuration a list of arcs which may be tested (using the arc type, the context free test on the arc, and the eurrent input) to determine whether a transition can be made to extend the path.",
        "We will consider each type of arc in turn, since the effects of taking various types of arca are different, and explain for each case what happens if the arc is taken.",
        "Whether just one transition, or several, or all possible transitions are made from an active"
      ]
    },
    {
      "heading": "Page 26",
      "text": [
        "configuration is a matter to be discussed in Section Five.",
        "Some JUMP arcs do not look at the current item, so they may be taken whether the input pointer is set to a word match or to NIL.",
        "The transition which results from taking an arc of this type has a null item associated with it, even if there is a word match in the theory at this point.",
        "The positions of the configurations at each end of the transition are the same; this corresponds to the fact that an ATN parser would not move the input pointer as a ponsequence of taking this arc.",
        "Rarely, a JUMP arc may test the current item in some way, for example, to make a feature check.",
        "If there is no word match for input, an arc of this type cannot be taken.",
        "If there is a word match, it is noted on the trahsition wh oh is created, but the configurations at each end of the transition have the same position.",
        "(It is then the case that theinext input-using or input-consuming transition on the path including this transition must use the same word match.)",
        "These are TST, CAT, and WRD arcs which end in a (TO nextstate) action.",
        "The operation is exactly the same as that above except that the configuration on which the transition terminates has the position of the right boundary of the current word match.",
        "Taking a POP arc results in the creation of a transition which has a null final configuration and a null item, because POP arcs are not permited to look at input.",
        "Page 27 When a PUSH arc is encountered, a monitor is placed in the Well-Formed Substring Table (WFST) at the current position to await the occurrence of a constituent of the required type.",
        "If one or more such constituents are already in the table, then Cor each one there are three possibilities: it may be composed of word matches which are in the current theory, it may be composed of word matches some of which are not in the current theory but which could be added without violating the non-overlapping constraint, or it may be composed of word matches some of which are Incompatible with the current theory.",
        "In the first case a transition is set up using the constituent as the current word.",
        "The transition terminates on a configuration whose state is determined from the termination of the PUSH arc and whose position is that of the right boundary of the rightmost word match in the constituent.",
        "In the second case, a notice is created and sent to the control component.",
        "A notice is a request that SPARSER be called to enlarge a theory by adding some new information, in this case, some additional word matches which form a constituent that the theory can use.",
        "SPARSER does not try to determine when (or even whether) the theory should be so enlarged.",
        "That is an issue for the main controller to decide (see Rovner, et.al.",
        "[23]).",
        "We will discuss below how SPARSER enlarges a theory if called upon to do so.",
        "In the final case, if there are no usable constituents in the WFST, a new configuration is set up to start looking for one and is added to the list of active configurations.",
        "Its state is Page 28 the state specified by the PUSH arc and its position is the same as the current configuration.",
        "There is a considerable amount of processing that can happen any time one of the transitions lust discussed is made.",
        "Whenever an initial configuration is constructed, this fact is recorded in the configuration.",
        "Whenever a transition is made from such configuration, the information that there is a path from some Initial configuration is recorded on the subsequent configuration.",
        "Similarly, whenever a POP transition is made, the configuration it emanates from and all previous configurations on any path which can terminate with the POP transition are marked to indicate that they can reach a POP transition.",
        "Whenever a transition is made which completes a path from an initial configuration to a POP transition, the path is executed, one transition at a time, and the register setting actions and context sensitive tests are executed.",
        "If a test fails or an arc aborts, the transitions and configurations of the path are removed from the list of configurations and transitions which are in the syntactic part of the current theory (unless they are used by another path in the theory) but not removed from the map.",
        "If the execution is successful, a deep structure tree is produced.",
        "That structure together with its features is given a score, whioh may include evaluations by other compotents such as semantics and prosodies, and is entered in the WFST.",
        "It is quite important that sources of knowledge other than syntax be called upon to verify and to rank syntactic constituents.",
        "This is because there are likely to be many Page 29 combinations of plausible words from the word lattice which form syntactically reasonable constituents but which may be ruled out on other grounds.",
        "To allow immediate use of this information which syntax cannot provide alone, SPARSER has an interface to the seantic component so that constituents can be verified directly without going through the control component.",
        "It will be amtrivial modification to insert verification calls to pragmatics and prosodics when they become available.",
        "In the meantime, even semantic knowledge can be turned off; if the parser gets no Information from the call to semantics, it proceeds without it.",
        "Placement of a constituent in the WFST causes a number of things to happen.",
        "First, any monitors which have been set by the current theory at that position ese activated.",
        "That is, for each configuration which was waiting for this constituent, a PUSH transition Is made which uses the constituent as its input item.",
        "If no nonitors have been set which can use this constituent, it is treated exactly as if it were the first word of an island: all the PUSH arcs which can use it are found in the grammar index and appropriate configurations and transitions (including lead-in transitions, if appropriate) are set up.",
        "Next, if there are any nonitors for other theories which can use the constituent, notices are created and output to Control as was described above in the section on PUSH transitions.",
        "Figure 4.6 shows SPARSER $ map after our example island has been completely processed.",
        "The parsing results-in the creation or a CAT N transition to configuration NP/N:30 using the word wanalyses\" The PUSH PP/ arc at state NP/N would oause Page 30 configuration PP/:30 to be created.",
        "Similarly, PP/:22 would be created when the configuration NP/N:22 is picked up to be extended.",
        "The POP arc transitions from each of the configurations for state NP/N result in the formation of complete paths, resulting in the creation of two noun phrases (\"chemical analyses\" and \"chemical\").",
        "Since there were no monitors for them, they result in the creation of configuration PP/PREP:14 and"
      ]
    },
    {
      "heading": "Map after processing island",
      "text": [
        "Page 31 Ending an Island It may be the case that no path can be found from one end of an island to the other.",
        "(This would occur when all active configurations block.)",
        "In this case, there is no possible way that the island could form part of a grammatical string, so SPARSER can inform the control component that the theory is wrong.",
        "When an active configuration is picked up to be extended and there is no word match at that point, the end of the island has been reached.",
        "That does not mean that no more transitions can be made, since arcs which do not test the input word can be taken as usual.",
        "Arcs which do use input cannot be taken, but they can be used to predict what sort of input would be acceptable at that position.",
        "For example, a CAT V arc which has a test requiring the verb to be untensed would allow SPARSER to predict an untensed verb beginning at the position of the current configuration.",
        "CAT and WRD arcs cause the prediction of syntactic categories and specific words, respectively, modified by the context-free test on the arc.",
        "TST arcs provide only the test which must be satisfied, and PUSH arcs cause a monitor to be set in the WFST as well as a TST monitor for the the lookahead test (if any) on the arc.",
        "Ending a Theory When 'all the islands of a theory have been processed in the manner just described, it is time to deal with the gaps between the islands.",
        "As we have seen, arcs in the grammar which can Page 32 enter configurations at the left end of an island or which can leave configurations at the right end of an island can be used to make predictions about words that may be adjacent to the island.",
        "The prediction is a list of the arc, the configuration it would connect to, and an indication of whether the transition caused by the arc will enter the configuration from the left or leave it to the right.",
        "If a gap between two islands is small enough that it may contain just one word, then it is likely that the arc which would process that word may have caused a prediction from both the left and right sides of the gap.",
        "If this is the case, and if the predictions intersect in a single possibility, it is highly probable that the word (or syntactic class) so predicted is correct.",
        "If the predictions do not intersect, parsing is continued from the active configurations which were not tried earlier because of their scores and from the configurations which could begin constituents at the right end of an island.",
        "This continued parsing is an attempt to find a path which results in a common prediction across the gap.",
        "If that too fails, then the configurations which were saved because they could lead up a chain of PUSHes or POPs to new configurations are tried.",
        "If no possibilities are left to try and there is still no prediction to fill the gap, this information is noted, but it does not definitely mean that the islands are incompatible, since in some cases the gap could actually be filled by two words instead of one.",
        "Page 33 SPARSER has two kind of predictions - those which seem highly likely and those which seema less likely.",
        "A highly likely prediction, such as one which is made from both sides of a small gap, is output in the form of a proo2sall which is a request to the rest of the system to find a word meeting the requirements of the proposal.",
        "A proposal contains:",
        "1) the item being proposed, which is either a particular word or list of words (from a WRD arc), or a syntactic olass (from a CAT arc), dr NIL, 'meaning any word (from a TST arc) 2) the left and/or right boundary point(s) of the item 3) a test which the item must satisfy (the context free test from the arc)",
        "4) the context of the proposal, i.e. the word match(es) on the left apd/or right side of the item being proposed.",
        "(This is to help the lexical retrieval component take into account phonological phenomena which may occur across word boundaries.)",
        "All predictions whether or not they are confident enough to become proposals are output as monitors.",
        "A monitor is a notification to the control component that if a word meeting the requirements of the monitor is somehow found (perhaps by the action of a proposal) , it may be added to the theory.",
        "Thus a monitor acts like a demon which sits at a particular point in the word lattice and watches for the appearance of a word match which it can use.",
        "A monitor contains: I) the item being monitored for (generally a syntactic categcry, but may be a word or a test) 2) the left or right boundary position of the item being monitored far Page 34",
        "3) a test which the item must satisfy (same as for proposals) 4) the theory which generated the monitor 5) the arc in the grammar which will process the item if found 6) the configuration frnm which the prediction was made 7) a score, indicating roughly how important the monitor is,",
        "i.e. how much information is likely to be gained by processing an event for that monitor.",
        "(Notice that monitors which are sent to the control component are very much like monitors which are set in the WFST by the occurrence of PUSH arcs.)",
        "Once the proposals have been made and the monitors have been setj SPARSER bundles up the information it knows about the current theory, such as the configurations and transitions in the theory, any configurations which are still candidates for expansion, the constituents in the theory, the notices, proposals, and monitors which have been created, etc.",
        "and associates the bundle with the theory number.",
        "This insures that SPARSER will be able to pickup where it left off if it is later given the thetry to process further."
      ]
    },
    {
      "heading": "Processing Multiple Theories",
      "text": [
        "Thus far we have seen only the operations which SPARSER performs on a single theory, but we made the assumption that SPARSER would be given a number of theories to process in sequence.",
        "Let us now examine what will happen when the second (or nth) theory is processed.",
        "SPARSER will no longer have a blank map and WFST; instead it will have all the configurations, transitions, and constituents which have been constructed by all previous theories.",
        "For concreteness, let us imagine that the theory (1 CHEMICAL 14 22) (2 ANALYSES 22 30) has been processed, resulting in the map shown in Figure it Now we Bre going to process a theory containing the island (4 NICKEL 16 22) (2 ANALYSES 22 30), which results in the map of Figure 4.7 Where the configurations and transitions added by this theory are shown in dotted lines.",
        "The process begins as usual with the creation of configuration NP/ADJ:16 and three possible lead-in transitions.",
        "The transitions for the two CAT N arcs, however terminate on configurations which already existed in the map, so the complete paths from configuration NP/:16 to configurations NP/N:30 and NP/N:22 will be discovered and processed, resulting in the construction of two new noun phrases.",
        "Those new constituents would then result in the creation of configuration PP/PREP:16 and two new transitions.",
        "Thus we have constructed only five new configurations and seven new transitions and have been able to take advantage of six old configurations and six old transitions."
      ]
    },
    {
      "heading": "In this fashion any information which has once been",
      "text": [
        "discovered about a possible parse path is made available to any other path which can use it.",
        "No reoarsing is ever done SPARSER merely realizes the existence of relevant configurations and transitions and incorporates them into the current theory.",
        "Map after processing island for \"nickel analyses\" If the new word (or wolids) in a theory are at the end (or in the middle) of an island, when SPARSER begins to parse the island it will discover the existing configurations and transitions from the previous theory.",
        "Whenever a transition which can be used in the current theory is discovered in the map, it and its terminating configuration are added to the syntactic part of the current theory.",
        "This is called tracing the transition.",
        "In addltion, all paths beginning with that transition which do not require the next word of input are also included in the syntactic part of the theory.",
        "This is accomplished by tracing from the terminating configuration all transitions which use either the Page 37 same word of input as the previous transition or no input word at all.",
        "(A similar process is used to trace backwards, i.e. right to left, when necessary.)",
        "When a configuration is reache4 which has no traceable transitions emanating from it, the tracing process, stops.",
        "Since both transitions and configurations are stored in such a way as to facilitate tracing (for example, each transition has a code attached to indicate whether or not it consumes or tests input), this process is considerably faster than creating that portion of the map in the first place.",
        "(To illustrate this, a theory was processed twice, once with an empty map and once starting with the map previously created; the time required for processing the theory fell from 47.5 seconds tel 16.5.)",
        "Configurations which can end traced paths are put on the active configurations list.",
        "If, when one of' them is picked up for extension, it is discovered that the next word of input was used on a transition already in the map, the tracing process is repeated.",
        "If the next word of input is new (or at least has not caused any transitions from the configuration being considered) then parsing continues in the normal manner.",
        "Processing Events As.",
        "was mentioned earlier, SPARSER can be called upon to add some new word matches to a theory it has previously processed.",
        "In this case, SPARSER is said to process an event.",
        "An event may be thought of rather abstractly as the discovery of a piece of information that has been syntactically proposed, monitored for,",
        "or noticed.",
        "Concretely, an event is a piece of data consistiniz of:",
        "1) the old theory that proposed or set a monitor for the event 2) something to be added to the theory (a new word match or constituent) 4) the arc in the grammar which will process the new information 4) the confipuration ih the old theory which will be at one end of the transition created by the above arc",
        "When SPARSER is given an event, it retrieves from its tables the bundle of configurations, transition, etc.",
        "in the old theory.",
        "Then using the arc and the new word or constituent in the event, it creates the appropriate transition(s).",
        "Then processing continues as usual, that is, any complete paths are noticed and processed, and any new active configurations are exbended, if possible.",
        "New predictions may be made as a result of this increased information.",
        "(A record is kept of previous predictions so none are remade unless with a more liberal score.)",
        "Finally SPARSER returns the new, larger theory.",
        "This new theory may be processed as part of another event at some later time, thus gradually reducing the number and size of the gaps in the theory.",
        "If an event results in filling the final gap in a theory, and if the resultinp complete sequence of words can he parsed, SPARSER notifies the control component of this fact, since the entire utterance may have been discovered.",
        "Of course, this may"
      ]
    },
    {
      "heading": "Page 39",
      "text": [
        "not be the correct solution -- it is up to the control component to look at the acoustic goodness, semantic meaningfulness, pragmatic likelihood, etc.",
        "of the result as well as the syntactic structure before declaring the utterance to have been understood.",
        "If for reasons other thin syntactic, the utterance appears to be bad, the control component of the system could go on to try to find another, more suitable, possibility.",
        "Section 5"
      ]
    },
    {
      "heading": "More Details of the Parsing Process 5.1 DEPTH vs BREADTH",
      "text": [
        "The parsing strategy just outlined works bottom up when beginning to parse an island and when a constituent is created which was not monitored for by the current theory.",
        "It works top down after an island has been started and to make syntactic predictions at the ends of islands.",
        "Both top down and bottom up techniques can be either depth or breadth first.",
        "Depth first processing takes at every step the fj!ost piece of information available and pursues its consequences.",
        "Breadth first processing considers at every step every possible next step of every alternative and pursues all paths in parallel.",
        "Breadth first processing generally takes much more space than the depth first process since many paths would have to be remembered at once Rage 40 instead of having just one stack which could be popped and reused when necessary.",
        "The breadth first process might save some computation steps and might produce several ambiguous parsings simultaneously while tlae depth first process would find one before the others (the latter is a'small difference, since both processes would have to be run to exhaustion to insure that all possible parsings had been found).",
        "In parsing speech, some mixture of breadth first and depth first processing can be extremely useful.",
        "To illustrate an advantage of breadth first processing in the speech environment, consider what miRht happen if, during the processing of an island the parser picks up a confiruration to extend which has several possible arcs emanating from it.",
        "If one arc is chosen and all the others are held as alternatives (i.e. depth first), but the chosen arc is wrong, all subsequent paths beginning with that arc would have to block before the alternatives would be tried.",
        "However, if the end of the island were reached before the success or failure of the first choice were confirmed, the only way that backup would ever take place would be to have one or more events add words to the theory so that the path could be extended until it failed.",
        "Since the gap would be likely to be filled by (incorrect) words predicted by the erroneous path, or by no words at all if the (incorrect) predictions were not satisfied, it is not at all clear how the process would ever know to back up.",
        "This problem cannot be eliminated completely without pursuing all alternatives to their fullest extent (a combinatorially unacceptable solution) but it can be modified to a great extent by a judicious combination of depth and breadth first processing to find the best path, not just the first one, through the island.",
        "This \"best path\" is not guaranteed to be the correct one, so it is possible to continue processing by extending paths with were suspended earlier.",
        "SPARSER handles the problem by assigning a score lo every configuration which reflects the likelihood of the path which terminates on that configuration to be correct.",
        "The score can also be thought of as a measure of how good that configuration looks in relation to others as a candidate for extension.",
        "One question which was previously left unanswered, how a subset of the active configurations is chosen for extension, can now be answered: the subset of maximally scoring configurations is chosen at each step until the maximal score of active configurations begins to fall.",
        "(The score on a configuration and the score of a path terminatinr on that configuration are the same thing -- we will use which ever terminology seems most natural at the time.)",
        "The result of this process is a sort of modified breadth first approach, where at one step all the alternatives are tried but at the next step only the best ones are chosen for further extension.",
        "This is similar to the best-first parser described by Paxton in [18] but it can be applied to the sort of partial paths which SPARSER generates rather than requiring the perfect Page 42 information resulting from a strictly left to riFht approach.",
        "The success of this method is directly dependent on the relative accuracy of the scores which are assigned to the paths."
      ]
    },
    {
      "heading": "5.2 SCORING PATHS",
      "text": [
        "Several attempts have ben made to develop rigorous systems for parsing errorful or speech-like input based on probabilities [1, 14, 27].",
        "These atteMpts have all simplified the problem to such an extent that it is no lonFer realistic or extendible, e.g. by assuming the input is a sequence (rather than a lattice) of probability distributions, by assuming that all the necessary Information is present in the search space to begin with so the only problem is to find an optimal path through the space, by requiring a small vocabulary, and/or by limiting the grammar to be context free.",
        "The ideal scoring mechanism for SPARSER would be one which accurately reflected at every step the probability that the path as correct.",
        "Bayes rule could be used, but it would be necessary to know, at any point in the parsing process, what the probability is that the next arc under consideration is correct, given that the entire path up to the current step is correct.",
        "In order to use this application of Bayes rule it would be necessary to pre-calculate the probabilities for every possible path and partial path which could be generated a clearly impossible task since there are an infinite numbtx of such paths.",
        "Page 43 Given that we cannot calculate the probabilities we need exactly, what is the next best option?",
        "If we ignore the effect of the path traversed up to the current point, but can say for any given state how likely each arc emanating from that state is to be correct, we would have a model which uses only local Information rather than one which takes into account accurately all the Left context which is available.",
        "Since it was not practical to run large amounts of data through a parser in order to obtaid accurate measurements even for the limited model, the author relied on considerable experience with ATN grammars to assign a weight to each arc of thipgrAmmar representing the intuitive likelihood that the arc (if it can be taken) is thy correct Qre to choose from that state.",
        "These weights are small integers (0 through 5) -- the larger the weight the more likely the arc.",
        "The question might arise as to why the score on the word match used by an arc should not be used to influence the score of the path using it.",
        "SPARSER tries to treat each theory as independently as possible and to assign scores based only on the syntactic information which is available.",
        "The one exception to this rule is the semantic information which is used to score constituents.",
        "If lexical word match scores were used, the control component would not be able to separate the lexical goodness from the syntactic goodness of the theory and make judgments as to their relative importance.",
        "In a syntax-driven speech understanding system, however, it would prObably be useful to combine lexical scores with syntactic information.",
        "Page 44 As was described in the previous section, when SPARSER begins to parse an island each possible partial path is begun by creating a configuration at the head of a transition for an arc which can use the current word.",
        "Rather arbitrarily, it was decided to give this configuration a score of one.",
        "This starts all partial paths out equally, a technique which is not quite accurate, since some contexts are more likely than others.",
        "For example, the words \"to\" and \"for\" are more likely to occur in prepositional phrases than in sentential complements.",
        "If this simplification appears to harm the overall performance of SPARSER, it coulo be remedied by giving each state an a priori score similar to the weights on arcs.",
        "Configurations on lead-id paths are also given a score of one.",
        "After the initial step, whenever a transition (other than a PUSH or POP) is made, the score of the subsequent configuration is influenced by the score of the configuration being extended and the weight on the arc being used.",
        "If the score& were actual probabilities; they would be multiplied; since they are not, it was arbitrarily decided to add them.",
        "When attempting to create a configuration which already exists (a situation encountered whenever two or more parse paths for the same theory merge), the configuration is given the maximum of the existing score and the score which would have been assigned had the configuration been created anew.",
        "When a PUSH arc is encountered and a configuration created to begin the search for the required constituent, the score of that configuration is set to be the sum of the score of the Page 45 configuration causing the PUSH, and the value (if any) of the lookahead test on the PUSH arc.",
        "For example, upon encountering an arc such as (PUSH NP/ ((NPSTART) T T) ...) the lookahead function NPSTART returns a high integer value IT the next word is a noun and a lower, value if it is a verb (e.g. \"accounting costs\").",
        "Of course, if the lookahead funcCion fails altogether, the configuration is not set up, although the monitor in the WFST remains.",
        "When a constituent is completed (or found in the WFST) and a PUSH transition is about to be made, the score of the configuration on 4hich the transition terminates is a function of the score of the confiruration being extended the weirht on the arc, and the score of tbe constituent itself.",
        "The score of tht.",
        "constituent is currently very ad hoc, being a function of the number of words in the constituent (less a function of the number of sub-constituents subsumed by this constituent, boosted if the constituent is a major one) and the score which is determined by semantic verification.",
        "Thus semantically \"rood\" constituents will Iloost the scores of the paths which use them more than semantically \"bad\" ones.",
        "Due to the level of effort required to gather accurate statistics on the relative frequencies of arcs, the current scores are admittedly ad hoc.",
        "It is not clear whether different scoring mechanisms would be better, however it is clear that the current scoring strategy is better than no scoring at all, as preliminary measurements indcate that the number of transitions created (as well as the number of configurations and predicions) Page 46 is reduced about 25% by the current strategy.",
        "(It is reasonable to ask why semantic scores are used to influence parse paths, since it was iust argued that lexical scores should not be used in this way Semantic scores may he more reliable than lexical ones hecause we are assuming that the utterande is semantically meanihgful.",
        "Under this assumption, a constituent like \"range remainder\" as a noun-noun modifier analogous to \"surplus money\" should be ruled out as early as possible.",
        "Since such constituents cannot be ruled out on syntactic grounds alone, since prosodic information (which might help to rule them out) is not available (see discussion in Section 7.2), an4 since they would seriously overrun the parser with a plethora of false paths if they were not rejected, it seems reasonable to permit semantics to influence the parser.)"
      ]
    },
    {
      "heading": "5.3 SCORING PREDICTIONS",
      "text": [
        "The previous section discussed three ways in which SPARSER can make predictions about what could fill in gaps between islands.",
        "Monitors wait for the occurrence of a word in the word lattice (or a constituent in the WFST), proposals request a search for a particular set of words, and notices indicate the presence of a usable word in the word lattice (or a constituent in the WFST).",
        "Since the processing of a typical theory is likely to result in a number of predictions it is necessary eo be able to order them so that predictions most likely to be correct or most likely to yield important information will be acted upon Page 47 first.",
        "For example, it is more important to fill a gap between two islands than to extend a single islabd, since by filling the gap one can check the consistency of information which was locally good in each island individually but may not be consistent when they are joined.",
        "Since two words can occur together in (usually) many contexts but longer sequences are generally more restrictive, adding a word to a one word island is likely to be less profitable in terms of the number of possible paths which are eliminated by the addition than adding a word to a multi-word island.",
        "It is up to the syntactic component to indicate to the control component the relative importance attached to each notice and monitor; the higher the score, the stronger the prediction.",
        "Several factors influence the score attached to predictions.",
        "One is the length of the island to which the prediction is attached.",
        "One word islands, if they are processed at all, yield very little information anc many predictions, hence the predictions are not scored high.",
        "Proposals are less important if there is already a noticeable word in the word lattice (since that word is acoustically better than the word to be proposed, else it would have been found earlier.",
        "However, if a proposal fills a gap between two islands, it is given a higher score.",
        "Notices are boosted in importance if an entire constituent may be added and penalized if they will add onto a one word island.",
        "Scores range from 0 to 95 for proposals, 0 to 40 for notices, and 0 to 15 for monitors.",
        "Page 48 These scores appear to work fairly well with the rest of the BBN SPEECHLIS system, but have been developed by a process of interaction with the other components (in order to make the scores of syntactic predictions commensurate with those of semantic predictions) and may be changed considerably as the entire system evolves.",
        "Small syntactic classes (e.g. determrners and prepositions) are proposed in their entirety (that is, their elvments are to be enumerated and given to the lexical matching component for verification) if the island which monitored for them is more than one word long.",
        "If a gap between two islands is small enough for just one word and if a syntactic class has been monitored for from both sides of the gap, it is proposed ir its entirety also."
      ]
    },
    {
      "heading": "Section 6 Examples and Result",
      "text": []
    },
    {
      "heading": "SPARSER is written in INTERLIgP and runs on a PDP-10 under",
      "text": [
        "the TENEX operating system The program and initial data structures occupy approximately 90000 words of virtual memory.",
        "(The other components of the BBN speech understanding system occupy separate forks from the syntactic component.)",
        "Page 49 At the time the 'examples in this section were run, the algorithm controlling the decision-making process in the control component was undergoing revision and was not solidified into a function which could operate automatically.",
        "Rather, there were a number of primitive operations such as scanning an utterance (or some specified portion of it), creating theories, calling SPARSER with a theory or event, calling for the processing of proposals, etc., which could be invoked by a human simulator The following examples were produced in this mode, with the user acting as the control component in a way which could be modelled by later implementation.",
        "Several conventions have been used in tracing the operation of SPARSER.",
        "Configurations are represented as NUMBER : STATE : POSITION (SCORE).",
        "For example, the configuration written as 30:NP/HEAD:23(39) is the configuration for state NP/HEAD at position 23 which has been given the (unique) number 30 and which currently has a score of 39.",
        "The creation of a transition is indicated by naming the type of arc causing the transition, the (unique) number of the transition, and the configurations at each end of the transition.",
        "For example, CAT N TRANS #9 FROM 14:NP/DET:6(1) TO 15:NP/DET:19(4).",
        "Annotations have been inserted within brackets { }, typeout in upper case was produced by the program.",
        "Page 50"
      ]
    },
    {
      "heading": "EXAMPLE 1",
      "text": [
        "This example parallels that given in Section Four.",
        "A word lattice was artificially created which contained only the following three word matches:",
        "(In this version or the system, regular inflectional endings are included in word matches after the element representing the score, hence the somewhat peculiar word match for the word \"trips\".)",
        "Two theories were constructed, one for word matches 2 and 3, the other for 1 and 3.",
        "What follows is an annotated (but otherwise unedited except for considerations of spacing) transcript of SPARSER processing these two theories in sequence, using the MINIGRAMMAR of Figure 3.3 and Appendix I. SPARSER PROCESSING THEORY' 1: 0 12 WINTER 16 TRIP S 21 30 (This is a linear representation of the thegry being processed.",
        "The endpoints are 0 and 30, but the words occupy-only the middle part of the utterance.)"
      ]
    },
    {
      "heading": "STARTING AN ISLAND \"WINTER\" TRYING CAT N ARC FROM NP/ADJ TO NP/ADJ",
      "text": [
        "(This is the first of two arcs retrieved from the index tables } CAT N TRANS #1 FROM 1:NP/ADJ:12(1) TO 2:NP/ADJ:16(3) (The first transition is created, and since there is a CAT N arc which enters state NP/ADJ, a monitor is set up to monitor for nouns which end at position 12.}"
      ]
    },
    {
      "heading": "ENDING AT 12: MONITORING [ N ] JUMP TRANS #2 FROM 3:NP/QUANT:12(1) TO 1:NP/ADJ:12(1)",
      "text": [
        "(Now the lead-in transitions are being created, along with the monitors for syntactic categories which may precede the newly constructed configurations.",
        "Configurations along the lead-in path are all assigned a score of 1.}",
        "[The lead-in transitions are all made.",
        "Now the second arc which can use the noun is about to be processed.)",
        "\"WINTER\" TRYING CAT N ARC FROM NP/ADJ TO NP/N CAT N TRANS #5 FROM 1:NP/ApJ:12(1) TO 6:NP/N:16(6) (This is the second of the two arcs obtained from the index table for \"winter\".",
        "The lead-in transitions to configuration 1 have already been Constructed, so they are not remade.",
        "Now we are ready to choose configurations to extend.",
        "The pool of candidates for extension contains configurations 2 and 6.)"
      ]
    },
    {
      "heading": "SELECTED CONFIGS (6) FOR EXTENSION",
      "text": [
        "(Only this one is chosen because it has a higher score than configUration 2, since the use of a noun as a head noun of a noun phrase is more likely than its use as a modifier.)"
      ]
    },
    {
      "heading": "PICKING UP CONFIG 6:NP/N:16(6) WITH WORD TRIP TRYING PUSH PP/ ARC",
      "text": [
        "(No.",
        "action is taken about starting a configuration for state PP/ because the lookahead test which checks that the next word can begin a prepositional phrase fails on the word trip.)"
      ]
    },
    {
      "heading": "TRYING POP ARC POP TRANS #6 FROM 6:NP/N:16(6)",
      "text": [
        "(Creating the POP transition completes a .path from configuration 5.",
        "The path is expressed as a list of transition numbers.",
        "We are about to execute the path, that is, check the context-sensitive tests and do the register building actions along it.)"
      ]
    },
    {
      "heading": "EXECUTING PATH (4 3 2 5 6) BEGINNING AT TRANS 4, CONFIG 5",
      "text": [
        "(We must begin executin, the path at the first transition, because no part of it has been executed before.",
        "Later we will see that it is possible to begin execution of a path in the middle, since the repister contents are stored at each step.)"
      ]
    },
    {
      "heading": "DOING JUMP ARC FROM 5:NP/ TO 4:NP/ART DOING JUMP ARC FRPM 4:NP/ART TO 3:NP/QUANT DOING JUMP ARC FROM 3:NP/QUANT TO 1:NP/ADJ DOING CAT ARC WITH WINTER FROM 1:NP/ADJ TO 6:NP/N DOING POP ARC FROM 6:NP/N TEST FAILED",
      "text": [
        "(The test failed because there is no determiner, and MINIGRAMMAR requires that singular, undetermined nouns can be complete noun phrases only if they are mass nouns.",
        "\"Winter\" is not marked as a mass noun in our dictionary, hence it will not parse as a complete noun phrase.)",
        "{Since there is no next word to test, a configuration is set up to begin processing a prepositional phrase, and the syntactic categories which ean begin such a phrase -- in this case, only one -- are monitored for.)",
        "{Creation of the POP trans completed a path, the first part of which has already been executed.",
        "We can therefore pick up in the middle of the path and execute only the last three transitions.]"
      ]
    },
    {
      "heading": "ALL ARCS TRIED AT THIS CONFIG",
      "text": [
        "(Now the theory has been processed.",
        "There follows a summary of the proposals, monitors, and notices constructed.",
        "The syntactic score assigned to the theory is given -- here just the score of the constituent constructed.",
        "Than there is a summary of statistics.)",
        "{This time there are monitors in the WFST, one which is looking for a NP starting at position 12 and one which is looking for a NP ending at position 21.",
        "One transition is sufficient to satisfy both of these, and the preposition needed to complete a PP/ is monitored for.)",
        "NP/ MAY LEAD TO CONFIG 11 {This is caused by the fact that there was a monitor for a. noun phrase ending at confipuration 11 -- the one created when constituent 1 was made.",
        "The transition which would be set up is the transition just created, so it is not remade.",
        "All of the processing which resulted from the completion of a constituent is finished; however there are tonitors still to be set for configurations along the path.)"
      ]
    },
    {
      "heading": "ENDING AT 12: MONITORING [ PREP ENDING AT 12: MONITORING [ N j ENDING AT 12: MONITORING [ QUANT MONITORING [ ADJ ENDING AT 12: MONITORING [ ART ]",
      "text": [
        "{Since each monitor consists of the item being monitored for, its associated test (if any) the theory which is to be notified when the monitor is satisfied, and the configuration and arc causing the monitor, monitors must be made anew each time one of the elements changes, although some of the list structure can be shared, hence the seeming proliferation of ,monitors.}"
      ]
    },
    {
      "heading": "monitors.) SELECTED CONFIGS (8) FOR EXTENSION PICKING UP CONFIG 8:NP/ADJ:21(5) ALL ARCS TRIED AT THIS CONFIG PREDICTIONS: MONITORING [ N ] STARTING AT 21, SCORE 10 MONITORING [s PREP ] ENDING AT 12, SCORE 10 MONITORING [ N ] ENDING AT 12, SCORE 10 MONITORING [ QUANT ] ENDING AT 12, SCORE 10 MONITORING ADJ ] ENDING AT 12, SCORE 10 MONITORING [ ART ] ENDING AT 12; SCORE 10 PROPOSING (PREP QUANT ART) ENDING AT 12 FINISHED THEORY 2 WITH SYN SCORE 10",
      "text": []
    },
    {
      "heading": "{The processing of this theory tbok approximately 4.5 seconds.)",
      "text": [
        "This example has shown the trace produced by running SPARSER on input which is analogous to the example presented with illustrations of the map in Section Four.",
        "The interested reader is urged to draw his own maps while reading the following Pare 96 examples in order to best understand the dynamic operation of SPARSER."
      ]
    },
    {
      "heading": "EXAMPLE 2",
      "text": [
        "This example is more roalistic than the previous one -- it shows the operation of SPARSER in the context of an utterance which has been automatically segmented and labeled, with the lexical retrieval and match component in operation.",
        "It demonstrates how SPARSER can help to select the best set of words from a more complex word lattice.",
        "This example uses the SPEECHGRAMMAR described in Ni).",
        "The utterance \"What is the registration fee?\" was spoken by an adult male speaker in a quite room and was record on tape.",
        "The utterance was automatically digitized and passed through the segmentation and labeling routines of the BBN speech understanding system.",
        "The initial scan of the utterance, using the lexical retrieval component, produced a word lattice of fifteen entries, including several for inflectional endings.",
        "(In this version of the system, they were not combined with the root form into a single word 'hatch?",
        "and hence could match even without a root word.)",
        "The format for a word match is:"
      ]
    },
    {
      "heading": "STARTING AN ISLAND STARTING AT LEFT END OF SENTENCE",
      "text": [
        "(Knowing that it is not necessary to go through the usual startup procedure for islands when beginning an"
      ]
    },
    {
      "heading": "TEST FAILED",
      "text": [
        "(This test failed because the grammar does not allow \"what\" to be a complete sentence.)",
        "(Here all the words which can start quantifiers, like \"a hundred\" or \"point five\", are proposed.",
        "The grammar does not preclude a quantifier following a queStion-determiner, e.g. \"What three men traveled to Spain?\".)",
        "(MONITORING [ INTEGER ZERO NO POINT A] For considerations of space, long listings of monitors and proposals in this example will be compacted as shown here.",
        "Such alterations to the actual trace produced will be surrounded by brackets.)"
      ]
    },
    {
      "heading": "TRYING JUMP NP/QUANT ARC JUMP TRANS #5 FROM 5:NP/ORD:3(16) TO 7:NP/QUANT:3(21) SELECTED CONFIGS (7) FOR EXTENSION PICKING UP CONFIG 7:NP/QUANTO(21) TRYING JUMP NP/DET ARC JUMP TRANS #6 FROM 7:NP/QUANT:3(21) TO 8:NP/DET:3(26) SELECTED CONFIGS (8) FOR EXTENSION PICKING UP CONFIG 8:NP/DET:3(26) STARTING AT 3: MONITORING [ NPR/ NP?! ]",
      "text": [
        "(There are two PUSH NPR/ arcs from this state so two monitors are created, but only one configuration is set up.)"
      ]
    },
    {
      "heading": "SETTING UP CONFIG 9:NPR/:3(26) {MONITORING [NPR NPR N ADJ N V ADV]) TRYING JUMP NP/HEAD ARC JUMP TRANS #7 FROM 8:NP/DET:3(26) TO 10:NP/HEAD:3(29) SELECTED CONFIGS (10) FOR EXTENSION PICKING UP CONFIG 10:NP/HEAD:3(29)",
      "text": [
        "{This is an example of the fallibility of using only context free tests on partial paths.",
        "The parser thinks it has successfully reached state NP/HEAD, while in fact tbis cannot be the case because no head noun has been discovered for the noun phrase.",
        "Thus it is incorrect to"
      ]
    },
    {
      "heading": "DOING CAT ARC WITH WHQ PROM 3:NP/ TO 5:NP/ORD DOING JUMP ARC FROM 5:NP/ORD TO 7:NP/QUANT DOING JUMP ARC FROM 7:NP/QUANT TO 8:NP/DET DOING JUMP ARC FROM 8:11P/DET TO 10:NP/HEAD TEST FAILED",
      "text": [
        "(A question-determiner alone cannot be a complete noun phrase; although this is permitted by considering \"what\" as a QWORD as in transition #2.)"
      ]
    },
    {
      "heading": "STARTING AN ISLAND \"REGISTRATION\" TRYING CAT N ARC FROM NP/DET TO NP/DET",
      "text": []
    },
    {
      "heading": "JUMP TRANS #14 FROM 20:NP/:6(1) TO 19:NP/ONLY:6(1) REGISTRATION\" TRYING CAT N ARC FROM NP/DET TO NP/HEAD",
      "text": [
        "CAT N TRANS #15 FROM 14:NP/DET:6(1) TO 21:NP/HEAD:19(6)",
        "(This notice is in response to the lookahead test on the push arc to state R/NIL.",
        "Since \"fee\" can start a reduced re*ative clause, it is noticed, but there is not a specific monitor set up because the arc within the relative clause network which will actually process the word \"fee\" is not known.)",
        "{Proposals were made to fill the gap because there were monitors from both sides of a gap small enough to contain one word.)",
        "FINISHED THEORY 1 WITH SYN SCORE 0 (It took 11.9 seconds to process this theory.)",
        "{Processing the proposals just made results, notably, in the detection of the word \"other\" between \"what\" and \"registration\", but the word match score is very low.",
        "Word matches for \"is\" and \"are\" from position 3 (next to \"what\") to position 4 are also found, but since they do Page 61 not fill the gap, the event scores are low.",
        "The best event is that for the word \"fee\".",
        "Processing it is fairly uninteresting, since it completes no tonstituent, so we will omit the trace of that event.",
        "After it has been processed, however, the best event is that for the word \"the\" and the theory just created.)"
      ]
    },
    {
      "heading": "SYNTAX PROCESSING EVENT FOR THEORY#2 WITH NEW WORD (4 THE 6)",
      "text": []
    },
    {
      "heading": "FEATS NU.SG",
      "text": [
        "(The format of this noun phrase is slightly different from that in the previous example because tpe structure building action for noun phrases in SPEECHGRAMMAR is different from that in MINIGRAMMAR.",
        "There are many places in the SPEECHGRAMMAR which push for noun phrases, and since there were no monitors in the WFST which can us b this constituent, all of them must be tried, resulting in a number of predictions and notices.)",
        "(PROPOSING \"WHICH\" \"THAT\" \"WHO\" \"WHOM\" \"WHICH\" \"WHOM\") (There are two arcs entering state R/WH which use the words \"which\" and \"whom\".",
        "There is a check made to see that duplicate proposals are not actually communicated to the control component, although they appear to he duplicated in the trace.)"
      ]
    },
    {
      "heading": "{The creation of transition #27 cvmpleted 3 paths. The",
      "text": [
        "first two have been executed, resulting respectively in failure and the completion of a constituent with all the processing that entails.",
        "Now the third path is still pending and is about to be executed.)"
      ]
    },
    {
      "heading": "N REGISTRATION FEATS NU SG",
      "text": [
        "(This constituent can now satisfy the monitors set by the discovery of the larger one, resulting in the creation of many new transitions but no new predictions.)",
        "SYN WEIGHT + SEM WT = 10 + 0 = 10 NP/ WAS PUSHED FOR AT CONFIG 34 PUSH NP/ TRANS #46 FROM 34:FOR/FOR:4(1) TO 53:TO/:19(8) (Similar NP/ transitions are set up at configurations 36,38,40,41,43146,47,49, and 51 because of the monitors set when the first constituent was found.)",
        "SELECTED CONFIGS (55 54 39 17) FOR EXTENSION (Because these are the maximally scoring configurations from the large pool of possibilities.)"
      ]
    },
    {
      "heading": "NP DET ART THE ADJ NP N REGISTRATION NU SG N FEE FEATS NU. SG WITH FEATURES (NPU)",
      "text": [
        "(Here is an example of a constituent which has features attached to it.",
        "The feature NPU can be tested by tne semantic component to determine that the constituent is a noun phrase utterance.",
        "If necessary, it could also be tested on a PUSH S/ arc in the grammar, since there are some times, e.g. during the construction of a Page 64 sentential complement, when an embedded sentence must contain a verb.)"
      ]
    },
    {
      "heading": "DET ART THE N FEE FEATS NU SO WITH FEATURES (NPU)",
      "text": [
        "discovery of the noun phrases which were not monitored for.)",
        "(Processing the proposals from this theory results in the best event being the one for \"is\" in the last gap.",
        "The word \"are\" also fills the gap, but the lower lexical score prevents the event for it from surfacing.",
        "If it were syntactically processed, however, no new theory would be created since the completed string would be ungrammatical.)"
      ]
    },
    {
      "heading": "SYNTAX PROCESSING EVENT FOR THEORY#3 WITH NEW WORD (3 IS 4)",
      "text": []
    },
    {
      "heading": "FEATS NU SG AUX TNS PRESENT VOICE ACTIVE VP V BE OBJ NP N WHAT FEATS NU SO/PL",
      "text": [
        "{This is the complete parse of the utterance, but SPARSER continues the operations it has pending before returning to Control.)"
      ]
    },
    {
      "heading": "NO SEMANTICS FOR HEAD",
      "text": [
        "{This is a comment from the semantic component indicating that it cannot currently interpret the construction.)"
      ]
    },
    {
      "heading": "JUMP TRANS 071 FROM 67:VP/V:0(1) TO 66:VP/HEAD:0(1) JUMP TRANS #72 FROM 68:S/AUX:0(1) TO 67:VP/V:0(1) JUMP TRANS 073 FROM 69:S/NO-SUBJ:0(1) TO 67:VP/V:0(1) JUMP TRANS #74 FROM 68:S/AUX:0(1) TO 69:S/NO-SUBJ:0(1) JUMP TRANS #75 FROM 69:S/NO-SUBJ:0(1) TO 67:VP/V:0(1) JUMP TRANS #76 FROM 70:S/THERE:0(1) TO 67:VP/V:0(1)",
      "text": [
        "{One of the pending operations is to check the other arcs which caused monitors for the verb \"is\".)"
      ]
    },
    {
      "heading": "\"IS\" TRYING (CAT V --) FROM STATE S/NP TO CONFIG 45",
      "text": [
        "\"IS\" TRYING (CAT V --) FROM STATE FOR/TO TO CONFIG 49 CAT V TRANS #77 FROM 71:FOR/T0:3(5) TO 49:VP/V:4(20) \"IS\" TRYING (CAT V --) FROM STATE VP/V TO CONFIG 49 CAT V TRANS t78 FROM 72:VP/V:3(5) TO 49:VP/V:4(20)"
      ]
    },
    {
      "heading": "CREATING THEORY 4: 0 WHAT 3 IS 4 THE 6 BEGISTRATION 19 FEE 23 WITH SYN SCORE 15",
      "text": []
    },
    {
      "heading": "{This processing took 34.45 seconds.]",
      "text": [
        "This example was run with a very simple, mechanical control structure.",
        "After the processing of the initial theory, the proposals which had been made by SPARSER were processed by the lexical retrieval component and the results added to the word lattice -- a process which can set off monitors and result in the creation of event notices.",
        "The events are scored by a combination of the monitor score assigned by SPARSER and the lexical score assigned by the word match component.",
        "In this sentence, syntax and lexical score alone were sufficivnt to make the best scoring event at each step be one which resulted in a correct extension of the theory.",
        "We now show how the same utterance use0 in the previous example cab be recognized when different theories ye created and when events and theories are processed in a different orde&r from that in Example 2.",
        "Suppose that after the initial scan of the atterance the semantic component created two theories, one for the words \"what\" and \"fee\" and the other for the wol-ids \"what' and \"reristration\" Let us see what happens in SPARSER when we bepin by processipg these two theories in sequence."
      ]
    },
    {
      "heading": "DOING CAT ARC WITH WHQ FROM 3:NP/ TO 5:NP/ORD DOING JUMP ARC FROM 5:NP/ORD TO 7:NP/QUANT DOING JUMP ARC FROM 7:NP/QUANT TO 8:NP/DET DOING JUMP ARC FROM 8:NP/DET TO 10:NP/HEAD TEST FAILED STARTING AN ISLAND \"FEE\" TRYING CAT N ARC FROM NP/DET TO NP/DET CAT N TRANS #9 FROM 14:NP/DET:19(1) TO 15:NP/DET:23(4) ENDING AT 19:",
      "text": []
    },
    {
      "heading": "STARTING AN ISLAND STARTING AT LEFT END OF SENTENCE",
      "text": []
    },
    {
      "heading": "SELECTED CONFIGS (1) FOR EXTENSION",
      "text": [
        "(Upon picking up this configuration to extend it, SPARSER finds the transitions which were created during: the processing of the word \"what\" by the previous theory.",
        "It \"traces\" them all, that is, it does not recreate them but simply puts the transition numbers on a list which will form part of the syntactic information Page 71 associated with the current theory.",
        "The tracing process also involves the creation of monitors (and notices, where applicable) for constituents along the path.",
        "These monitprs and notices must be remade, since the previous monitors will activate only the previous theory.",
        "Due to the recursive nature of the tracing process, the transitions are not necessarily followed in the same order that they were originally created, nor are the monitors made in exactly the same order.",
        "Notice that the many arcs which were tried but which did not result in the creation of transitions in the previous theory are not retried here.)",
        "(This does not mean that configuration 6 was just created.",
        "Since it already existed in the map, having been created during the processing of the previous theory, the configuration number is merely put on the list of configurations in the current theory.)",
        "(No proposals were made here because proposals are not theory Oependent; that is, the word proposals which were made during the processing of the previous theory resulted in some words being placed in the word lattice which were noticed here.",
        "Remaking the proposals would not lead to the discovery of any new information.)"
      ]
    },
    {
      "heading": "TRACING POP TRANS 8 FROM 10:NP/HEAD:3(29)",
      "text": []
    },
    {
      "heading": "Page 72",
      "text": [
        "{The processing of the island for \"registration\" is idéntical to that in the last example, so the remainder of the trace will be omitted.",
        "The total processing took 12.2 seconds.)",
        "fLet us now process the event which adds the word \"theft to the theory just processed.",
        "This will result in the creation of a constituent event.)",
        "(This constituent cannot be used immediately by this theory because it contains a word (\"fee\") which is not in the theory.",
        "Therefore a notice is sent to Control which may be turned into an event at some later time.",
        "Nothing further is done with this constituent at this time, i.e., no transitions using it are created.",
        "It is, however, placed in the WFST for later use.)"
      ]
    },
    {
      "heading": "NP DET ART THE N REGISTRATION FEATS NU SG WITH FEATURES (NPU) **** SYN WEIGHT + SEM WT = 10 + 0 = 10 PICKING UP CONFIG 44:VP/V:19(7) STARTING AT 19: {MONITORING [ NP/ N-QDET ADJ INTEGER ARE QUANT PRO NPR POSS V V ADV TEST(NOT (CAT V)) ]) SETTING UP CONFIG 20:NP/:19(7) NOTICING \"FEE\" ALL ARCS TRIED AT THIS CONFIG PICKING UP CONFIG 48:VP/NP:19(7) STARTING AT 19: {MONITORING [ COMPL/ TO/ COMPL/ NP/ FOR THAT TO FOR THAT N QDET ADJ INTEGER ARE QUANT PRO NPR FOSS V PARTICLE ])",
      "text": [
        "(Now we will process the constituent event for the theory just created.",
        "Because of the constituent for \"the registration\" there are now monitors in the WFST for a noun phrase beginning at position 4, so the appropriate transitions are made.)"
      ]
    },
    {
      "heading": "SYNTAX PROCESSING EVENT FOR THEORY#3 WITH CONSTITUENT #1 TO GET NEW THEORY#4 0 WHAT 3 4 THE 6 REGISTRATION 19 FEE 23",
      "text": [
        "{Processing begins exactly where it left off when the"
      ]
    },
    {
      "heading": "Page 76",
      "text": [
        "constituent was made -- the constituent is semantically evaluated with respect to this theory so that the constituent weight may be altered.",
        "In this case, however, Semantics has been turned off, so there is no increment in the score.)"
      ]
    },
    {
      "heading": "SYN WEIGHT + SEM WT = 15 + 0 = 15 NP/ WAS PUSHED FOR AT CONFIG 34",
      "text": [
        "PUSH NP/ TRANS #51 FROH 34:FOR/FOR:4(1) TO 56:TO/:23('11) (Similar transitions are set up for all 9 other confipurations where an NP/ was used in the previous theory.",
        "The monitors set by these paths are copied from the previous theory, so there is no indication here of a new monitor beinp created.)"
      ]
    },
    {
      "heading": "NP DET ART THE ADJ NP N REGISTRATION NU SG N FEE FEATS NU SG WITH FEATURES (NPU)",
      "text": []
    },
    {
      "heading": "PROPOSING (V MODAL) FROM 3 TO 4 PROPOSING (MODAL PREP) STARTING AT 1 PROPOSI6G (PREP MODAL NEG QADV) ENDING AT 4",
      "text": [
        "(Again, the monitor list is omitted for considerations of space.)"
      ]
    },
    {
      "heading": "CREATING THEORY 4: 0 WHAT 3 4 THE 6 REQISTRATION 19 FEE 23 WITH SYN SCORE 15",
      "text": []
    },
    {
      "heading": "(This event took only 9.7 seconds.)",
      "text": [
        "Page 77 The processing of the final event, that which adds the word \"is\" to the theory just created, will not he shown.",
        "These examples have shown that SPARSER is a useful tool in the automatic recognition df speech.",
        "The timing measurements indicate that considerable processing is done when the parser is forced to work in bottom pp mode, especially with a large grammar Of course there is some implementaion overhead involved in doing the timings themselves.",
        "If the parsing algorithm were to be carefully recoded in assembly language a speed up of at least a factqr of 20 (and perhaps much more) could be achieved.",
        "Another way to cut down the time-consuminp processing mipht be to attempt to obtain more semantic guidance.",
        "For example, if the semantic hypothesis associated with a theory indicates that a particular noun is likely to be used in a noun phrase modifier (e.g. \"tomorrow\"), then SPARSER should be able to take advantage of this informetion by scorimtz the PUSH NP/ transition from a configuration for state PP/ (i.e. to get something like \"by tomorrow\") higher than those PUSH NP/ transitions for other syntactic slots.",
        "In fact, the others may not need.",
        "to be constructed at all.",
        "The grammar could also be further tuned to eliminate some spurious predictions and reduce the time spent following erroneous paths."
      ]
    },
    {
      "heading": "Page 78 Section 7 Conclusions and Further Research 7.) STRENGTHS AND WEAKNESSES OF SPARSER",
      "text": [
        "One of the weak points of the current system is the fact that some context information is not used until a path is complete, resulting in the creation of false paths and predictions which should not have been made.",
        "This is partly mitigated by the fact that this avoids a too great dependence on left context and allows the creation of partial paths which may be followed if an earlier word is changed.",
        "It is important, however, to minimize the number of predibtions which are made and to make the predictions as accurate as possible.",
        "In this regard, it is unforitunatc, that the currept system makes predictions on the left of an island solely on the basis of the first word in the island and .maskes predictions On the right end from configurations which, if context sensitive tests had been done along the path, would never nave been created.",
        "One way to help tighten the predictions would be to take each context free path through an island and walk it in a special mode after the island has been processed but before predictions are communicated to the control component.",
        "This mode would set and check registers, assuming that any tests which require unknown left context are true.",
        "Only if the path did not fail Page 79 under this mode of operation would the predictions at either end of it be made.",
        "If a really efficient way of handlinp unknown left context and of storinp this informaCion were developed, it could be used in place of the context free pass in the first place, thus eliminatinp all inconsistent paths.",
        "The problem with storing all possible contexts is that they must be recomputed each time a new step is added to the path.",
        "lhis is relatively easy if the next step is taken to the ripht of an existing path, since ATN s are more suited to left to ripht processing, but it becomes extremely complex when a transition is added to the left end of a path (or set of paths) or when a transition joins two sets of paths together.",
        "To be absolutely sure that no contexts have been missed, all the paths would have to be walked and their contexts reprocessed and copied in whole or in part (since the new step may be wrong, the old context must be preserved.)",
        "Of eourse this is not the only approach which could be .used a merffing technique like that of Earley's algorithm might be feasible, if the structure of the grammar were also changed to make it less left to right oriented.",
        "One great strength of the system is its ability to store and merge information in such a way that it does not have to be redone when the context is changed.",
        "For example, once an arc has been tried with a particular word match, a transition will be created if the arc may be taken and the arc will be removed from further consideration if it may not be taken.",
        "Then, if the conriguration should ever be reached with the same word match agarn (perhaps in a later theory) not only will any relevant Pape 80 transitions be recognized without having to go throur!h the work of recreating them, but also no arcs which had prev,iously Hal txgr hg rvIrigA.",
        "Another feature of SPARSER.",
        "is the fact that it was desirned and implemented with many unsolved problems and unavailable data in mind, and therefore many \"holes\" have been left on which to \"hook\" further developments.",
        "For exnmnle, althourh prosodic verification of constituents is not yet available, the scorinr mechanism for constituents is structured in such a way that it would be easy to include the results of verification by prosodies (or any other component).",
        "The original implementation of SPARSER used a depth first search but Oas implemented in such a way that the change to Modified breadth first was quite simple.",
        "This foresight has paid off in a flexible system which has shown that it can be readily experimented with in order to explore many still unsolved problems concerning the nature and use of syntactic information in understanding"
      ]
    },
    {
      "heading": "7.2 NIOSODICS",
      "text": [
        "A tremendous amount of information in speech is conveyed by prosodic features: stress, intonation duration, loudness, pauses, pitch.",
        "For example, if John mumbles to Bill, \"The mailman left something for you,\" Bill may reply either \"What?\" with much energy and a sharply risifig intonation or \"What?\" with a flat di' falling intonation.",
        "In the first case John is very likely to shout \"I said, The mailman left something for you \""
      ]
    },
    {
      "heading": "Page 81",
      "text": [
        "interpreting .\"What?\" to mean \"What did you say?\" whereas in the second case he is likely to say something like \"A package from your mother,\" interpreting \"What?\" to mean \"What is it?\" To ignore prosbdics is to ignore a source of information which has been shown repeatedly to be an extremely important factor in human understanding.",
        "Consider the following examples of sentences and sentence frapments which illustrate some of the ways prosodies are used:"
      ]
    },
    {
      "heading": "1. I stepped on the man with black shoes. (Who was wearing",
      "text": []
    },
    {
      "heading": "the shoes?)",
      "text": [
        "2a.",
        "The new gnu knew news.",
        "2b.",
        "The gnu knew new gnus."
      ]
    },
    {
      "heading": "3. I m going to move on Thursday. (stress on \"move\"",
      "text": [
        "implies moving to a new house; stress on \"on\" implies traveling to a new place.)"
      ]
    },
    {
      "heading": "Prosodic verification could help a lot in rejecting",
      "text": [
        "semantically correct, syntactically consistent phrases which are nonetheless wrong.",
        "If the constituent \"speech understandinp\" were identified and relied upon, it might be very difficult to produce a correct analysis of the utterance: \"Because of peculiarities in his speech, understanding Joe is not easy.\" Page 82 Besides indicating syntactic boundaries and/or providinp intonation contours for certain constituents, prosodic features can be used to mark emphasis, introduce new topics, convey information about the speaker s internal mental and emotional state (e.g. whether he is teasinp or serious), and probably more.",
        "It is particularly interestinP to note that some well known phenomena such as \"pronouns are almost never stressed\" and \"ip discourse when a new topic word is mentioned it is almost always stressed\" hhve very natural explainations in light of what we know about acoustic processing.",
        "stressed words are renerdlly easier to identify because there is less acoustic ambiguity, but unstressed words may differ greatly from their ideal pronunciation and hence are harder to reliably identify Pronouns refer to antecedents which are presumably known to the listener, so he can anticipate them or at least verify them easily, hence they need not have rood acoustic characteristics.",
        "A new topic may not have been anticipated, so the listener will have to depend heavily on identifying the word from acoustic information alone and the speaker can provide this extra reliable information by stressing the word.",
        "Unfortunately, not a great deal is known about either the acoustic correlates of prosodic features or the ways in which they are used.",
        "Many, of the rules which have been developed thus far are speaker dependent and are sufficient for conveying information but are not necessary.",
        "This makes them difficult to use in the analysis mode.",
        "Although a good start has been made in exploring prosodies (see, for example, Lea [52, 13] and Bates and Wolf [8]), much more work remains to be done before prosodic Page 83 information can be reliably used by speech understanding systems.",
        "SPARSER could use prosodic information in several ways.",
        "Verification of constituents would be a prest help, but local prosicid information could be used even earlier in the parsing process.",
        "For example, if major constituent boundaries could be accurately determined, them instead of both POPing a constituent and continuing it in parallel, as is done now, one alternative could be chosen instead of the other on the badis of prosodic information.",
        "If, as is more likely, some major boundaries could be reliably detected, then it would be easy to revise SPARSER to begin processing at such places even •ithin an island at states which can begin constituents.",
        "This would again reduce the number of partial paths created when parsing an island."
      ]
    },
    {
      "heading": "7.3 EXTENSIONS AND FURTHER RESEARCH",
      "text": []
    },
    {
      "heading": "Ungrammatical input",
      "text": [
        "One of the obvious extensions to a basic speech understanding.",
        "system is to relax the restrictions on the input to the system.",
        "Syntactically, this can mean removing the requirement that the initial utterance be grammatical.",
        "Since people frequently speak ungrammatically in informal discourse, this is a natural step to want to take.",
        "In order to extend SPARSER to handle such input, several approaches are possible.",
        "Certain types of errors may be called errors of style (and may not be called errors at all by some Page 84 people) such as the use of \"ain't\" and the occurrence of a prepositjon at the end of a sentence.",
        "These regularities ray simply be declared grammatical by modifying the grdmmar to accept them.",
        "Many speech errors have been shown to follow rerul.fr patterns and hence may be amenable to this approacol.",
        "Other common errors viol.ite specific tests which appear the arcs of the grNmmar, for example, to prohibit douIc neratives and to check for number agreement between sub:eot and verb or between determiner and noun (e.g. *\"There is sore verv severe restrictions on this rule.\").",
        "In this case, rather than removinp the tests from the grammar it would be rore suitable to modify them so that if the' .4rc could still hy though with a ruch reduced weight or with an indication in sore register that an error has occurred.",
        "One way to implement this would be to have all tests return a nurber as their value indicating how well they succeeded on some scade from \"perfectly\" to \"not at all\".",
        "Not all arc tests are of this relaxable nature, however, since certain types of errors are so rare, if they occur at all, that they may be judged unacceptatio.",
        "Examples of such tests are the case checks for pronouns (e.g. *\"I gave it to he\") and the requirement that a verb modifying .a noun must be in either the present or past participle form (e.g. \"the singing brook\" vs. *flthe sinF brook\").",
        "These methods would not allow all types of grammatical errors to be handled (in particular it irnores the problem of constituent ordering errors such as \"Throw Mama from the train a"
      ]
    },
    {
      "heading": "Pare 85",
      "text": [
        "kiss\"), but would handle many of the most common syntactic errors."
      ]
    },
    {
      "heading": "AD experiment",
      "text": [
        "Keepinr in mind that SPARSER is not intended to be a Medel of human syntactic analysis, it is nonetheless reasonable to ask whether there are any similarities which may be seen.",
        "The followinr experiment is suprested with the hypothesis that it pill indicate that people do considerable processing at the end of svntactic constituents in a way similar to.some rerister settiny and testing actions and semantic (or other) verification The experiment is this: a subject is seated in front of a switch which 11.' is asked td press whenever he is sure that he is bearing an anomalous sentence, He is then presented with a ;number of recorded utterancs, some of which are incorrect, e.g. \"The cat and dog which lives next door are friendly.",
        "\"I sawia red big barn on the farm.",
        "I hypothesize that the subject will indicate the presence of an error at a point shortly after the end of the constituent in which the error occurred more often thand shortly after the earliest possible place wher4 the error coul be detected."
      ]
    },
    {
      "heading": "1 4 CONCLUSION",
      "text": [
        "In concluslon, it is obvibds ttigt there is mlich work yet to be done in the.",
        "problem.of speech understanding, but it is hoped bhat the system presented her has not only advanced our current undstancfln of t'r] r(-)1 of sntotIc nnw1c-',1c-c-' hut will continue to h o uoful tco_1 Isor t h fiol."
      ]
    },
    {
      "heading": "APPENDIX I MINIGHAmmAR",
      "text": []
    },
    {
      "heading": "This appendix contains a listing (slightly edited for",
      "text": [
        "clarity) of theagrammar called MINIGRAMMAR which was discussed in Section Three (illustrated in Figure 3.3) and which was used In"
      ]
    },
    {
      "heading": "APPENDIX II",
      "text": []
    },
    {
      "heading": "Vocabulary and Syntax C/aaaes",
      "text": [
        "This appendix lists the 351 words whigh were In the dictionary of the BBN speech understanding system wtien the examples in Chapter Six were run (July 1975).",
        "(A 569. word dictionary and one with 1000 entries are now available.",
        "After the listing of the words in the dictionary, they are.",
        "broken into syntactic classes, with the number of words im each alass indicated beside the clmss name.",
        "Finally, the syntactic features are given together with a list of the words which carry each feature.",
        "Features may be of the form FEATURE, (FEATURE), or (FEATURE VALUE).",
        "This is not a listing of the dictionary as 4t appears to the system, but rather a derived cross reference which indicates the various parts of speech and syntactic features for each word The words:"
      ]
    },
    {
      "heading": "(A ABOUT ABOVE ACL ACOUSTICAL ACOUSTICS ADDITIONAL AFFORD AFTER Al AIR AIRPLANE ALL ALREADY ALSO AM AMHERST AMOUNT AN AND ANTICIPATE ANY ANYONE ANYWHERE APRIL ARE ABPA ARRANGE ASA ASK 4SSOCIATION ASSUME ASSUMPTION AT ATTEND AUGUST AVAILABLE BATES 3E BECAUSE BEEN BEFORE BEGINNING BEING BIG BILL BPNNIE BOSTON BOTH BREAKDOWN BUDGET BUS BY CALIFORNIA CAN CANCEL CAR CARNEGIE CENT CHANGE CITY COLARUSSO COMPUTATIONAL CONFERENCE CONTINUE COSELL COST COSTING COSTS COUNTRY CRAIG CURRENT DATE, DWE )AY DECEMBER DENNIS DID DIVISION DO DOES DOLLAR DONE DUE@TO DURING EACH EIGHT EIGHTEEN EIGHTEENTH EIGHTH EIGHTY EITHER ELEVEN ELEVENTH END ENGLAND EtIOUGH ESTIMATE-N ESTIMATE-V EVERY EVERYONE EXPECT EXPENSE EXPENSIVE FALL FARE FEBRUARY FEE FIFTEEN FIFTEENTH FIFTH FIFTY FIGURE FINAL FIRST FISCAL FIVE FOR FORTY FOUR FOURTEEN",
      "text": [
        "Page Q0"
      ]
    },
    {
      "heading": "FOURTEENTH FOURTH GET GETS GETTING GIVE GIVEN GIVES GIVING GO GOES GOING GONE GOT GOTTEN GROUP HAD HALF HALVES HAS HAVE HAVING HE HER HIM HIS Row HOWMANY HOMMUCH HUNDRED I IF IFIP IJCAI IN INTERNATIONAL IS IT JANUARY JERRY JOHN JULY JUNE K KNOW L.A. LAST LATE LEFT LINDA LINGUISTICS LIST LONDON LONG LOS@ANGELES LYN LYNN MADE MAKE .MAKES MAKHOUL MAKING MANY MARCH MASSACHUSETTS MAY ME MEETING MEMBER MISCELLANEOUS MONEY MONTH MORE MOST MUCH MY NEED NEWYORK NEXT NINE NINETEEN NINETEENTH NINETY NINTH NO NOT NOTE NOVEMBER NOW OCTOBER OF ON ONE ONLY OR OTHER OUT OVERHEAD PAJARRO@DUNES PARTICIPANT PAUL PAY PENNSYLVANIA PEOPLE PER PERSON PHONOLOGY PITTSBURGH PLACE PLEASE PLUS PRINT PROJECT-N PROJECT-V PURPOSE QUARTER REGISTRATION REMAIN REST REVISE RICH RICHARD ROUND@TRIP SANTA@BARBARA SCHEDULE SECOND SEND SENDING SENDS SENT SEPTEMBER SEVEN SEVENTEEN SEVENTEENTH SEVENTH SEVENTY SHE SINCE UTE SIX SIXTEEN SIXTEENTH SIXTH SIXTY SO SOCIETY SOME SOMEONE SPEECH SPEND SPENDING SPENDS SPENT SPRING ST.LOUIS START STATUS STOCKHOLM SUMMER SUPPOSE SUPPOSED SUPPOSITION SUR SWEDEN TAKE TAKEN TAKES TAKING \"TEN TENTH THAN THANKPYOU THAT THE THEIR THEM THERE THESE THEY THIRD THIRTEEN THIRTEENTH THIRTIETH THIRTY THIS THOSE THOUSAND THREE TIME TO TOO TOOK TOTAL TRAVEL TRIP TWELFTH TWELVE TWENTIETH TWENTY TWO UNANTICIPATED UNBUDGETED UNSPENT UNTAKEN US VARIOUS VISIT WANT WAS WASHINGTON WE WEEK WENT WERE WHAT WHEN WHERE WHICH WHO WHOM WHOSE WILL WINTER WISCONSIN WITH WITHIN WORKSHOP YEAR YES YOU)",
      "text": [
        "The syntactic categories: (ADJ 23 (ACOUSTICAL ADDITIONAL AVAILABLE BIG COMPUTATIONAL"
      ]
    },
    {
      "heading": "CURRENT EACH ENOUGH EXPENSIVE FINAL FISCAL INTERNATIONAL LATE LEFT LONG MANY MISCELLANEOUS OTHER UNANTICIPATED\" UNBUDGETED UNSPENT UNTAKEN VARIOUS)) (ADV 18 (ALREADY ALSO ANYWHERE EITHER ENOUGH HOW LATE LONG MORE MOST MUCH NOW ONLY PLEASE SO THERE TOO YES)) (ART 8 (A AN NO THAT THE THESE THIS THOSE)) (cou 8 (AND BECAUSE BOTH IF OR PLUS SINCE SO)) (INTECIER 27 (EIGHT EIGHTEEN EIGHTY ELEVEN FIFTEEN FIFTY FIVE FORTY FOUR FOURTEEN NINE NINETEEN NINETY ONE SEVEN SEVENTEEN SEVENTY SIX SIXTEEN SIXTY TEN THIRTEEN THIRTY THREE TWELVE TWENTY TWO))",
      "text": [
        "(NODAL 5 (CAN DID DO DOES WILL))"
      ]
    },
    {
      "heading": "(N 70 (ACOUSTICS AIR AIRPLANE AMOUNT ASSOCIATION ASSUMPTION BEGINNING BREAKDOWN BUDGET BUS CAR CENT CHANGE CITY CONFERENCE COST COUNTRY DATE DAY DIVISION END ESTIMATE-N EXPEN$E FALL FARE FEE FIGURE GROUP HALF HALVES LINGUISTICS LIST MEETING MEMBER MONEY MONTH MUCH NEED NOTE OVERHEAD PARTICIPANT PEOPLE PERSON PHONOLOGY PLACE PROJECT-N PURPOSE QUARTER REGISTRATION REST ROUND@TRIP SCHEDULE SITE SOCIETY SOME SPEECH SPRING STATUS SUMMER SUPPOSITION",
      "text": []
    },
    {
      "heading": "(NPR 53 (ACL Al AMHERST APRIL ARPA SA AUGUST BATES BILL BONNIE BOSTON CALIFORNIA CARNEGIE C)LARUSSU COSELL CRAIG DECEMBER DENNIS ENGLAND FEBRUARY IFIP IJCAI JANUARY JERRY JOHN JULY JUNE L.A. LINDA LONDON LOS @ NGELES LYN LYNN MAKHOUL MARCH MASSACHUSETTS MAY NEWOYORK NOVEMBER OCTOBER PAJARROODUNa PENNSYLVANIA PITTSBURGH RICH RICHARD SANTA@BARBARA SEPTEMBER ST.LOUIS STOCKHOLM SUR SWEDEN WASHINGTON WISCONSIN))",
      "text": [
        "(ORD 23 (EIGHTEENTH EIGHTH ELEVENTH FIFTEENTH FIFTH FIRST"
      ]
    },
    {
      "heading": "FOURTEENTH FOURTH LAST NEXT NINETEENTH NINTH SECOND S5VENTEENTH SEVENTH SIXTEENTH SIXTH TENTH THIRD TH1$TEENTH THIRTIETH TWELFTH TWENTIETH))",
      "text": [
        "(PARTICLE 3 (IN ON OUT)) (FOSS 5 (HER HIS MY THEIR WHOSE)) (PRECONJ 2 (BOTH EITHER)) (PREP 18 (ABOUT ABOVE AFTER AT BEFORE BY DUEOTO DURING FOR IN OF ON OUT PER SINCE TO WITH WITHIN))"
      ]
    },
    {
      "heading": "(PRO 23 (ANYONE EVERYONE HE HER HIM I IT ME ONE SHE SOMEONE THAT THEM THESE THEY THIS THOSE US WE WHAT WHO WHOM YOU)) (QADV 2 (WHEN WHERE)) (QDET 5 (HOWMANY HOWMUCH WHAT WHICH WHOSE)) (QUANT 14 (ALL ANY BOTH EACH EITHER ENOUGH EVERY HOWMANY HOWMUCH MANY MOPE MUCH OTHER SOME)) (QWORD 7 (HOW HOWMANY HOWMUCH WHAT WHICH WHO WHOM)) (SPECIAL 8 (DOLLAR HUNDRED K NO THAN THANKOYOU THOUSAND YES) ) CV 85 (AFFORD AM ANTICIPATE ARE ARRANGE ASK ASSUME ATTEND BE BEEN BEGINNING BEING BUDGET CAN CANCEL CHANGE CONTINUE COST COSTING COSTS DID DO DOES DONE END ESTIMATE-V EXPECT FIGURE GET GETS GETTING GIVE GIVEN GIVES GIVING GO GOES GOING GONE GOT GOTTEN HAD HAS HAVE HAVING IS KNOW LAST LEFT LIST MADE MAKE MAKES MAKING NEED NOTE PAY PRINT PROJECT-V REMAIN REVISE SCHEDULE SEND SENDING SENDS SENT SPEND SPENDING SPENDS SPENT START SUPPOSE TAKE TAKEN TAKES TAKING TOOK TOTAL TRAVEL VISIT WANT WAS. WENT WERE WILL))",
      "text": []
    },
    {
      "heading": "Pag 92",
      "text": [
        "The syntactic features:"
      ]
    },
    {
      "heading": "(INGCOMP (CANCEL CONTINUE GIVE START START)) (INTRANS (CONTINUE GO GO GO START)) (PASSIVE (CANCEL CONTINUE FIGURE GET GIVE MAKE MAKE SEND SEND START START TAKE) ) (QCOMP (CANCEL CONTINUE FIGURE GIVE SEND SEND TAKE)) (THATCOMP (END FIGURE)) (TRANS (CANCEL CONTINUE END FIGURE GET GIVE MAKE MAKE SEND SEND START START TAKE)) ((ANAPHORIC) (WHICH)) ((DETERMINED) (ANYONE I IT ME THAT THESE THIS THOSE US WE WHO WHOM YOU)) ((INDOBJ FOR) (MAKE MAKE)) ((NUMBER PL) (HALVES PEOPLE THEM THESE THEY THOSE US WE)) ((NUMBER SG) (ANYONE HE HER HIM I IT ME ONE SHE SOMEONE THAT THIS WHO WHOM WHOM)) ((NUMBER SG/PL) (WHAT WHAT WHICH WHO YOU)) ((PARTICLEOF (LEAVE PUT ADD)) (IN)) ((PARTICLEOF (ADD CONTINUE)) (ON)) ((PARTICLEOF (LEAVE PRINT SEND MAKE CANCEL FIGURE FIND GO)) (OUT)) ((PASTPART) (BEEN COST DONE GIVEN GONE GOTTEN HAD LEFT MADE SENT SPENT TAKEN)) ((PNCODE 13SG) (WAS)) ((PNCODE 1SG) (AM)) ((PNCODE 3SG) (COST COST COST COST COSTS DOES DOES GETS GIVES GOES HAS IS MAKES SENDS SPENDS TAKES)) ((PNCODE mvx) (CAN CAN DID)) ((PNCODE X13SG) (ARE WERE)) ((PNCODE X3SG) (COST DO WILL)) ((PRESPART) (BEGINNING BEING COSTING GETTING GIVING GOING HAVING MAKING SENDING SPENDING TAKING)) ((ROLE OBJ) (HER HIM ME THEM US WHOM WHOM)) ((ROLE SUBJ) (HE I SHE WE)) ((ROLE SUBJ/OBJ) (WHAT WHICH WHO WHO)) ((TNS FUTURE) (WILL WILL)) ((MS PAST) (COST DID DID GOT HAD MADE SENT SPENT TOOK WAS WENT WERE)) ((TNS PRESERT) (AM ARE CAN CAN CJST COST COST COST COST COSTS DO DOES DOES GETS GIVES GOES HAS IS MAKES SENDS SPENDS TAKES WILL))",
      "text": []
    }
  ]
}
