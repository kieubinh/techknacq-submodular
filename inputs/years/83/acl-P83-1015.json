{
  "info": {
    "authors": [
      "Raymond C. Perrault"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P83-1015",
    "title": "On the Mathematical Properties of Linguistic Theories",
    "url": "https://aclweb.org/anthology/P83-1015",
    "year": 1983
  },
  "references": [
    "acl-J82-3001",
    "acl-P80-1012",
    "acl-P81-1001",
    "acl-P83-1002",
    "acl-P83-1004"
  ],
  "sections": [
    {
      "heading": "AlISTRACT",
      "text": [
        "Meta-theoretical results on the decidability, generative capacity, and recognition complexity of several syntactic theories are surveyed.",
        "These include context-free grammars, transformational grammars, lexical functional grammars, generalized phrase structure grammars, and tree adjunct grammars."
      ]
    },
    {
      "heading": "1. introduction.",
      "text": [
        "The development of new formalisms in which to express linguistic theories has been accompanied, at least since Chomsky and Miller's early work on context-free languages, by the study of their meta-theory.",
        "In particular, numerous results on the ciecidability, generative capacity, and more recently the complexity of recognition of these formalisms have been published (and rumoured!).",
        "Strangely enough, much less attention seems to have been devoted to a discussion of the significance of these mathematical results.",
        "As a preliminary to the panel on formal properties which will address the significance issue, it seemed appropriate to survey the existing results.",
        "Such is the modest goal of this paper.",
        "We will consider context-free languages, transformational grammars, lexical functional grammars, generalized phrase structure grammars, and tree adjunct grammars.",
        "Although we will not examine them here, formal studies of other syntactic theories have been undertaken: e.g. Warren [51] for Montague's PTQ 30], and Borgida 1_71 for the stratificational grammars of Lamb ;251.",
        "There follows a brief summary of some comments in the literature about related empirical issues, but we avoid entirely the issue of whether one theory is more descriptively adequate than another."
      ]
    },
    {
      "heading": "2. Preliminary Definitions",
      "text": [
        "We assume the reader is familiar with the basic definitions of regular, context-free (CF), context-sensitive (CS), recursive, and recursively enumerable (re.)",
        "languages and with their acceptors as can be found in 141 Some elementary definitions from complexity theory may be useful.",
        "Further details may be found in :2].",
        "Complexity theory is the study of the resources required of algorithms, usually space and time.",
        "Let fez) be a function, say the recognition function for a language L The most interestlag results we could obtain about f would be a lower bound on the resources needed to compute f on a machine of a given architecture, say a von Neumann This research was sponsored by the National Science and Engineering Research Council of Canada under Grant computer or a parallel array of neurons.",
        "These results over whole classes of machines are very difficult to obtain, and none of any significance exist for parsing problems.",
        "Restricting ourselves to a specific machine model and an algorithm M for f, we can ask about the cost (e.g time or space) c(x) of executing M on a .specific input x.",
        "Typically c is too fine-grained to be useful: what one studies Instead is a function cw whose argument is an integer it denoting the size of the input to M. and which gives some measure of the cost of processing inputs of length re.",
        "Complexity theorists have been most interested in the asymptotic behaviour of cw, i.e. the behaviour of c as it gets Large.",
        "ifl If one is interested in tipper bounds on the behaviot:-.",
        "of M. one usually defines cw(n) as the rruzrtmum of c(a, over all inputs x of size n.-This is called the worst-casc complexity function for M. Notice that other definition: are possible: one could define the expected complexity function c(n) for M as the average of c(z) over an inpu.,.",
        "of length n. c. might be more useful than cw if one had an Idea of what the distribution of inputs to M could be.",
        "Unfortunately, the introduction of probabilistic considerations makes the study of expected complexity technically more difficult that of worst case complexity For a given problem, expected and worst case measures may be quite different.",
        "it is quite difficult to get detailed descriptions of cw and for many purposes a cruder estimate is sufficient.",
        "The next abstraction involves \"lumping\" classes of cw functions into simpler ones that more clearly demonstrate their asymptotic behaviour and are easier to manipulate.",
        "This is the purpose of 0-notation.",
        "Let f(n) and g(n) be two functions.",
        "f is said to be 0(g) if a constant multiple of g is an upper bound for f, for all but a finite number of values of n. More precisely, f is 0(g) if there ,s are constants K and no such that for all nano, f(n) < 107(n).",
        "Given an algorithm M. we will say that its worst-case time complexity is 0(g) if the worst-case time cost function c(m) for M is 0(g).",
        "Notice that this merely says that almost all aiputs to M of size it can be processed in time at most a constant times g (n).",
        "It does not say that all inputs require g(n) time, or even that any do even on M. let alone on any other machine that implements f. Also.",
        "if two algorithms A, and A2 are available for a function/ and if their worst-case complexity can be given respectively as O(g1) and 0(g).",
        "and g1 < g2.",
        "it may still.be the case that for a large number of cases (maybe even for all cases one is likely to encounter in practice) that A2 will be the preferable algorithm, simply because the constant K, for g may be much smaller than K2 for g 2.",
        "In examining known results about the recognition complexity of various theories, it is useful to consider how \"robust\" they are in the face of changes in the machine model from which they were derived.",
        "These models can be divided into two classes: sequential models and parallel models.",
        "Sequential models [2] include the familiar single and multi-tape Turing Machines (TMs) as well as Random Access Machines (RAMs) and Random Access Stored Program.",
        "Machines (RASPs).",
        "A RAM is like a TM except that its working menory is random access rather than sequential.",
        "A RASP is like a RAM but stores its program in its memory.",
        "Of all these models, it is most Like a von Neumann computer.",
        "All these sequential models can simulate each other in ways that do not cause great changes in time complexity.",
        "For example, a k-tape Turing Machine that runs in time 0(t) can be simulated by a RAM in time 0(t).",
        "and conversely, a RAM running in 0(t) can be simulated by a k-tape TM in time 0(t2).",
        "In fact, all familiar sequential models are polynomially related: they can simulate each other with at most a polynomial loss in efficiency.",
        "Thus if a syntactic model is known to have a difficult recognition problem on one sequential model, then it will not have a much easier one on another.",
        "Transforming a sequential algorithm to one on a parallel machine with a fixed number Kof processors provides at most a factor K improvement in speed.",
        "More interesting results are obtained when the number of processors is allowed to grow with the size of the problem, e.g. with the Length of the string to be parsed.",
        "If we view these processors as connected together in a circuit, with inputs values entering at one end and outputs being produced at the other, then a problem that has a solution on a sequential machine in polynomial time and in space s will have a solution on a parallel machine with a polynomial number of processors and circuit depth (or maximum number of processors data must be passed through from input to output) 0(r).",
        "Since the depth of a parallel circuit corresponds to the (parallel) time required to complete the computation, this means that algorithms with sequential solutions requiring small space (such as deterministic CSLs) have fast parallel solutions.",
        "For a comprehensive survey of parallel computation, see Cook[9]."
      ]
    },
    {
      "heading": "3. Context-Free Languages.",
      "text": [
        "Recognition techniques for context-free languages are well-known 7,31.",
        "The so-called \"CKY' or \"dynamic pro-gramming\" method is attributed by Hays [151 to J. Cocke, and it was discovered independently by Kasami 54] and Younger :531 who showed it to be 0(n.j).",
        "It requires the grammar to be in Chomsky Normal Form, and putting an arbitrary grammar in CNF may square the size of the grammar.",
        "Earley's algorithm recognizes strings in arbitrary CFCs in time 0(n3) and space 0(n2), and in time 0(n2) for unambiguous CFGs.",
        "Graham, Harrison and Ruzzo 1131 give an algorithm that unifies CKY and Earley's 'L101 algorithm, and discuss implementation details.",
        "Valiant [50] showed how to interpret the CKY algorithm as the finding of the transitive closure of a matrix and thus reduced CF recognition to matrix multiplication, for which sub-cubic algorithms exist.",
        "Because of the enormous constants of proportionality associated with this method, it is not likely to be of much practical use, either an implementation method or as a description of the function of the brain.",
        "Ruzzo [55] has shown how CFLs can be recognized by boolean circuits of depth 0(log(n)2), and thus that parallel recognition can be done in time 0(log(n)2).",
        "The required circuit has size polynomial inn.",
        "So as not to get mystified by the upper bounds on CF recognition, it is useful to remember that no known CFI, requires more than linear time, nor is there a (nonconstructive) proof of the existence of such a For an empirical comparison of various parsing methods, see Slocum [441."
      ]
    },
    {
      "heading": "4. Transformational Grammar.",
      "text": [
        "From its earliest days, discussions of transformational grammar (TG) have included mention of matters computational.",
        "Peters and Ritchie [33] provided the first non-trivial results on the generative power of TOs.",
        "Their model reflects the \"Aspects\" version quite closely, including transformations that could move and add constituents, and delete them subject to recoverability.",
        "All transformations are obligatory, and applied cyclically from the bottom up.",
        "They show that every recursively enumerable (re.)",
        "set can be generated by a TG using a context-sensitive base.",
        "The proof is quite simple: the right-hand sides of the type-0 rules that genet-ate the re.",
        "set are padded with a new \"blank\" symbol to make them at least as long as their left-hand sides.",
        "Rules are added to allow the blank symbols to commute with all others.",
        "These context-sensitive rules are then used as the base of a TO whose only transformation deletes the blank symbols.",
        "Thus if the transformational formalism itself is supposed to characterize the grammatical strings of possible natural languages, then the only languages being excluded are those which are not enumerable under any model of computation.",
        "At the expense of a considerably more intricate argument, the previous result can be strengthened L321 to show that every re.",
        "set can be generated by a context-free based TG, as long as a filter (intersection with a regular set) can be applied to the phrase-markers output by the transformations.",
        "In fact, the base grammar can be independent of the language being generated.",
        "The proof involves simulating a TM by a TG.",
        "The transformations first generate an \"input tape\" for the TM being simulated, and then apply the TM productions, one per cycle of the grammar.",
        "The filter insures that the base grammar generated just as many S nodes as necessary to generate the input string and do the simulation.",
        "Again, if the transformational formalism is supposed to characterize the possible natural languages, then the Universal Base Hypothesis [31] according to which all natural languages can be generated from the same base grammar is empirically vacuous: any recursively enumerable language can.",
        "Several attempts were then made to find a restricted form of the transformational model that was descriptively adequate and yet whose generated languages are recursive (see e.g. [27]).",
        "Since a key part of the proof in 321 involves the use of a filter on the final derivation trees, Peters and Ritchie examined the consequences of forbidding final filtering [35].",
        "They show that if 5' is the only recursive symbol in the CF base then the generated language L is predictably enurn.erable and exponentially bounded.",
        "A language L is predictably enumerable if there is an \"easily\" computable function t(n) that gives an upper bound on the number of tape squares needed by its enumerating TM to enumerate the first n elements of L. L is exponentially bounded if there is a constant K such that for every string x in L there is another string x' in L whose length is at most K times the length of z.",
        "The class of non-filtering languages is quite unusual, including all the CFLs (obviously), but also some (but not all) CSLs, some (but not all) recursive languages, and some (but not all) r.e.",
        "languages.",
        "The source of non-recursivity in transformationally generated languages is that transformations can delete arbitrarily large parts of the tree, thus producing surface trees arbitrarily smaller than the deep structure trees they were derived from.",
        "This is what Chomsky's recoverability of deletions condition was meant to avoid.",
        "In his thesis, Petrick [36] defines the following terminal-length-increasing condition on transformational derivations: consider the following two p-markers from a derivation, where the right one is derived from the left one by applying the cycle of transformations to subtree c producing the subtree a>.",
        "cycle 1 Continuing the derivation., apply the cycle to tree t yielding tree u.",
        ".>• cycle 2 A derivation satisfies the terminal-length-increasing condition it the yield of u is always longer than the yield of Petrick shows that if all recursion in the base \"passes through 5\" and if all derivations satisfy the terminal-length-increasing condition, then the generated language is recursive.",
        "Using a slightly more restricted model of transformations, Rounds [42] strengthens this result by showing that the resulting languages are in fact context-sensitive.",
        "In an unpublished paper.",
        "Myhill shows that if the condition is weakened to terminal-length-non-decreasing, then the resulting languages can be recognized in space at most exprrn.entia/ in the length of the input.",
        "This implies that the recognition can be done in at most double-exponential tune, but Rounds :43] shows that not only can recognition be done in exponential time, but that every language recognizable in exponential time can be generated by a TO satisfying the terminal-length-nondecreasing condition and recoverability of deletions.",
        "This is a very strong result, because of the closure properties of the class of exponential-time languages.",
        "To see why this is so requires a few more definitions.",
        "Let P be the class of all languages that can be recog-razed in polynomial time on a deterministic TM, and NP the class of all Languages that can be recognized in polynomial time on a non-deterministic TM.",
        "P is obviously contained in NP, but the converse is not known, although there is much evidence that is false.",
        "There is a class of problems.",
        "the so-called NP-complete problems.",
        "which are in NP and \"as difficult\" as any prr,blem in NY' in the following sense if any of them could be shown to be in P, then ail the problems in NP would also be in P. One way to show that a language L is NP-complete is to show that L is in NP and that every other language 1,, in NP can be polynomially transformed into L, i.e. that there is a deterministic TM, operating in polynomial time., that will transform an input /I, to L into an input wo to Lo such that w is in L if and only wo is in Lo.",
        "In practice, to show that a language is NP-complete, one shows that it is in NP, and that some already-known NP-complete language can be polynomially transformed to it.",
        "All the known NP-complete languages can be recognized in exponential time on a deterministic machine, and none are known to have sub-exponential solutions.",
        "Thus sinee the restricted transformational languages of Rounds characterize the exponential languages.",
        "then if all of them were to be in P, then P would be equal to NP.",
        "Putting it another way, if P is not equal to NP, then some transformational languages (even those satisfying the terminal-length-non-increasing condition) have n \"tractable\" (i.e. polynomial time) recognition pro:.",
        "on any deterministic TM.",
        "Note that this result also holds for all the other known sequential models of computation, and even for parallel machines with as many as a polynomial number of processors."
      ]
    },
    {
      "heading": "5. Lexical Functional Grammar.",
      "text": [
        "In part, transformational grammar seeks to account for a range of constraints or dependencies within sentences.",
        "Of particular interest are subcategorization dependencies and predicate-argument dependencies.",
        "These dependencies can hold over arbitrarily large distances.",
        "Several recent theories suggest different ways of accounting for these dependencies, but without making use of transformations.",
        "We will examine three of these, Lexical Functional Grammar, Generalized Phrase Structure Grammar.",
        "and Tree Adjunct Grammars, in tne next few sections.",
        "Lexical Functional Grammar (LFG) of Kaplan and Bresnan [241 aims to provide a descriptively adequate syntactic formalism without transformations.",
        "All the work done by transformations is instead encoded in structures in the lexicon and in links established between nodes in the constituent structure LEG languages are CS and properly include the CFLs :24-.1.",
        "Berwick :5] shows that a set of strings whose recognition problem is known to be NP-complete, namely the set of satisfiable boolean formulas, is an LFG language.",
        "Therefore, as was the case for Rounds's restricted class of TGs, if P is not equal to NP, then some languages generated by LFGs do not have polynomial time recognition algorithms.",
        "Indeed only quite \"basic\" parts of the LEG mechanism are necessary to the reduction.",
        "This includes mechanisms necessary for feature agreement, for forcing verbs to take certain cases, and lexical ambiguity.",
        "Thus no simple change to the formalism is likely to avoid the combinatorial consequences of the full mechanism.",
        "Berwick has also examined the relation between LEG and the class of languages generated by indexed grammars [11, a class known to be a proper subset of the CSLs, but including some NP-complete languages :42].",
        "He claims (personal communication) that the indexed languages are a proper subse of the LEG languages."
      ]
    },
    {
      "heading": "6. Generalized Phrase Structure Grammar.",
      "text": [
        "In a series of papers, Gerald Gazdar and his colleagues I] have argued for a joint account of the syntax and semantics of English like LFG in eschewing the use of transformations but unlike it in positing only one level of",
        "syntactic description.",
        "The syntactic apparatus is based on a non-standard interpretation of phrase-structure rules and on the use of meta-rules.",
        "The formal consequences of both these moves have been investigated."
      ]
    },
    {
      "heading": "6.1. Node Admi.saibility",
      "text": [
        "There are two ways of interpreting the function of CF rules.",
        "The first, and most usual, is as rules for rewriting strings.",
        "Derivation trees can then be seen as canonical representatives of classes of derivations producing the s?rne string, and differing only in the order of application o!' the same productions.",
        "The second interpretation of CF rules is as constraints on derivation trees: a legal derivation tree is r..rie where each node is \"admitted\" by a rule, i.e. each node dominates a sequence of nodes in a way sanctioned by a rule.",
        "For CF rules, the two interpretations obviously generate the same strings aiid the same set of trees.",
        "Following a suggestion of McCawley's. Peters and Ritchie [341 showed that if one considered context-sensitive rules from the node-admissibility point of view, the languages defined were still CF.",
        "Thus the use of CS rules in the base to impose sub-categorization restrictions, for example, does not increase the weak generative capacity of the base component.",
        "(For some different restrictions of context-sensitive rules that guarantee that only CFLs will be generated, see Baker [41.)",
        "Rounds [40] gives a simpler proof of Peters and Ritchie's node-admissibility result using the techniques from tree-automata theory, a generalization to trees of elute state automata theory for strings.",
        "Just as a finite state automaton (FSA) accepts a string by reading it one character at a time, changing its state at each transition, a finite state tree automaton (.1,'I'A) traverses trees, propagating states.",
        "The top-down FSTA \"attaches\" a starting state (from a finite set) to the root of the tree.",
        "Transitions are allowed by productions of the form (q, a, n.)--> (q1,..„qn) such that if state q is being applied to a node Labelled a and dominating n descendants, then state gi should be applied to its -ith descendant.",
        "Acceptance occurs if all leaves of the tree end up labelled with states in the accepting subset.",
        "The bottom-up FM is similar: starting st&tes are attached to the leaves of the tree and the productions are of the form (a, n. q q indicating that if a node labelled a dominating n descendants each labelled with states q to gn, then node a gets labelled with state q.",
        "Acceptance occurs when the root is labelled by a state from the subset of accepting states.",
        "As is the case with FSAs, l'As of both flavours can be either deterministic or non-deterministic.",
        "A set of trees k said to be recognizable if it is accepted by a nondeterministic bottom-up Again as with FSA.s.",
        "any set of trees accepted by a non-deterministic bottom-up F5TA IJ accepted by a deterministic bottom-up FSTA, but the reilt does not hold for top-down although the recognizable sets arc exactly the Languages retognized by non-deterministic top-down rSI'As.",
        "A set of trees is local if it is the set of derivation trees of a CF grammar Clearly, every local set !s recognizable by a one-state bottom-up FSTA that checks at each node that it satisfies a CF production.",
        "Also, the yield of a recognizable set of trees (the set of strings it generetes) is CF.",
        "Although not all recognizable sets are local, hey can all be mapped into local sets by a simple (homo;-norphic) mapping.",
        "Rounds's proof 41] that CS rules under node-admissibility generate only CFLs involves showing that the set of trees accepted by the rules is recognizable, i.e. that there is a non-deterministic bottom-up 1.'5VA that can check at each node that some node-admissibility condition holds there.",
        "This requires checking that the \"strictly context-free\" part of the rule holds, and that some proper analysis of the tree passing thre-f...the node satisfies the \"context-sensitive\" part of the rule.",
        "The difficulty comes from the fact that the bottom-up automaton cannot generate the set of proper analyses, but must instead propagate (in its state set) the proper analysis conditions necessary to \"admit\" the nodes of its subtrees.",
        "It must, of course, also check that those rules get satisfied.",
        "A more intuitive proof using tree transducers as well as I.",
        "-1•As ,s sketched inthe Appendix.",
        "Joshi and Levy [21] strengthened Peters and Ritchie's result by showing that the node admissibility conditions could also include arbitrary Boolean combinations of dominance conditions: a node could specify a bounded set of labels that must occur immediately above it along a path to the root, or Immediately below it on a path to the frontier.",
        "In general the CF grammars constructed in the proof of weak equivalence to the CS grammars under node admissibility are much larger than the original, and not useful for practical recognition.",
        "Joshi, Levy and Yueh [221, however, show how Earley's algorithm can be extended to a parser that uses the local constraints directly."
      ]
    },
    {
      "heading": "8.2. Metarules.",
      "text": [
        "The second important mechanism used by Gazdar [11] is rnetarules.",
        "or rules that apply to rules to produce other rules.",
        "Using standard notation for CF rules, one example of a metarule that could replace the transformation known as \"particle movement\" is: V--> V N Pt X ==> V--> VPt IV[-PRO] X X here is a variable behaving like variables in structural analyses of transformations.",
        "If such variables are restricted to being used as abbreviations, that is if they are only allowed to range over a finite subset of strings over the vocabulary, then closing the grammar under the metarules produces only a finite set of derived rules, and thus the generative power of the formalism is not increased.",
        "If, on the other hand, X is allowed to range over strings of unbounded length, as are the essential variables of transformational theory, then the consequences are less clear.",
        "It is well known, for example, that if the right-hand sides of phrase structure rules are allowed to be arbitrary regular expressions, then the generated languages are still context-free.",
        "Might something like this not be happening with essential variables in meLarules?",
        "It turns out not.",
        "The formal consequences of the presence of essential, variables in metarules depends on the presence of another device, the so-called phantom categories.",
        "It may be convenient in formulating rnetarules to allow, in the left-hand sides of rules, occurrences of syntactic categories that are never introduced by the grammar, i.e. that never appear in the right-hand sides of rules.",
        "In standard CFLs, these are called 'useless categories, and rules containing them can simply be dropped, with no change in generative capacity.",
        "Not so with metarules: it is possible for metarules to rewrite rules containing phantom categories into rules without them.",
        "Such a device was proposed at one time as a way to implement passives in the GPSG framework.",
        "Uszkoreit and Peters [49] have shown that essential variables in metarules are powerful devices indeed: CF grammars with metarutes that use at most one essential variable and allow phantom categories can generate all recursively enumerable sets.",
        "Even if phantom categories are banned, as long as the use of at least one essential variables is allowed, then some non-recursive sets can be generated.",
        "Possible restrictions on the use of metarules are suggested in Gazdar and Pullum [12].",
        ",Shieber et al.",
        "[45] discuss some empirical consequences of these moves."
      ]
    },
    {
      "heading": "7. Tree Adjunct Grammar.",
      "text": [
        "The Tree Adjunct Grammars (TAGs) of Joshi and his colleagues presents a different way of accounting for syntactic dependencies ([17], [19]).",
        "A TAG consists of two (finite) sets of (finite) trees, the centre trees and the adjunct trees.",
        "The centre trees correspond to the surface structures of the \"kernel\" sentences of the languages.",
        "The root of the adjunct trees is labelled with a non-terminal symbol which also appears exactly once on the frontier of the tree.",
        "All other frontier nodes are labelled with terminal symbols.",
        "Derivations in TAGs are defined by repeated application of the operation of adjunction If c is a centre tree containing an occurrence of a non-terminal A. and if a is an adjunct tree whose root (and one node n on the frontier) is labelled A. then the adjunction of a to c is performed by \"detaching\" from c the subtree t rooted at A. attaching a in its place, and reattaching t at node re.",
        "Adjunction may then be seen as a tree analogue of a context-free derivation for strings (40].",
        "The string languagea obtained by taking the yields of the tree languages generated by TAGs are called Tree Adjunct Languages, or TALs.",
        "In TAGs all long-distance dependencies are the result of adjunctions separating nodes that at one point in the derivation were \"close\".",
        "Both crossing and non-crossing dependencies can be represented [18].",
        "The formal properties of TAGs are fully discussed in [20].",
        "52], [23].",
        "Of particular interest are the following.",
        "TALs properly contain the CFLs and are properly contained in the indexed languages, which in turn are properly contained in the Ca.s.",
        "Although the indexed languages contain NP-complete languages, TALs are much better behaved: Joshi and Yokomori report [personal communication] an 0(n4) recognition algorithm and conjecture that an 0(n3) bound may be possible."
      ]
    },
    {
      "heading": "B. A Pointer to Empirical Discussions",
      "text": [
        "The literature on the empirical issues underlying the formal results reported here is not extensive.",
        "Chomsky argues convincingly r..81 that there is no argument for natural languages necessarily being recursive.",
        "This, or course, is different from the possibility that languages are contingently recursive.",
        "Putnam [39] gives three reasons he claims \"point in this direction\": (1) speakers can presumably classify sentences as acceptable or unacceptable, deviant or non-deviant, at cetera, without reliance on extralinguistic contexts.",
        "There are of course exceptions to this rule '', (2) grammaticality judgements can be made for nonsense sentences, and (3) grammars can be learned.",
        "(2) and (3) are irrelevant and (1) contains its own counterargument.",
        "Peters and Ritchie [33] contains a suggestive but hardly open-and-shut case for contingent recursivity (*..) every TG has an exponentially bounded cycling function, and thus generates only recursive languages, (2) every natural language has a descriptively adequate TG, and (3) the complexity of languages investigated so far is typical of the class.",
        "Hintikka[16] presents a very different argument against the recursivity of English based on the distribution of the words any and every.",
        "His account of why John knows everything is grammatical while John knows anything is not is that any can appear only in contexts where replacing it by every changes the meaning.",
        "Taking meaning to be logical equivalence, this means that grammaticality is dependent on the determination of logical equivalence of logical formulas, an undecidable problem.",
        "Chomsky [8] argues that a simpler solution is available, namely one that replaces logical equivalence by syntactic identity of some kind of logical form.",
        "Pullum and Gazdar [38] is a thorough survey of, and argument against, published claims (mainly the -respectively\" examples (26], Dutch cross-serial dependencies, and nominalization in Mohawk [37]) that some natural languages cannot be weakly generated by CF grammars.",
        "No claims are made about the strong adequacy of CFGs."
      ]
    },
    {
      "heading": "9. Seeking Significance.",
      "text": [
        "When can the supporter of a weak (syntactic) formalism (i.e. low recognition complexity, low generative capacity) claim that it superior to a competing more powerful formalism?",
        "Linguistic theories can differ along several dimensions, with generative capacity and recognition capacity being only two (albeit related) ones.",
        "The evaluation must take into consideration at least the following others: Coverage.",
        "Do the theories make the same grammatical predictions?",
        "Extensibility.",
        "The linguistic theory of which the syntactic theory is a part will want to express well-tormedness constraints other than syntactic ones These constraints may be expressed over syntactic representations, or over different representations, presumably related to the syntactic ones.",
        "One theory may make this connection possible when another does not.",
        "This of course underlies the arguments for strong descriptive adequacy.",
        "Also relevant here is how the linguistic theory as a whole is decomposed.",
        "The syntactic theory can obviously be made simpler by transferring some of the explanatory burden to another constituent.",
        "The classic example in programming languages is the constraint that all variables must be declared before they are used.",
        "This constraint cannot be imposed by a CFG but can be by an indexed grammar, at the cost of a dramatic increase in recognition complexity.",
        "Typically, however, the requirement is simply not considered part of 'syntax\", which thus remains CF. and imposed separately.",
        "In this case, the overall recognition complexity remains some low-order polynomial.",
        "Sortie arguments of this kind can be found in [33] Separating the constraints into different sub-theories will riot in general make the problem of recognizing strings that satisfy all the constraints any more efficient, but it may allow limiting the power of each constituent.",
        "To take an extreme example, every r.e.",
        "set 3 the homomorphic image of the intersection of to.)",
        "context-free Languages.",
        "Implementation.",
        "This is probably the most subtle ssi of issues determining the significance of the forms,I results, and I don't claim to understand them.",
        "Comparison between theories requires agreement between the machine models used to derive the complexity results.",
        "As mentioned above, the sequential models are all polynornially related, and no problem not having a",
        "polynomial time solution on a sequential machine is likely to have one on a parallel machine limited to at most a polynomial number of processors, at least if P is not equal to NP.",
        "Both these results restrict the improvement one can obtain by changing implementation.",
        "but are of little use in comparing algorithms of Low complexity.",
        "Berwick and Weinberg [6] give examples of how algorithms of low complexity may have different implementations differing by large constant factors.",
        "In particular, changes in the form of the grammar and in its representation may have this effect.",
        "But of more interest I believe is the fact that implementation is often accompanied by some form of resource limitation that has two effects.",
        "First it is also a change in specification.",
        "A context-free parser implemented with a bounded stack recognizes only a finite-state Language.",
        "Second.",
        "very special implementations can be used if one is willing to restrict the size of the problem.",
        "to be solved, or even use special-purpose methods for limited problems.",
        "Marcus's parser [28] with its bounded lookahead is another good example.",
        "Sentences parsable within the allowed lookahead have \"quick\" parses, but some grammatical sentences, such as \"garden path\" sentences cannot be recognized without an extension to the mechanism that would distort the complexity measures.",
        "There is obviously much more of this story to be told.",
        "Allow me to speculate as to how it might go.",
        "We may end up with a space of linguistic theories, differing in the idealization of the data they assume, in the way they decompose constraints, and in the procedural specifications they postulate (I take it that two theories may differ in that the second simply provides more detail than the first as to how constraints specified by the first are to be used.)",
        "Our observations, in particular our measurements of necessary resources, are drawn from the \"ultimate implementation\", but this does not mean that the \"ultimately Low-level theory\" is necessarily the most informative, witness many examples in the physical sciences, or that less procedural theories are not useful stepping stones to more procedural ones.",
        "It is also not clear that theories of different computational power may not be useful as descriptions of different parts of the syntactic apparatus.",
        "For example, it may be easier to learn statements of constraints within the framework of a general machine.",
        "The constraints once learned might then be subjected to transformation to produce more efficient special-purpose processors also imposing resource limitations.",
        "Indeed, the \"possible languages\" of the future may be more complex than the present ones, just as earlier ones may have been syntactically simpler.",
        "Were ancient languages regular?",
        "Whatever we decide to make of existing formal results, it is clear that continuing contact with the complexity community is important.",
        "The driving problems there are the P= NP question.",
        "the determination of lower bounds, the study of time-space tradeoffs, and the complexity of parallel computations.",
        "We still have some methodological housecleaning to do, but I don't see how we can avoid being affected by the outcome of their investigations."
      ]
    },
    {
      "heading": "ACKNOWLEDGEMENTS",
      "text": [
        "Thanks to Bob Berwick, Aravind Joshi, Jim Hoover, and Stan Peters for their suggestions."
      ]
    },
    {
      "heading": "APPENDIX",
      "text": [
        "Rounds [41] proves that context-sensitive rules under node-admissibility generate only context-free languages by constructing a non-deterministic bottom-up tree automaton to recognize the accepted trees.",
        "We sketch here a proof that makes use of several deterministic transducers instead.",
        "FSTAs can be generalized so that instead of simply accepting or rejecting trees, they transform them, by adding constant trees, and deleting or duplicating subtrees.",
        "Such devices are called finite state tree transducers (len'T), and like the FSTA they can be top-down or bottom-up.",
        "First motivated as models of syntax-directed translations for compilers, they have been extensively studied (e.g. [V], [48], [40]) but a simple subset is sufficient here.",
        "The idea is this.",
        "Let Tbe the set of trees accepted by the CS-based grammar.",
        "Let t be in T. r'Srl's can be used to label each node it of t with the set of all proper analyses passing through n. It will then be simple to check that each node satisfies one of the node admissibility conditions by sweeping through the labelled tree with a bottom-up FSTA.",
        "The node labelling is done by two FblTs r, and r2.",
        "Let in.",
        "be the maximum length of any left or right-context of any node admissibility condition.",
        "Thus we need only label nodes with sets of strings of length at most in, and over a finite alphabet there are only a finite number of such strings.",
        "7-1 operates bottom-up on a tree t, and labels each node it of t with three sets Prefiz(n), Suffix(n).",
        "and Yield(n) of proper analyses: if P is the set of all proper analyses of the subtree rooted at TL, then Prefix(n) is the set of all substrings of length at most m that are prefixes of strings of P. Similarly, 5Uffix(n) is the set of all suffixes of length at.",
        "most in, and Yield(n) is the set of all strings of P of length at most in.",
        "It can easily be shown that for any set of trees T T is recognizable if and only if r (T) is.",
        "Applying to the output of r1, the second transducer r2, operating top-down, labels each node it with all the proper analyses going through it, i.e. with a pair of sets of strings.",
        "The first set will contain all left-contexts of node n and the second all right-contexts.",
        "1-2 also preserves recognizability.",
        "A bottom-up FSI'A can now be defined to check at each node that both the context-free part of a rule as well as its context conditions are satisfied.",
        "This argument also extends easily to cover the dominance predicates of Joshi and Levy: transducers can be added to label each node with all its top contexts\" and all its \"bottom- contexts\" The final FSTA must.",
        "then check that the nodes satisfy whatever Boolean combination of dominance and proper analysis predicates are required by the node admissibility rules."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": []
    }
  ]
}
