{
  "info": {
    "authors": [
      "G. Edward Barton Jr."
    ],
    "book": "Computational Linguistics",
    "id": "acl-J85-4001",
    "title": "On the Complexity of ID/LP Parsing",
    "url": "https://aclweb.org/anthology/J85-4001",
    "year": 1985
  },
  "references": [
    "acl-J82-3001",
    "acl-P85-1018"
  ],
  "sections": [
    {
      "text": [
        "Modern linguistic theory attributes surface complexity to interacting subsystems of constraints.",
        "For instance, the ID/LP grammar formalism separates constraints on immediate dominance from those on linear order.",
        "An ID/LP parsing algorithm by Shieber shows how to use ID and LP constraints directly in language processing, without expanding them into an intermediate context-free \"object grammar\".",
        "However, Shieber's purported runtime bound underestimates the difficulty of ID/LP parsing.",
        "ID/LP parsing is actually NP-complete, and the worst-case runtime of Shieber's algorithm is actually exponential in grammar size.",
        "The growth of parser data structures causes the difficulty.",
        "Some computational and linguistic implications follow; in particular, it is important to note that, despite its potential for combinatorial explosion, Shieber's algorithm remains better than the alternative of parsing an expanded object grammar."
      ]
    },
    {
      "heading": "1 INTRODUCTION",
      "text": [
        "It is common in recent linguistic theories for various surface characteristics of a language to be described in terms of several different kinds of underlying constraints.",
        "ID/LP grammars involve immediate-dominance rules and linear-order constraints; more broadly, GPSG systems can also involve feature relationships and metarules (Gazdar et al.",
        "1985).",
        "The tree adjunction grammars of Kroch and Joshi (1985) separate the statement of local constraints from the projection of those constraints to larger structures.",
        "The GB-framework of Chomsky (1981:5) identifies the subtheories of bounding, government, 0-marking, binding, Case, and control.",
        "When several independent constraints are involved, a system that explicitly multiplies out their effects is large, cumbersome, and uninformative.2 If done properly, the disentanglement of different kinds of constraints can result in shorter and more illuminating language descriptions.",
        "With any such modular framework, two questions immediately arise: how can the various constraints be put back together in parsing, and what are the computational characteristics of the process?",
        "One approach is to compile a large object grammar that expresses the combined effects of the constraints in a more familiar format such as an ordinary context-free grammar (CFG).",
        "The context-free object grammar can then be parsed with Earley's (1970) algorithm or any of several other well-known procedures with known computational characteristics.",
        "However, in order to apply this method, it is necessary to expand out the effects of everything that falls outside the strict context-free format: rule schemas, metarules, ID rules, LP constraints, feature instantiations, case-marking constraints, etc.",
        "The standard algorithms operate on CFGs, not on extended variants of them.",
        "Unfortunately, the object grammar may be huge after the effects of all nonstandard devices have been expanded out.",
        "Estimates of the object-grammar size for typical systems vary from hundreds or thousands3 up to trillions of rules (Shieber 1983:4).",
        "With some formalisms, the context-free object-grammar approach is not even possible because the object grammar would be infinite (Shieber 1985:145).",
        "Grammar size matters beyond questions of elegance and clumsiness, for it typically affects processing complexity.",
        "Berwick and Weinberg (1982) argue that the effects of grammar size can actually dominate complexity for a relevant range of input lengths.",
        "Given the disadvantages of multiplying out the effects of separate systems of constraints, Shieber's (1983) work on direct parsing leads in a welcome direction.",
        "Shieber considers how one might do parsing with ID/LP grammars, which involve two orthogonal kinds of rules.",
        "ID rules constrain immediate dominance irrespective of constituent order (\"a sentence can be composed of V with NP and SBAR complements\"), while LP rules Copyright 1 985 by the Association for Computational Linguistics.",
        "Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.",
        "To copy otherwise, or to republish, requires a fee and/or specific permission.",
        "constrain linear precedence among the daughters of any node (\"if V and SBAR are sisters, then V must precede SBAR\").",
        "Shieber shows how Earley's (1970) algorithm for parsing context-free grammars (CFGs) can be adapted to use the constraints of ID/LP grammars directly, without the combinatorially explosive step of converting the ID/LP grammar into standard context-free form.",
        "Instead of multiplying out all of the possible surface interactions among the ID and LP rules, Shieber's algorithm applies them one step at a time as needed.",
        "Surely this should work better in a parsing application than applying Earley's algorithm to an expanded grammar with trillions of rules, since the worst-case time complexity of Earley's algorithm is proportional to the square of the grammar size!",
        "Shieber's general approach is on the right track.",
        "On pain of having a large and cumbersome rule system, the parser designer should first look to linguistics to find the correct set of constraints on syntactic structure, then discover how to apply some form of those constraints in parsing without multiplying out all possible surface manifestations of their effects.",
        "Nonetheless, nagging doubts about computational complexity remain.",
        "Although Shieber (1983:15) claims that his algorithm is identical to Earley's in time complexity, it seems almost too much to hope for that the size of an ID/LP grammar should enter into the time complexity of ID/LP parsing in exactly the same way that the size of a CFG enters into the time complexity of CFG parsing.",
        "An ID/LP grammar G can enjoy a huge size advantage over a context-free grammar G' for the same language; for example, if G contains only the rule S ID abcde, the corresponding G' contains 5!",
        "= 120 rules.",
        "In effect, the claim that Shieber's algorithm has the same time complexity as Earley's algorithm means that this tremendously increased brevity of expression comes free (up to a constant).",
        "The paucity of supporting argument in Shieber's article does little to allay these doubts: We will not present a rigorous demonstration of time complexity, but it should be clear from the close relation between the presented algorithm and Earley's that the complexity is that of Earley's algorithm.",
        "In the worst case, where the LP rules always specify a unique ordering for the right-hand side of every ID rule, the presented algorithm reduces to Earley's algorithm.",
        "Since, given the grammar, checking the LP rules takes constant time, the time complexity of the presented algorithm is identical to Earley's.",
        ".",
        ".",
        ".",
        "That is, it is 0( I G 2 n3), where I G I is the size of the grammar (number of ID rules) and n is the length of the input.",
        "(:14f) Many questions remain; for example, why should a situation of maximal constraint represent the worst case, as Shieber claims?4 The following sections will investigate the complexity of ID/LP parsing in more detail.",
        "In brief, the outcome is that Shieber's direct-parsing algorithm usually does have a time advantage over the use of Earley's algorithm on the expanded CFG, but that it blows up in the worst case.",
        "The claim of O(I G 12 n3) time complexity is mistaken; in fact, the worst-case time complexity of ID/LP parsing cannot be bounded by any polynomial in the size of the grammar and input, unless S' = ID/LP parsing is NP-complete.",
        "As it turns out, the complexity of ID/LP parsing has its source in the immediate-domination rules rather than the linear precedence constraints.",
        "Consequently, the precedence constraints will be neglected.",
        "Attention will be focused on unordered context-free grammars (UCFGs), which are exactly like standard context-free grammars except that when a rule is used in a derivation, the symbols on its right-hand side are considered to be unordered and hence may be written in any order.",
        "UCFGs represent the special case of ID/LP grammars in which there are no LP constraints.",
        "Shieber's ID/LP algorithm can be used to parse UCFGs simply by ignoring all references to LP constraints."
      ]
    },
    {
      "heading": "2 GENERALIZING EARLEY'S ALGORITHM",
      "text": [
        "Shieber generalizes Earley's algorithm by modifying the progress datum that tracks progress through a rule.",
        "The Earley algorithm uses the position of a dot to track •linear advancement through an ordered sequence of constituents.",
        "The major predicates and operations on such dotted rules are these:",
        "• A dotted rule is initialized with the dot at the left edge, as in X -* .ABC .",
        "• A dotted rule is advanced across a terminal or nonterminal that was predicted and has been located in the input by simply moving the dot to the right.",
        "For example, X -4.",
        "A.BC is advanced across a B by moving the dot to obtain X AB.0 .",
        "• A dotted rule is complete iff the dot is at the right edge.",
        "For example, X -* ABC.",
        "is complete.",
        "• A dotted rule predicts a terminal or nonterminal iff the dot is immediately before the terminal or nonterminal.",
        "For example, X -* A.BC predicts B.",
        "UCFG rules differ from CFG rules only in that the right-hand sides represent unordered multisets (that is, sets with repeated elements allowed).",
        "It is thus appropriate to use successive accumulation of set elements in place of linear advancement through a sequence.",
        "In essence, Shieber's algorithm replaces the standard operations on dotted rules with corresponding operations on what will be called dotted UCFG rules.5",
        "• A dotted UCFG rule is initialized with the empty multi-set before the dot and the entire multiset of right-hand elements after the dot, as in X -3.",
        "{} • {A, B, C}.",
        "• A dotted UCFG rule is advanced across a terminal or nonterminal that was predicted and has been located in the input by simply moving one element from the multiset after the dot to the multiset before the dot.",
        "For example, X {A} • {B, C} is advanced across a B by moving the B to obtain X B} • {C}.",
        "Similar206 Computational Linguistics, Volume 11, Number 4, October-December 1985 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing ly, X -•• {A} • {B, C, C} may be advanced across a C to obtain X -* {A, C} {B, C}.",
        "• A dotted UCFG rule is complete iff the multiset after the dot is empty.",
        "For example, X {A, B, C} {} is complete.",
        "• A dotted UCFG rule predicts a terminal or nonterminal iff the terminal or nonterminal is a member of the multiset after the dot.",
        "For example, X {A} {B, C} predicts B and C.",
        "Given these replacements for operations on dotted rules, Shieber's algorithm operates in the same way as Earley's algorithm.",
        "As usual, each state in the parser's state sets consists of a dotted rule tracking progress through a constituent plus the interword position defining the constituent's left edge (Earley 1970:95, omitting lookahead).",
        "The left-edge position is also referred to as the return pointer because of its role in the complete operation of the parser."
      ]
    },
    {
      "heading": "3 THE ADVANTAGES OF SHIEBER'S ALGORITHM",
      "text": [
        "The first question to ask is whether Shieber's algorithm saves anything.",
        "Is it faster to use Shieber's algorithm on a UCFG than to use Earley's algorithm on the corresponding expanded CFG?",
        "Consider the UCFG G1 that has only the single rule S abcde.",
        "The corresponding CFG Get has 120 rules spelling out all the permutations of abcde: S abcde, S abced, and so forth.",
        "If the string abcde is parsed using Shieber's algorithm directly on G1the state sets of the parser remain sma11.6",
        "In contrast, consider what happens if the same string is parsed using Earley's algorithm on the expanded CFG with its 120 rules.",
        "As Figure 1 illustrates, the state sets of the Earley parser are much larger.",
        "In state set Si, the Earley parser uses 4!",
        "= 24 states to spell out all the possible orders in which the remaining symbols {b,c,d,e} could appear.",
        "Shieber's modified parser does not spell them out, but uses the single state [S {a} • {b,c,d,e}, 0] to summarize them all.",
        "Shieber's algorithm should thus be faster, since both parsers work by successively processing all of the states in the state sets.",
        "Similar examples show that the Shieber parser can enjoy an arbitrarily large advantage over the use of the Earley parser on the expanded CFG.",
        "Instead of multiplying out all surface appearances ahead of time to produce an expanded CFG, Shieber's algorithm works out the possibilities one step at a time, as needed.",
        "This can be an advantage because not all of the possibilities may arise with a particular input.",
        "enjoy a large advantage over the use of the Earley parser on the corresponding expanded CFG.",
        "After having processed the terminal a while parsing the string abcde as discussed in the text, the Shieber parser uses the single state shown in (a) to keep track of the same information for which the Earley parser uses the 24 states in (b)."
      ]
    },
    {
      "heading": "4 COMBINATORIAL EXPLOSION WITH SHIEBER'S ALGORITHM",
      "text": [
        "The answer to the first question is yes, then: it can be more efficient to use Shieber's parser than to use the Earley parser on an expanded object grammar.",
        "The second question to ask is whether Shieber's parser always enjoys a large advantage.",
        "Does the algorithm blow up in difficult cases?",
        "In the presence of lexical ambiguity, Shieber's algorithm can suffer from combinatorial explosion.",
        "Consider the following UCFG, G2, in which x is five-ways ambiguous:",
        "What happens if Shieber's algorithm is used to parse the string xxxxa according to this grammar?",
        "After the first three occurrences of x have been processed, the state set of Shieber's parser will reflect the possibility that any three of the phrases A, B, C, D, and E might have been encountered in the input and any two of them might remain to be parsed.",
        "There will be (5) = 10 states reflecting progress through the rule expanding S, in addition to 5 states reflecting phrase completion and 10 states reflecting phrase prediction (not shown):",
        "In cases like this, Shieber's algorithm enumerates all of the combinations of k elements taken i at a time, where k is the rule length and i is the number of elements already processed.",
        "Thus it can be combinatorially explosive.",
        "It is important to note that even in this case Shieber's algorithm wins out over parsing the expanded CFG with Earley's algorithm.",
        "After the same input symbols have been processed, the state set of the Earley parser will reflect the same possibilities as the state set of the Shieber parser: any three of the required phrases might have been located, while any two of them might remain to be parsed.",
        "However, the Earley parser has a less concise representation to work with.",
        "In place of the state involving S {A,B,C} • {D,E}, for instance, there will be 3!",
        "• 2!",
        "= 12 states involving S ABC.DE, S BCA.ED, and so forth.",
        "Instead of a total of 25 states, the Earley state set will contain 135 = 12 10 + 15 states.'",
        "In the above case, although the parser could not be sure of the categorial identities of the phrases parsed, at least there was no uncertainty about the number of phrases and their extent.",
        "We can make matters even worse for the parser by introducing uncertainty in those areas as well.",
        "Let G3 be the result of replacing every x in G2 with the empty string r:",
        "Then an A, for instance, can be either an a or nothing.",
        "Before any input has been read, the first state set S'c, in Shieber's parser must reflect the possibility that the correct parse may include any of the 25 = 32 possible subsets of A, B, C, D, and E as empty initial constituents.",
        "For example, So must include [S {A,B,C,D,E} • { }, 0] because the input might turn out to be the null string.",
        "Similarly, it must include [S {A,C,E} • {B,D}, 0] because the input might turn out to be bd or db.",
        "Counting all possible subsets in addition to other states having to do with predictions, completions, and the parser's start symbol, there are 44 states in So.",
        "(There are 338 states in the corresponding state when the expanded CFG GI3 is used.)"
      ]
    },
    {
      "heading": "5 THE SOURCE OF THE DIFFICULTY",
      "text": [
        "Why is Shieber's algorithm potentially exponential in grammar size despite its \"close relation\" to Earley's algorithm, which has time complexity polynomial in grammar size?",
        "The answer lies in the size of the state space that each parser uses.",
        "Relative to grammar size, Shieber's algorithm involves a much larger bound than Earley's algorithm on the number of states in a state set.",
        "Since the main task of the Earley parser is to perform scan, predict, and complete operations on the states in each state set (Earley 1970:97), an explosion in the size of the state sets will be fatal to any small runtime bound.",
        "Given a CFG Ga, how many possible dotted rules are there?",
        "Resulting from each rule X A, ... Ak, there are k+ 1 possible dotted rules.",
        "Then the number of possible dotted rules is bounded by I Go I , if this notation is taken to mean the number of symbols that it takes to write Ga down.",
        "An Earley state is a pair [r,i], where r is a dotted rule and i is an interword position ranging from 0 to the length n of the input string.",
        "Because of these limits, no state set in the Earley parser can contain more than 0( I Go I -n) (distinct) states.",
        "The limited size of a state set allows an 0(1 G I 2 • n3) bound to be placed on the runtime of the Earley parser.",
        "Informally, the argument (due to Earley) runs as follows.",
        "The scan operation on a state can be done in constant time; the scan operations in a state set thus contribute no more than 0(1 GaI • n) computational steps.",
        "All of the predict operations in a state set taken together can add no more states than the number of rules in the grammar, bounded by I Gal , since a nonterminal needs to be expanded only once in a state set regardless of how many times it is predicted; hence the predict operations need not take more than 0( I Go I • n+ I Ga I)= 0(1 Gal • n) steps.",
        "Finally, there are the complete operations to be considered.",
        "A given completion can do no worse than advancing every state in the state set indicated by the return pointer.",
        "Therefore, any bound k on state set size leads to a bound of k2 on the number of steps it takes to do all the completions in a state set.",
        "Here k = 0(1 G I • n), so the complete operations in a state set can take at most 0(1 G 12 • n2) steps.",
        "Overall, then, it takes no more than 0(1 Ga 12 • n2) steps to process one state set and no more than 0(1 Ga 12 • n3) steps for the Earley parser to process them all.",
        "In Shieber's parser, though, the state sets can grow much larger relative to grammar size.",
        "Given a UCFG Gb, how many possible dotted UCFG rules are there?",
        "Resulting from a rule X ... Ak, there are not k+1 possible dotted rules tracking linear advancement, but 2k possible dotted UCFG rules tracking accumulation of set elements.",
        "In the worst case, the grammar contains only one rule and k is on the order of Gb ; hence the number of possible dotted UCFG rules for the whole grammar is not bounded by I Gb I, but by 21Gb .",
        "(The bound can be reached; recall that exponentially many dotted rules are created in the processing of G3 from section 4.)",
        "Informally speaking, the reason why Shieber's parser sometimes suffers from combinatorial explosion is that there are exponentially more possible ways to progress through an unordered rule expansion than an ordered one.",
        "When disambiguating information is scarce, the parser must keep track of all of them.",
        "In the more general task of parsing ID/LP grammars, the most tractable case occurs when constraint from the LP relation is strong enough to force a unique ordering for every rule expansion.",
        "Under such conditions, Shieber's parser reduces to Earley's.",
        "However, the case of strong constraint represents the best case computationally, rather than the worst case as Shieber (1983:14) claims."
      ]
    },
    {
      "heading": "6 ID/LP PARSING IS INHERENTLY DIFFICULT",
      "text": [
        "The worst-case time complexity of Shieber's algorithm is exponential in grammar size rather than quadratic as Shieber (1983:15) believed.",
        "Did Shieber simply choose a poor algorithm, or is ID/LP parsing inherently difficult in the general case?",
        "In fact, the simpler problem of recognizing sentences according to a UCFG is NP-complete.8 Consequently, unless .9 = <4;9, no algorithm for ID/LP parsing can have a runtime bound that is polynomial in the size of the grammar and input.",
        "The proof of NP-completeness involves reducing the vertex cover problem (Garey and Johnson 1979:46) to the UCFG recognition problem.",
        "Through careful construction of the grammar and input string, it is possible to \"trick\" the parser into solving a known hard problem.",
        "The vertex cover problem involves finding a small set of vertices in a graph with the property that every edge of the graph has at least one endpoint in the set.",
        "Figure 2 shows a trivial example.",
        "To construct a grammar that encodes the question of whether the graph in Figure 2 has a vertex cover of size 2, first take the vertex names a, b, c, and d as the alphabet.",
        "Take START as the start symbol.",
        "Take H1 through H4 as special symbols, one per edge; also take U and D as special dummy symbols.",
        "Next, write the rules corresponding to the edges of the graph.",
        "Edge e1 runs from a to c, so include the rules H1 -÷ a and H1 c. Encode the other edges similarly.",
        "Rules expanding the dummy symbols are also needed.",
        "Dummy symbol D will be used to soak up excess input symbols, so D -* a through D d should be rules.",
        "Dummy symbol U will also be used to soak up excess input symbols, but U will be allowed to match only when there are four occurrences in a row of the same symbol (one occurrence for each edge).",
        "Take U aaaa, U - bbbb, U cccc, and U dddd as the rules expanding U.",
        "Now, what does it take for the graph to have a vertex cover of size k = 2?",
        "One way to get a vertex cover is to go through the list of edges and underline one endpoint of each edge.",
        "If the vertex cover is to be of size 2, the underlining must be done in such a way that only two distinct vertices are ever touched in the process.",
        "Alternatively, since there are 4 vertices in all, the vertex cover will be of size 2 if there are 4-2=2 vertices left untouched in the underlining process.",
        "This method of finding a vertex cover can be translated into a UCFG rule as follows: START H1ll2H3H4UUDDDD That is, each H-symbol is supposed to match the name of one of the endpoints of the corresponding edge, in accordance with the rules expanding the H-symbols.",
        "Each U-symbol is supposed to correspond to a vertex that was left untouched by the H-matching, and the D-symbols are just there for bookkeeping.",
        "Figure 3 lists the complete grammar that encodes the vertex-cover problem of Figure 2.",
        "To make all of this work properly, take a = aaaabbbbccccdddd as the input string to be parsed.",
        "(In general, for every vertex name x, include in a a contiguous run of occurrences of x, one occurrence for each edge in the graph.)",
        "The grammar encodes the underlining procedure by requiring each H-symbol to match one of its endpoints in a.",
        "Since the right-hand side of the START rule is unordered, the grammar allows an H-symbol to match anywhere in the input, hence to match any vertex name (subject to interference from other rules that have already matched).",
        "Furthermore, since there is one occur",
        "rence of each vertex name for every edge, all of the edges could conceivably be matched up with the same vertex; that is, it's impossible to run out of vertex-name occurrences.",
        "Consequently, the grammar will allow either endpoint of an edge to be \"underlined\".",
        "The parser will have to figure out which endpoints to choose – in other words, which vertex cover to select.",
        "However, the grammar also requires two occurrences of U to match somewhere.",
        "U can only match four contiguous identical input symbols that have not been matched in any other way, and thus if the parser chooses a vertex cover that is too large, the U-symbols will not match and the parse will fail.",
        "The proper number of D-symbols is given by the length of the input string, minus the number of edges in the graph (to account for the H,.-matches), minus k times the number of edges (to account for the U-matches): in this case, 16 – 4 – (2.4) = 4, as illustrated in the START rule.",
        "The net result of this construction is that in order to decide whether a is in the language generated by the UCFG, the parser must in effect search for a vertex cover of size 2 or less.9 If a parse exists, an appropriate vertex cover can be read off from beneath the H-symbols in the parse tree; conversely, if an appropriate vertex cover exists, it indicates how to construct a parse.",
        "Figure 4 shows the parse tree that encodes a solution to the vertex-cover problem of Figure 2.",
        "The construction shows that vertex-cover problem is reducible to UCFG recognition.",
        "Furthermore, the construction of the grammar and input string can be carried out in polynomial time.",
        "Consequently, UCFG recognition and the more general task of ID/LP parsing must be computationally difficult.",
        "For a more careful and detailed treatment of the reduction and its correctness, see the appendix."
      ]
    },
    {
      "heading": "7 COMPUTATIONAL IMPLICATIONS",
      "text": [
        "The reduction of Vertex Cover shows that the ID/LP parsing problem is NP-complete.",
        "Unless .9 = , the time complexity of ID/LP parsing cannot be bounded by any polynomial in the size of the grammar and input.10 An immediate conclusion is that complexity analysis must be done carefully: despite its similarity to Earley's algorithm, Shieber's algorithm does not have complexity 0(1 G 12 • n3).",
        "For some choices of grammar and input, its internal structures undergo exponential growth.",
        "Other consequences also follow."
      ]
    },
    {
      "heading": "7.1 PARSING THE OBJECT GRAMMAR",
      "text": [
        "Even in the face of its combinatorially explosive worst-case behavior, Shieber's algorithm should not be immediately cast aside.",
        "Despite the fact that it sometimes blows up, it still has an advantage over the alternative of parsing the expanded object grammar.",
        "One interpretation of the NP-completeness result is that the general case of ID/LP parsing is inherently difficult; hence it should not be surprising that Shieber's algorithm for solving that problem can sometimes suffer from combinatorial explosion.",
        "More significant is the fact that parsing with the expanded CFG blows up in cases that should not be difficult.",
        "There is nothing inherently difficult about parsing the language that consists of all permutations of the string abcde, but while parsing that language the Earley parser can use 24 states or more to encode what the Shieber parser encodes in only one (section 3).",
        "To put the point another way, the significant fact is not that the Shieber parser can blow up; it is that the use of an expanded CFG blows up unnecessarily."
      ]
    },
    {
      "heading": "7.2 IS PRECOMPILATION POSSIBLE?",
      "text": [
        "The present reduction of Vertex Cover to ID/LP Parsing involves constructing a grammar and input string that both depend on the problem to be solved.",
        "Consequently, the reduction does not rule out the possibility that through clever programming one might concentrate most of the computational difficulty of ID/LP parsing into a separate precompilation stage, dependent on the grammar but independent of the input.",
        "According to this optimistic scenario, the entire procedure of preprocessing the grammar and parsing the input string would be as difficult as any NP-complete problem, but after precompilation, the time required for parsing a particular input would be bounded by a polynomial in grammar size and sentence length.",
        "Regarding the case immediately at hand, Shieber's modified Earley algorithm has no precompilation step.11 The complexity result implied by the reduction thus applies with full force; any possible precompilation phase has yet to be proposed.",
        "Moreover, it is by no means clear that a clever precompilation step is even possible; it depends on exactly how I G I and n enter into the complexity function for ID/LP parsing.",
        "If n enters as a factor multiplying an exponential, precompilation cannot help enough to ensure that the parsing phase will run in polynomial time.",
        "For example, suppose some parsing problem is known to require 21 G I • n3 steps for solution.12 If one is willing to spend, say, 10 • 21G1 steps in the precompilation phase, is it possible to reduce parsing-phase complexity to something like I G 1 8 • n3?",
        "The answer is no.",
        "Since by hypothesis it takes at least 21 G I • n3 steps to solve the problem, there must be at least 21 G I • n3 – 10 • 21 GI steps left to perform after the precompilation phase.",
        "The parameter n is necessarily absent from the precompilation complexity, hence the term 21 G I n3 will eventually dominate.",
        "In a related vein, suppose the precompilation step is conversion from ID/LP to CFG form and the runtime step is the use of the Earley parser on the expanded CFG.",
        "Although the precompilation step does a potentially exponential amount of work in producing G' from G, another exponential factor still shows up at runtime because I G' I in the complexity bound I G' 12 • n3 is exponentially larger than the original I GI ."
      ]
    },
    {
      "heading": "7.3 POLYNOMINAL-TIME PARSING OF A FIXED GRAMMAR",
      "text": [
        "As noted above, both grammar and input in the current vertex-cover reduction depend on the vertex-cover problem to be solved.",
        "The NP-completeness result would be strengthened if there were a reduction that used the same fixed grammar for all vertex-cover problems, for it would then be possible to prove that a precompilation phase would be of little avail.",
        "However, unless .9 = it is impossible to design such a reduction.",
        "Since grammar size is not considered to be a parameter of a fixed-gram"
      ]
    },
    {
      "heading": "Computational Linguistics, Volume 11, Number 4, October-December 1985",
      "text": [
        "mar parsing problem, the use of the Earley parser on the object grammar constitutes a polynomial-time algorithm for solving the fixed-grammar ID/LP parsing problem.",
        "Although ID/LP parsing for a fixed grammar can therefore be done in cubic time, that fact represents little more than an accounting trick.",
        "The object grammar G' corresponding to a practical ID/LP grammar would be huge, and if I G' I 2 • n3 complexity is too slow, then it remains too slow when I G' I 2 is regarded as a constant.",
        "The practical irrelevance of polynomial-time parsing for a fixed grammar sheds some light on another question that is sometimes asked.",
        "Can't we have our cake and eat it too by using the ID/LP grammar G directly when we want to see linguistic generalizations, but parsing the object grammar G' when we want efficient parsing?",
        "After all, the Earley algorithm runs in cubic time based on the length of the input string, and its dependence on grammar size is only I G' 12.",
        "Essentially, the answer is that using the object grammar doesn't help.",
        "The reduction shows that it's not always easy to process the ID/LP form of the grammar, but it is no easier to use the Earley algorithm on the expanded form.",
        "As the examples that have been presented clearly illustrate, both the Shieber parser and the Earley parser for a given language can end up with state sets that contain large numbers of elements.",
        "The object grammar does not promote efficient processing; the Shieber parser operating on the ID/LP grammar can often do better than the Earley parser operating on the object grammar, because of its more concise representation (section 4).",
        "The Earley-algorithm grammar-size factor I G' I 2 looks smaller than the Shieber factor 21 G I until one recalls that G' can be exponentially larger than G. In other words, we can hide the factor 21 G I inside I G' I , but that doesn't make it any smaller.",
        "Thus parsing is likely to take a great many steps even if we parse the object grammar, which might mistakenly be thought to be more efficient than direct parsing.",
        "If an algorithm runs too slowly, it doesn't make it faster if we cover up the exponential factor and make it a constant K, and it's unlikely that ID/LP parsing can be done quickly in the general case."
      ]
    },
    {
      "heading": "7.4 THE POWER OF THE UCFG FORMALISM",
      "text": [
        "The Vertex Cover reduction also helps pin down the computational power of the UCFG formalism.",
        "As G1 and G'1 in section 3 illustrated, a UCFG (or an ID/LP grammar) can enjoy considerable brevity of expression compared to the equivalent CFG.",
        "The NP-completeness result illuminates this property in two ways.",
        "First, the result shows that this brevity of expression is sufficient to allow an instance of any problem in .4'9 to be stated in a UCFG that is only polynomially larger than the original problem instance.",
        "In contrast, if an attempt is made to replicate the current reduction with a CFG rather than UCFG, the necessity of spelling out all the orders in which the H-, U-, and D-symbols might appear makes",
        "the CFG more than polynomially larger than the problem instance.",
        "Consequently, the reduction fails to establish NP-completeness, which indeed does not hold.",
        "Second, the result shows that the increased expressive power does not come free; while the CFG recognition problem can be solved in time 0(( G 2•n3) unless g = the general UCFG recognition problem cannot be solved in polynomial time.",
        "The details of the reduction show how powerful a single UCFG rule can be.",
        "If the UCFG formalism is extended to permit ordinary CFG rules in addition to rules with unordered expansions, the grammar that expresses a vertex-cover problem needs only one UCFG rule, although that rule may need to be arbitrarily long.",
        "'3"
      ]
    },
    {
      "heading": "7.5 THE ROLE OF CONSTRAINT",
      "text": [
        "Finally, the discussion of section 5 illustrates the way in which the weakening of constraints can often make a problem computationally more difficult.",
        "It might erroneously be thought that weak constraints represent the best case in computational terms, for \"weak\" constraints sound easy to verify.",
        "However, oftentimes the weakening of constraint multiplies the number of possibilities that must be considered in the course of solving a problem.",
        "In the case at hand, the removal of constraints on the order in which constituents can appear causes the dependence of parsing complexity on grammar size to grow from I G 1 2 to 216I."
      ]
    },
    {
      "heading": "8 LINGUISTIC IMPLICATIONS",
      "text": [
        "The key factors that cause difficulty in ID/LP parsing are familiar to linguistic theory.",
        "GB-theory and GPSG both permit the existence of constituents that are empty on the surface, and thus in principle they both allow the kind of pathology illustrated by G3 in section 4, subject to amelioration by additional constraints.",
        "Similarly, every current theory acknowledges lexical ambiguity, a key ingredient of the vertex-cover reduction.",
        "Though the reduction illuminates the power of certain mechanisms and formal devices, the direct implications of the NP-completeness result for grammatical theory are few.",
        "The reduction does expose the weakness of attempts to link context-free generative power directly to efficient parsability.",
        "Consider, for instance, Gazdar's (1981:155) claim that the use of a formalism with only context-free power can help explain the rapidity of human sentence processing: Suppose .",
        ".",
        ".",
        "that the permitted class of generative grammars constituted a subset of those phrase structure grammars capable only of generating context-free languages.",
        "Such a move would have two important metatheoretical consequences, one having to do with learnability, the other with processability .",
        ".",
        ".",
        ".",
        "We would have the beginnings of an explanation for the obvious, but largely ignored, fact that humans process the utterances they hear very rapidly.",
        "Sentences of a context-free language are provably parsable in a time which is proportional to the cube of the length of the sentence or less.",
        "As the arguments and examples in this paper have illustrated, context free generative power does not guarantee efficient parsability.",
        "Every ID/LP grammar technically generates a context-free language, but the potentially large size of the corresponding CFG means that we can't count on that fact to give us efficient parsing.",
        "Thus it is impossible to sustain this particular argument for the advantages of such formalisms as (early) GPSG over other linguistic theories; instead, GPSG and other modern theories seem to be (very roughly) in the same boat with respect to complexity.",
        "In such a situation, the linguistic merits of various theories are more important than complexity results.",
        "(See Berwick (1982), Berwick and Weinberg (1982, 1984), and Ristad (1985) for further discussion.)",
        "The reduction does not rule out the use of formalisms that decouple ID and LP constraints; note that Shieber's direct parsing algorithm wins out over the use of the object grammar.",
        "However, if we assume that natural languages are efficiently parsable (EP), then computational difficulties in parsing a formalism do indicate that the formalism itself does not tell the whole story.",
        "That is, they point out that the range of possible languages has been incorrectly characterized: the additional constraints that guarantee efficient parsability remain unstated.",
        "Since the general case of parsing ID/LP grammars is computationally difficult, if the linguistically relevant ID/LP grammars are to be efficiently parsable, there must be additional factors that guarantee a certain amount of constraint from some source.",
        "'4 (Constraints beyond the bare ID/LP formalism are required on linguistic grounds as well.)",
        "Note that the subset principle of language acquisition (cf. Berwick and Weinberg 1984:233) would lead the language learner to initially hypothesize strong order constraints, to be weakened only in response to positive evidence.",
        "However, there are other potential ways to guarantee efficient parsability.",
        "It might turn out that the principles and parameters of the best grammatical theory permit languages that are not efficiently parsable in the worst case – just as grammatical theory permits sentences that are deeply center-embedded (Miller and Chomsky 1963).15 In such a situation, difficult languages or sentences would not be expected to turn up in general use, precisely because they would be difficult to process.16 The factors that guarantee efficient parsability would not be part of grammatical theory because they would result from extragrammatical factors, i.e. the resource limitations of the language-processing mechanisms.",
        "This \"easy way out\" is not automatically available, depending as it does on a detailed account of processing mechanisms.",
        "For example, in the Earley parser, the difficulty of parsing a construction can vary widely with the amount of lookahead used (if any).",
        "Like any other theory, an explanation based on resource limitations must make the right predictions about which constructions will be difficult to parse.",
        "In the same way, the language-acquisition procedure could potentially be the source of some constraints relevant to efficient parsability.",
        "Perhaps not all of the languages permitted by the principles and parameters of syntactic theory are accessible in the sense that they can potentially be constructed by the language-acquisition component.",
        "It is to be expected that language-acquisition mechanisms will be subject to various kinds of limitations just as all other mental mechanisms are.",
        "Again, however, concrete conclusions must await a detailed proposal."
      ]
    },
    {
      "heading": "9 APPENDIX",
      "text": [
        "This appendix contains the details of a more careful reduction of the vertex-cover problem to the UCFG recognition problem.",
        "This version of the reduction establishes that the difficulty of UCFG recognition is not due either to the possibility of empty constituents (E-rules) or to the possibility of repeated symbols in rules (i.e., to the use of multisets rather than sets).",
        "Consequently, it is somewhat different from and more complex than the one sketched in the text."
      ]
    },
    {
      "heading": "9.1 DEFINING UNORDERED CONTEXT-FREE GRAMMARS",
      "text": [
        "Definition: An unordered CFG (UCFG) is a quadruple (N, 2, R, 5), where (a) Nis a finite set of nonterminals.",
        "(b) E disjoint from Nis a finite, nonempty set of terminal symbols.",
        "(c) R is a nonempty set of rules (A, a), where A e N and a c (N U E )*.",
        "The rule (A, a) may be written as A a.",
        "(d) S c N is the start symbol.",
        "Convention: The grammar G and its components N,1, R, S need not be explicitly mentioned when clear from context.",
        "Convention: Unless otherwise noted,",
        "(a) A, A', At, ... denote elements of N; (b) a, a', at ... denote elements of 2; (c) X, Y, X', Y', X,, ... denote elements of N U E; (d) a, u, u', ... denote elements of E*; (e) a, 0, y, 4), denote elements of (NU E)*.",
        "Definition: G = (N, 2, R, S) is simple iff it is E-free, duplicate-free, and branching.",
        "Note.",
        "The notion of a simple UCFG is introduced in order to help pin down the source of any computational"
      ]
    },
    {
      "heading": "Computational Linguistics, Volume 11, Number 4, October-December 1985",
      "text": [
        "difficulties associated with UCFGs.",
        "For example, since simple UCFGs are restricted to be duplicate-free, a difficulty that arises with simple UCFGs cannot result from the possibility that a symbol may occur more than once on the right-hand side of a rule.",
        "Definition: 0,44it0c4) (by r) just in case (for some) r = (A', Y1... e R and for some permutation p of [1,n], A = A' and a = ,(1) Y.",
        "If E 1*, also write 4)A4)+lm 04.",
        "Definition: An n-step derivation of 4) from is a sequence (00, ..., 0,,) such that 00, = 0, = and for all i E [0, n-1], (pi If it is also true for all i that lm say that the derivation is leftmost."
      ]
    },
    {
      "heading": "9.2 DEFINING THE COMPUTATIONAL PROBLEMS",
      "text": [
        "Definition: A possible instance of the problem VERTEX COVER is a triple (V,E,k), where (V,E) is a finite graph with at least one edge and at least two vertices, k e N, and k < 1 V1.17 VERTEX COVER itself consists of all possible instances (V,E,k) such that for some V g V, 1 < k and for all edges e e E, at least one endpoint of e is in V'.",
        "(Figure 5 gives an example of a VERTEX COVER instance.)",
        "with the e as indicated k= 3",
        "Notation: Take II II to be any reasonable measure of the encoded input length for a computational problem; continue to use I • I for set cardinality and string length.",
        "It is reasonable to require that if S is a set, k E N, and I SI > k, then 11 S 11 > 11 k 11 ; that is, the encoding of numbers is better than unary.",
        "It is also reasonable to require that V (..., x, >11 > 11 X 11."
      ]
    },
    {
      "heading": "9.3 THE UCFG RECOGNITION PROBLEM IS IN NP",
      "text": [
        "Lemma 9.1: Let (00, ..., ok) be a shortest leftmost derivation of \"k from 00 in a branching c-free UCFG.",
        "If k > INI +1 then I Ok I > 14o I • Proof.",
        "There exists some sequence of rules (A0, ... ak_i) such that for all i E [0, k – 1], cb, + lm c5i+1 by",
        "only one permutation.",
        "Then ow cAj+i, Ok) is a leftmost derivation of 4k from 00 and has length less than k, which is also impossible.",
        "Then 14k I > 100 1 Corollary 9.2: If G is a branching c-free UCFG and a E L(G), then a has a leftmost derivation of length at most lal •mwherem= 1N1+2.",
        "Proof.",
        "Let (cio, ok) be a shortest leftmost derivation of a from S. Suppose k > I a 1 • m. Consider the sub-derivations",
        "Lemma 9.3: II = SIMPLE UCFG RECOGNITION is in the computational class .4;9.",
        "Proof.",
        "Let G = (N, E, R, 5) be a simple UCFG and a E E*.",
        "Consider the following nondeterministic algorithm with input (G, a): Step 1.",
        "Write down 00 = S. Step 2.",
        "Perform the following steps for i from 0 to I a I • m-1, where m= INI +2.",
        "(a) Express 0, as u,A,y, by finding the leftmost nonterminal, or loop if impossible.",
        "(b) Guess a rule (A., Yo • • • E R and a permutation p, of [IA], or loop if there is no such rule.",
        "(c) Write down 0,4.1 = 14,Y, 0) • • • Y„ pi(k) Y, (d) If 0,4_1 = a then halt.",
        "It should be apparent that the algorithm runs in time at worst polynomial in II (G, a) 11 ; note that the length of 0, increases by at most a constant amount on each iteration.",
        "Assume (G, a) c H. Then a has a leftmost derivation of length at most I a I • m by Corollary 9.2; hence the nondeterministic algorithm will be able to guess it and will halt.",
        "Conversely, suppose the algorithm halts on input (G, a).",
        "On the iteration when the algorithm halts, the sequence (Po, • • • , 0,+i) will constitute a leftmost derivation of a from S; hence a E L(G) and ( G, a) E Then there is a nondeterministic algorithm that runs in polynomial time and accepts exactly H. Hence"
      ]
    },
    {
      "heading": "H E",
      "text": []
    },
    {
      "heading": "9.4 THE UCFG RECOGNITION PROBLEM IS NP-COMPLETE",
      "text": [
        "Lemma 9.4: Let (V,E,k) = k) be a possible instance of VERTEX COVER.",
        "Then it is possible to construct, in time polynomial in 11 VII , 11E11 , and k, a simple UCFG G(V,E,k) and a string a(V,E,k) such that",
        "Proof.",
        "Construct G(V,E,k) as follows.",
        "Let the set N of nonterminals consist of the following symbols not in V: START, U, D, H, for i E [1, 1 E 1 ], U, for i E [1, I VI -k], D, for i E [1, IEI (k-1)].",
        "II Nil will be at worst polynomial in II Ell , II VII , and k for a reasonable length measure.",
        "Define the terminal vocabulary E to consist of subscripted symbols as follows:",
        "Designate START as the start symbol.",
        "Include the following as members of the rule set R:",
        "Take G(V,E,k) to be (N, E, R, START).",
        "(Figure 6 shows the results of applying this construction to the VERTEX COVER instance of Figure 5.)",
        "Let h : [1, I VI -* V be some standard enumeration of the elements of V. Construct a(V,E,k) as h(1)1 h(1)1E1 h( I VI ), ••• h( I VI )1E1 thus a(V,E,k) will have length I E I • I VI.",
        "It is easy to see that II (G(V,E,k), a(V,E,k)) 0 will, be at worst polynomial in 0 E II VII , and k for reasonable II • II .",
        "It will also be possible to construct the grammar and string in polynomial time.",
        "Finally, note that given the definition of a possible instance of VERTEX COVER, the grammar will be branching, E-free, and duplicate-free, hence simple.",
        "Now suppose (V,E,k) E VERTEX COVER.",
        "Then there exist V c V and f : E V such that I V' I < k and for every e E E, f(e) is an endpoint of e. E is nonempty by hypothesis and V must hit every edge, hence I V I cannot be zero.",
        "Construct a parse tree for a (V ,E,k) according to G(V,E,k) as follows.",
        "Step I.",
        "Number the elements of V – V as {x1 : i E [1, I V – V' I ]}.",
        "For each x, where i < IVI-k, construct a node dominating the substring (x,)1 (x1)151 of a(V,E,k) and label it U.",
        "Then construct a node dominating only the U-node and label it U.",
        "Note that the available symbols U, are numbered from 1 to I VI-k, so it is impossible to run out of U-symbols.",
        "Also, IVI <k and",
        "all of the U-symbols will be used.",
        "Finally, note that U al ... al Elis a rule for any a E S and that U, U is a rule for any U,.",
        "Step 2.",
        "For each e, E E, construct a node dominating the (unique) occurrence of f(e,), E a(V,E,k) and label it H..",
        "Step 2 cannot conflict with step 1 because f(e,) E , hence f(e,) V – V. Different parts of step 2 cannot conflict with each other because each one affects a symbol with a different subscript.",
        "Also note that f(e;) is an endpoint of ei and that H. is a rule for any e, E E and a an endpoint of e..",
        "Step 3.",
        "Number all occurrences of terminals in a(V,E,k) that were not attached in step 1 or step 2.",
        "For the ith such occurrence, construct a node dominating the occurrence and label it D. Then construct another node domi",
        "nating the D-node and label it D,.",
        "Note that the stock of D-symbols runs from 1 to (k-1) • IEI .",
        "Exactly (IVI – k)' I E 1 syrhbols of a(V,E,k) were accounted for in step 1.",
        "Also, exactly 1 E 1 symbols were accounted for in step 2.",
        "The length of a(V,E,k) is 1 VI IEl, hence exactly",
        "symbols remain at the beginning of step 3.",
        "D a is a rule for any a in E; D -).D is a rule for any D,.",
        "START -* ... ••• D I E is in the grammar.",
        "Note also that nodes labeled Hi,' were constructed in step 2, nodes labeled U1,1_,, were constructed in step 1, and nodes labeled D1, •• • DIE1.",
        "(k-i) were constructed in step 3.",
        "Hence the application of the rule is in accord with the grammar.",
        "Then a(V,E,k) e L(G).",
        "(Figure 7 illustrates the application of this parse-tree construction procedure to the grammar and input string derived from the VERTEX COVER example in Figure 5.)",
        "Conversely, suppose a(V,E,k) e L(G).",
        "Then the derivation of a(V,E,k) from START must begin with the application of the rule START -> H1... HIEIUIVI_kDl... Di E I '(k-1) and each H., must later be expanded as some subscripted terminal g(H,).",
        "Define f(e,) to be g(H1) without the",
        "subscript; then by construction of the grammar, f(e,) is an endpoint of e, for all e e E. Define 1/1 = {1*(0 : e, e E}; then it is apparent that V' c V and that V' contains at least one endpoint of e, for all e, e E. Also, each U, , for i E [1, I V I – k] must be expanded as U, then as some substring (a,), .",
        ".",
        ".",
        "(a) 1E1 of a(V,E,k).18 Since the substrings dominated by the H, and U, must all be disjoint, and since there are only I E I subscripted occurrences of any single symbol from V in a(V,E,k), there must be I VI – k distinct elements of V that are not dominated in any of their subscripted versions by any H Then I V – I > I V I – k. Since in addition V c V' ,",
        "complete.",
        "Proof.",
        "SIMPLE UCFG RECOGNITION is in the class Ar.9 by Lemma 9.3, hence a polynomial-time reduction of VERTEX COVER to SIMPLE UCFG RECOGNITION is sufficient.",
        "Let (V,E,k) be a possible instance of VERTEX COVER.",
        "Let G be G(V,E,k) and a be (V,E,k) as constructed in Lemma 9.4.",
        "Note that G is simple.",
        "The construction of G and a can, by lemma, be carried out at time at worst polynomial in II E II, II VII , and k. Also by lemma (G,a) E SIMPLE UCFG RECOGNITION iff (V,E,k) E VERTEX COVER.",
        "k is not polynomial in k II under a reasonable encoding scheme.",
        "However, E I > k, hence II E ?_ ii k II ; also II (V,E,k) II > II E hence II (V,E,k) II > k, all by properties assumed to hold of II • Then G and a can in fact be constructed in time at worst polynomial in II (V,E,k) Hence the VERTEX COVER problem is polynomial-time reduced to SIMPLE UCFG RECOGNITION.",
        "❑"
      ]
    },
    {
      "heading": "REFERENCES",
      "text": [
        "Useful guidance and commentary during the writing of this paper have been provided by Bob Berwick, Michael Sipser, and Joyce Friedman.",
        "The paper was also improved in response to interesting remarks from the anonymous referees for Computational Linguistics.",
        "The author gratefully acknowledges the assistance of Blythe Heepe in preparing the fitures.",
        "representation actually suffers to some extent from the same problem.",
        "Shieber (1983:10) uses an ordered sequence instead of a multiset before the dot; consequently, in place of the state involving S IA,B,C1 • {D,E}, Shieber would have the 3!",
        "= 6 states involving S a • {D,E}, where a ranges over the six permutations of ABC.",
        "8.",
        "Recognition is simpler than parsing because a recognizer is not required to recover the structure of an input string, but only to decide whether the string is in the language"
      ]
    }
  ]
}
