{
  "info": {
    "authors": [
      "Pamela K. Fink",
      "Alan W. Biermann"
    ],
    "book": "Computational Linguistics",
    "id": "acl-J86-1002",
    "title": "The Correction of Ill-Formed Input Using History-Based Expectation With Applications to Speech Understanding",
    "url": "https://aclweb.org/anthology/J86-1002",
    "year": 1986
  },
  "references": [
    "acl-C80-1027",
    "acl-J80-2001",
    "acl-J80-2003",
    "acl-J81-2002",
    "acl-J83-3001",
    "acl-J83-3002",
    "acl-J83-3003",
    "acl-J83-3005"
  ],
  "sections": [
    {
      "heading": "THE CORRECTION OF ILL-FORMED INPUT USING HISTORY-BASED EXPECTATION WITH APPLICATIONS TO SPEECH UNDERSTANDING",
      "text": [
        "A method for error correction of ill-formed input is described that acquires dialogue patterns in typical usage and uses these patterns to predict new inputs.",
        "Error correction is done by strongly biasing parsing toward expected meanings unless clear evidence from the input shows the current sentence is not expected.",
        "A dialogue acquisition and tracking algorithm is presented along with a description of its implementation in a voice interactive system.",
        "A series of tests are described that show the power of the error correction methodology when stereotypic dialogue occurs.",
        "This material is based upon work supported by The National Science Foundation under Grant number MCS 7904120 and Grant number MCS 8113491 and by the Air Force Office of Scientific Research, Air Force Systems Command, USAF, under Grant 81-0221."
      ]
    },
    {
      "heading": "1 INTRODUCTION",
      "text": [
        "In an environment where stereotypic discourse commonly occurs, the repetitiveness and predictability of the interactions may enable a machine to effectively anticipate some inputs.",
        "For a speech understanding system, such anticipation can greatly enhance the processor's capabilities for error correction so that proper action will take place despite inaccuracies at the voice recognition phase.",
        "This paper is concerned with the automatic construction of a model of user behaviors in typical interactions and the use of such a model in the correction of misrecognition errors.",
        "It is assumed that a user approaches the machine in a typical application with a problem to be solved.",
        "He or she inputs a series of sentences requesting action or information that will lead to a solution and then leaves when the task is complete.",
        "In the early examples of such an interaction, the machine will have little or no expectation and will be dependent on its basic capabilities for understanding and carrying out commands.",
        "However, if repetitive behaviors occur, the processor will effectively use them to anticipate inputs and correct errors.",
        "This will enable the user to speak less precisely and more quickly while still achieving reliable performance.",
        "Such repetitive behaviors may occur within a single dialogue where a user may utter sentences with similar meanings again and again (as in \"Is there a plane on Thursday?",
        "What time does it leave?",
        "Is there one on Friday?",
        "When does it leave?\").",
        "They may also occur when a given dialogue resembles earlier ones.",
        "The expectation system will thus continuously monitor inputs, looking for repetition.",
        "If no repetitious behavior occurs, the natural language processor is allowed to proceed without intervention in handling a dialogue.",
        "However, if repetitiveness is detected, the expectation system will supply the processor with anticipated behaviors which can be used to help remove uncertainties in sentence recognition when they occur.",
        "In the following sections, an overview of the history based expectation system is given.",
        "Then a representation for user behaviors is described, followed by an algorithm for creating and tracking such models along with a meth-Copyright1986 by the Association for Computational Linguistics.",
        "Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.",
        "To copy otherwise, or to republish, requires a fee and/or specific permission.",
        "0362-613X/86/010013-36$03.00 Computational Linguistics, Volume 12, Number 1, January-March 198613 Pamela K. Fink and Alan W. BiermaimThe Correction of III-Formed Input od for using them in error correction.",
        "Finally, an implementation of this methodology is described in the domain of speech recognition and results from a series of tests investigating the system's performance in various situations are presented."
      ]
    },
    {
      "heading": "2 AN OVERVIEW OF THE HISTORY-BASED EXPECTATION SYSTEM",
      "text": [
        "The general goal of the history-based expectation system is to merge a series of dialogues, each of which consists of a sequence of sentences, into a more general dialogue that reflects the patterns that exist between and within the separate dialogues.",
        "Thus, the expectation system must: save incoming dialogues, find patterns between and within these dialogues so that they can be merged into a more general dialogue which becomes a formula for a more general situation, and use this information to help predict what will be said by a user in a given situation.",
        "This ability to predict what might be said by a user can help error correct what is input to the natural language system through errorful means, such as a voice recognizer.",
        "We will call this ability expectation.",
        "Figure 1 shows an overview of the structure of the history-based expectation system.",
        "Expectation is acquired at two levels, the sentence level and the dialogue level.",
        "A special parser, called the expectation parser, is used to analyze at the sentence level.",
        "The expected dialogue is a data structure used to store the history-based expectation that is acquired using an expectation acquisition algorithm.",
        "This constitutes the dialogue level.",
        "As each sentence is entered into the system, such as through a speech recognition device, it is parsed and a meaning representation is produced and saved by an expectation acquisition algorithm in the expectation module (see 1 in Figure 1).",
        "The parse is also output for use in the next step in the system's processing of the sentence.",
        "This process builds a sequence of sentence meanings, which are then incorporated into an expected dialogue (see 2 in Figure 1).",
        "After an expected dialogue is partially or completely built, the expectation module attempts to determine where the user is in a given dialogue using information from the expected dialogue and the current parsed sentence (see 1 and 3 in Figure 1).",
        "If it succeeds, it creates and transmits (see 4 in Figure 1) an expected sentence set to the expectation parser.",
        "The expectation parser will then use this information to improve its ability to recognize the next incoming sentence."
      ]
    },
    {
      "heading": "EXPECTED DIALOGUE",
      "text": []
    },
    {
      "heading": "3 A REPRESENTATION FOR USER BEHAVIORS",
      "text": [
        "Suppose a user inputs the following sequence: SentenceLabel Display my mail summary for today.Si Show me this letter.",
        "(with touch input)S2 (the letter appears on the screen) Remove this letter.S3 Display the letter from JA.S4 (letter appears on the screen) Delete it.S5 Log off.S6 We denote the meaning of each sentence Si with the notation M(Si).",
        "The exact form of M(Si) need not be discussed at this point; it could be a conceptual dependence graph (Schank and Abelson 1977), a deep parse of Si, or some other representation.",
        "A user behavior is represented by a network, or directed graph, of such meanings.",
        "At the beginning of a task, the state of the interaction is represented by the start state of the graph.",
        "The immediate successors of this state are the typical opening meaning structures for this user, and succeeding states represent, historically, paths that have been followed by this user.",
        "It is important that if two sentences, Si and Sj, have approximately the same meaning this should be clear in the representations M(Si) and M(Sj).",
        "Our algorithm, described below, merges two meanings M(Si) and M(Sj) into a single node in the behavior representation if they are sufficiently similar, and appear in similar contexts.",
        "Thus, in the above example it would appear that M(S3) and M(S5) play similar roles and could be represented by one structure: after a letter is read, one might expect to see it deleted.",
        "Often, two commands will be similar except for the instantiation of certain constituents.",
        "This is the case in sentences S2 and S4, which request the display of, respectively, the message indicated by a touch and the letter from JA.",
        "Again, it is desired to represent such similar meanings in a behavior graph with a single node if they appear in similar environments.",
        "Thus, a routine will be needed to find a generalization of two such sentences that can represent their common meaning.",
        "In the example, the generalization of S2 and S4 might be \"display (LETTER)\" where \"(LETTER)\" is a noun group referring to a letter.",
        "In tracking a dialogue, we may arrive at a node in the behavior graph with meaning Ml.",
        "This means a command is expected with meaning M2 that is either identical to, or a special case of, Ml.",
        "If such an M2 is input at this time, we will say that M1 predicts M2 and define the predicate: Predicts(M1, M2) = true if and only if meaning M1 is identical or similar to M2.",
        "It is quite possible, as with M(S2) and M(S4) above, that a common generalization can be found for two sentences that appear in similar contexts.",
        "Then one will be able to merge them into a single node in the behavior graph.",
        "Thus, it is necessary to have a predicate to check whether these conditions hold and a function to find the desired generalization.",
        "The following two routines do this: Mergeable(M1, M2) = true if and only if an M can be found such that Predicts(M, M1) and Predicts(M, M2).",
        "Merge(M1, M2) yields a meaning M that is identical to, or a generalization of, M1 and M2.",
        "A user behavior is represented as a network of sentence meanings with transitions from one meaning to another that indicate traversals observed in actual dialogues and their frequencies.",
        "For example, the above six-sentence sequence could be represented as shown in Figure 2.",
        "Each node i has a meaning Mi and a count Ci, which gives the number of times in observed dialogues this node has been visited.",
        "The integer on each transition gives the number of times it has been traversed in observed dialogues.",
        "More formally, a behavior graph B will consist of a set of nodes named 0, 1, 2, 3, ..., bsize-1.",
        "Each node i will have its associated Mi and Ci and the first node will have a special meaning MO = 'start'.",
        "The transitions will be represented as triples (i, j, k) where the traversal is from node i to node k and has been observed j times.",
        "The example six-command sequence would be represented by Computational Linguistics, Volume 12, Number 1, January-March 198615 Pamela K. Fink and Alan W. Biermann The Correction of III-Formed Input the nodes 0 through 4 with Mi's and Ci's as shown and with the triples {(0,1,1) (1,1,2) (2,2,3) (3,1,2) (3,1,4)}.",
        "Notice that the observed probability of crossing transition (i, j, k) is j/Ci, a fact that is used by the expectation parser.",
        "result in similar processing, the addition of states 2 and 3, and the creation of transitions (1, 1, 2) and (2, 1, 3) as shown in Figure 3."
      ]
    },
    {
      "heading": "4 THE EXPECTATION MODEL BUILDING ANDSTART TRACKING ALGORITHMM(SI)",
      "text": [
        "It is desired to have an algorithm to monitor the discourse, collect the history of inputs, and invoke expectation when any kind of repetition occurs.",
        "Such an algorithm is described below.",
        "To do so, however, some additional notation is needed:M(S2) current = an integer giving the state number in B corre-M(S3) sponding to the most recently recognized sentence.",
        "bsize = the total number of states in B.",
        "The behavior graph B begins with one state numbered \"0\" and with MO = start, C(0) = 0.",
        "Thus, the size of the graph is bsize = 1 and the most recently recognized sentence is assumed to be this start state, current = 0.",
        "Suppose that the first sentence in the above sample dialogue is read: Si = \"Display my mail summary for today.\" Then the processor will begin with no expectation since E(0) is currently the empty set, and find",
        "This will result in the creation of a second state in B with the following statements: Create a NEW NODE: Put(current, 1, bsize) into B; (a transition to the new state is created) C(current) := C(current) + 1; (state O's count is incremented)",
        "Thus, the first two states shown in Figure 2 will exist with the single transition (0, 1, 1).",
        "Sentence S2 and S3 Figure 3.",
        "Constructing the behavior graph.",
        "The input sentence will yield a different action, however, if its meaning M(S) is determined to be merge able with the meaning of an existing node Mk on the graph.",
        "While the details of mergeability have not yet been discussed, let us assume for the current example that M(S4) is mergeable with M(S2).",
        "Then a new meaning will appear in the graph that is a generalization of these two, Merge(M(S2), M(S4)), and a graph transition will be built to this new meaning.",
        "Transfer to the existing meaning Mk would proceed as follows:",
        "Figure 4 shows the updated graph.",
        "At this point, current = 2, and the expectation set, E(2), is non-empty for the first time.",
        "So, now we compute P(S5, {M3}), meaning that S5 is read with the expectation that its meaning will be \"remove this one\".",
        "Given this expectation, the parser will prefer any transitions down paths that lead to some paraphrase of this sentence and, unless the system clearly recognizes that something else has been said, a sentence meaning \"remove this one\" should be recognized.",
        "If it is, then current will be advanced to this expected node.",
        "In general, there may be several expected sentence meanings, and the processor will select the one most similar to the incoming utterance unless that sentence is clearly not any member of the expected set.",
        "16Computational Linguistics, Volume 12, Number 1, January-March 1986 Pamela K. Fink and Alan W. Biermann The Correction of III-Formed Input Thus, if a successor k to the current state predicts the incoming sentence, we track that successor.",
        "Tracking the expected meaning Mk would proceed as follows:",
        "The final sentence S6 in the dialogue will cause the creation of a termination state and complete the graph of Figure 2.",
        "The behavior graph creation and tracking algorithm is thus the collection of the above code segments: if no behavior graph B exists then",
        "until M(S) is a dialogue termination.",
        "This code creates a finite state model of the dialogue based on equivalence or similarity classes defined by the functions Predicts, Mergeable, and Merge.",
        "As will be discussed in the next section, similarity classes are based not only on the similarity of the sentences themselves, but also on the environment in which they occur.",
        "Thus, there is only one state for each such similarity class in the finite state model created.",
        "When the user enters the system again, this algorithm can be reinvoked using the existing B graph.",
        "If the next dialogue is very similar to a previous one, then the expectation dialogue will powerfully support error correction.",
        "If the next dialogue has little resemblance to previous ones, then no expectation will be available, and the user will be dependent on basic processor recognition capabilities.",
        "This section has given an overview of the approach to history-based expectation processing.",
        "The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.",
        "The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve."
      ]
    },
    {
      "heading": "5 AN IMPLEMENTATION",
      "text": [
        "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.",
        "An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann and Ballard 1980).",
        "The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.",
        "The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).",
        "[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al.",
        "1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]",
        "It should be emphasized, of course, that the central issue here is the study of expectation mechanisms and the details of the design decisions could have been made in rather different ways.",
        "Thus one could have implemented expectation error correction with a typed input system or with a speech input system that integrates voice signal processing with higher level functions in a way not possible with a commercial recognizer.",
        "This implementation shows only one way in which the functions P, Predicts, Mergeable, and Merge can be constructed to achieve expectation capabilities.",
        "The conclusion of this paper is that in this particular situation substantial error correction is achieved, and thus one may suspect that similar results can be achieved in other applications.",
        "The implementation, as in the overview of the general system presented in section 2, consists of two major parts, an expectation parser and an expectation module, and their respective data structures.",
        "The expectation parser embodies the function P, while the major functions of the expectation module are Predicts, Mergeable, and Merge.",
        "An expected sentence set, E(current), along with the most recent input sentence S, are inputs to the expectation parser P. The expectation parser P uses these two inputs to determine the meaning M(S) of the input sentence S. Thus, M(S) is a deep parse of S. The function Predicts determines if one of the sentences in E(current) predicts M(S).",
        "If so, then M(S) is merged with this sentence meaning and dialogue tracking is begun from that point.",
        "Otherwise the function Mergeable determines how \"similar\" M(S) is to any other sentences in the expected dialogue.",
        "In this implementation, the function Mergeable is actually much more cautious about determining whether or not a set of sentences should be merged.",
        "For the implementation, if Mergeable determines that certain nodes in the expected dialogue are mergeable with M(S), then it adds the successors of these nodes to E, creating an expanded expected sentence set.",
        "Then, if the next sentence input is predicted by one or more of these sentences, they are merged through the action of Predicts and Merge."
      ]
    },
    {
      "heading": "5.1 THE EXPECTATION PARSER",
      "text": [
        "The purpose of the expectation parser in this implementation of a speech understanding system is to take input from the scanner and the expectation module, and use this information to determine what was said by the user.",
        "Thus, during the parsing process, the expectation parser must reconcile the sequence of words input from the scanner with the expected sentence set from the expectation module, or determine that the scanner input is not like anything that was expected and, thus, ignore expectation.",
        "In this way, the expectation parser parses from two inputs.",
        "It is constantly trying to maintain an equilibrium between the input from the scanner and the input from the expectation module.",
        "This balancing is kept in line by a set of rating factors that are used during the parsing procedure to help guide the search for a reasonable sentence structure.",
        "These rating factors, at times, will be referred to as probabilities in the following discussion.",
        "However, in reality, the ratings are one thousand times the values of the logarithms of numbers between 0 and 1.",
        "Thus, the ratings span the values 999 to 0, where 0 is equivalent to a probability of one.",
        "These ratings are computed this way because they remain integral and still fairly accurately represent the correct values.",
        "Also, they can simply be added and subtracted rather than multiplied and divided in the hundreds of calculations required for a single sentence parse.",
        "The expectation parser uses an ATN-like representation for its grammar (Woods 1970).",
        "Its strategy is top-down.",
        "The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (Ballard 1979).",
        "An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.",
        "Sentences have the same \"meaning\" if they result in identical tasks being performed.",
        "The various sentence structures that have the same meaning we call paraphrases.",
        "We have studied the following types of paraphrasing:",
        "1) WORD <= > WORD 'entry' <=> 'number' 2) ADJ NOUN <=> NOUN QUALIFIER 'positive entries' <=> 'entries which are positive' 3) NOUN NUMBER <=> DET ORDINAL NOUN 'row 2' <=> 'the second row' 4) CLASSIFIER NOUN <=> NOUN of/in CLASSIFIER 'the row 1 entries' <=> 'the entries in row 1' 5) EQUIVALENT SETS 'row 1' <=> 'entries in row 1' 6) QUANTIFIERS 'all (of) (the) entries' <=> 'the entries' 7) CONJUNCTION OF NOUNS 'double rows one and two' <=> 'double row one and row two' 18Computational Linguistics, Volume 12, Number 1, January-March 1986 Pamela K. Fink and Alan W. BiermaimThe Correction of III-Formed Input 8) DEFAULT CONTEXT 'the rows' <=> 'the rows in matrix 1' 9) NAMES 'column 1' <=> 'testA' 10) PRONOUNS 'it' <=> 'row 1' 11) ORDINAL NOUN <=> NOUN X in COLUMN or ROW NOUN NUMBER <=> NOUN X in COLUMN or ROW 'sixth entry' <=> 'entry 2 in column 3' 'entry 6' < => 'entry 3 in row 2' 12) NUMBER <=> ENTRY X '9.75' <=> 'entry 3' 13) WORD <=> {WORDS} 'double' <=> 'multiply by two' 14) CONJUNCTION OF VERBS 'double row two and zero matrix one.'",
        "<=> 'double row two.",
        "zero matrix one.'",
        "It is obvious from this list that there are varying levels of paraphrasing.",
        "Some arise at the vocabulary level (number 1), some at the syntactic level (numbers 2, 3, 4, 5, 6, and 7), some at the semantic level (numbers 8, 9, and 10), some at the current world level (numbers 11 and 12), and some at a combination of levels (numbers 13 and 14).",
        "Some are domain dependent, especially at the vocabulary level such as entry <=> number.",
        "Others are not, such as ADJ NOUN <=> NOUN QUALIFIER.",
        "Those that only require knowledge of the vocabulary or of the grammar are implemented in the current history based expectation system.",
        "This means that paraphrases one through seven are handled currently as part of the parsing process itself.",
        "The last seven may be dealt with at some future date.",
        "However, they are somewhat more complicated because they require temporal-type knowledge such as the current referent of a pronoun or the current size of a matrix.",
        "The lexical and grammatical paraphrases, on the other hand, will always have the same meaning, regardless of the current state of the world.",
        "By handling the seven lexical and syntactic paraphrases, a stored parse can aid in recognizing many sentences with the same \"meaning\" but different surface structures.",
        "To simplify representation of the parser output we have developed a special notation to indicate the deep parse of a sentence.",
        "For example, the parse of the sentences: Double the positive row 1 entries.",
        "Double the positive entries in row 1.",
        "Double the row 1 entries which are positive.",
        "Double the entries in row 1 which are positive.",
        "is notated as:",
        "istics of each grammatical structure could be accounted for individually.",
        "Thus in some cases, certain error correction alternatives were checked immediately while in others it was wiser to determine whether normal processing would fail at deeper levels before attempting those same corrections.",
        "The network represents a tree structure which is searched by the expectation parser.",
        "Succession in the network is represented by the parent child relationship, which is indicated in Figure 6 by indentation.",
        "Thus, the node containing the command ADV is the parent of the node containing the command CHEK PART ADJ, and so is succeeded by it.",
        "Should a command fail, the parser backs up to the parent node of the node that has just failed.",
        "Thus, if a check for an adjective in CHEK PART ADJ fails, control will back up to the node containing ADV.",
        "Choice is represented by the sibling relationship which is indicated in Figure 6 by the vertical lines connecting nodes.",
        "Thus, ADV, EXPADV, SKIPWORD, EXTRAWS, and LOSTWS are all siblings in the tree network and are choices that the parser can make when parsing a sentence.",
        "Note that, in this case, these five choices represent the five possible attempts that are made in trying to parse a word slot that were discussed above.",
        "A choice is made by picking the siblings in the order in which they appear in the network.",
        "Thus, when the CHEK PART ADJ fails and control backs up to ADV, the expectation parser will back up to the START node and then take the second choice, EXPADV, and attempt to proceed down that chain of commands.",
        "The scoring mechanism within the parser serves to aid in the evaluation of the alternative paths during the parse process and the pruning of improbable choices.",
        "A typical spoken input to the system is \"add row one to row two\" and the speech recognition machine will often return such errorful output as \"and row * to row\".",
        "The asterisk indicates that the device guesses the existence of a word but has failed to identify it.",
        "The parser must be able to extract the user's original intent and its operation is guided by rating factors which evaluate the quality of the path through the parser, the word selection, the level of agreement with expectation, and the self consistency (or compatibility) of the sentence.",
        "These individual ratings work as follows:"
      ]
    },
    {
      "heading": "1) The Transition Value",
      "text": [
        "Every time the parser moves over a SKIPWORD, EXTRAWS, or LOSTWS command a charge is made to the value of the transition.",
        "Normally, a transition does not cost anything, but each SKIP WORD, EXTRAWS, and LOSTWS executed results in a lowering of the transition's value.",
        "This charge is made for the rest of the parse unless the SKIPWORD, EXTRAWS, or LOSTWS is backed over.",
        "This charge can be seen in the sample grammar net appearing in Figure 6 after the words SKIPWORD, EXTRAWS, and LOSTWS.",
        "The charge in this example for each of the three commands is 1000*log[0.7] = 35."
      ]
    },
    {
      "heading": "2) The Word Value",
      "text": [
        "We define the synophones of a given vocabulary word to be the words a user might speak that could possibly be recognized as that word.",
        "Because of the nature of the dynamic programming algorithm in the NEC machine, it yields only one guess at each word slot.",
        "So it is necessary for our software to provide the set of synophones for each guessed word.",
        "This, in effect, simulates the situation where the speech recognition device provides a larger number of possible matches.",
        "Thus, in the case of the above recognizer output, the following synophones would be produced to represent the sequence of possible words spoken:",
        "LOSTWS at word slot x Each alternative word is given a rating.",
        "The words selected by the recognizer are given maximum ratings and alternatives are given lower values.",
        "If two words have the same pronunciation as with to and two, they are given the same values."
      ]
    },
    {
      "heading": "3) The Expectation Value",
      "text": [
        "This value is based on whether or not there is an expected sentence, how well the current parse is matching the current expected sentence from the expected sentence set, and how much the current parse is using this expected sentence.",
        "Whenever a slot is filled by the parser, it is compared with the corresponding slot in the expected sentence.",
        "If they do not match, the expectation value decreases, otherwise the expectation value remains the same."
      ]
    },
    {
      "heading": "4) The Compatibility Value",
      "text": [
        "This value differs from the other three in that it is simply true or false.",
        "Verb-operand, noungroup-noungroup, and expectation are checks made during the parse.",
        "If compatibility fails, then the expectation parser backs up, otherwise it continues forward.",
        "Each of these components has a value assessed at each word slot in the incoming sentence as well as one for the entire sentence.",
        "The word slot values are assumed to have a top rating until the parser reaches that word slot.",
        "Thus, the parser is always examining a best case situation based on what it has already done.",
        "For example, all word slot transition values are assumed, initially, to have the value 1000*log[1] = 0.",
        "The transition value at a word slot is only lowered if it is necessary for the parser to execute a SKIPWORD, EXTRAWS, or LOSTWS command in parsing that word slot.",
        "The charge made is according to the value indicated at the particular command in the grammar network.",
        "The average of the current values of all word slot transition values creates the sentence transition rating for the parse so far.",
        "The word slot and sentence values for the expectation and word values are computed similarly.",
        "The compatibility value differs, however, since it does not have degrees of ratings but rather indicates acceptability or lack thereof.",
        "Thus, it is not included in the formula for determining a rating for the parse.",
        "Rather, if it fails, then parsing automatically backs up.",
        "If it succeeds, then parsing continues forward.",
        "The values of the transition, word, and expectation components are used to determine two sentence parse ratings.",
        "At each word slot, the values of the three factors are averaged together to produce a general word slot parse rating.",
        "Also, the sentence values for the three components are averaged together to obtain a general sentence parse rating.",
        "Thus, we have the following equations that define the various rating values, where n is the number of word slots in the sentence:",
        "999, for example, all possibilities in the grammar are checked.",
        "On the other hand, setting all the minimum ratings to 0 results in the expectation parser behaving like a normal parser since this essentially turns off the use of the SKIPWORD, EXTRAWS, and LOSTWS commands, the use of synophones, and expectation.",
        "In theory, the parsing algorithm is admissible.",
        "That is, it is capable of finding the best possible parse.",
        "The various rating factors can initially be set high and gradually lowered until a parse is found.",
        "This parse would have the highest rating possible.",
        "However, this is impractical in practice due to the amount of time required to repeatedly search a growing space.",
        "Thus, minimum rating values are set and the search is conducted once.",
        "In this way, the first parse found is the \"best\" parse in the sense that it is the first one found whose rating was higher than the minimum preset value."
      ]
    },
    {
      "heading": "5.2 ROUTINES OF THE EXPECTATION MODULE",
      "text": [
        "The task of the expectation module is to acquire a general dialogue from a series of dialogues spoken by a user.",
        "The dialogues essentially contain examples of how to go about solving a particular kind of problem.",
        "In acquiring these dialogues and merging them into one generalized dialogue, the expectation system learns how to solve this particular kind of problem through examples.",
        "In a sense, by building this generalized dialogue the expectation system is creating a procedure that can solve a particular subset of problems.",
        "This is a future goal of the project.",
        "However, the current application is for the generalized dialogue to be used as an aid in the voice recognition process by offering predictions about what might be said next.",
        "The types of problems that can be learned by the existing history-based expectation system include linear algebra applications such as matrix multiplication, simultaneous linear equations, and Gaussian elimination.",
        "Non-linear algebra problems that require matrix-type representations can also be learned, such as gradebook maintenance and invoice manipulation.",
        "Though the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.",
        "The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.",
        "Thus, for example, it can acquire a \"script\" such as the one for going to a restaurant as defined in Schank and Abelson (1977).",
        "The expectation module takes two inputs and produces two outputs.",
        "The inputs are the user behavior graph discussed earlier, called the expected dialogue D, and the meaning of the most recently input sentence, M(S).",
        "Its outputs are a new expected dialogue D modified according to the latest input sentence M(S) and an expected sentence set E. These outputs are produced based upon the inputs and the functions Predicts, Merge able, and Merge.",
        "The role of the predicate Predicts can be best understood by recalling the function of the parser P. P uses the set of expected sentences E(current) to try to error correct the incoming sentence S. P may do this by discovering that some Mk in E(current) is quite similar to M(S).",
        "If P does select such an Mk and uses it to help parse S, then Predicts (Mk, M(S)) is true.",
        "Otherwise, Predicts (Mk, M(S)) is false.",
        "Thus the function of Predicts is to select the Mk which the parser used in parsing S. If the parser did not use expectation, then Predicts always is false.",
        "If the incoming sentence was not predicted by existing transitions in D, perhaps it can be found to be similar to some node Mk in D and a new transition could be added to that node.",
        "The routine Mergeable has the job of finding one or more such Mk's into which the current sentence meaning M(S) can be merged.",
        "The question of similarity of two sentences is determined by the meanings of the sentences themselves and the \"environment\" in which they occur in the dialogue.",
        "Sentence \"meanings\" are based on the sentence deep parses produced by the expectation parser, while a sentence \"environment\" is based on the meanings of the sentences preceding and following it in the expected dialogue.",
        "Similarity is based on the notion of \"distance\".",
        "Currently two sentences are considered similar in meaning if their parses differ in only one slot in the noun group template.",
        "This means that their noun group distance cannot be greater than one to be considered similar.",
        "For example, the following two sentences are similar: M(\"double the first row\") = double (r 1 ) M(\"double row 2\") = double (r2) The environment of one sentence matches that of another if the sentence meanings preceding the two sentences being compared are identical and/or the sentence meanings following them are identical.",
        "Clearly, these definitions are quite arbitrary and many other strategies could be tried.",
        "However, for the purposes of this study, they were quite satisfactory.",
        "Based on the question of how well the environment and the sentence itself matches previously seen environments and sentences, five different matches are possible between the current incoming sentence and the elements of the expected dialogue:",
        "Thus, if the sentences \"Double (r1)\" and \"Double (r2)\" are inputs to Merge, the output would be \"Double (rARG)\"."
      ]
    },
    {
      "heading": "6 EXPERIMENTAL RESULTS",
      "text": [
        "the expectation dialogue has a probability of being spoken next.",
        "IV) Totally-Ordered Schema with Arguments This test is an example of a totally-ordered schema, but the system does not know exactly what will be said all the time because one or more of the expected sentences contain an argument.",
        "Each of the four tests was run on three different test subjects to acquire data concerning how fast a user speaks, what types of errors are produced by the voice recognizer, and how well the expectation system acquires and uses the expected dialogue to help error correct the input.",
        "To begin the experiment session, the subject trained the voice recognizer, a NEC DP-200, on a specific vocabulary of 49 different words in connected speech mode.",
        "The DP-200 can handle only 150 word slots in connected speech mode, so 49 allowed for some repetitive training.",
        "The subject was then given a brief tutorial that lead him/her through a few features of the VNLCE system and gave him/her some practice in talking to the NEC device.",
        "This training session usually took a total of about 45 minutes.",
        "The subject was then given one or more of the test sheets representing the problems to be solved.",
        "The number was based on the amount of time that the subject was willing to donate to the effort.",
        "Each test dialogue had a similar overall structure in that it required a certain amount of repetition, thus creating a loop structure in the expected dialogue.",
        "In all tests, except test II, the subject was provided with the specific sequence of sentences to be spoken.",
        "This guaranteed that the desired level of repetition was actually achieved.",
        "How much repetition there was in each dialogue depended on the expected dialogue schema being imitated.",
        "In test I, which was done to demonstrate a totally-ordered schema, the test subject had to repeat an identical sequence of six sentences nine times in a row except for the seventh time when four new sentences were inserted into the loop.",
        "A sample schema can be seen in Figure 7.",
        "In test II, the user had much more freedom, since its purpose was to demonstrate a partially ordered schema.",
        "Here the subject had to solve six sets of simultaneous linear equations with two equations and two unknowns and he/she spoke whatever sentences that seemed appropriate.",
        "A sample schema is shown in Figure 8.",
        "Notice that in one case an argument was created.",
        "The third test was done to show how well error correction works when the dialogue seems random, creating a totally-unordered schema.",
        "To create such an environment, the user was asked to repeat four sentences in random order eight times.",
        "An example expected dialogue schema that resulted from this test is shown in Figure 9.",
        "In the last test, test IV, the subject was asked to repeat a sequence of four sentences six times, each time through changing the value of the row number spoken.",
        "This demonstrates the argument creation facility in a totally ordered dialogue schema.",
        "The expected dialogue generated from this test appears in Figure 10.",
        "Each test has associated with it three charts indicating the results.",
        "The first graph represents the average sentence error and correction rates, the second shows the average word error and correction rates, while the third illustrates the average rate-of-speech in words-per-second spoken by the subject while doing the experiment.",
        "The charts indicating the average error and correction rates of the four tests reflect the loop structure of the dialogues.",
        "Each chart is a series of bar graphs, each bar graph representing the average error and correction rates over the sentences spoken by the subjects in a particular loop of the dialogue.",
        "The highest point on each of these bars represents the raw error rate of the voice recognizer.",
        "The different markings within the bars themselves represent the percentage of the errors that were corrected by a particular facility of the expectation system.",
        "The horizontal design associated with \"loosening\" indicates the percentage of the errors that were corrected by the use of the flexible parsing techniques, such features as the synophones and the parser commands SKIP WORD, EXTRAWS, and LOSTWS.",
        "The vertical design associated with expectation indicates the percentage of the errors that were corrected by use of the expected sentence set alone.",
        "The blank area indicates the percentage of the errors that were corrected by using both of the above facilities.",
        "Finally, the dot design shows the percentage of the errors that were not corrected.",
        "Thus, for example, in the top chart in Figure 11, the eighth loop of the dialogue had an 85% sentence error rate from the voice recognizer.",
        "Of those errors, 6% were corrected using the facilities associated with loosening the search, while 25% were corrected by using only expectation.",
        "Another 63% were corrected using features from both categories.",
        "Only 6% could not be corrected.",
        "Test III demonstrates the error correction capabilities of the system when expectation only knows that one of a group of sentences will be said next.",
        "It produces a totally-unordered dialogue schema.",
        "The results of the systems error correction capabilities in such a situation appear in Figure 13.",
        "Test IV uses a totally-ordered dialogue schema, but with a variation from test I.",
        "Each sentence sooner or later contains an argument so that the system does not know everything about the sentence that will be said next.",
        "The data given in Figure 14 shows the error correction rates for this dialogue.",
        "It clearly shows how error correction failures increase until after the third loop when argument creation begins so that the system no longer error corrects incorrectly.",
        "Figure 15 shows the graphs of the average speech rate of the speakers for each of the four tests.",
        "Like the other eight graphs, these graphs reflect the loop structure of the dialogues.",
        "As can be seen, the speakers tended to increase their speech rate as they talked to the system.",
        "This behavior was hoped for because as the speech rate increased, so did the error rate of the speech recognizer, thus placing more of a burden on the error correcting abilities of the expectation system.",
        "Note that, in all eight graphs in Figures 11 through 14, the word and sentence error rates from the voice recognizer generally increased with the progress through the dialogue.",
        "This is due to the increased rate of speech.",
        "However, the actual failure rate of VNLCE did not increase by the same amount.",
        "These extra errors were corrected by the expectation system.",
        "Figure 16 gives a summary of the average error and correction rates for each test and over all."
      ]
    },
    {
      "heading": "7 RELATED LITERATURE",
      "text": [
        "A number of speech understanding systems have been developed during the past fifteen years (Barnett et al.",
        "1980, Dixon and Martin 1979, Erman et al.",
        "1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980).",
        "Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.",
        "While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",
        "A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).",
        "The problem of handling ill-formed input has been studied by Carbonell and Hayes (1983), Granger (1983), Jensen et al.",
        "(1983), Kwasny and Sondheimer (1981), Riesbeck and Schank (1976), Thompson (1980), Weischedel and Black (1980), and Weischedel and Sondheimer (1983).",
        "A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.",
        "However, these methodologies have not used historical information at the dialogue level as described here.",
        "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.",
        "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.",
        "Thus, an error in this work has no pattern but occurs probabilistically.",
        "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.",
        "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.",
        "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.",
        "Another dialogue acquisition system has been developed by Ho (1984).",
        "However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.",
        "The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.",
        "It self activates to bias recognition toward historically observed patterns but is not otherwise observable.",
        "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al.",
        "(1984).",
        "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969), assertional statements as in Michalski (1980), or semantic nets as in Winston (1975).",
        "That is, the current system learns procedures rather than data structures.",
        "There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al.",
        "(1984) and the PROLOG synthesis method of Shapiro (1982).",
        "However, the latter methodologies have not been applied to dialogue acquisition."
      ]
    },
    {
      "heading": "8 CONCLUSIONS AND AREAS FOR FUTURE RESEARCH",
      "text": [
        "We have shown that the ability to use expectation in the form of knowledge about the dialogue being spoken, as with humans, is a tremendous aid to speech recognition by computer.",
        "Since expectation, in this research, has been based on repetition of patterns, the expectation system's ability to correct varies, of course, with the repetitiveness of the dialogue itself.",
        "We have attempted, in sections 5 and 6, to justify this decision by demonstrating how the expectation system can acquire common programming constructs such as loops and arguments.",
        "It is our belief that repetitious patterns occur in everyday life, and that the expectation system is capable of dealing with such patterns, resulting in a generalized situation similar to a Schankian script.",
        "Finally, we have tested the expectation system's correction power in some representative situations, as discussed in section 6.",
        "It has been demonstrated that the expectation system has the capabilities of reducing a large sentence error rate to nearly zero in many situations.",
        "At the word level, error rates to the expectation system climbed as high as 47% in certain user dialogues when the user was speaking fast.",
        "At the same time, the error rate leaving the expectation system remained fairly low at between zero and fifteen percent.",
        "On the average, the system was able to lower a sentence error rate of 53% to 8%, and a word error rate of 13.5% to 2%.",
        "The use of expectation, along with an ability to ignore or add words to the input stream of the parser, is all that is needed to achieve this error correction rate on randomly erroneous input.",
        "The parser design, with the five choices at each word slot, has the potential to run into problems with the exponential growth of the search and to result in unacceptably long parse times.",
        "However, when the rating scheme is used intelligently, it not only aids in finding the best parse of a word sequence, but it also helps to lower the search time necessary by pruning unreasonable search choices.",
        "The average parse time for a sentence, from the tests discussed above, was 5.1 seconds while the average total processing time for a sentence was 10.5 seconds.",
        "This was on a highly loaded PDP 11/70 under the UNIX' operating system.",
        "In the event that a particular word sequence leads the parser down a garden path, a time out facility has been implemented that causes the parser to fail after one minute of real-time.",
        "However, out of a total of 629 sentences spoken in the above four tests, this feature was needed only 19 times.",
        "The research reported on here was divided into two parts, the theory and the implementation.",
        "Most of the theory developed was implemented in the VNLCE system.",
        "This theory has been aimed at error correction of random errors using expectation based on historical information.",
        "However, there are many possible extensions that could be examined in the future and added to the implementation if the investigation indicates that it would create a yet more usable system.",
        "These include the following: use of low level knowledge from the speech recognition phase, use of high level knowledge about the domain in particular and the dialogue task in general, a \"continue\" facility and an \"auto-loop\" facility as described by Biermann and Krishnaswamy (1976), a \"conditioning\" facility as described by Fink et al.",
        "(1985), implementation of new types of paraphrasing, checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen, and examining inter-speaker dialogue patterns.",
        "All but two of these areas for expansion are aimed at moving the expectation system from one that finds patterns in a user's dialogues and acquires historical knowledge about them to one that can acquire true procedures.",
        "The first two areas for expansion have nothing to do with creating a true procedure acquisition module but would be highly desirable from the point of view of the speech recognition application.",
        "Features three and four would simply make the system easier to use and would require little theoretical investigation.",
        "The final three would require research efforts.",
        "In conclusion, we have designed a system that is capable of correcting ill-formed input and implemented the Computational Linguistics, Volume 12, Number 1, January-March 198635 Pamela K. Fink and Alan W. BiermannThe Correction of III-Formed Input design in the area of speech recognition.",
        "The system performs error-correction through a mechanism also used by humans in the same situation, that of expectation.",
        "We have shown that the expectation algorithm is general enough to handle almost any dialogue structure.",
        "It is possible to predict approximately what kind of error correction to expect from the system based on the dialogue structure and the word error rate.",
        "We have also shown that the theory on which the implemented expectation system is based is capable of acquiring and generalizing real-world, script-like situations.",
        "This research can serve as a starting point for further research into the field of computer expectation, procedure acquisition, and learning."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": []
    }
  ]
}
