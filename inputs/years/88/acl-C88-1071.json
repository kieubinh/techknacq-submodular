{
  "info": {
    "authors": [
      "Roland Kuhn"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C88-1071",
    "title": "Speech Recognition and the Frequency of Recently Used Words: A Modified Markov Model for Natural Language",
    "url": "https://aclweb.org/anthology/C88-1071",
    "year": 1988
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Speech recognition systems incorporate a language model which, at each stage of the recognition task, assigns a probability of occurrence to each word in the vocabulary.",
        "A class of Markov language models identified by Jelinek has achieved consider able success in this domain.",
        "A modification of the Markov approach, which assigns higher probabilities to recently used words, is proposed and tested against a pure Markov model.",
        "Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Speech recognition systems consist of two components.",
        "An acoustic component matches the most recent acoustic input to words in its vocabulary, producing a list of the most plausible word candidates together with a probability for each.",
        "The second component, which incorporates a language model, utilizes the string of previously identified words to estimate for each word in the vocabulary the probability that it will occur next.",
        "Each word candidate originally selected by the acoustic component is thus associated with two probabilities, the first based on its resemblance to the observed signal and the second based on the linguistic plausibility of that word occurring immediately after the previously recognized words.",
        "Multiplication of these two probabilities produces an overall probability for each word candidate.",
        "Our work focuses on the language model incorporated in the second component.",
        "The language model we use is based on a class of Markov models identified by Jelinek, the \"n-gram\" and \"Mg-gram\" models [Jelinek 1985, 1983].",
        "These models, whose parameters are calculated from a large training text, produce a reasonable non-zero probability for every word in the vocabulary during every stage of the speech recognition task.",
        "Our model incorporates both a Markov 3g-gram component and an added \"cache\" component which tracks short-term fluctuations in word frequency.",
        "We adopted the hypothesis that a word used in the recent past is much more likely to be used soon than either its overall frequency in the language or a Markov model would suggest.",
        "The cache component of our model estimates the probability of a word from its recent frequency of use.",
        "The overall model uses a weighted average of the Markov and cache components in calculating word probabilities, where the relative weights assigned to each component depend on the part of speech (POS).",
        "For each POS, the overall model may therefore place more reliance on the cache component than on the Markov component, or vice versa; the relative weights are obtained empirically for each POS from a training text.",
        "This dependance on POS arises from the hypothesis that a content word, such as a particular noun or verb, will occur in bursts.",
        "Function words, on the other hand, would be spread more evenly across a text or a conversation; their short-term frequencies of use would vary less dramatically from their long-term frequencies.",
        "One of the aims of our research was to assess this hypothesis empirically.",
        "If it is correct, the relative weight calculated from the training text for the cache component for most content POSs will be higher than the cache weighting for most function POSs.",
        "We intend to compare the performance of a standard 3g-gram Markov model with that of our model (containing the same Markov model along with a cache component) in calculating the probability of 100 texts, each approximately 2000 words long.",
        "The texts are taken from the Lancaster-Oslo/Bergen (LOB) Corpus of modern English ]Johanson et al. 1986, 19821; the rest of the corpus is utilized as a training text which determines the parameters of both models.",
        "Comparison of the two sets of probabilities will allow one to assess the extent of improvement over the pure Markov model achieved by adding a cache component.",
        "Furthermore, the relative weights calculated from the training text for the two components of the combined model indicate those POSs for which short-term frequencies of word use differ drastically from long-term frequencies, and those for which word frequencies stay nearly constant over time."
      ]
    },
    {
      "heading": "2 A Natural Language Model with Markov and Cache Components",
      "text": [
        "The \"trigram \" Markov language model for speech recognition developed by F. Jelinek and his colleagues uses the context provided by the two preceding words to estimate the probability that the word Wi occurring at time i is a given vocabulary item W. Assume recursively that at time i we have just recognized the word sequence Ws, • • Wi--2, Wi--1.",
        "The trigram model approximates P(W1=W I We • • W;i-1) by f W1_2, Wi...1) where the frequencies f are calculated from a huge \"training text\" before the recognition task takes place.",
        "One adaptation of the trigram model employs trigrams of POSs to predict the POS of Wi , and frequency of words within each POS to predict IN; itself.",
        "Thus, this \"3g-gram\" model gives",
        "where we let P(Wi=W Ig(Wi) =g,i)",
        "Here G denotes the set of all parts of speech, 95 denotes a particular part of speech, and g ( Wi) denotes the part of speech category to which word Wi belongs (abbreviated to gi from now on); f denotes a frequency calculated from the training text.",
        "This \"3g-gram\" model was used by Derouault and Merialdo for French language modeling [Derouault and Merialdo 1986, 1984], and forms the Markov component of our own model.",
        "In practice many POS triplets will never appear in the training text but will appear during the recognition task, so Derouault and Merialdo use a weighted average of triplet and doublet POS frequencies plus a low arbitrary constant to prevent zero estimates for the probability of occurrence of a given POS :",
        "The parameters 11,12 are not constant but can be made to depend on the count of occurrences of the sequence or on the POS of the preceding word, In either ease these parameters must sum to 0.9999 and can be optimized iteratively; Derouault and Merialdo found that the two weighting methods performed equally well.",
        "The 3g-gram component of our model is almost identical to that of Derouault.",
        "and Merialdo, although the 153 POSs we use are those of the LOB Corpus.",
        "We let 11 and 12 depend on the preceding POS 9i-_l.",
        "The cache component keeps track of the recent frequencies of words within each POS; it assigns high probabilities to recently used words.",
        "Now, let Cj(W,i) denote the cache-based probability of word W at time i for POS gj .",
        "g ( W) o g• then Cj(W,i) at all times 1, i.e. if W does not belong to POS its cache-based probability for that POS is always 0.",
        "Similarly, let M•( W) denote the Markov probability due to the rest of the pure 3g-gram Markov model.",
        "This is approximated by Mi(W) f Ig(Wi) i.e, the frequency of word W among all words with POS = gi in the training text.",
        "The final, combined model is then P(Wi",
        "Here + kclj=-1; kmd denotaes the weighting given to the \"frequency within POS\" component and k01 the weighting of the \"cache-based probability\" component of POS g1.",
        "One would expect relatively \"insensitive\" POSs, whose constituent words do not vary much hi frequency over time, to have high values of km j and low values of Ic01; the reverse should be true for \"sensitive\" POSs.",
        "As is described in the next section, approximate values kej and kmd were determined empirically for two POSs yi to see if these expectations were correct.",
        "Them cache-based probabilities C')( W,i) were calculated as follows.",
        "For each POS, a \"cache\" (just a buffer) with.",
        "room for 200 words is maintained.",
        "Each new word is assigned to a single POS.",
        "g1 and pushed into the corresponding buffer.",
        "As soon as there are 5 words in a cache, it begins to output probabilities which correspond to the relative proportions of words it contains.",
        "The lower limit of 5 on the size of the cache before it !tarts producing probabilities, and the upper size limit of 200, are arbitrary; there are many possible heuristics for producing cache•based probabilities.",
        "3 implementation and Testing of the Combined Model"
      ]
    },
    {
      "heading": "3.1 The LOB Corpus",
      "text": [
        "The Lancaster-Oslo/Bergen Corpus of British English consists of 500 samples of about 2000 words each; each word in the corpus is tagged with exactly one of 153 POSs.",
        "The samples were extracted from texts published in Britain in 1961, and have been grouped by the LOB researchers into 15 categories spanning a wide range of English prose [Johansson et al. 1986, 1982].",
        "We split the tagged LOB Corpus into two unequal parts, one of which served as a training text for our models and the other of which was used to test and compare them.",
        "The comprehensiveness of the LOB Corpus made it an ideal training text and a tough test of the robustness of the language model.",
        "Furthermore, the fact that it has been tagged by an expert team of grammarians and lexicographers freed us from having to devise our own tagging procedure."
      ]
    },
    {
      "heading": "3.2 Parameter Calculation",
      "text": [
        "400 sample texts form the training text used for parameter calculation; the remaining 100 samples form a testing text used for testing and comparison of the pure 3g-gram model with the combined model.",
        "Samples were allocated to the training text and the testing text in a manner that ensured that each had similar proportions of samples belonging to the 15 categories identified by the LOB researchers.",
        "All parameters for both the pure 3g-grain model and the combined model were calculated from the 400-sample training text.",
        "The two models share a POS prediction component which is estimated by the Derouault-Merialdo method.",
        "Triplet and doublet POS frequencies were obtained from 75% (300 of the 400 samples) of the training text; the remaining 25% (100 samples) gave the weights, /1(gi_1) and 12(g,_4), needed for smoothing between these two frequencies.",
        "These were computed iteratively using the Forward-Backward algorithm ( Deronault and Merialdo [DSC Rabiner and Juang [1986]).",
        "Now die pure 3g-gram model is complete - it remains to find kmj arid Iced for the combined model.",
        "This can be calculated by means of the Forward-Backward method from the 400 samples."
      ]
    },
    {
      "heading": "3.3 Testing the Combined Model",
      "text": [
        "As demribecl in 4.2, 80% of the LOB Corpus is used to find the best-fit parameters for a. the pure 3g-gram model h. the combined model, made up of the 3g-gram model plus a cache component.",
        "These two models will then be tested on the remaining 20% of the LOB Corpus as follows.",
        "Each is given this portion of the LOB Corpus word by word, calculating the probability of each word as it goes along.",
        "The probability of this sequence of about 200,000 words as estimated by either model is simply .the product of the individual word, probabilities as estimated by that model.",
        "The higher this overall probability, the better the model.",
        "Thus the o'erall probability calculated for the pure) 3g-gram trilodel and for the combined model; the increase achieved by the latter over the former, is the measure of the improvement due to addition of the cache'conaponent.",
        "Note that in order to calculate word probabilities, both models must have guessed the POSs of the two preceding words.",
        "Thus every word encountered must be assigned a P05.",
        "There are three cases : a).",
        "the word did not occur in the tagged training text and therefore is not in the vocabulary; b).",
        "the word was in the training text, and had the sane tag wherever it occurred; c).",
        "the word was in the training text, and had more than one tag (e.g. the word 'light\" might have been tagged as a noun, verb, and adjective).",
        "The heuristics employed to assign tags were as follows : a).",
        "in this case, the two previous POSs are substituted in the Derouault-Merialdo weighted-average formula and the program tries all 153 possible tags to find the one that maximizes the probability given by the formula.",
        "b).",
        "in this case, there is no choice; the tag chosen is the unique tag associated with the word in the training text.",
        "c).",
        "when the word has two or more possible tags, the tag chosen is the one which makes the largest contribution to the word's probability (i.e. which gives rise to the largest component in the summation on pg.",
        "1).",
        "Thus, although the portion of the LOB Corpus used for testing is tagged, these tags were not employed in the implementation of either model; in both cases the heuristics given above guessed POSs.",
        "A separate part of the program compared actual tags with guessed ones in order to collect statistics on the performance of these heuristics."
      ]
    },
    {
      "heading": "4 Preliminary Results",
      "text": [
        "1.",
        "The first results of our calculations are the values /i(gi_i) and 12(m_1) , obtained iteratively to optimize the weighting between the POS triplet frequency I (gi and the POS doublet frequency f (gi I _i) in the estimation of P(gi=gi (m-2,gi_1).",
        "As one might expect, /i(gi_i) tends to be high relative to 12(8i_1) when g4_1 occurs often, because the ' triplet frequency is quite reliable in this case.",
        "For instance, the most frequent tag in the LOB Corpus is \"NN\", singular common noun; we have 11(NN) 0.61 .",
        "The tag \"MO\", attached only to the word \"having\", is fairly rare; we have li(HVG) 0.13 .",
        "However, there are other factors to consider.",
        "Derouault and Merialdo state that for equal to an article, 11 was relatively low because we need not know the POS.",
        "to predict that g; is a noun or adjective.",
        "Thus doublet frequencies alone were quite reliable in this case.",
        "On the other hand, when is a negation, knowing gi.4 was very important in making a prediction of , because of French phrases like \"il ice vent\" and \"je ne veux\".",
        "Our results from English texts show somewhat different patterns.",
        "The tag \"AT\" for singular articles had an llthat was neither high nor low, 0.47 .",
        "The tag \"CC\" for coordinating conjunctions, including \"not\", had a high 11 value, 0.80 .",
        "Adjectives (\"Jr) and adverbs (\"RB\") had II values even higher than one would expect on the basis of their high frequencies of occurrence : 0.90 and 0.86 respectively.",
        "2.",
        "We collected statistics on the success rate of the pure Markov component in guessing the POS of the latest word (using the tag actually assigned the word in the LOB Corpus as the criterion).",
        "This rate has a powerful impact on the performance of both models, especially the one with a cache component; each incorrectly guessed POS leads to looking in the wrong cache and thus to a cache-based probability of 0.",
        "We are particularly interested in forming an idea of how fast this success rate will increase as we increase the size of the training text.",
        "Of the words that had occurred at least once in the training text, 83.9 % had tags that were guessed correctly (16.1 % incorrectly).",
        "Words that never occurred in the training text were assigned the correct tag only 22 % of the time (78 % incorrect).",
        "Apparently the information contained in the counts of POS triplets, doublets, and singlets is a good POS predictor when combined with some knowledge of the possible tags a word may have, but not nearly as good on its own.",
        "Among the words that appeared at least once in the training text, a surprisingly high proportion - 42.8 % - had more than one possible POS.",
        "Of these, 66.7 % had POSs that were guessed correctly, Thus it might appear that performance is degraded when the program must make a choice between *Bible tags.",
        "This analysis is a faulty.",
        "a given word might have",
        "many POSs, and perhaps the correct one was not found in the training text at all.",
        "The most important statistic , therefore, is the proportion of words in the testing text whose tag was guessed correctly among the words that had also appeared with the correct tag in the training text.",
        "This proportion is 94.0 %.",
        "It seems reasonable to regard this as being an indication of the upper limit for the success rate of POS prediction with training texts of manageable size; it provides an estimate of the success rate when the two main sources of error ( words found in the testing text but not the training text, words found in both texts which are tagged in the testing text with a POS not attached to them anywhere in the training text ) are eliminated.",
        "3.",
        "We have not yet tested the full combined model ( with a cache component and a Markov component ) against the 3g-gram Markov model.",
        "However, we have examined the effect on the predictive power of the Markov model of including cache components for two POSs : singular common noun ( label \"NN\" in the LOB Corpus ) and preposition ( label \"IN\" in the LOB Corpus ).",
        "These two were chosen because they occur with high frequency in the Corpus, in which there are 148,759 occurrences of \"NN' and 123,440 occurrences of \"IN\", and because \"NN\" is a content word category and \"IN\" a function word category.",
        "Thus they provide a means of testing the hypothesis outlined in the Introduction, that a cache component will increase predictive power for content POSs but not make much difference for function POSs.",
        "For both POSs, the expectation that the 200-word cache will often contain the current word was abundantly fulfilled.",
        "On average, if the current word was an NN-word, it was stored in the NN cache 25.8 % of the time; if it was an IN-word, it was stored in the IN cache 64.7 % of the time.",
        "The latter is no surprise - there are relatively few different prepositions - but the former figure is remarkably high, given the large number of different nouns.",
        "Note that the figure would be higher if we counted plurals as variants of the singular word ( as we may do in future implementations ).",
        "We have not yet obtained the best-fit weighting for the combined model.",
        "However, we tried 3 different combinations for the NN-words and the IN-words.",
        "If \"a\" is the weight for the cache component and \"b\" the weight for the Markov component, the 3 combinations (a, b) are (0.2, 0.8), (0.5, 0.5), and (0.9, 0.1); the pure Markov model corresponds to the weighting (0.0, 1.0).",
        "To assess the performance of each combination for NN-words and IN-words, we calculated i), the log product of the estimated probabilities for NN-words only under each of the 4 formulas ii).",
        "the log product of the estimated probabilities for IN words only under each of the 4 formulas.",
        "It is then straightforward to calculate the improvement per word obtained by using a cache instead of the pure Markov model.",
        "For NN-words, the (0.2, 0.8) weighting yielded an average multiple of 2.3 in the estimated probability of a word in the testing text over the probability as calculated by the pure Markov model ; the (0.5, 0.5) weighting yielded a multiple of 2.0 per word, and the (0.9, 0.1) actually decreased the probability by a factor of 1.5 per word.",
        "For IN-words, the (0.2, 0.8) weighting gave an average multiple of 5.1, the (0.5, 0.5) weighting a multiple of 7.5 and the (0.9, 0.1) weighting a multiple of 6.2 ."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "The preliminary results listed above seem to confirm our hypothesis that recently-used words have a higher probability of occurrence than the 3g-gram model would predict.",
        "Surprisingly, if the above comparison of the POS categories \"NN\" and \"IN\" is a reliable guide, this increased probability is more dramatic in the case of content-word categories.",
        "Perhaps the smaller number of different prepositions makes the cache-based probabilities more reliable in this case.",
        "Since the cost of maintaining a 200-word cache, in terms of memory and time, is modest, and the increase in predictive power can be great, the approach outlined above should be considered as a simple way of improving on the performance of a 3g-gram language model for speech recognition.",
        "If memory is limited, one would be wise to create caches only for POSs that occur with high frequency and ignore other POSs.",
        "Our immediate goal is to build caches for a larger number of POSs, and to obtain the best-fit weighting for each of them, in order to test the full power of the combined model.",
        "Eventually, we may explore the possibility of ignoring variations in the exact form of a word, merging the singular form of a noun with its plural, and different tenses and persons of a verb.",
        "This line of research has more general implications.",
        "The results above seem to suggest that at a given time, a human being works with only a small fraction of his vocabulary.",
        "Perhaps if we followed an individual's written or spoken use of language through the course of a day, it would consist largely of time spent in language \"islands\" or sublanguages, with brief periods of time during which he is in transition between islands.",
        "One might attempt to chart these \"islands\" by identifying groups of words which often occur together in the language.",
        "If this work is ever carried out on a large scale, it could lead to pseudo-semantic language models for speech recognition, since the occurrence of several words characteristic of an \"island\" makes the appearance of all words in that island more probable."
      ]
    },
    {
      "heading": "Bibliography",
      "text": []
    }
  ]
}
