{
  "info": {
    "authors": [
      "Douglas B. Paul"
    ],
    "book": "Workshop on Speech and Natural Language",
    "id": "acl-H89-1024",
    "title": "The LINCOLN Continuous Speech Recognition System: Recent Developments and Results",
    "url": "https://aclweb.org/anthology/H89-1024",
    "year": 1989
  },
  "references": [],
  "sections": [
    {
      "heading": "THE LINCOLN CONTINUOUS SPEECH RECOGNITION SYSTEM: RECENT DEVELOPMENTS AND RESULTS'",
      "text": []
    },
    {
      "heading": "ABSTRACT",
      "text": [
        "The Lincoln stress-resistant HMM CSR has been extended to large vocabulary continuous speech for both speaker-dependent (SD) and speaker-independent (SI) tasks.",
        "Performance on the DARPA Resource Management task (991 word vocabulary, perplexity 60 word-pair grammar) [1] is 3.4% word error rate for SD training of word-context-dependent triphone models and 12.6% word error rate for SI training of (word-context-free) tied mixture triphone models."
      ]
    },
    {
      "heading": "INTRODUCTION",
      "text": [
        "Our earlier development efforts [2,3,4,5,6,7,8,9] centered on improving the SD speaker-stress robustness for both IWR and CSR tasks.",
        "Since our IWR database included a normal speech test section, we were able to determine that our enhancements for robustness also improved performance for normally spoken speech (0 errors/1680 test tokens, 105 word vocabulary, multi-style training).",
        "An independent test on the TI-20 word database [10] confirmed this normal speech performance with 3 errors out of 5120 test tokens on our first run on this database and no errors after a small amount of development [3].",
        "Our robust CSR database was not useful for determining the large vocabulary normal speech performance.",
        "In order to work on a large vocabulary normal speech CSR task, we switched to the DARPA Resource Management database [1].",
        "The SD portion of this database has 12 speakers with 600 training sentences and 100 development test sentences per speaker.",
        "This provided a total of 1,200 test sentences containing 10,242 words.",
        "For SI work we used the same development test sentences, but trained on 2,880 sentences from 72 speakers from the SI training portion of the database.",
        "(There was an overlap of 8 speakers between the SI and SD training sets making the total of 80 speakers reported in [1].)",
        "When additional SI training data was needed, we added the designated \"SI development test\" data, again avoiding test speaker overlaps, to the designated SI training data.",
        "This provided a total 3,990 training sentences from 109 speakers.",
        "The vocabulary of the Resource Management database is 991 words.",
        "There is also an \"official\" word-pair recognition grammar [11].",
        "This grammar is just a list of allowable word pairs without probabilities for the purpose of reducing the recognition perplexity to about 60.",
        "(Including the probabilities slightly more than halves the perplexity.)",
        "later system may enjoy an advantage over an earlier system due to the training to the test set.",
        "The results provided below will be identified according to which test set was used: June 88 or development test.",
        "Error rates for these systems will be quoted as \"% word error rate\" in the text.",
        "This number is:",
        "algorithm are performed using monophone (context-free phone) models from a uniform initial state.",
        "This, in effect, automatically marks the data.",
        "The monophone models are then used to provide initial values for the (single Gaussian) triphone models and a few more iterations are performed.",
        "If mixtures are to be used, minor random perturbations of the single Gaussian mean vectors are used to initialize the Gaussian mixtures and a few final iterations are performed.",
        "During recognition, the system extrapolates (guesses based upon a linear combination of the available triphones) the triphones which were not observed during training.",
        "The recognition environment is modeled by adaptive background states.",
        "In order to control the relative number of word insertions and deletions, the likelihood is multiplied by a penalty for each word.",
        "A Viterbi beam search using a finite state grammar with optional interword silences produces the recognized output."
      ]
    },
    {
      "heading": "RESULTS OF THE JUNE 88 TEST SYSTEM",
      "text": [
        "1.",
        "Three or more phones: each word end has a fan of initial (final) phones.",
        "2.",
        "Two phones: each word end has a list of initial and final phones with a crossbar of interconnections between them.",
        "3.",
        "One phone: a crossbar between beginnings and endings with a triphone on each link.",
        "Links between two adjacent words are formed according to the following priority list: 1.",
        "Both boundary triphones exist: link them.",
        "2.",
        "Only one of the boundary triphones exist: link to a WCF triphone on the other word.",
        "3.",
        "Neither boundary triphone exists: link WCF boundary triphones from both words.",
        "Thus, as more word boundaries are observed in the training data, the system gradually builds from the original WCF system toward a system with full word context models.",
        "The SD development test results for this system showed a significant improvement over the WCF system: 3.39% versus 5.19% word error rate.",
        "An earlier system which extrapolated all missing boundary triphones rather the defaulting to WCF triphones did not show an improvement.",
        "Thus it is better to use observed WCF triphones rather than extrapolate boundary triphones.",
        "The SI results were worse than the WCF system, both with and without the additional training data.",
        "The word-context-dependent system appears to be too detailed a model for the available SI training data."
      ]
    },
    {
      "heading": "VARIABLE MIXTURES",
      "text": [
        "Variable order mixtures show a small improvement for the SI task.",
        "The number of mixtures for the states in a triphone was chosen by:",
        "min(n, sqrtRnumber of instances of triphone in data)]) This attempts to match the complexity of the distribution to the amount of available training data.",
        "It has been tested for n = 4 and n = 8 with both the normal and augmented training sets.",
        "The results are in Table 3.",
        "The variable mixtures show an improvement for n = 4 for the standard training and n = 8 for the augmented training.",
        "These results show that the basic idea improves performance but the function is not optimum for choosing the mixture order.",
        "If the function were optimum, the best results would be obtained for any large n."
      ]
    },
    {
      "heading": "TIED MIXTURES",
      "text": [
        "A version of tied mixtures [15,16] has been tested and shown to provide a small improvement for the SI task.",
        "In this system, each monophone group is given a set of Gaussians.",
        "All triphones of each monophone group use mixtures chosen from the same set of Gaussians.",
        "The mixture weights for each triphone are independent of all other triphones.",
        "This reduces the total number of Gaussians by a significant factor.",
        "Training is again performed using a bootstrapping procedure.",
        "After the monophones are trained, small random perturbations of their mean vectors are used to initialize the mixture Gaussians for the monophone group.",
        "The triphone weights, along with the parameters of the Gaussians, are then trained with a number of iterations of the Baum-Welch algorithm.",
        "The recognizer used here is the simpler WCF system.",
        "Three SI systems were tried using 10, 20, and 40 Gaussians per monophone group.",
        "Only the 40 system showed an improvement over the original SI system: 12.62% versus 13.25% development test word error rate.",
        "This system also reduced the number of Gaussians by a factor of five.",
        "Tied mixtures have not been tried on the SD task."
      ]
    },
    {
      "heading": "SPEAKER GROUPING",
      "text": [
        "Another approach to improving the SI (WCF) performance was tried.",
        "The training speakers were segregated by sex and two separate sets of models were trained.",
        "The recognizer kept the sets of models separate by using two separate networks.",
        "Thus, the system co-recognizes both the speech and the sex of the speaker.",
        "Systems which lump both sexes together in training do not discriminate against cross-group spectral matches of individual sounds.",
        "Mixtures were not used to save CPU time.",
        "The results shown in Table 4 show a significant increase in the error rate."
      ]
    },
    {
      "heading": "DISCUSSION AND CONCLUSIONS",
      "text": [
        "Word context modeling reduced the word error rate of the SD system by 35%.",
        "This significantly increased both the number of triphones and the complexity of the recognizer.",
        "Fully, 39% of the triphones occurred only once in the training data compared to 19% for the WCF system.",
        "However, since the system is speaker dependent and (almost) only Gaussian means were being trained, the system was able to improve in spite of the limited training data.",
        "Word-context modeling did not help the SI system, probably due to insufficient training data.",
        "Thirty-two percent of the triphones (mostly word boundary), in contrast to 3.7% for the WCF system, occurred only one or two times in the training data.",
        "This is not sufficient to train an SI system.",
        "Since many of the word-context-dependent triphones were adapted to only one or two speakers, more damage than good was done.",
        "The larger SI training data set reduced the number of single and double occurrence triphones to 22%, which helped, but was not enough to overcome the problem.",
        "Variable order mixtures (WCF) improved the SI results by matching the complexity of the distributions to the amount of training data.",
        "This approach required essentially no increase in the complexity of the trainer.",
        "The (WCF) tied mixture system achieved a small improvement over the June 88 SI system.",
        "This was probably due to the high-order mixtures of the shared Gaussians.",
        "This allowed more detailed modeling where there was sufficient training data while allowing the system to automatically reduce its degrees of freedom where there was insufficient training data by placing very low or zero weights on unneeded mixture components.",
        "Training, however, requires so much computation that it will hamper exploration of this class of system.",
        "Grouping the speakers (WCF) yielded sufficiently poor results that mixtures were not tested.",
        "The recognizer did appear to correctly identify the sex of the speaker.",
        "The reduction in performance may be due to an effect similar to multi-style training [5] which may be enhanced by mixing the sexes during training."
      ]
    }
  ]
}
