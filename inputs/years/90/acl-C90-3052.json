{
  "info": {
    "authors": [
      "Martin C. Emele",
      "Remi Zajac"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C90-3052",
    "title": "Typed Unification Grammars",
    "url": "https://aclweb.org/anthology/C90-3052",
    "year": 1990
  },
  "references": [
    "acl-C86-1071",
    "acl-E85-1016",
    "acl-E89-1009",
    "acl-E89-1024",
    "acl-P87-1032",
    "acl-P89-1001"
  ],
  "sections": [
    {
      "text": [
        "Keplerstraf3e 17, ll â€“ 7000 Stuttgart 1, Federal Republic of Germany femele,zajac)Ois.informatik.uni-stuttgart.dhp.de"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "We introduce TI'S, a computer formalism in the class of logic formalisms which integrates a powerful type system.",
        "Its basic data structures are typed feature structures.",
        "The type system encourages an object-oriented approach to linguistic description by providing a multiple inheritance mechanism and an inference mechanism winch allows the specification of relations between levels of linguistic description defined as classes of objects.",
        "We illustrate this approach starting from a very simple DCG, and show how to make use of the typing system to enforce general constraints and modularize linguistic descriptions, and how further abstraction leads to a HPSG-like gram"
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Various proposals have been made for the integration of type information in unification-based grammar formalisms to enforce constraints described in a hierarchical way where types are partially ordered with a subtype relation.",
        "Authors describe these extensions as \"inheritance grammars\", \"inheritance networks\" , \"feature sorts\", \"typed feature structures\" ,... [1, 3, 5, 13, 17, 15, 9, 11, 7, 8].",
        "These formalisms exhibit, to various degrees, one or several of the following properties, characteristic of the so-called object-oriented paradigm: a high level of abstraction, a capacity of inference, modularity and distributed control.",
        "Abstraction and modularity are needed when the linguist wants to describe a hierarchy of concepts (like a lexical hierarchy or the hierarchy of phrasal categories), and to describe linguistic data at different levels (e.g. morphology, syntax, semantics).",
        "At first glance it seems rather natural to develop separate modules for different linguistic levels, and to describe separately their interactions; however, great difficulties are encountered when these modules have to be integrated.",
        "Usually, there are two choices.",
        "Either everything is described in a single place using a deeply intricate data structure, like packing both syntactic and semantic equations in CF rules in some LFG extensions (e.g. [10]); the price is a loss in understandability and generality.",
        "Or descriptions are kept separate and the processing is done accordingly: first, a morphological phase, then a syntactic analysis, and then a semantic analysis, without any communication between these different steps [4].",
        "The price is that interdependent constraints between these levels are lost, resulting in inadequate linguistic description or very complex control strategies at the implementation level.",
        "In this paper, we argue that typed unification grammars give the linguist a formal framework which has the desirable properties.",
        "We will give an introduction to such a formalism, called TFS (Typed Feature Structure), which integrates disjunctions, conjunctions and conditional expressions of typed feature structures.",
        "This introduction will start from a very simple DCG, and will show how one can write a DCG-like grammar in TFS, making use of the typing system to enforce general constraints valid for classes of objects and to modularize linguistic descriptions.",
        "We then show that further abstraction leads to a HPSG-like grammar.",
        "It is not our goal to give here a formal account of the formalism (the interested reader should refer to [2] where a very clear formal semantics on which TES is based is given), and we will use an informal approach wherever possible."
      ]
    },
    {
      "heading": "2 Typed feature structures and unification",
      "text": [
        "The basic data structure of the language is a typed feature structure: a feature structure (FS in the following) with which a type can be associated.",
        "Compared to untyped FSs (as presented in [16] for example), the TES system offers the possibility to name complex FSs, and to associate constraints with these names, thus defining a type.",
        "We write feature names in small caps letters (F, G, H), type symbols in upper case letters (A, B), and we use symbols inside a box W, called tags, for denoting shared values.",
        "For example, the typed I'S, written in a linear form A[F: [1:1B[H: C], a: [1]], is an FS of type A with two features F and G, F having as a value the typed FS B[u: A] and G having the same shared value as P. In the system, one can specify type definitions which can, as a first approximation, he seen as a kind of template definition like in e.g. PATR-II.",
        "There is, however, a major difference.",
        "The system uses a type inference mechanism to derive new types dynamically during computation whereas templates in PATR-II are expanded statically at compile time.",
        "A type that encodes agreement features can be written: AGR = [num: NUM,gender: GEN] and types NUM and GEN being themselves defined as NUM = SING V PLUR (where the symbol \"v\" denotes the logical OR) and GEN = MASC V FEM V NEU.",
        "The types NUM, SG,... do not have definitions: they are called atomic types.",
        "AGR, NUM and GEN are called complex types.",
        "From a set of type definitions, one can extract a partial order on type symbols.",
        "For example, from the *Research reported in this paper is partly supported by the German Ministry of Research and Technology (BMFT, Bun-desminister far Forschung mid Technologic), under grant No.",
        "08 B3116 3.",
        "The views and conclusions contained herein are those of the authors and should not be interpreted as representing official policies.",
        "set of definitions above, we can derive the following partial order on type symbols (Fig.1) where T represents the greatest element (no information) and 1 the smallest element (inconsistent information, leading to failure in unification).",
        "This partial order is in turn used to derive a lattice of type symbols, which is then extended to typed FSs ordered by (typed) sub-sumption, forming a lattice on which the interpreter works (see a formal account in [2])."
      ]
    },
    {
      "heading": "SING PLUR MASC FEM NEU",
      "text": [
        "For example, the FS fl AGR[num: NUM] subsumes the FS t2 AGR[num: PLUR, gender: FEM] because fp, has more specific information than fl : no gender is specified in fl, and the number value of f2 PLUR is more specific than the number value of fl, NUM.",
        "Typed unification proceeds as ordinary unification for FSs, recursively combining substructures at the same paths.",
        "When two (typed) FSs are unified, first the type symbols are unified, and if this unification succeeds, the FSs are unified.",
        "Unification of two types x and Y is defined as the (set of) most general type(s) which is smaller than both x and Y: it is the greatest lower bound (OLD) of these two symbols in the lattice of type symbols.",
        "If two types are directly comparable, the smallest is the result of the unification: NUM fl PLUR = PLUR.",
        "This extension is consistent with the definition of the unifier of two FSs as the GILD of these structures (see, for example, [16]).",
        "3 Feature types as data types and feature types as relations"
      ]
    },
    {
      "heading": "3.1 The LIST type as a data type",
      "text": [
        "A list of words will be defined in a LISP-like fashion as either the END of a list or a CONS with two attributes first and rest:",
        "WORD denotes the set of word forms, an the list.",
        "of words \"John likes Mary\" will be encoded as"
      ]
    },
    {
      "heading": "FIRST: JOHN",
      "text": []
    },
    {
      "heading": "FIRST: CONS LIKES R REST: CONS [FinsT:: M END AR/ nrsT",
      "text": [
        "which is a well-fori led list with respect to the LIST definition.",
        "(We shall use in the following t more concise syntax for lists: END will be written as 0; CONS FinsT:WORD, 'IF:Tr:LIST] will be written as (WORD .",
        "LIST); lists will be written using the usual abbreviation for lists: the list of words \"John likes Mary\" will then be written as (JOHN LIKES MARY))."
      ]
    },
    {
      "heading": "3.2 The APPEND type as a relation",
      "text": [
        "One can also understand feature types as relations much like those in PROLOG.",
        "Let us recall the classical PROLOG definition of append: append( 0 ,L,L).",
        "append( DE I Ll] ,2 , EX I L3]) append(L1 , L2 , L3) .",
        "In PROLOG, the arguments of a term are identified by their positions in the term, and the presence",
        "of all arguments is mandatory.",
        "In an FS, arguments (feature values) are not identified by their position but.",
        "by a label, the feature, and the absence of an attribute-value pair will denote any kind of value for this attribute (type T).",
        "Using the TFS syntax, where the symbol ':-' after an FS introduces a condition, a definition for append can he as follows:",
        "Note that the tagging syntax allows to specification of identity between structures and a partial instance of the structure.",
        "This possibility (together with the fact that typing is enforced by the system) allows the writing of a typed version of append, in contrast to the untyped PROLOG version.",
        "3.3 Type checking as deduction Contrary to PROLOG, there is no distinction in TFS between top-level types (which could be interpreted as predicates) and inner types (which could be interpreted as arguments): they are all typed FSs, and the same deduction mechanism applies for the top-level structure as well as for all substructures.",
        "A (typed) FS is consistent with respect to a set of type definitions if it unifies with the definition of its type, and if each of its substructures is also consistent.",
        "Conditions like in the definition of append above introduce additional constraints which are erased after having been successfully evaluated.",
        "When a type is defined as a disjunction, a structure has to be consistent with at least one element of the disjunction (but all possibilities are explored, creating as many possible solutions as there are disjuncts).",
        "When a type is defined as a conjunction (using the AND operator noted \"A\"), a structure has to be consistent with every single element of the conjunction.",
        "The order used for type checking (roughly top-down) guarantees that the solution the system finds is the OLD of the set of definitions augmented by the initial structure [2].",
        "For example, the (typed) FS AGR[num:PLUR] is consistent with regard to the set of definitions above (Sect.1).",
        "The interpreter will apply the definition of AGR at the root of the FS : AGR[num:PLUR] fl num:NUM,gender:GEN] AGR num:PLUR,gender:GEN AGRinum:MASCI is an inconsistent (typed) FS : AGR num:MASC fl [num:NUM,gender:GEN] 1 because the types MASC and NUM have only I, the bottom of the lattice, as a common subtype representing inconsistent information.",
        "Note that this type checking process may introduce new type symbols also used for checking, thus defining a type inheritance mechanism.",
        "A full evaluation of APPEND[w:(A B)] produces a set of three FSs:",
        "F: 0, 13: tm (A B), W: NI ] V F:(D A .",
        "0), B: Q(B), w: ( F: (IE A .",
        "(fl 13)), [3: Id 0, w: ("
      ]
    },
    {
      "heading": "4 Typed unification grammars",
      "text": []
    },
    {
      "heading": "4.1 DCGs",
      "text": [
        "In this section, we describe how one can (hut should not) write grammars using this formalism.",
        "To make comparisons easier, we will start from the small example of DCG presented in [Pereira and Warren 80] and show how this grammar (Fig.2) can he written in TFS.",
        "is_determiner(all, plural).",
        "is_noun(man, singular, ma is_noun(rnen, plural, man).",
        "is_name(mary).",
        "is_trans(likes,singular, like) is_trans(like, plural, like).",
        "In a specification like this, there are three different kinds of information mixed together.",
        "Take for example the rule \"noun_phrase(Num, np(Det, Noun)) determiner(Num, Det), noun(Num, Noun)\".",
        "In this rule we find:",
        "1. a specification of a set of well-formed sub-strings using the CF skeleton: noun_phrase determiner, noun; 2. a specification of well-formed (partial) syntactic structures: the structure np(Det, Noun) is well-formed if Det and Noun are a well-formed structure and if its agreement value (variable Num) is the same for the Det, the Noun, and the noun_plirase; 3. a specification of a relation between well-formed (partial) syntactic structures and well-formed substrings by augmenting the CF skeleton with annotations representing those structures."
      ]
    },
    {
      "heading": "4.2 A TFS specification",
      "text": [
        "All this information mixed together can be separated out and specified in a more modular way.",
        "1.",
        "The set of well-formed strings of words is defined as in Sect.2.1, where WORD = allVmen... 2.",
        "The set of well-formed partial syntactic structures, i.e. every syntactic constraint like agree-rnent or sub categorisation, should be expressed in this part of the specification.",
        "3.",
        "The relation between strings and structures should be stated independently of well-formedness conditions on syntactic structures.",
        "It is expressed here in CF manner by using the APPEND relation on strings.",
        "(However, we do not advocate the exclusive use of CF-like relations; more complex ones can be specified to gain expressive power, e.g. by incorporating linear precedence rules)."
      ]
    },
    {
      "heading": "4.3 Parsing and generation",
      "text": [
        "Both parsing and generation in the system amount to type inference.",
        "Either (1) for parsing or (2) generation yield the same result (3).",
        "(1) SENTENCE[sTRING: (Mary likes all men)] (2) SENTENCE C-STR: S [ NP: NP[NAME: MARY] [v: LIKE VP: VP NP: [DET: ALL, NOUN: MAN (3) SENTENCE STRING: (n Mary ri likes IN all IS men) C-STR: S -NP: NP[NAME: MARY[woRD: ], AGR: OSG] VP: VP"
      ]
    },
    {
      "heading": "AGR: I:I",
      "text": [
        "This shows that the formalism has the same power as PROLOG to synthesize unspecified arguments, and the same evaluation mechanism can be used for both generation and parsing, depending on the input.",
        "= FP - (7) SYN: HEAD[ [HEAD: Iheadli DTRS: [HEAD-DTR: [SYN: [HEAD: Ila]li1 4.4 From DCG to HPSG",
        "In the following, we explain how one can generalize the principles used for describing a DCG grammar in TES to write an HPSG-like grammar.",
        "HPSG linguistic objects of all kinds, be they syntactic, phrase-structural, or semantic, are modeled by feature structures [14].",
        "In addition, HPSG relies heavily on the notion of type.",
        "Hence, TES is perfectly suited for an implementation of HPSG.",
        "The grammar itself is purely declarative in the sense that it characterizes what constraints should hold on linguistic objects independently of the order in which these constraints are actually applied.",
        "We first generalize the description of linguistic structures: instead of defining explicit types for sentences, noun phrases, etc., we define a generic constituent structure for any kind of phrase.",
        "According to the specification of IIPSG linguistic objects, we define SIGNs as being either of type PHRASALSIGN or of type LEXICALSIGN [15].",
        "A SIGN has a phonological value, represented as a list of words, and syntactic and semantic information (omitted for this comparison).",
        "The subtypes PHRASAL_SIGN and LEXICAL-SIGN inherit all the attributes and type restrictions of SIGN.",
        "(4) SIGN = PHRASALSIGN V LEXICALSIGN) A PHON: LIST_OF_STRINGS syN: CATEGORY"
      ]
    },
    {
      "heading": "SEM: SEMANTIC OBJECT",
      "text": [
        "PHRASAL_SIGNs (5) differ from LEXICAL_SIGNs (6) by having an additional dtrs (\"daughters\") attribute that gives information about the (lexical or phrasal) signs which are their immediate constituents.",
        "This attribute encodes the kind of information about constituency conventionally described as constituent structures.",
        "In addition, the various daughters are distinguished according to what kinds of information they contribute to the sign as a whole.",
        "Thus, daughters are classified as heads and complements as in the standard X-bar theory.",
        "In order to be a well formed object of type PHRASAL_SIGN, a linguistic object has to obey some general principles such as the \"Head Feature Principle\" and the \"Subcategorization Feature Principle\".",
        "(5) phrasal_sign = (HEAD-FP A SUBCAT_FP A .",
        ".",
        ".",
        "A (CH_CO_FP V HC*_CO_FP ...)) A DTRS: C_STRUCTURE [ HEAD-DTR: SIGN COMP-DTRS: LIST_OF_SIGNS] (6) lexical_sign = VERB V PNOUN V NOUN V DET V ..",
        "General principles The \"Head Feature Princi-ple\" ensures that the head features of the head-daughter always be shared with their phrasal projections.",
        "It generalizes the passing of agreement information from e.g. a verb to the VP for all kind of constituent and for all information related to agreement and subcategorization.",
        "In the DCG example, subcategorization was expressed by introducing different kinds of lexical categories like transitive verb (TV) vs. intransitive verbs",
        "(Iv).",
        "In HPSG, subcategorization is expressed by using a list of signs.",
        "This list specifies the number and kind of signs that the head subcategorizes for the formation of a complete sign.",
        "Subcategorization information is described in lexical entries.",
        "The \"Subcat Feature Principle\" ensures that in any phrasal sign, the subcat list of the head-daughter is the concatenation of the list of complement daughters and the subcat list of the mother.",
        "(The order of the elements in the complements list does not reflect the surface order but rather the more abstract \"obliqueness hi-erarchy\" ([14] Chap.7)).",
        "Grammar rules Just as we have generalized the notion of constituency, we are also able to generalize the relations between phonological representations and their desired constituent structure representations.",
        "The specialized CF-like relations for a sentence, a noun phrase, and so on in the DCG example can be replaced by two more general rules which specify constituent structure configurations according to the X-bar theory.",
        "The \"Complement Head Constituent Order Feature Principle\" (9) simply states that a \"saturated phrasal sign\" (i.e. with [syn.",
        ": [subcat: ()]]) is the combination of an unsaturated phrasal head with one phrasal complement (e.g. S NP VP)."
      ]
    },
    {
      "heading": "W: phon",
      "text": [
        "The \"Head Complements Constituent Order Feature Principle\" (13) states that an \"unsaturated phrasal sign\" is the combination of a lexical head and any number of complements (e.g. VP â€“ 4 V XP*).",
        "The relation ORDER_COMPL is used for specifying the ordering of the phonological values of all complements.",
        "The phonological value of the whole phrase can then be specified as the concatenation of the head phonology value with the complement phonology value.",
        "Either (10) for parsing or (11) generation, the evaluation yields the same fully specified sign (12)."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "The main characteristics of the formalism we presented are (1) type inheritance which provides a clean way of defining classes and subclasses of objects, and (2) an evaluation mechanism based on typed unification which provides a very powerful and semantically clear means of specifying and computing relations between classes of objects.",
        "The possibility of defining types as (conditional) expressions of typed FSs encourages a very different approach to grammar specification than integrated CU' based approaches like DCC or LEG: the grammar writer has to define the set of linguistic objects relevant for the problem, define the possible relations between these objects, and specify explicitly the constraints between objects and relations.",
        "The TES system has been implemented in Common-Lisp and has been tested on Symbolics, TI Explorer, VAX and Allegro Common-Lisp.",
        "Sample grammars have been developed([6], [18]) in order to demonstrate the feasibility of the approach.",
        "Acknowledgments The current system is based on a previous implementation carried out by the authors at ATR, Kyoto, as a part of a visiting research program.",
        "We would like to thank Dr. Akira Kure-matsu, president of ATR Interpreting Telephony Research Laboratories for making our stay possible, and Mr. Teruaki Aizawa, head of the Natural",
        "Language Understanding Department for his constant support.",
        "We owe many clarifications to Sondra Ahlen with whom we had many lively discussions.",
        "This paper has benefited from many comments from our collegues at the IMS of the University of Stuttgart."
      ]
    },
    {
      "heading": "'eferences",
      "text": []
    }
  ]
}
