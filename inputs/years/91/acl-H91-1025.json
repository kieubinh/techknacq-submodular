{
  "info": {
    "authors": [
      "Peter F. Brown",
      "Stephen A. Della Pietra",
      "Vincent J. Della Pietra",
      "Robert L. Mercer"
    ],
    "book": "Workshop on Speech and Natural Language",
    "id": "acl-H91-1025",
    "title": "A Statistical Approach to Sense Disambiguation in Machine Translation",
    "url": "https://aclweb.org/anthology/H91-1025",
    "year": 1991
  },
  "references": [
    "acl-P91-1034"
  ],
  "sections": [
    {
      "heading": "ABSTRACT",
      "text": [
        "We describe a statistical technique for assigning senses to words.",
        "An instance of a word is assigned a sense by asking a question about the context in which the word appears.",
        "The question is constructed to have high mutual information with the word's translations."
      ]
    },
    {
      "heading": "INTRODUCTION",
      "text": [
        "An alluring aspect of the statistical approach to machine translation rejuvenated by Brown, et al., [1] is the systematic framework it provides for attacking the problem of lexical disambiguation.",
        "For example, the system they describe translates the French sentence Je vais prendre la decision as I will make the decision, thereby correctly interpreting prendre a.s make.",
        "The statistical translation model, which supplies English.",
        "translations of French words, prefers the more common translation take, but the trigra.m language model recognizes that the three-word sequence make the decision is much more probable than take the decision..",
        "The system is not always so successful.",
        "It incorrectly renders Je vais prendre ma propre decision a,s I will take my own decision.",
        "Here, the language model does not realize that take my own decision is improbable because take and decision no longer fall within a, single trigram.",
        "Errors such as this are common because our statistical models only capture local phenomena; if the context necessary to determine a translation falls outside the scope of our models, the word is likely to be translated incorrectly.",
        "However, if the relevant, context is encoded locally, the word should be translated correctly.",
        "We can achieve this within the traditional paradigm of analysis – transfer – synthesis by incorporating into the analysis phase a sense-disambiguation.",
        "coin penent that assigns sense labels to French words.",
        "1.1 prendre is labeled with one sense in the context of decision but with a. different sense in other contexts, then the translation model will learn from training data that the first sense usually translates to make, whereas the other sense usually translates to take.",
        "In this paper, we describe a statistical procedure for constructing a. sense-disambiguation component that label words so as to elucidate their translations."
      ]
    },
    {
      "heading": "STATISTICAL TRANSLATION",
      "text": [
        "As described by Brown, et al.",
        "[1], in the statistical approach to translation, one chooses for the translation of a. French sentence F, that English sentence E which has the greatest probability, Pr (El F), according to a model of the translation process.",
        "By Ba.yes' rule, Pr (El F) Pr (.0 Pr (FIE) / Pr (F).",
        "Since the denominator does not depend.",
        "on E, the sentence for which Pr (ELF) is greatest is also the sentence for which the product Pr (E) Pr (FIE) is greatest.",
        "The first term in this product is a statistical characterization of the English language and the second term is a statistical characterization of the process by which English sentences are translated into French.",
        "We can compute neither of these probabilities precisely.",
        "Rather, in statistical translation, we employ a. language model P,„,,dei(E) which.",
        "provides a.n estimate of Pr (E) and a translation, model which provides an estimate of Pr ( PIE).",
        "The performance of the system depends on the extent to which these statistical models approximate the actual probabilities.",
        "A useful gauge of this is the cross entropy 1 H(E I F) E – >2 Pr (E, F) log Prnod,i(E F) (I) E,F which measures the average uncertainty that the model has about the English translation E of a French sentence F. A better model has less uncertainty and thus a lower cross entropy.",
        "A shortcoming of the architecture described above is that it requires the statistical models to deal directly with English and French sentences.",
        "Clearly the probability distributions Pr (E) and Pr (F E) over sentences are immensely complicated.",
        "On the other hand, in practice the statistical models must be rela,tively simple in order that their parameters can be reliably estimated from a manageable amount of training data.",
        "This usually means that they are restricted to the modeling of local linguistic phenomena.",
        "As a result, the estimates .1),flodej(E) and Pmodo(F E) will be inaccurate.",
        "This difficulty can be addressed by integrating statistical models into the traditional machine translation architecture of analysis-transfer-synthesis.",
        "The resulting system employs",
        "1.",
        "An analysis component which encodes a French sentence F into an intermediate structure F\".",
        "2.",
        "A statistical transfer component which translates F' a corresponding intermediate English structure E'.",
        "This component incorporates a. language model, a. translation model, and a decoder as before, but here these components deal with the intermediate structures rather than the sentences directly.",
        "3.",
        "A synthesis component which reconstructs an English sentence E from E'.",
        "For statistical modeling we require that the synthesis transformation E be invertible.",
        "Typically, analysis and synthesis will involve a sequence of successive transformations in which F' is incrementally 'In this equation and in the remainder of the paper, we use bold face letters (e.g. E) for random variables and roman letters (e.g. E) for the values of random variables.",
        "constructed from F, or E is incrementally recovered.",
        "from E'.",
        "The purpose of analysis and synthesis is to facilitate the task of statistical transfer.",
        "This will be the case if the probability distribution Pr (E', F') is easier to model then the original distribution Pr (E, F).",
        "In practice this means that E' and F' should encode global linguistic facts about E and F in a local form.",
        "The utility of the analysis and synthesis transformations can be measured.",
        "in terms of cross-entropy.",
        "Thus transformations F F' and E' E are useful if we can construct models P`modd(F' E') and P',„„d,i(E') such that 1(E' F') < H(E F)."
      ]
    },
    {
      "heading": "SENSE DISAMBIGUATION",
      "text": [
        "In this paper we present a statistical method for automatically constructing analysis and synthesis transformations which perform cross-lingual word-sense labeling.",
        "The goal of such transformations is to label the words of a. French sentence so as to elucidate their English translations, and, conversely, to label the words of an English sentence so as to elucidate their French translations.",
        "For example, in some contexts the French verb prendre translates as to take, but in other contexts it translates as to make.",
        "A sense disambiguation transformation, by examining the contexts, might la.bel occurrences of prendre that likely mean to take with one label, and other occurrences of prendre with another label.",
        "Then the uncertainty in the translation of prendre given the label would be less than the uncertainty in the translation of prendre without the label.",
        "Although the label does not provide a.ny information that is not already present in the context, it encodes this information locally.",
        "Thus a local statistical model for the transfer of labeled sentences should be more accurate than one for the transfer of unlabeled ones.",
        "While the translation of a. word depends on many words in its context, we can often obtain information by looking at only a single word.",
        "For example, in the sentence le vais prendre ma propre decision (I will make my own decision), the verb prendre should be translated a.s make because its object is decision.",
        "If we replace decision by voiture then prendre should be translated a.s take: Je vais prendre ma, propre voiture (I will take my own car).",
        "Thus we can reduce the uncertainity in the translation of prendre by asking a question about its object, which is often the first noun",
        "to its right, and we might assign a sense to prendre based upon the answer to this question.",
        "In.",
        "II doute que les nOtres gagnent (lie doubts that we will win), the word it should be translated as he.",
        "On the other hand, if we replace doute by faut then it should be translated as it: Il faut que les notres gagnent (It is necessary that we win).",
        "Here, we might assign a sense label to it by asking about the identity of the first verb to its right.",
        "These examples motivate a sense-labeling scheme in which the label of a word is determined by a question about an informant word in its context.",
        "In the first example, the informant of prendre is the first noun to the right; in the second example, the informant of it is the first verb to the right.",
        "If we want to assign n senses to a word then we can consider a question with n answers.",
        "We can fit this scheme into the framework of the previous section as follows: The Intermediate Structures.",
        "The intermediate structures E' and F' consist of sequences of words labeled by their senses.",
        "Thus F' is a sentence over the expanded vocabulary whose 'words' f' are pairs (f, l) where f is a word in the original French vocabulary and 1 is its sense label.",
        "Similarly, F' is a sentence over the expanded vocabulary whose words e' are pairs (e,l) where e is an.",
        "English word and 1 is its sense label.",
        "The analysis and synthesis transformations.",
        "For each French word and each English word we choose an informant site, such as first noun to the left, and an n-a.ry question about the value of the informant at that site.",
        "The analysis transformation F F' and the inverse synthesis transformation E i E' map a sentence to the intermediate structure in which each word is labeled by a sense determined by the question about its informant.",
        "The synthesis transformation E' 1-4 E maps a. labeled sentence to a sentence in which the labels have been removed.",
        "The probability models.",
        "We use the translation model that was discussed in [1] for both P I E') and for P,-,,,,,dd(F I E).",
        "We use a. trigram language model [1] for P„,„,id(E) and",
        "to choose for each English and French word an informant and a. question.",
        "As suggested in the previous section, a, criterion for doing this is that of minimizing the cross entropy 14E' F').",
        "In the remainder of the paper we present a.n algorithm for doing this."
      ]
    },
    {
      "heading": "THE TRANSLATION MODEL",
      "text": [
        "We begin by reviewing our statistical model for the translation of a sentence from one language to another [1].",
        "In statistical French to English translation system we need to model transformations from English sentences E to French sentences F, or from intermediate English structures E' to intermediate French structures F'.",
        "However, it is clarifying to consider transformations from an arbitrary source language to an arbitrary target language."
      ]
    },
    {
      "heading": "Review of the Model",
      "text": [
        "The purpose of a translation model is to compute the probability P modei(T S) of transforming a source sentence S into a. target sentence T. For our simple model, we assume that each word of S independently produces zero or more words from the target vocabulary and that these words are then ordered to produce T. We use the term alignment to refer to an association between words in T and words in S. The probability P mod el(T I 8) is the sum of the probabilities of all possible alignments A between S and T",
        "here .i,t(t) is the word of S aligned with t in the alignment A, and A (s) is the number of words of T aligned with sin A.",
        "The distortion model Pdi.,t„tion describes the ordering of the words of T. We will not give it explicitly.",
        "The parameters in (3) a,re",
        "1.",
        "The probabilities p(n I s) that a, word s in the • source language generates 7?, target words; 2.",
        "The probabilities p(t I s) that s generates the word t;",
        "In order to construct these transformations we need 3.",
        "The parameters of the distortion model.",
        "We determine values for these parameters using maximum likelihood training.",
        "Thus we collect a large bilingual corpus consisting of pairs of sentences (S,T) which are translations of one another, and we seek parameter valves that maximize the likelihood of this training data as computed by the model.",
        "This is equivalent to minimizing the cross entropy"
      ]
    },
    {
      "heading": "The Viterbi Approximation",
      "text": [
        "The sum over alignments in (2) is too expensive to compute directly since the number of alignments increases exponentially with sentence length.",
        "It is useful to approximate this sum by the single term corresponding to the alignment, A.",
        "(S,T), with greatest probability.",
        "We refer to this approximation as the Viterbi approximation and to A(S,T) as the Viterbi alignment.",
        "Let c(s, t) be the expected number of times that s is aligned with t in the Viterbi alignment of a pair of sentences drawn at random from the training data.. Let c(s, n) be the expected number of times that s is aligned with n words.",
        "Then",
        "where c(s,t I A) is the number of times that s is aligned with t in the alignment A, and c(s, n I A) is the number of times that s generates n target words in A.",
        "It can be shown [2] that these counts are also averages with respect to the model S,T By normalizing the counts c(s, t) and c(s, n) we obtain probability distributions p(s, t) and p(s, n) 2",
        "The conditional distributions p(t I s) and p(n J s) are the Viterbi a.pproxima.tion estimates for the parameters of the model.",
        "The marginals satisfy",
        "where u(s) and u(t) are the unigram distributions of s and t and ii(s) = En p(n J s) n is the average number of target words aligned with s. These formulae reflect the fact that in any alignment each target word is aligned with exactly one source word."
      ]
    },
    {
      "heading": "CROSS ENTROPY",
      "text": [
        "In this section we express the cross entropies 11 ( S J T ) and MS' J T') in terms of the information between source and target words.",
        "In the Viterbi approximation the cross entropy",
        "where LT is the average length of the target sentences in the training data, and H(t J s) and H(n I s) are the conditional entropies for the probability distributions p(s, t) and p(t, s) :",
        "We want a similar expression for the cross entropy 1[(S I T).",
        "Since",
        "this cross entropy depends on both the translation model, I'modet(T I S), and the language model, P - model(S)• We now show that with a suitable additional approximation",
        "use the generic symbol noirm to denote a normalizing factor that converts counts to probabilities.",
        "We let the actual value of noirn., be implicit from the context.",
        "Thus, for example, in the left hand equation of (7), the normalizing factor is norm = c(s, t) which equals the average length of target sentences.",
        "In the right hand equation of (7), the normalizing factor is the average length of source sentences.",
        "where H(S) is the cross entropy of P model(S) and I(s, t) is the mutual information between t and s for the probability distribution p(s, t).",
        "The additional approximation that we require is H(T) LTII(t)= – p(t) log p(t) (12) where p(t) is the marginal of p(s, t).",
        "This amounts to approximating P modei(T) by the unigram distribution that is closest to it in cross entropy.",
        "Granting this, formula (11) is a consequence of (9) and of the identities"
      ]
    },
    {
      "heading": "Target Questions",
      "text": [
        "For sensing target sentences, a question about an informant is a function a from the target vocabulary into the set of possible senses.",
        "If the informant of t is x , then t is assigned the sense a(x).",
        "We want to choose the function (x) to minimize the cross entropy H(S I T').",
        "From formula (14), we see that this is equivalent to maximizing the conditional mutual information I(s, t' t) between s and t'",
        "T' be sense labeling transformations of the type discussed in Section 2.",
        "Assume that these transformations preserve Viterbi alignments; that is, if the words s and t are aligned in the Viterbi alignment for (5,T), then their sensed versions s' and t` are aligned in the Viterbi alignment for (S' ,T').",
        "It follows that the word translation probabilities obtained from the Viterbi alignments satisfy p(s, t) = EvEt P(S) t') EstG, p(s', t) where the sums range over the sensed versions t' of t and the sensed versions s' of s. By applying (11) to the cross entropies If (5 I T), H(S I T'), and H(S' I T), it is not hard to verify that",
        "Here I(s, t' I t) is the conditional mutual information given a target word t between its translations s and its sensed versions t'; (t ,s' I s) is the conditional mutual information given a source word s between its translations t and its sensed versions s'; and /(n, s' I s) is the conditional mutual information given s between n and its sensed versions s'."
      ]
    },
    {
      "heading": "SELECTING QUESTIONS",
      "text": [
        "We now present a.n algorithm for finding good informants and questions for sensing.",
        "An exhaustive search for the best a requires a computation that is exponential in the number of values of x and is not practical.",
        "In previous work [3] we found a good r using the flip-flop algorithm [4], which is only applicable if the number of senses is restricted to two.",
        "Since then, we have developed a different algorithm that can be used to find a for any number of senses.",
        "The algorithm uses the technique of alternating minimization, and is similar to the k-means algorithm for determining pattern clusters and to the generalized Lloyd algorithm for designing vector quantitizers.",
        "A discussion of alternating minimization, together with references, can be found in Chou [5].",
        "The algorithm is based on the fact that, up to a constant independent of a, the mutual information I(s, t' I t) can be expressed as an infimum over conditional probability distributions q(s I C))",
        "The best value of the information is thus an infimum over both the choice for C' and the choice for the q.",
        "This suggests the following iterative procedure for obtaining a good a':"
      ]
    },
    {
      "heading": "Source Questions",
      "text": [
        "For sensing source sentences, a question about an informant is a function C' from the source vocabulary into the set of possible senses.",
        "We want to chose e: to minimize the entropy H(S' I T).",
        "From (114) this is equivalent to maximizing the sum",
        "and we can again find a good C' by alternating minimization."
      ]
    },
    {
      "heading": "CONCLUSION",
      "text": [
        "In this paper we presented a general framework for integrating analysis and synthesis with statistical translation, and within this framework we investigated cross-lingual sense labeling.",
        "We gave an algorithm for automatically constructing a simple labeling transformation that assigns a. sense to a word by asking a question about a single word of the context..",
        "In a companion paper [3] we present results of translation experiments using a sense-labeling component that employs a similar algorithm.",
        "We are currently studying the automatic construction of more complex transformations which utilize more detailed contextual information."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": []
    }
  ]
}
