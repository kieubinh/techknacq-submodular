{
  "info": {
    "authors": [
      "Renato de Mori",
      "Roland Kuhn"
    ],
    "book": "Workshop on Speech and Natural Language",
    "id": "acl-H91-1043",
    "title": "Some Results on Stochastic Language Modelling",
    "url": "https://aclweb.org/anthology/H91-1043",
    "year": 1991
  },
  "references": [
    "acl-H89-1043"
  ],
  "sections": [
    {
      "heading": "ABSTRACT",
      "text": [
        "The paper will discuss three issues.",
        "The first is the derivation of precise probability scores for partial hypotheses containing islands, in the context of a Stochastic-Context-Free-Grammar (SCFG) for Language Modeling (LM).",
        "The second issue is the possibility of adding a cache component to a LM.",
        "This component alters the expected probability of words to reflect the speaker's patterns of word use.",
        "Finally, the idiosyncratic properties of dialogue are being studied; this work will indicate how knowledge about the discourse state can be incorporated into the LM and into the semantic component."
      ]
    },
    {
      "heading": "ISLAND-DRIVEN PARSING",
      "text": []
    },
    {
      "heading": "Language Modeling and Theories with Islands",
      "text": [
        "Automatic Speech Understanding (ASU) is based on a search process that generates partial interpretations of a spoken sentence called theories; theories are scored on the basis of a likelihood L = O(Pr(A th) Pr(th)).",
        "We are interested in the computation of Pr(th) when this a partial interpretation of a spoken sentence generated by a Stochastic Context-Free Grammar (SCFG) G,.",
        "A recent report [2] reviews this problem and gives interesting results.",
        "The most popular parsers used in Automatic Speech Recognition (ASR) generate new theories in a left-to-right fashion.",
        "To score the theories generated by these parsers, the probability of all parse trees generating the first p words of a sentence must be computed; the appropriate algorithms are given in [8].",
        "Parsers that are \"island-driven\" proceed outward in both directions from islands of words that have been hypothesized with high acoustic evidence.",
        "Interesting island-driven parsers have been proposed by [13], [12], [6] who have also discussed the motivations for considering these parsers for ASU.",
        "None of these parsers uses a stochastic grammar.",
        "If island-driven parsers are used for generating partial interpretations of a spoken sentence, it is important to compute Pr(th), which is the probability that a SCFG generates sequences of words intermixed with gaps corresponding to portions of the acoustic signal that are still uninterpreted.",
        "Recent work provides a precise theoretical framework for this computation [3].",
        "Many different cases involving islands and gaps have been examined; space considerations do not permit us to give here the lengthy formulas obtained for each of these cases.",
        "Instead, this paper will list the cases along with the worst-case time complexity of the computation of Pr(th) for each.",
        "Perhaps the most striking result of this work was the sharp division between the cases where one must compute the probability that a partial tree generates substrings of a sentence intermixed with a gap of unknown length, and the cases where the gap has a known length.",
        "The former computation appears to have an unacceptable time complexity; the latter computation is quite tractable.",
        "For this reason a later section considers ways in which one might estimate the length of a gap."
      ]
    },
    {
      "heading": "Definitions",
      "text": [
        "A SCFG is a quadruple G, = (N, E, P, S), where N is a finite set of nontermina/ symbols, E is a finite set of terminal symbols disjoint from N, P is a finite set of productions of the form H a, HEN, a E (E U N)*, and S E N is a special symbol called start symbol.",
        "Each production is associated with a probability, indicated with Pr(H a).",
        "If the grammar is proper the following relation holds:",
        "An SCFG G, is in Chomsky Normal Form (CNF) if all productions in G, are in one of the following forms: H FG H H, F,G N, w E. (2) In the following we will always refer to SCFGs in CNF.",
        "In the adopted formalism u, v and t represent strings of already recognized terminals; j and 1 are position indices;",
        "p, q and r are shift indices; m indicates a (known) gap length and k, h are used as running indices.",
        "Furthermore, z(m) stands for a gap of unknown terminals with specified length while a gap of unknown terminals with unknown length is represented by z(*).",
        "Finally, E* represents the set of all strings of finite length over E, while En', m > 0 is the set of all strings in E\" of length m. The derivation of a string in G, is usually represented as a parse (or derivation) tree, which shows the rules employed.",
        "It is also possible to associate with each derivation tree the probability that it was generated from a nonterminal symbol H by the grammar Ga.",
        "This probability is the product of the probabilities of all the rules employed in the derivation.",
        "Given a string x E Es, the notation H< z >, H E N, indicates the set of all trees with root H generated by G, and spanning z.",
        "Therefore Pr(H < x >) is the sum of the probabilities of these subtrees, i.e. the probability that the string z has been generated by Ge starting from symbol H. We assume that the grammar G, is consistent.",
        "This means that the following condition holds:",
        "From this hypothesis it follows that a similar condition holds for all nonterminals.",
        "We are concerned with the computation of probabilities of strings involving islands.",
        "The assumed model of computation is the Random Access Machine, taken under the uniform cost criterion (see [1]).",
        "We will indicate with 1/1 the size of set P, i.e. the number of productions in G1.",
        "We will also write f(z) = 0(g(x)) whenever there exist constants c, a > 0 such that f(x) > c g(z) for every x > E. In the following section, we give the worst-case time complexity results we have derived."
      ]
    },
    {
      "heading": "Complexity Results",
      "text": [
        "First, consider the computation of the probability that a given nonterminal H generates a tree whose yield is the string uz(*)ity(*), where u = wi wi+p and v = are two already recognized substrings, while z(*) and y(*) represent two unspecified length gaps, i.e. two not yet specified strings of terminal symbols that can be generated in those positions by Ga.",
        "Such a probability will be indicated by Pr(H < uz(*)vy(*) >).",
        "For H = S, this probability gives the syntactical plausibility of the partial theory uz(*)vy(*), which may be used for computing hypothesis scores in search of the most plausible interpretation of a spoken sentence.",
        "The asterisk indicates that nothing is known about gap z.",
        "We have determined that calculation of such island probabilities with unknown gap length requires solving a rather huge non-linear system of 1N1(q + 1)2 equations, q being the length of the island.",
        "If an approximate solution is of any interest, such a system can be rendered linear and can be solved by means of the computation of an INI(q + 1)2 x INI (4 + 1)3 inverse square matrix; this takes an 0(INI3e) amount of time.",
        "For practical values of N and q the required computational effort seems unaffordable.",
        "Tables 1, 2, and 3 list the remaining cases that have been examined, along with the worst-case time complexity of calculating each probability given a known SCFG G,.",
        "Table 1 is self-explanatory.",
        "Table 2 deals with a problem of great practical interest - the computation of a theory that has been obtained from a previous theory by means of a single-word extension.",
        "In these cases the only calculation required concerns the new terms whose introduction is due to the added word.",
        "Table 3 shows the complexity of additional computation when not the theory, but the gap, is extended by one term.",
        "Since suffixes and prefixes are symmetric, the tables show only one of two symmetric cases (results still valid if strings are reversed).",
        "The computations shown in Table 3 are particularly worth studying because we do not know exactly the number of words filling the gap but often know a probability distribution for this quantity; hence we have to take into account more than one value for the gap length.",
        "Rows 3 and 5 in Table 3 show that a one-unit extension of a gap within a string costs a cubic amount of time (on top of work already done).",
        "If it is possible to get bounds on the number of (possible) words in a gap, this extra work will be repeated a fixed (in practical cases small) number of times."
      ]
    },
    {
      "heading": "Island-Driven Parsing Strategies",
      "text": [
        "Given a method for scoring partial sentence interpretations in ASU systems, how can the method be utilized?",
        "This section discusses how the computations listed previously support island-driven bidirectional strategies for ASU.",
        "In speech recognition and speech understanding tasks, partial theories are created and a strategy is used to select the most probable theory (theories) for growing.",
        "The score of a theory th can be expressed as:",
        "A parsing strategy can be considered that starts from left to right generating a sequence of word hypotheses u; subsequently, syntactic or semantic predictions generate a sequence v. An upper bound for Pr(Aluz(*)vy(*)) can be obtained by running the Viterbi algorithm using a model for a, followed",
        "by a looped model of the lexicon (or the phonemes) for z(*), followed by a model for v and by a looped model for y(*).",
        "Starting from th, a theory can grow by trying to fill the gap z(*) with a sequence of words.",
        "The hypotheses used for filling the gap may have one word, two words, three words, etc.. For each size of the gap an upper bound of the probability coming from the language model is Pr(uz(m)vy(*)).",
        "Reasonable assumptions about possible values of m can be obtained if suprasegmental acoustic cues such as energy contour descriptors are available.",
        "Based on a string A9 describing these features in the gap, it is possible to express the probability Pr(A,Im) of observing A9 given a gap of m words as follows:",
        "where at, indicates the number of syllables in the gap, and Pr(A9 I n, = a) denotes the a-priori probability of observing A9 given that there are a syllables in the gap.",
        "It is reasonable to assume that this probability is a good approximation of the probability of observing A9 given that there are a syllables and m words in the gap.",
        "Pr(n, = a m) is the probability that a string of m words is made up of a syllables, and it can be estimated from a written text.",
        "The limits and Amax are chosen in such a way that Pr(n, = a I m) < e for a < amin and a > sm.",
        ":, so that they depend on m and on the language model, but not on the input string and can be computed off-line.",
        "Thanks to (5) it is possible to delimit practical values between which m can vary.",
        "Let mi and ana be the lowest and the highest value for m. An upper bound for the probability of the language model relative to theory \"th\" can be expressed as:",
        "We are mainly interested in ASU systems performing sentence interpretation in restricted domains.",
        "In this kind of task, non-syntactic information is usually available to predict words on the basis of previously obtained partial interpretations of the uttered sentence.",
        "Predicted words may be \"islands\" in the sense that they do not follow an existing partial theory in a strictly left-to-right manner.",
        "The acoustic evidence of these islands can be evaluated using word-spotting techniques.",
        "For these situations, island-driven parsers can be used.",
        "These parsers produce partial parses in which sequences of hypothesized words can be interleaved by gaps, making theories of the kind listed in the previous section (whose probabilities are calculated as described in [3]).",
        "The same methods permit assessment of word candidates adjacent to an already recognized string - i.e., computation of the probability that the first (last) word of the gap al (z,,,,) is a certain a E E. This new word will extend the current theory.",
        "Normally, the system would select the word candidate(s) which maximize the prefix-string-with-gap probability of the theory augmented with it.",
        "Instead of computing these probabilities for all the elements in the dictionary, it is possible to restrict such an expensive process to the preterminal symbols (as in [8]).",
        "The approach discussed here should be compared with standard lattice parsing techniques, where no restriction is imposed by the parser on the word search space (see, for example [4] and the discussion in [11]).",
        "Our framework accounts for bidirectional expansion of partial analyses; this improves the predictive capabilities of the system.",
        "In fact, bidirectional strategies can be used in restricting the syntactic search space for gaps surrounded by two partial analyses.",
        "This idea has been discussed without reference to stochastic grammars in [12] for the case of one word length gaps.",
        "We propose a generalization torn-length gaps and to cases where partial analyses do not represent entire parse trees but partial derivation trees.",
        "A fair comparison between island-driven and left-to-right theory growing in stochastic parsing is not possible at present.",
        "In practice, island-driven parsers may remarkably accelerate the theory-growing process if island predictions are made by a lookahead mechanism that leads to a correct partial theory with a limited number of competitors and if a limited number of predictions can be made for the words that can fill the gap."
      ]
    },
    {
      "heading": "HEURISTICS FOR IMPROVED LANGUAGE MODELING",
      "text": [
        "The domains of discourse into which we might wish to introduce speech recognition systems vary widely.",
        "Often, the way in which human beings employ speech within a given domain has idiosyncracies which should be incorporated into the probabilistic language model, because they greatly increase its predictive power.",
        "In this section, we discuss two heuristics which may improve language modeling in specific situations.",
        "Adding a Cache Component to a Standard Language Model Our work on cache-based language modeling began with the simple observation that a given speaker or writer is likely",
        "to use the same words repeatedly, and gave rise to a heuristic which greatly improved the performance of a standard probabilistic language model (the 3g-gram model).",
        "This heuristic is likely to be useful in any context where a speaker interacts with the speech recognition system for some length of time (the longer the interaction continues, the more accurate the system's estimate of the speaker's characteristic word use frequencies).",
        "Dictation systems are an obvious application, as is any interactive system in which the interaction is prolonged, or where the same people use the system repeatedly.",
        "This section summarizes our work on cache components; see [9] for more details.",
        "Since the cache is superimposed on a standard language model (we used the 3g-gram model) we will begin with an overview of such models.",
        "Consider the most straightforward kind of language modeling, where the task of the LM is to determine Pr(th) = Pr(wi...w.).",
        "The trigram model [7] approximates this by setting Pr(wiltot – wi-t) = Pr(wilwi-2wi-1), which can be estimated from a training text.",
        "Thus we have The 3g-gram model uses grammatical parts of speech (POS).",
        "Let g(wi) = g, denote the POS of the word that appears at time i.",
        "Based on git and gi_2, one part of the model gives the probability that gi is a noun or a verb or an article, etc: Pr(g; = X) = Pr(gi Xigi_3,gi_i).",
        "Another part gives the probability of a particular word if the POS is known.",
        "Both parts are estimated from frequencies in training texts.",
        "Thus, for a word W that has only one possible POS, g(W), the probability Pr(wi = W) is estimated by Our contention was that while someone is speaking, a word used in the immediate past is very likely to be used again - much more likely than would be predicted by either of the models just described.",
        "We believed that these short-term word frequency fluctuations depend on the POS.",
        "Therefore, we used the 3g-gram model along with a cache component as the basis for a combined model which could weight the short-term cache component heavily for some POSs and not for others.",
        "The relative weights assigned to the cache and 3g-gram components within each POS category were obtained by maximum-likelihood estimation.",
        "To assess the improvement achieved by incorporating the cache component, we ran the combined model and a pure 3g-gram model (both trained on exactly the same data) on a text and compared the perplexities obtained.",
        "The combined model gives a probability to each POS in the same way as the 3g-gram model.",
        "For a fixed POS, the probability of any word W which belongs to it is a weighted average of W's frequency in that POS category in the training text - the 3g-gram component - and W's frequency in the cache belonging to the POS category - the cache component.",
        "During the speech recognition task, the cache for a POS will contain the last N words which were guessed to have that POS (we set N to 200).",
        "If a word has occurred often in the recent past, it will occur many times in the cache for its POS.",
        "Let CAW, i) be the cache-based probability estimate for word W at time i for POS gi.",
        "This is calculated from the frequency of W among the N=200 most recent words belonging to POS gi.",
        "Our combined model estimates Pr(wi = W1g(W)) by kmdf(wi = Wig(W)) -1-kc,iCi(W,i), where km,; kci = 1, instead of by AIN = Wig(W)) alone.",
        "The POS component Pr(gi = g(W)Igi-2,gi-i) of the combined model was estimated as described in [5].",
        "To train and test the pure 3g-gram model and the combined model, we utilized different portions of the LOB Corpus of British English: 100 sample texts drawn from this corpus form the testing text.",
        "The results exceeded our ex",
        "• pectations.",
        "On the testing text, the pure 3g-gram model gave a perplexity of 332; the combined model gave a perplexity of 107.",
        "Thus, the incorporation of a cache component yielded a 3-fold improvement in perplexity.",
        "The results confirm our hypothesis that recently-used words have a higher probability of occurrence than the 3g-gram model would predict, and that incorporating this knowledge into the LM via a cache component gives a significant improvement in performance.",
        "The following are some ideas for extending this work:",
        "• The weighting of the cache component could be made to depend on the number of words in the cache.",
        "• The idea of tracking the speaker's recent behaviour could be extended to POSs - that is, recently employed POSs could be assigned higher probabilities.",
        "• A word association matrix could be built, so that the occurrence of a word would increase the estimated probability of words that often co-occur with it.",
        "• A cache component could be incorporated into a SCFG,",
        "where it would affect the probabilities of those productions which give rise to terminals."
      ]
    },
    {
      "heading": "Language Modeling and Semantics in Dialogue Systems",
      "text": [
        "More recent work focuses on the special characteristics of dialogue.",
        "Given the context provided by the state of the discourse, there will be strong constraints on both syntax",
        "and word choice which should be expressed probabilistically in the LM.",
        "Development of such an LM is one of our main current goals.",
        "We have recently begun to consider the influence of discourse state on semantics.",
        "It has usually been tacitly assumed that almost all the words in an utterance must be correctly recognized for its meaning to be determined.",
        "This is true for isolated sentences, but it is seldom true during a dialogue.",
        "We wish to design a dialogue system capable of extracting semantic content even from distorted utterances, by using Bayesian criteria to decide between possible meanings.",
        "This work has links with the results for island-driven parsers, since meaningful word sequences often form islands within a user utterance.",
        "A rigorous quantitative theory linking these themes, together with the appropriate parsing algorithm, is under development.",
        "11.",
        "R.M.Moore F.Pereira and H.Murveit, \"Integrating Speech and Natural Language Processing\", Proceedings of the Speech and Natural Language Workshop, 1989, Philadelphia, Pennsylvania, pp.243-247.",
        "12.",
        "O.Stock R.Falcone and P.Insinnarno, \"Bidirectional Chart: A Potential Technique for Parsing Spoken Natural Lan"
      ]
    },
    {
      "heading": "REFERENCES",
      "text": [
        "probabilities of theories extended by means of incrementing by one unit the known length gap."
      ]
    }
  ]
}
