{
  "info": {
    "authors": [
      "L. R. Bahl",
      "Peter V. DeSouza",
      "P. S. Gopalakrishnan",
      "David Nahamoo",
      "Michael A. Picheny"
    ],
    "book": "Workshop on Speech and Natural Language",
    "id": "acl-H91-1051",
    "title": "Context Dependent Modeling of Phones in Continuous Speech Using Decision Trees",
    "url": "https://aclweb.org/anthology/H91-1051",
    "year": 1991
  },
  "references": [
    "acl-H89-1053"
  ],
  "sections": [
    {
      "heading": "ABSTRACT",
      "text": [
        "In a continuous speech recognition system it is important to model the context dependent variations in the pronunciations of words.",
        "In this paper we present an automatic method for modeling phonological variation using decision trees.",
        "For each phone we construct a decision tree that specifies the acoustic realization of the phone as a function of the context in which it appears.",
        "Several thousand sentences from a natural language corpus spoken by several talkers are used to construct these decision trees.",
        "Experimental results on a 5000-word vocabulary natural language speech recognition task are presented."
      ]
    },
    {
      "heading": "INTRODUCTION",
      "text": [
        "It is well known that the pronunciation of a word or subword unit such as a phone depends heavily on the context.",
        "This phenomenon has been studied extensively by phoneticians who have constructed sets of phonological rules that explain this context dependence [8, 14].",
        "However, the use of such rules in recognition systems has not been extremely successful.",
        "Perhaps, a fundamental problem with this approach is that it relies on human perception rather than acoustic reality.",
        "Furthermore, this method only identifies gross changes, and the more subtle changes, which are generally unimportant to humans but may be of significant value in speech recognition by computers, are ignored.",
        "Possibly, rules constructed with the aid of spectrograms would be more useful, but, this would be very tedious and difficult.",
        "In this paper we describe an automatic method for modeling the context dependence of pronunciation.",
        "In particular, we expand on the use of decision trees for modeling allophonic variation, which we previously outlined in [4].",
        "Other researchers have modeled all distinct sequences of tri-phones (three consecutive phones) in an effort to capture phonological variations [16, 10].",
        "The method proposed in this paper has the advantage that it allows us to account for much longer contexts.",
        "In the experiments reported in this paper, we model the pronunciation of a phone as a function of the five preceding and five following phones.",
        "This method also has better powers of generalization, i.e. modeling contexts that do not occur in the training data.",
        "Use of decision trees for identifying allophones have been considered in [4, 7, 11, 15].",
        "However, apart from [4], these methods have either not been used in a recognizes or have not provided significant improvements over existing modeling methods.",
        "In the next section we describe the algorithms used for constructing the decision trees.",
        "In Section 3 we present recognition results for a 5000-word natural language continuous speech recognition task.",
        "We also present results showing the the effect of varying tree size and context on the recognition accuracy.",
        "Concluding remarks are presented in Section 4."
      ]
    },
    {
      "heading": "CONSTRUCTING THE DECISION TREE",
      "text": [
        "The data used for constructing the decision trees is obtained from a database of 20,000 continuous speech natural language sentences spoken by 10 different speakers.",
        "For more details about this database, see [4].",
        "Spectral feature vectors are extracted from the speech at a rate of 100 frames per second.",
        "These frames are labeled by a vector quantizer using a common alphabet for all the speakers.",
        "This data is used to train a set of phonetic Markov models for the words.",
        "Using the trained phonetic Markov model statistics and the Viterbi algorithm, the labeled speech is then aligned",
        "against the phonetic baseforms.",
        "This process results in an alignment of a sequence of phones (the phone sequence obtained by concatenating the phonetic baseforms of the words in the entire training script) with the label sequence produced by the vector quantizer, For each aligned phone we construct a data record which contains the identity of the current phone, denoted as Po, the context, i.e. the identities of the K previous phones and K following phones in the phone sequence, denoted as P_K,... P_i, ... PK, and the label sequence aligned against the current phone, denoted as y.",
        "We partition this collection of data on the basis of Po.",
        "Thus we have collected, for each phone in the phone alphabet, several thousand instances of label sequences in various phonetic contexts.",
        "Based on this annotated data we construct a decision tree for each phone.",
        "If we had an unlimited supply of annotated data, we could solve the context dependence problem exhaustively by constructing a different model for each phone in each possible context.",
        "Of course, we do not have enough data to do this, but even if we could carry out the exhaustive solution, it would take a large amount of storage to store all the different models.",
        "Thus, because of limited data, and a need for parsimony, we combine the contexts into equivalence classes, and make a model for each class.",
        "Obviously, each equivalence class should consist of contexts that result in similar label strings.",
        "One effective way of constructing such equivalence classes is by the use of binary decision trees.",
        "Readers interested in this topic are urged to read Classification and Regression Trees by Breiman, Friedman, Olshen and Stone M. To construct a binary decision tree we begin with a collection of data, which in our case consists of all the annotated samples for a. particular phone.",
        "We split this into two subsets, and then split each of these two subsets into two smaller subsets, and so on.",
        "The splitting is done on the basis of binary questions about the context Pi, for i = ±1, ± K. In order to construct the tree, we need to have a. goodness-of-split evaluation function.",
        "We base the goodness-of-split evaluation function on a probabilistic measure that is related to the homogeniety of a set of label strings.",
        "Finally, we need some stopping criteria.",
        "We terminate splitting when the number of samples at a node falls below a threshold, or if the goodness of the best split falls below a threshold.",
        "The result is a binary tree in which each terminal node represents one equivalence class of contexts.",
        "Using the label strings associated with a. terminal node we can construct a. fen.onic Markov model for that node by the method described in [1, 2].",
        "During recognition, given a. phone and its context, we use the decision tree of that phone to determine which model should be used.",
        "By answering the questions about the context at the nodes of the tree, we trace a path to a terminal node of the tree, which specifies the model to be used.",
        "Let Q denote a set of binary questions about the context.",
        "Let n denote a node in the tree, and m(q, n) the goodness of the split induced by question q E Q at node n. We will need to distinguish between tested and untested nodes.",
        "A tested node is one on which we have evaluated rn(q, n) for all questions q E Q and either split the node or designated it as a terminal node.",
        "It is well-known that the construction of an optimal binary decision tree is an NP-hard problem.",
        "We use a suboptimal greedy algorithm to construct the tree, selecting the best question from the set Q at each node.",
        "In outline, the decision tree construction algorithm works as follows.",
        "We start with all samples at the root node.",
        "In each iteration we select some untested node n and evaluate ni(q, n) for all possible questions q E Q at this node.",
        "If a stopping criterion is met, we declare node n as terminal.",
        "otherwise we associate the question q with the highest value of in(n, q) with this node.",
        "We make two new successor nodes.",
        "All samples that answer positively to the question q are transferred to the left successor and all other samples are transferred to the right successor.",
        "We repeat these steps till all nodes have been tested.",
        "The most important aspects of this algorithm are the set of questions Q, the goodness-of-split evaluation function m(q, n), and the stopping criteria.",
        "We discuss each of these below."
      ]
    },
    {
      "heading": "The Question Set",
      "text": [
        "Let P denote the alphabet of phones, and Np the size of this alphabet.",
        "In our case Np = 55.",
        "The question set Q consists of questions of the form [ Is E S where S C P. We start with singleton subsets of P, e.g. S {p}, S {t), etc.",
        "In addition, we use subsets corresponding to phonologically meaningful classes of phones commonly used in the analysis of speech [9], e.g., S = t, k} (all unvoiced stops), S = {p, t, k, d, g} (all stops), etc.",
        "Each question is applied to each element Pi for i = ±1, ± K, of the context.",
        "If there are Ns subsets in all, the number of questions NQ is given by NQ = 2K Ns.",
        "Thus there will be NQ splits to be evaluated at each node of the tree.",
        "In our experiments K = 5 and Ns = 130, leading to a total of 1300 questions.",
        "Note that, in general, there are 2NP different subsets of P, and, in principle, we could consider all 2K2NP questions.",
        "Since this would be too expensive, we have chosen what we consider to be a meaningful subset of all possible questions and consider only this fixed set of questions during tree construction.",
        "It is possible to generalize the tree construction procedure to use variable questions which are constructed algorithmically as part of the tree construction process, as in [5, 13].",
        "Furthermore, the type of questions we use are called simple questions, since each question is applied to one element of the context at a time.",
        "It is possible to construct complex questions which deal with several context elements at once, as in [5].",
        "Again, we did not use this more complicated technique in the experiments reported in this paper."
      ]
    },
    {
      "heading": "The Goodness-of-Split Evaluation Function",
      "text": [
        "We derive the goodness-of-split evaluation function based on a probabilistic model of collections of label strings.",
        "Let M denote a particular class of parametric models that assign probabilities to label strings.",
        "For any model M E M let PrM(y) denote the probability assigned to label string y.",
        "Let Y„ be the set of label strings associated with node n. Prm(Y„) = flyEyn Prm(y) is a measure of how well the model 1W fits the data at node n. Let Mn, E M be the best model for Y„, i.e. Prmn(Y„)> Prm(112) for all M. Prmn (Y„) is a measure of the purity of Y.",
        "If the label strings in Y„ are similar to each other, then Prmn(Yu) will be large.",
        "A question q will split the data at node n into two subsets based on the outcome of question q.",
        "Our goal is to pick q so as to make the successor nodes as pure as possible.",
        "Let YE and Yr denote the subsets of label strings at the left and right successor nodes, respectively.",
        "Obviously, Y U Yr = Y.",
        "Let MI and Mr be the corresponding best models for the two subsets.",
        "Then",
        "is a measure of the improvement in purity as a result of the split.",
        "Since our goal is to divide the strings into subsets containing similar strings, this quantity serves us well as the goodness-of-split evaluation function.",
        "Since, we will eventually use the strings at a terminal node to construct a Markov model, choosing M to be a class of Markov models would be the natural choice.",
        "Unfortunately, this choice of model is computationally very expensive.",
        "To find the best model M„ we would have to train the model, using the forward-backward algorithm using all the data at the node mt.",
        "Thus for computational reasons, we have chosen a simpler class of models – Poisson models of the type used in [3] for the polling fast match.",
        "Recall that y is a sequence of acoustic labels al, a2, ...at.",
        "We make the simplifying assumption that the labels in the sequence are independent of each other.",
        "The extent to which this approximation is inaccurate depends on the length of the units being modeled.",
        "For strings corresponding to single phones, the inaccuracy introduced by this approximation is relatively small.",
        "However, it results in an evaluation function that is easy to compute and leads to the construction of very good decision trees in practice.",
        "A result of this assumption is that the order in which the labels occur is of no consequence.",
        "Now, a string y can be fully characterized by its histogram, i.e. the number of times each label in the acoustic label alphabet occurs in that string.",
        "We represent the string y by its histogram y1y2...yF, a vector of length F where F is the size of the acoustic label alphabet and each yi is the number of times label i occurs in string y.",
        "We model each component yi of the histogram by an independent Poisson model with mean rate pi.",
        "Then, the probability assigned to y by M is 11 lit el t=1 The joint probability of all the strings in the set Y„, is then",
        "It can be easily shown that Prm(Y„) is maximized by choosing the mean rate to be the sample average, i.e., the best model for Y„ has as its mean rate 117,i = – N„ E yi for i =1,2... F (4) yEYy, Let ph and pri for i 1, 2 ... F, denote the optimal mean rates for Yi and Yr respectively.",
        "Using these expressions in (1) and eliminating common terms, we can show that the evaluation function is given by",
        " – No.t„i log itni} (5) where N, is the total number of strings at the left node and .Arr is the total number of strings at the right node",
        "resulting from split q.",
        "At each node, we select the question q that maximizes the evaluation function (5).",
        "The evaluation function given in equation (5) is very general, and arises from several different model assumptions.",
        "For example, if we assume that the length of each string is given by a Poisson distribution, and the labels in a string are produced independently by a. multinomial distribution, then the evaluation function of equation (5) results.",
        "There are also some interesting relationships between this function and a minimization of entropy formulation.",
        "Due to space limitations, the details are omitted here."
      ]
    },
    {
      "heading": "The stopping criteria",
      "text": [
        "We use two very simple stopping criteria.",
        "If the value m(q, n) of the best split at a node n is less than a threshold T„ we designate it to be a terminal node.",
        "Also, if the number of samples at a node falls below a threshold T, then we designate it to be a terminal node.",
        "The thresholds T, and Ts are selected empirically.",
        "Using the Decision Trees During Recognition The terminal nodes of a tree for a phone correspond to the different allophones of the phone.",
        "We construct a fenonic Markov model for each terminal node from the label strings associated with the node.",
        "The details of this procedure are described in [1, 2].",
        "During recognition, we construct Markov models for word sequences as follows.",
        "We construct a. sequence of phones by concatenating the phonetic baseforms of the words.",
        "For each phone in this sequence, we use the appropriate decision tree and trace the path in the tree corresponding to the context provided by the phone sequence.",
        "This leads to a terminal node, and we use the fenonic Markov model associated with this node.",
        "By concatenating the fenonic Markov models for each phone we obtain a Markov model for the entire word sequence.",
        "For the last few phones in the phone sequence, the right context is not fully known.",
        "For these phones, we make tentative models ignoring the unknown right context.",
        "When the sequence of words is extended, the right context for these phones will be available, and we can replace the tentative models by the correct models and recompute the acoustic match probabilities.",
        "This procedure is quite simple and the details are omitted here."
      ]
    },
    {
      "heading": "EXPERIMENTAL RESULTS",
      "text": [
        "We tested this method on n ri000,,word, continnons speech, natural language task.",
        "The test vocabulary consists of the 5000 most frequent words taken from a large quantity of IBM electronic mail.",
        "The training data consisted of 2000 sentences read by each of 10 different talkers.",
        "The first 500 sentences were the same for each talker, while the other 1500 were different from talker to talker.",
        "The training sentences were covered by a 20,000 word vocabulary and the allophonic models were constructed for this vocabulary.",
        "The test set consisted of 50 sentences (591 words) covered by our 5000 word vocabulary.",
        "Interested readers can refer to [4] for more details of the task and the recognition system.",
        "We constructed the decision trees using the training data described above.",
        "The phone alphabet was of size 55, K was chosen to be 5, and on the average the number of terminal nodes per decision tree and consequently the number of allophones per phone was 45.",
        "We tested the system with 10 talkers.",
        "The error rates reported here are for the same 10 talkers whose utterances were used for constructing the decision trees.",
        "Each talker provided roughly 2,000 sentences of training data for constructing the vector quantizer prototypes and for training the Markov model parameters.",
        "Tests were also done using context independent phonetic models.",
        "In both, the same vector qua.ntizer prototypes were used and the models were trained using the same data.",
        "Table 1 shows the error rates for the phonetic (context independent) and allophonic (context dependent) models for the 10 talkers.",
        "On the average, the word error rate decreases from 10.3% to 5.9%.",
        "We tested the performance of our allophonic models for talkers who were not part of the training database.",
        "The error rates for five new test talkers using the allophonic models are shown in Table 2.",
        "As can be seen, the error rates obtained using the allophonic models are comparable to those given in Table 1.",
        "We also trained and decoded using triphone-based IIMMs [16].",
        "In these experiments, only intra-word triphone models were used; we did not attempt to construct crossword triphone models.",
        "The number of phonetic models in our system is 55; approximately 10000 triphone models were required to cover our 20000 word vocabulary.",
        "No attempt was made to cluster these into a. smaller number as is done for generalized tri-phones [10].",
        "Both phonetic and triphone models were trained using the forward-backward algorithm in the usual manner; the triphone statistics were smoothed back onto the underlying phonetic models via deleted",
        "estimation.",
        "The topology of the triphone and and phonetic models were seven-state models with independent distributions for the beginning, middle, and end of each phone as described in [12].",
        "Results are shown in the fourth column of Table 1.",
        "These results are significantly worse than the results obtained with our allophonic models.",
        "However, it should be noted that these tri-phone models do not incorporate several techniques that are currently in use [11]."
      ]
    },
    {
      "heading": "Varying Context and Tree Size",
      "text": [
        "The number of preceding and following phones that are used in the construction of the decision tree influences the recognition performance considerably.",
        "We constructed several decision trees that examine different amounts of context and the recognition error rates obtained using these models is shown in Table 3.",
        "The second column shows results for models constructed using decision trees that examine only one phone preceding and following the current one.",
        "The third column shows results for trees that examine two phones preceding and following the current phone and so on to the last column for trees that examine five preceding and following phones.",
        "The stopping criterion used in all cases was the same, as was the training and test set.",
        "These results show that increasing the amount of context information improves the recognition performance of the system using these models.",
        "An important issue in constructing decision trees is when to stop splitting the nodes.",
        "As we generate more and more nodes, the tree gets better and better for the training data but may not be appropriate for new data.",
        "In order to find an appropriate tree size, we conducted several decoding experiments using models constructed from decision trees of various sizes built from the same training data.",
        "We constructed decision trees of different sizes using the following scheme.",
        "We first constructed a set of decision trees using the algorithms given in Section 2, but without using the stopping criterion based on the goodness-of-split evaluation function.",
        "The splitting is terminated only when we are left with one sample at a node or when all samples at a node have identical context so that no question can split the node.",
        "The context used consisted of the 5 preceding and following phones.",
        "Now, sets of trees of varying sizes can be obtained from these large trees by pruning.",
        "We store the value of the goodness-of-split evaluation function rn(q, n) obtained at each node.",
        "The tree for each phone is pruned back as follows.",
        "We examine all nodes 7i both of whose successor nodes are terminal nodes.",
        "From among these we select the node 77.",
        "* which has the smallest value for the evaluation function in(q , n*).",
        "If this value is less than a theshold Tin we discard this split, and mark the node n* as a leaf.",
        "This process is repeated until no more pruning can be done.",
        "By varying the pruning threshold Trn, we can obtain decision trees with different number of nodes.",
        "Table 4 shows the decoding error rates using models obtained for trees of various sizes.",
        "The second column shows the results obtained with trees having an average of 23 terminal nodes (allphones) per phone.",
        "The third, fourth, and fifth columns show the error rates for 33, 45, and 85 allophones per phone respectively.",
        "The training and test sets were the same as that described earlier in this section.",
        "As can be seen, increasing the number of allophones beyond 45 did not result in increased accuracy."
      ]
    },
    {
      "heading": "CONCLUSIONS",
      "text": [
        "Acoustic models used in continuous speech recognition systems should account for variations in pronunciation arising from contextual effects.",
        "This paper demonstrates that such effects can be discovered automatically, and represented very effectively using binary decision trees.",
        "We have presented a method for constructing and using decision trees for modeling allophonic variation.",
        "Experiments with continuous speech recognition show that this method is effective in reducing the word error rate."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": []
    }
  ]
}
