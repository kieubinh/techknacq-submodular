{
  "info": {
    "authors": [
      "Eiichiro Sumita",
      "Hitoshi Iida"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P91-1024",
    "title": "Experiments and Prospects of Example-Based Machine Translation",
    "url": "https://aclweb.org/anthology/P91-1024",
    "year": 1991
  },
  "references": [
    "acl-C86-1023",
    "acl-C88-2091",
    "acl-C90-3044",
    "acl-P87-1018"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "EBMT (Example-Based Machine Translation) is proposed.",
        "EBMT retrieves similar examples (pairs of source phrases, sentences, or texts and their translations) from a datalrse of examples, adapting the examples to translate a new input.",
        "EBMT has the following features: (1) It is easily upgraded simply by inputting appropriate examples to the database; (2) It assigns a reliability factor to the translation result; (3) It is accelerated effectively by both indexing aid parallel computing;",
        "(4) It is robust because of best-match reasoning; and (5) It well utilizes translator expertise.",
        "A prototype",
        "system has been implemented to deal with a difficult translation problem for conventional Rule-Based Machine Translation (RBMT), i.e., translating Japanese noun phrases of the form \"N, no N,\" into English.",
        "The system has achieved about a 78% success rate on average.",
        "This paper explains the basic idea of EBMT, illustrates the experiment in detail, explains the bread applicability of EBMT to several difficult translation problems for RBMT and discusses the advantages of integrating EBMT with RBMT."
      ]
    },
    {
      "heading": "1 INTRODUCTION",
      "text": [
        "Machine Translation requires handcrafted axl complicated large-scale knowledge (Nirenburg 1987).",
        "Conventional machine translation systems use rules as the knowledge.",
        "This framework is called Rule-Based Machine Translation (RBMT).",
        "It is difficult to scale up from a toy program to a practical system because of the problem of building such a large-scale rule-base.",
        "It is also difficult to improve translation performance because the effect of adding a new rule is hard to anticipate, and because translation using a large-scale rule-based system is time-consuming.",
        "Moreover, it is difficult to make use of situational or domain-specific information for translation.",
        "In order to conquer these problems in machine translation, a database of examples (pairs of source phrases, sentences, or texts and"
      ]
    },
    {
      "heading": "* Currently with Kyoto University",
      "text": [
        "their translations) has been implemented as the knowledge (Nagao 1984; Sumita and Tsutsumi 1988; Sato and Nagao 1989; Sadler 1989a; Sumita et al.",
        "1990a, b).",
        "The translation mechanism retrieves similar examples from the database, adapting the examples to translate the new source text.",
        "This framework is called Example-Based Machine Translation (EBMT).",
        "This paper focuses on ATR's linguistic database of spoken Japanese with English translations.",
        "The corpus contains conversations about international conference registration (Ogura et al.",
        "1989).",
        "Results of this study indicate that EBMT is a breakthrough in MT technology.",
        "Our pilot EBMT system translates Japanese noun phrases of the form \"N, no 142\" into English noun phrases.",
        "About a 78% success rate on average has been achieved in the experiment, which is considered to outperform RBMT.",
        "This rate can be improved as discussed below.",
        "Section 2 explains the basic idea of EBMT.",
        "Section 3 discusses the broad applicability of EBMT and the advantages of integrating it with RBMT.",
        "Sections 4 and 5 give a rationale for section 3, i.e., section 4 illustrates the experiment of translating noun phrases of the form \"N, no N2\" in detail, aid section 5 studies other phenomena through actual data from our corpus.",
        "Section 6 concludes this paper with detailed comparisons between RBMT and EBMT."
      ]
    },
    {
      "heading": "2 BASIC IDEA OF EBMT",
      "text": []
    },
    {
      "heading": "2.1 BASIC FLOW",
      "text": [
        "In this section, the basic idea of EBMT, which is general and applicable to many phenomena dealt with by machine translation, is shown.",
        "Figure 1 shows the basic flow of EBMT using translation of \"Icireru\"[cut / be sharp].",
        "From here on, the literal English translations are bracketed.",
        "(1) and (2) at examples (pairs of Japanese sentences and their English",
        "translations) in the database.",
        "Examples similar to the Japanese input sentence are retrieved in the following manner.",
        "Syntactically, the input is similar to Japanese sentences (1) and (2).",
        "However, semantically, \"Icachou\" [chief] is far from \"houchou\" [kitchen knife].",
        "But., \"kachou\" [chief] is semantically similar to \"kanojo\" [she] in that both are people.",
        "In other words, the input is similar to example sentence (2).",
        "By mimicking the similar example (2), we finally get \"The chief is sharp\".",
        "Although it is possible to obtain the same result by a word selection rule using fine-tuned semantic restriction, note that translation here is obtained by retrieving similar examples to the input.",
        "• Example Database (data for \"kireru[cut / be sharp]) (1) houchou wa klreru -> The kitchen knife cuts.",
        "(2) kanojo w a klreru -› She Is sharp.",
        "• Input kachou wa klreru ?",
        "• Retrieval of similar examples (Syntax) Input = (1), (2) (Semantics) kachou houchou kachou kanojo (Total) Input .",
        "(2) • Output ->Thechiefls sharp."
      ]
    },
    {
      "heading": "2.2 DISTANCE",
      "text": [
        "Retrieving similar examples to the input is done by measuring the distance of the input to each of examples.",
        "The smaller a distance is, the more similar the example is to the input.",
        "To define the best distance metric is a problem of EBMT not yet completely solved.",
        "However, one possible definition is shown in section 4.2.2.",
        "From similar examples retrieved, EBMT generates the most likely translation with a reliability factor based on distance and frequency.",
        "If there is no similar example within the given threshold, EBMT tells the user that it cannot translate the input."
      ]
    },
    {
      "heading": "3 BROAD APPLICABILITY AND INTEGRATION",
      "text": []
    },
    {
      "heading": "3.1 BROAD APPLICABILITY",
      "text": [
        "EBMT is applicable to many linguistic phenomena that are regarded as difficult to translate in conventional RBMT.",
        "Some are well-known among researchers of natural language processing and others have recently been given a great deal of attention.",
        "When one of the following conditions holds true for a linguistic phenomenon, RBMT is less suitable than EBMT.",
        "(Ca) Translation rule formation is difficult.",
        "(Cb) The general rule cannot accurately describe phenomena because it represents a special case, e.g., idioms.",
        "(Cc) Translation cannot be made in a compositional way from target words (Nagao 1984; Nitta 1986; Sadler 1989b).",
        "This is a list (not exhaustive) of phenomena in J-E translation that are suitable for EBMT:",
        "• optional cases with a case particle ( \" – de\", \" – ni\",...) • subordinate conjunction (\" – ba \" – nagara tara baai • noun phrases of the form \"NI no N2\" • sentences of the form \"N, wa N2 da\" • sentences lacking the main verb (eg.",
        "sentences of the form \" – o-negaishimasu\") • fragmental expressions (\"hai\", \"sou-desu\", \"wakarimashita\",...) (Furuse et al.",
        "1990) • modality represented by the sentence ending (\" – tainodesuga\", \" – seteitadalcimasu\", ...) (Furuse et al.",
        "1990) • simple sentences (Sato and Nagao 1989)",
        "This paper discusses a detailed experiment for \"N, no N2\" in section 4 and prospects for other phenomena, \"N, wa N2 da\" and \" – o-negaishimasu\" in section 5.",
        "Similar phenomena in other language pairs can be found.",
        "For example, in Spanish to English translation, the Spanish preposition \"de\", with its broad usage like Japanese \"no\", is also effectively translated by EBMT.",
        "Likewise, in German to English translation, the German complex noun is also effectively translated by EBMT."
      ]
    },
    {
      "heading": "3.2 INTEGRATION",
      "text": [
        "It is not yet clear whether EBMT can or should deal with the whole process of translation.",
        "We assume that there are many kinds of phenomena.",
        "Some are suitable for EBMT, while others are suitable for RBMT.",
        "Integrating EBMT with RBMT is expected to be useful.",
        "It would be more acceptable for users if RBMT were first introduced as a base system, and then incrementally have its translation performance improved by attaching EBMT components.",
        "This is in the line with the proposal in Nagao (1984).",
        "Subsequently, we proposed a practical method of integration in",
        "previous papers (Sumita et al.",
        "1990a, b)."
      ]
    },
    {
      "heading": "4 EBMT FOR \"N1 no N2\"",
      "text": []
    },
    {
      "heading": "4.1 THE PROBLEM",
      "text": [
        "\"N, no Ny\" is a common Japanese noun phrase form.",
        "\"no\" in the \"N, no N2\" is a Japanese adnominal particle.",
        "There are other variants, including \"deno\", \"Icarano\", \"madeno\" and so on.",
        "Roughly speaking, Japanese noun phrases of the form \"N, no N2\" correspond to English noun phrases of the form \"N2 of N,\" as shown in the examples at the top of Figure 2.",
        "youka no gogo the afternoon of the 8th kaigi no mokuteki the object of the conference expressions).",
        "EBMT has the advantage that it can directly return a translation by adapting examples without reasoning through a long chain of rules."
      ]
    },
    {
      "heading": "4.2 IMPLEMENTATION 4.2.1 OVERVIEW",
      "text": [
        "The EBMT system consists of two databases: an example database and a thesaurus; and also three translation modules: analysis, example-based transfer, and generation (Figure 3).",
        "Examples (pairs of source phrases and their translations) are extracted from ATR's linguistic database of spoken Japanese with English translations.",
        "The corpus contains conversations about registering for an international conference (Ogura 1989).",
        "(Example Database ( Thesaurus kaigi no sankaryou the application fee for the conf.",
        "(1) Analysis ?the application fee of the conf.",
        "kyouto deno kaigi the cord.",
        "In Kyoto ?the cont.",
        "of Kyoto isshukan no kyuka a week's holiday ?the holiday of a week mittsu no hoteru three hotels",
        "However, \"N2 of N,\" does not always provide a natural translation as shown in the lower examples in Figure 2.",
        "Some translations are too broad in meaning to interpret, others am almost ungrammatical.",
        "For example, the fourth one, \"the conference of Kyoto\", could be misconstrued as \"the conference about Kyoto\", ani the last one, \"hotels of three\", is not English.",
        "Natural translations often require prepositions other than \"of\", or no preposition at all.",
        "In only about one-fifth of \"N, no N2\" occurrences in our domain, \"N2 of N,\" would be the most appropriate English translation.",
        "We cannot use any particular preposition as an effective default value.",
        "No rules for selecting the most appropriate translation for \"N, no N2\" have yet been found.",
        "In other words, the condition (Ca) in section 3.1 holds.",
        "Selecting the translation for \"N, no N2\" is still an important and complicated problem in J-E translation.",
        "In contrast with the preceding research analyzing \"N, no N2\" (Shimazu et al.",
        "1987; Hirai and Kitahashi 1986), deep semantic analysis is avoided because it is assumed that translations appropriate for given domain can be obtained using domain-specific examples (pairs of source and target The thesaurus is used in calculating the semantic distance between the content words in the input and those in the examples.",
        "It is composed of a hierarchical structure in accordance with the thesaurus of everyday Japanese written by Ohno and Hamanishi (1984)."
      ]
    },
    {
      "heading": "Analysis",
      "text": [
        "kyouto deno kaigi"
      ]
    },
    {
      "heading": "Example-Based Transfer",
      "text": [
        "d Japanese English 0.4 toukyou deno taizai the stay In Tokyo 0.4 honkon denotaizai the stay In Hongkong 0.4 toukyou deno go-taizai the stay In Tokyo the conf.",
        "in Osaka the cord.",
        "in Tokyo Figure 4 illustrates the translation procedure with an actual sample.",
        "First, morphological analysis is performed for the input phrase,\"kyouto[Kyoto] deno kaigi [conference]\".",
        "In this case, syntactical",
        "analysis is not necessary.",
        "Second, similar examples are retrieved from the database.",
        "The top five similar examples are shown.",
        "Note that the top three examples have the same distance and that they are all translated with \"in\".",
        "Third, using this rationale, EBMT generates \"the conference in Kyoto\".",
        "The distance metric used when retrieving examples is essential and is explained here in detail.",
        "Here we suppose that the input and examples (I, E) in the database are represented in the same data structure, i.e., the list of words' syntactic and semantic attribute values (referred to as and l, Ei) for each phrase.",
        "The attributes of the current target, \"N1 no N2\", ate as follows: 1) for the nouns \"NI\" and \"1•12\": the lexical subcategory of the noun, the existence of a prefix or suffix, aid its semantic code in the thesaurus; 2) for the adnominal particle \"no\": the kinds of variants, \"deno\", \"lcarano\", \"madeno\" and so on.",
        "Here, for simplicity, only the semantic code and the kind of adnominal ate considered.",
        "Distances are calculated using the following two expressions (Stunita et al.",
        "1990a, b):",
        "(1) d(I,E)=1, d(II,Ej) *wi (2) wri E ( freq.",
        "of t. p. when Ej=lj ) 2 t. p.",
        "The attribute distance, cl(li,Ei) and the weight of attribute, we ate explained in the following sections.",
        "Each translation pattern (t.p.)",
        "is abstracted from an example and is stored with the example in the example database [see Figure 6]."
      ]
    },
    {
      "heading": "(a) ATTRIBUTE DISTANCE",
      "text": [
        "For the attribute of the adnominal particle \"no\", the distance is 0 or 1 depending on whether or not they match exactly, for example, d(\"deno\",\"deno\")= 0 and dCdeno\", \"no\") = 1.",
        "For semantic attributes, however, the distance varies between 0 and 1.",
        "Semantic distance d(0 5 d 5 1) is determined by the Most Specific Common Abstraction(MSCA) (Kolodner and Riesbeck 1989) obtained from the thesaurus abstraction hierarchy.",
        "When the thesaurus is (n+1) layered, (k/n) is assigned to the concepts in the k-th layer from the bottom.",
        "For example, as shown with the broken line in Figure 5, the MSCACkaigi\" [conference], \"taizai\" [stay]) is \"koudou\" [actions] and the distance is 2/3.",
        "Of course, 0 is assigned when the MSCA is the bottom class, for instance, MSCA(\"kyouto\" [Kyoto], \"toukyou\" [Tokyo]) = \"timer[place], or when nouns are identical ( MSCA(N, N) for any N).",
        "The weight of the attribute is the degree to which the attribute influences the selection of the translation pattern(t.p.).",
        "We adopt the expression (2) used by Stanfill and Waltz (1986) for memory-based reasoning, to implement the intuition.",
        "t.p.",
        "freq.",
        "t.p.",
        "freq.",
        "I t.p.",
        "freq.",
        "In Figure 6, all the examples whose E2 = \"deno\" are translated with the same preposition, \"in\".",
        "This implies that when E2= \"deno\", E2 is an attribute which heavily influences the selection of the translation pattern.",
        "In contrast to this, the translation patterns of examples whose El = \"timei\"[place], are varied.",
        "This implies that when El = \"timer[plaee], El is an attribute which is less influential on the selection of the translation pattern.",
        "According to the expression (2) , weights for",
        "The distance between the input and the first example shown in Figure 4 is calculated using the weights in section 4.2.2 (b), attribute distances as explained in section 4.2.2 (a) and expression (1) at the beginning of section 4.2.2."
      ]
    },
    {
      "heading": "4.3 EXPERIMENTS",
      "text": [
        "The current number of words in the corpus is about 300,000 and the number of examples is 2,550.",
        "The collection of examples from another domain is in progress."
      ]
    },
    {
      "heading": "4.3.1 JACKKNIFE TEST",
      "text": [
        "In order to roughly estimate translation performance, a jackknife experiment was conducted.",
        "We partitioned the example database(2,550) in groups of one hundred, then used one set as input(100) and translated them with the rest as an example database (2,450).",
        "This was repeated 25 times.",
        "It is difficult to fairly compare this result with the success rate of the existing MT system.",
        "However, it is believed that current conventional systems can at best output the most common translation pattern, for example, \"B of A\", as the default.",
        "In this case, the average success rate may only be about 20%."
      ]
    },
    {
      "heading": "4.3.2 SUCCESS RATE PER NUMBER OF EXAMPLES",
      "text": [
        "Figure 8 shows the relationship between the success rate and the number of examples.",
        "Of the twenty-five cases in the previous jackknife test, three are shown: maximum, average, and minimum.",
        "This graph shows that, in general, the more examples we have, the better the quality [see section 4.3.4]."
      ]
    },
    {
      "heading": "4.3.3 SUCCESS RATE PER DISTANCE",
      "text": [
        "Figure 9 shows the relationship between the success rate and the distance between the input and the most similar examples retrieved.",
        "This graph shows that in general, the smaller the distance, the better the quality.",
        "In other words, EBMT assigns the distance between the input and the retrieved examples as a reliability factor."
      ]
    },
    {
      "heading": "4.3.4 SUCCESSES AND FAILURES",
      "text": [
        "The following represents successful results: (1) the noun phrase \"kyouto-eki [Kyoto-station] no o-mise [store]\" is translated according to the translation pattern \"B at A\" while the similar noun phrase, \"kyouto[Kyoto] no shiten [branch]\" is translated according to the translation pattern \"B in A\"; (2) the noun phrase of the form \"NI no hou\" is translated according to the translation pattern \"A\", in other words, the second noun is omitted.",
        "We are now studying the results carefully and are striving to improve the success rate.",
        "(a) About half of the failures are caused by a lack of similar examples.",
        "They are easily solved by adding appropriate examples.",
        "(b) The rest are caused by the existence of similar examples: (1) equivalent but different examples are retrieved, for instance, those of the form, \"B of A\" and \"AB\" for \"roku-gatsu [June] no futsu-ka [second]\".",
        "This is one of the main reasons the graphs (Figure 7 and 8) show an up-and-down pattern.",
        "They can be regarded as a correct translation or the distance calculation may be changed to handle the problem; (2) Because the current distance calculation is inadequate, dissimilar examples are retrieved."
      ]
    },
    {
      "heading": "5 PHENOMENA OTHER THAN \"N1 no N,\"",
      "text": [
        "This section studies the phenomena, \"N1 wa N2 di\" and \" – o-negaishimasu\" with the same corpus used in the previous section."
      ]
    },
    {
      "heading": "5.1 \"N1 wa Ni da\"",
      "text": [
        "A sentence of the form \"N1 wa N2 di\" is called a \"di\" sentence.",
        "Here \"N,\" and \"N,\" are nouns, \"wa\" is a topical particle, awl \"da\" is a kind of verb which, roughly speaking, is the English copula \"be\".",
        "The correspondences between \"di\" sentences and the English equivalents are exemplified in Figure 10.",
        "Mainly, \"N1 wa N2 di\" corresponds to \"N1 be N2\" like (a-1) – (a-4).",
        "However, sentences like (b) (e) cannot be translated according to the translation pattern \"N1 be N2\".",
        "In example (d), there is no Japanese counterpart of \"payment should be made by\".",
        "The English sentence has a modal, passive voice, the verb make, and its object, payment, while the Japanese sentence has no such correspondences.",
        "This translation cannot be male in a compositional way from the target words which are selected from a normal dictionary.",
        "It is difficult to formulate rules for the translation and to explain how the translation is made.",
        "The conditions (Ca) and (Cc) in section 3.1 hold true.",
        "Conventional approaches leaf to the understanding of \"da\" sentences using contextual and extralinguistic information.",
        "However, many translations exist that are the result of human translators' understanding.",
        "Translation can be made by mimicking such similar examples.",
        "Example (e) is special, i.e., idiomatic.",
        "The condition (Cb) in section 3.1 holds.",
        "The distribution of N, and N2 in the examples",
        "of our corpus vary for each case.",
        "Attention should be given to 2-tuples of nouns, (Ni, N2).",
        "NA of (a-4), (b) au (c) am similar, i.e., both mean \"prices\".",
        "However Nis am not similar to each other.",
        "Nis of (a-4) and (d) are similar, i.e., both mean \"fee\".",
        "However, the N2s are not similar to each other.",
        "Thus, EBMT is applicable."
      ]
    },
    {
      "heading": "5.2 \"- o-negaishimasu\"",
      "text": [
        "Translations in examples (b) arid (c) am possible by fmding substitutes in Japanese for give me and pay by , respectively.",
        "The conditions (Ca) and (Cc) in section 3.1 hold.",
        "Usually, this kind of supplement is done by contextual analysis.",
        "However, the connection between the missing elements and the noun in the examples is strong enough to reuse, because it is the product of a combination of translator expertise and domain specific restriction.",
        "Examples (a), (d) and (e) are idiomatic expressions.",
        "The condition (Cb) holds.",
        "The distribution of the noun and the particle in the examples of our corpus varies for each case in the same way as in the \"da\" sentence.",
        "EBMT is applicable."
      ]
    },
    {
      "heading": "6 CONCLUDING REMARKS",
      "text": [
        "Example-Based Machine Translation (EBMT) has been proposed.",
        "EBMT retrieves similar examples (pairs of source and target expressions), adapting them to translate a new source text.",
        "The feasibility of EBMT has been shown by implementing a system which translates Japanese noun phrases of the form \"Ni no N2\" into English noun phrases.",
        "The result of the experiment was encouraging.",
        "Broad applicability of EBMT was shown by studying the data from the text corpus.",
        "The advantages of integrating EBMT with RBMT were also discussed.",
        "The system has been written in Common Lisp, and is running on a Genera 7.2 Symbolics Lisp Machine at ATR."
      ]
    },
    {
      "heading": "(1) IMPROVEMENT",
      "text": [
        "The more elaborate the RBMT becomes, the less expandable it is.",
        "Considerably complex rules concerning semantics, context, and the real world, are required in machine translation.",
        "This is the notorious Al bottleneck: not only is it difficult to add a new rule to the database of rules that are mutually dependent, but it is also difficult to build such a rule database itself.",
        "Moreover, computation using this huge and complex rule database is so slow that it forces a developer to abandon efforts to improve the system.",
        "RBMT is not easily upgraded.",
        "However, EBMT has no rules, ail the use of examples is relatively localized.",
        "Improvement is effected simply by inputting appropriate examples into the database.",
        "EBMT is easily upgraded, which the experiment in section 4.3.2 has shown: the more examples we have, the better the quality."
      ]
    },
    {
      "heading": "(2) RELIABILITY FACTOR",
      "text": [
        "One of the main reasons users dislike RBMT systems is the so-called \"poisoned cookie\" problem.",
        "RBMT has no device to compute the reliability of the result.",
        "In other words, users of RBMT cannot trust any RBMT translation, because it may be wrong without any such indication from system.",
        "Consider the case where all translation processes have been completed successfully, yet, the result is incorrect.",
        "In EBMT, a reliability factor is assigned to the translation result according to the distance between the input and the similar examples found [see the experiment in section 4.3.3].",
        "In addition to this, retrieved examples that are similar to the input convince users that the translation is accurate."
      ]
    },
    {
      "heading": "(3) TRANSLATION SPEED",
      "text": [
        "RBMT translates slowly in general because it is really a large-scale rule-based system, which consists of analysis, transfer, and generation modules using syntactic rules, semantic restrictions, structural transfer rules, word selections, generation rules, and so on.",
        "For example, the Mu system has about 2,000 rewriting and word selection rules for about 70,000 lexical items (Nagao et al.",
        "1986).",
        "As recently pointed out (Furuse et al.",
        "1990), conventional RBMT systems have been biased toward syntactic, semantic, and contextual analysis, which consumes considerable computing time.",
        "However, such deep analysis is not always necessary or useful for translation.",
        "In contrast with this, deep semantic analysis is avoided in EBMT because it is assumed that translations appropriate for given domain can be obtained using domain-specific examples (pairs of source and target expressions).",
        "EBMT directly returns a translation without reasoning through a long chain of rules [see",
        "sections 2 and 4].",
        "There is fear that retrieval from a large-scale example database will prove too slow.",
        "However, it can be accelerated effectively by both indexing (Sumita and Tsutsumi 1988) and parallel computing (Sumita and Iida 1991).",
        "These processes multiply acceleration.",
        "Consequently, the computation of EBMT is acceptably efficient."
      ]
    },
    {
      "heading": "(4) ROBUSTNESS",
      "text": [
        "RBMT works on exact-match reasoning.",
        "It fails to translate when it has no knowledge that matches the input exactly.",
        "However, EBMT works on best-match reasoning.",
        "It intrinsically translates in a fail-safe way [see sections 2 and 4]."
      ]
    },
    {
      "heading": "(5) TRANSLATORS EXPERTISE",
      "text": [
        "Formulating linguistic rules for RBMT is a difficult job and requires a linguistically trained staff.",
        "Moreover, linguistics does not deal with all phenomena occurring in real text (Nagao 1988).",
        "However, examples necessary for EBMT are easy to obtain because a large number of texts and their translations are available.",
        "These are realization of translator expertise, which deals with all real phenomena.",
        "Moreover, as electronic publishing increases, more and more texts will be machine-readable (Sadler 1989b).",
        "EBMT is intrinsically biased toward a sublanguage: strictly speaking, toward an example database.",
        "This is a good feature because it provides a way of automatically tuning itself to a sublanguage."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": []
    }
  ]
}
