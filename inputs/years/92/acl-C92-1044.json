{
  "info": {
    "authors": [
      "Benjamin L. Chen",
      "Von-Wun Soo"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C92-1044",
    "title": "An Acquisition Model for Both Choosing and Resolving Anaphora in Conjoined Mandarin Chinese Sentences",
    "url": "https://aclweb.org/anthology/C92-1044",
    "year": 1992
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Anaphoric reference is an important linguistic phenomenon to understand the discourse structure and content.",
        "In Chinese natural language processing, there are both the problems of choosing and resolving anaphora.",
        "In Mandarin Chinese, several linguists have attempted to propose criteria to ezplain the phenomenon of anaphora but with controversial results.",
        "On the other hand, search-based computational techniques for resolving anaphora are neither the best way to resolve Chinese anaphora nor to facilitate choosing anaphora.",
        "Thus, to facilitate both choosing and resolving anaphora with accuracy and efficiency, we propose a case-based learning model G-UNIMEM to automatically acquire anaphorie regularity from a sample set of training sentences i, which are annotated with a list of features.",
        "The regularity acquired from training was then tested and compared with other approaches in both choosing and resolving anaphora."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In discourse, there may be anaphora in two consecutive sentences.",
        "When anaphora appear in a pair of consecutive sentences, the two consecutive sentences are called conjoined sentences.",
        "In real life conversation, we frequently choose and resolve anaphora to understand the utterances.",
        "There are primarily three types of anaphora in Mandarin Chinese: zero (ellipsis), pronominal (using pronoun) and nominal anaphora[4].",
        "Let's take the conjoined Chinese sentences in (B) to illustrate the phenomenon.",
        "The conjoined sentence in (C) is the English translation of the Chinese sentences in (B).",
        "1. This paper is partially supported by the Minister of Economic Affairs, R.O.C. under the project no. 33H3100 at ITRI and by National Science Councial of R.O.C. under the grant no. NSCS1-0408-E007-02 at National Tsing Hua University.",
        "The authors also want to thank Dr. Martha Pollack for valuable comments during 1991 UCSC linguistic institute.",
        "Because the anaphora in (B) is a zero anaphora and there is no zero anaphora in English, the antecedent of zero anaphora in (B) must be resolved first before choosing an appropriate pronominal anaphora in Chinese to English translation.",
        "In the translation from (C) to (B), it is not good to directly translate an English pronoun to a Chinese pronoun.",
        "A better way is to resolve the anaphora in (C) and then choose an appropriate type of anaphora in Chinese.",
        "In natural language processing, better results seems to be attainable if rich linguistic or domain knowledge is available.",
        "However it generally costs much and doesn't seem to be realistic.",
        "The same situation applies for resolving and choosing anaphora in Mandarin Chinese.",
        "If we only used search-based ap-proaches(those that merely used heuristic and algorithmic methods without much linguistic knowledge), the performance was limited.",
        "However, when we intended to adopt linguistic knowledge, we found linguists' theories tended to be controversial and less computable.",
        "Thus, it motivated us to pursue an acquisition model that could acquire linguistic regularity from corpora and then used the regularity to resolve and choose anaphora."
      ]
    },
    {
      "heading": "2 Review of previous approaches",
      "text": []
    },
    {
      "heading": "2.1 Search-based approaches",
      "text": [
        "Both history list [1] and Hobbs's naive syntactic algorithm [7] are search-based approaches for resolving anaphora.",
        "However, it's not quite obvious to tell which was better than the other with only few examples.",
        "Thus, we collected 120 testing instances to test them.",
        "Those instances were selected from linguists' examples, textbooks, essays and novels.",
        "Half of them contained zero anaphora the other pronominal.",
        "The result showed that the correct number was 111(92.5%) with Hobbs's syntactic algorithm and 87(72.5%) with the history list approach if first matched were selected.",
        "There was 109(90.8%) correct for history list if the last matched were selected.",
        "It seemed that both approaches were applicable to resolve anaphora.",
        "However, when there are several NPs with the same semantic features, both approaches may get into troubles.",
        "Furthermore, both cannot be used to choose anaphora."
      ]
    },
    {
      "heading": "2.2 Linguist's criteria",
      "text": [
        "Among linguists' works [3] (5] [11] [12] [14], Tai's criteria [14] was applicable to both choose and resolve anaphora.",
        "Others' suffered from difficulties of extracting features or resolving anaphora.",
        "Table 1 shows 4 co-references for Tai's citeria, which are all applicable when co-referred NPs are human.",
        "For example, consider the following conjoined sentences:",
        "John came U.S.A after [ ] made many friends Since John came to the U.S.A., he has made many friends.",
        "The subject in the first sentence is human and co-referred by the subject in the second sentence, so this is a subject-subject co-reference.",
        "According to Table 1, zero anaphora is preferred to the pronominal one and nominal anaphora is not permitted in this example.",
        "Though Tai didn't propose the criteria for resolving anaphora, it was possible to get these criteria just by transforming the choosing criteria in reverse order.",
        "After Tai's criteria were applied to choose and resolve anaphora on the 120 testing instances, we got the success numbers 86(71.7%) and 65(54.2%) respectively.",
        "The results failed to meet our satisfaction.",
        "Through above paragraphs, it appears that search-based methods have their limitations due to lack of enough linguistisc knowledge and Tai's criteria seems to be applicable to both choose and resolve anaphora.",
        "It might be that Tai's criteria were too general to lead to a high success rate.",
        "More reliable method to acquire regularity might be required to promote the success rate.",
        "We hypothesized the regularity of anaphora could be accounted by causal relations between the features in the conjoined sentences and the antecedents.",
        "In the following section, an acquisition model is introduced."
      ]
    },
    {
      "heading": "3 G-UNIMEM: A Case-Based",
      "text": []
    },
    {
      "heading": "Learning Model",
      "text": [
        "In natural language acquisition problem, the restriction of positive-only examples [2] has prohibited many machine learning models as a feasible natural language model.",
        "However, a case-based learning approach such as Lebowitz's UNIMEM [9] [10] seems to be a candidate due to its capability to form concepts incrementally from a rich input domain.",
        "Nevertheless, to apply UNIMEM directly to the acquisition of anaphoric regularity in Mandarin Chinese is still not sufficient.",
        "We have therefore modified UNIMEM into G-UNIMEM.",
        "G-UNIMEM, a modified version of UNIMEM, is an incremental learning system that uses GBM(Generalized-based Memory) to generalize concepts from a large set of training instances.",
        "The program was implemented in Quintus PROLOG and on SUN workstation.",
        "G-UNIMEM differs from UNIMEM in two respects.",
        "Firstly, if a drinker got drunk many times after taking either whiskey and water or brandy and water, he would induce that water made him drunk with UNIMEM.",
        "This is intuitively incorrect.",
        "Whereas, with G-UNIMEM, he would induce that whiskey and water, brandy and water or water would cause him drunk.",
        "In this case, G-UNIMEM retains the possible causal accounts without committing to erroneous conclusion.",
        "Secondly, G-UNIMEM can extract explicit causal rules from memory hierarchy.",
        "Similar to UNIMEM, G-UNIMEM organizes input training instances into a memory hierarchy according to the frequencies of features.",
        "However, its goal is to explicitly express the generalized causal relationships between two specified types of features: cause features and goal features.",
        "Since there may be inconsistency due to lack of cause features, further refinement is needed to obtain consistent causal relations.",
        "Thus, there are four different modules in G-UNIMEM to complete different functions in order to achieve this purpose."
      ]
    },
    {
      "heading": "3.1 The classifier",
      "text": [
        "The classifier is the first module that processes all training instances for G-UNIMEM.",
        "Its function is close to UNIMEM that organizes a hierarchy structure to incrementally accommodate a training instance and at the same time generalize the features based on similarities among training instances.",
        "The forming hierarchy is organized as either a g-c-hierarchy or a c-g-hierarchy depending on the setup of system, which is defined in Definition 1.",
        "In Appendix A we show the basic classifier algorithm.",
        "Figure 1 and Figure 2 show the forming g-c-hierarchy and c-g-hierarchy respectively after 13 annotated training sentences are entered into G-UNIMEM.",
        "Generally, g-c-hierarchy would be chosen since it retained all possible causal accounts.",
        "For example, the drinker with g-c-hierarchy would induce that whiskey and water, brandy and water or water would cause him drunk; whereas, he would induce whiskey and water, brandy and water with c-g-hierarchy.",
        "The cg-hierarchy is more efficient since no rules are needed to be generated.",
        "Fig.",
        "3 and Fig. 4 show the updating of a GBM before and after inserting a new training instance."
      ]
    },
    {
      "heading": "3.2 The rule generator",
      "text": [
        "Once a hierarchy has been constructed by the classifier, the causal rules can be extracted.",
        "The rule generator module serves as the role to extract causal rules from the hierarchy.",
        "It generates all causal rules from the hierarchy as the regularity is retrieved for predictions.",
        "In Fig. 6, if a testing instance is given for choosing anaphora with a query feature list [ (g,type(*?",
        ")), (g,ante(theme)), (c,f1(theme)), (c,anaphor(theme)) (c,s2(obj)), (c,p(pv))], the retrieval process is searched with a post-order traverse, namely, in the order sequence of the node number 1, 2, 3, 4, 5 and 6.",
        "Since there may be more than one candidate, the system can be setup to select either the first or the most specific one.",
        "If the first one is preferred, type(nil) is yielded as the prediction.",
        "If the most specific answer is preferred, all possible rules will be tried and the one with the most number of contingent features matched will be the answer(i.e. type(pronoun) ).",
        "The sample rules generated from Fig. 1 are shown in Fig. 5.",
        "Before generating rules, the OHM is adjusted so that all children of a GEN-NODE are ordered according to their confidence scores of features.",
        "Then all rules are generated in a post-order traversal."
      ]
    },
    {
      "heading": "3.3 The rule filter",
      "text": [
        "The rule filter removes those rules that are ill-formed and useless.",
        "For example, the causal rule 5 in Fig. 5 has no causes which is not a well-formed rule.",
        "It also detects conflicting rules.",
        "Conflicting rules are those that have different goal feature descriptions, which are accounted by the same cause.",
        "For example, the rule 1 and rule 6 in Fig. 5 are conflicting.",
        "These rules will be detected in this module and then to be resolved by the feature selector."
      ]
    },
    {
      "heading": "3.4 The feature selector",
      "text": [
        "Any two conflicting rules are resolved by the feature selector through augmenting the two rules with mutual exclusive contingent cause features, which are prepared in advance.",
        "Dominant features were used in initial regularity acquisition stage; whereas contingent features were used in feature selection stage.",
        "The dominant features such as goal features are assumed to be those that must be present in every anaphoric rule.",
        "Contingent features are optional.",
        "Fig. 6. shows the GBM with g-c hierarchy after feature selection process."
      ]
    },
    {
      "heading": "4 Tests using sentences annotated with mixed features",
      "text": [
        "We trained G-UNIMEM with 30, 60, 90, 120 instances using those features mentioned by Tai, and used all the 120 instances as testing instances.",
        "It showed that the approach using Tai's criteria was not promising.",
        "There are two reasons.",
        "First, none of the success rates was as high as those using the history list approach or Hobbs's algorithm.",
        "Second, many conflicting rules remained conflicting due to either that no further features from feature selection were available or too many specific training leading to too many specific rules.",
        "These factors decreased the success rate."
      ]
    },
    {
      "heading": "4.1 Selecting mixed features",
      "text": [
        "Since Tai's features were not sufficient, more semantic features were considered.",
        "Among several linguists' works, we tentatively selected some computational feasible syntactic and semantic features from different sources [3] [5] [11] [12] (133 [14] [15] as in Table 2.",
        "An example with annotated features is shown below.",
        "The notation [ ] represents zero anaphora.",
        "(C)[Lao zheng]i qu-le ji-ge Inurenlj.",
        "[ jj hen hui zuo-cai.",
        "John married a woman ] well can cook.",
        "agent theme agent hm sub,bv nondefinite John married a woman, and the woman cooked well.",
        "The training feature list for the sentences (C)is : [Tante(theme)), (g,type(nil)), (c,f1(agent)), (c, f2 theme)), (c,anaphor(agent)), (c,p(bv)), (c,s2 sub)), (c,h(hm)), (c,d(nondefinite))] where the notations g and c represent goal and cause features respectively."
      ]
    },
    {
      "heading": "4.2 Testing using mixed features",
      "text": [
        "After semantic features has been determined, we trained G-UNIMEM with 30, 60, 90, 120 instances and used all the 120 instances as testing instances each time.We hypothesized to choose semantic roles(i.e. case) as dominant cause features.",
        "The features such as ante(CASE), type(X), anaphor(CASE) and fi (CASE) are dominant features and the number of fi is variant.",
        "The hypothesis was motivated by Sidner [13] who used semantic roles to determine focus and resolve definite anaphora.",
        "The others such as h(Hm), p(POS), s2(SYN), d(D), con(s) belong to contingent features."
      ]
    },
    {
      "heading": "4.3 The experimental results",
      "text": [
        "It is interesting that the success numbers in Table 3 increased with the number of training instances.",
        "Finally, our results showed that experiments with cg-hierarchy had a little high accuracy rates (95.8% for resolving and 90.8% for choosing anaphora with 120 training instances) than those with g-c-hierarchy.",
        "Both accuracy rates were higher than those with Tai's criteria [14].",
        "Thus, G-UNIMEM with semantic roles as dominant features promised much higher accuracy rate.",
        "In Appendix B we show some sample rules acquired in Hornlike clauses.",
        "After examination, either the agent or theme of first sentence is most likely to act as antecedents of anaphora.",
        "This phenomenon is in coincidence with the investigation on anaphora by Sidner.",
        "That is, the agent often appeared as actor focus and theme as default focus .",
        "This is similar to Tai's criteria but is in more compact interpretation."
      ]
    },
    {
      "heading": "5 Discussion",
      "text": [
        "There are two concerns in implementing G-UNIMEM: (1) The feature set : Is the assignment of dominant features and contingent features objective?",
        "If there is any contingent feature in the assignment that obviously improves the accuracy rate, it should be assigned as dominant feature.",
        "We use statistical methods [] to analyze if contingent features actually improve accuracy rates.",
        "If there is no obvious improvement with contingent features, the division of dominant and contingent features is acceptable.",
        "We made the null hypothesis \"G-UNIMEM with cg-hierarchy doesn't have obvious improvement with contingent features\" and the alternative hypothesis \"G-UNIMEM with c-g-hierarchy has obvious improvement with contingent features\".",
        "We then got two test values from test statistics: ti = 0.8472 and £2 < 0.",
        "Both test statistics were less than t„ = .05 (= 1.734 with d.f.",
        "18).",
        "Thus, the null hypothesis \"GUNIMEM with c-g-hierarchy doesn't have obvious improvement with contingent features\" was not rejected, which justified that G-UNIMEM using semantic roles as dominant features was valid.",
        "(2) The sample size : Compared with actual linguistic domain, the 120 training and testing instances are small.",
        "A large corpus is desirable to test the system's performance.",
        "If it becomes available, our results would be more objective and reliable."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We have illustrated a way of using machine learning techniques to acquire anaphoric regularity in conjoined Mandarin Chinese sentences.",
        "The regularity was used to both choose and resolve anaphora with considerable accuracy.",
        "Table 4 shows a comparison between different approaches.",
        "In comparison to other approaches, the proposal of using G-UNIMEM as the acquisition model and using semantic roles as dominant features is practical and serves multiple purposes."
      ]
    },
    {
      "heading": "References",
      "text": [
        "[1] James Allen (1987), Natural Language Understanding, The 'Benjamin/Cummings Publishing Co. [2] Robert Berwick (1986), Learning From Positive-only Examples: The Subjective Principle and Three Case Studies.",
        "In R.Michalski, J. Carbonell, and T. Mitchell (ed.)",
        "Machine Learning: An Artificial Intelligence Approach Vol.",
        "II, Chapter 21, Morgan Kaufmann Publishers, Inc. [3] Ping Chen (1984), A Discourse Analysis of Third Person Zero Anaphora in Chinese, reproduced by Indianan University Linguistics Club, Bloomington, Indiana.",
        "[4] Ping Chen (1987), A Discourse Approach to Zero Anaphora in Chinese, Zhonggua Yuwen, Beijing.",
        "[5] Chauncey C. Chu (1983), Definiteness, Presupposition, Topic and Focus in Mandarin Chinese, Student Book company, Taipei, Taiwan.",
        "• et io s purpose asis computa i ity for anaphora istory ist reso ye search easy 1 o s s syntactic reso ye seam easy al • orithm c nose pre.",
        "ict somewhat easy cloose pre.",
        "ct not easy I V V ommant c oose reso ve pre ict easy features"
      ]
    },
    {
      "heading": "Appendix A. The basic classifier algorithm",
      "text": [
        "input: The current node N of the concept hierarchy.",
        "The name I of an unclassified instance.",
        "The set of I's unaccounted features F. Results: The concept hierarchy that classifies the instance.",
        "Top-level call: classifier( Top-node, I, F) Variables: N, N', C and NC are nodes in the hierarchy.",
        "G, H, and K are sets of features.",
        "J is an instance stored on a node.",
        "R is a variable of set.",
        "Classifier(N, I, F).",
        "Let G be the set of features stores in N. Let H be the features in F that match features in G. Let K1 be the features in F that do not match features in G. Let K2 be the features in G that do not match features in H. Let H', K1' and K2' be the sets of features after Adjust( H,K1,K2,H',K1',K2') /* adjust goal and cause features for g-c-hierarchy or c-g-hierarchy */ if N is not the root node, then if H is empty set /* no features match 5/ then return False else if both H' and K1' are not empty sets then { /* split node N */ split N into N' and NC where NC is a child of N'; N' contains features in H' with confidence scores and I as a instance with features K1'; each confidence score in H' is increased by 1; the remaining features and instances belong to NC; return Split.",
        "else if II' and H are equal /5 all features match */ then increase each confidence score in N by 1."
      ]
    }
  ]
}
