{
  "info": {
    "authors": [
      "Tung-Hui Chiang",
      "Yi-Chung Lin",
      "Keh-Yih Su"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C92-1055",
    "title": "Syntactic Ambiguity Resolution Using a Discrimination and Robustness Oriented Adaptive Learning Algorithm",
    "url": "https://aclweb.org/anthology/C92-1055",
    "year": 1992
  },
  "references": [
    "acl-A88-1019",
    "acl-C88-2133"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, a discrimination and robustness oriented adaptive learning procedure is proposed to deal with the task of syntactic ambiguity resolution.",
        "Owing to the problem of insufficient training data and approximation error introduced by the language model, traditional statistical approaches, which resolve ambiguities by indirectly and implicitly using maximum likelihood method, fail to achieve high performance in real applications.",
        "The proposed method remedies these problems by adjusting the parameters to maximize the accuracy rate directly.",
        "To make the proposed algorithm robust, the possible variations between the training corpus and the real tasks are also taken into consideration by enlarging the separation margin between the correct candidate and its competing members.",
        "Significant improvement has been observed in the test.",
        "The accuracy rate of syntactic disambiguation is raised from 46.0% to 60.62% by using this novel approach."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Ambiguity resolution has long been the focus in natural language processing.",
        "Many rule-based approaches have been proposed in the past, However, when applying such approaches to large scale applications, they usually fail to offer satisfactory performance.",
        "As a huge amount of fine-grained knowledge is required to solve the ambiguity problem, it is quite difficult for role-based approach to acquire the huge and fine-grained knowledge, and maintain consistency among them by human [Su 90a].",
        "Probabilistic approaches attack these problems by providing a more objective measure on the preference to a given interpretation.",
        "Then, these approaches acquire huge and fine grained knowledge, or parameters in statistic terms from the corpus automatically.",
        "The uncertainty problem in linguistic phenomena is resolved on a more solid basis if a probabilistic approach is adopted.",
        "Moreover, the knowledge acquired by the statistical method is always consistent because the knowledge is acquired by jointly considering all the data in the corpus at the same time.",
        "Hence, the time for knowledge acquisition and the cost to maintain consistency are significantly reduced by adopting those probabilistic approaches.",
        "To resolve the problems resulting from syntactic ambiguities, a unified statistical approach for ambiguity resolution has been proposed by Su [Su 88, 92b].",
        "In that approach, all knowledge sources, including lexical, syntactic and semantic knowledge, are encoded by a unified probabilistic score function with a uniform formulation.",
        "This uniform probabilistic score function has been successfully applied in spoken language processing [Su 90b, 91b, 92a] and machine translation systems [Chen 91] to integrate different knowledge sources for ambiguity resolution.",
        "In implementing this unified probabilistic score function, values of score functions are estimated from the data in the training corpus.",
        "However, due to the problem of insufficiency of training data and incompleteness of model knowledge, the statistical variations between the training corpus and the real application are usually not covered by this approach.",
        "Therefore, the performance in the testing set sometimes gets poor in the real application.",
        "To enhance the capability of discrimination and robustness of those proposed score function, a discrimination-oriented adaptive learning is proposed in this paper.",
        "And then, the robustness of this proposed adaptive learning procedure is enhanced by enlarging the margin between the correct candidate and its confusing candidates to achieve maximum separation between different candidates.",
        "Since the implementation of this adaptive learning procedure is based on the uniform probabilistic score function, we will first briefly review the unified probabilistic score function.",
        "Readers who are AC1ES DE COL1NG-92, NAFTIES, 23-28 AoUr 1992352 PROC.",
        "OF COLING-92, NAFTTES, Auc.",
        "23-28, 1992 lexical and log syntactic scores, i.e.,"
      ]
    },
    {
      "heading": "9 (Sling)",
      "text": [
        "tui, log S,0 (Lea+ Wayry log S (Slink) ul E log Plo.-+ to, E log P (1,11,1-1)",
        "where A10 (1) = log P^ ,tv;'), and A,0 (1) =- log P (L,11,1-1) .",
        "Both stand for the log lexical score and the log syntactic score of the i-th word for the k-th syntactic ambiguity, respectively.",
        "In addition, latex and correspond to the weights of lexical and syntactic scores, respectively.",
        "If the parse tree of a sentence is misselected, the parameters (i.e., the lexical and the syntactic scores) are adjusted via the proposed adaptive learning procedure.",
        "Otherwise, no parameters would be adjusted.",
        "When misselection occurs, the misclassification distance, d, is less than zero.",
        "This misclassification distance is defined as the difference between the log integrated score of the correct candidate and that of the top one.",
        "A specific term of the syntactic score components in the (t+1 )-th iteration of the correct candidate, say (j), would be adjusted as follows:",
        "where AA(s (j) is the amount of adjustment.",
        "This value is represented as",
        "where do is a constant which stands for a window size, and r is the learning constant for controlling the speed of convergence.",
        "The learning rule for adjusting the lexical scores can be represented in a similar manner.",
        "Notice that only the parameters of the top candidate and those of the correct candidate would be adjusted when misselections occur.",
        "Those parameters of other wrong candidates would not be adjusted in this adaptive learning procedure.",
        "From Eq.",
        "(13), (14) and (15), it is clear that the score of the correct candidate will increase and that of wrong candidate will decrease from iteration to iteration until the correct candidate is selected.",
        "For the purpose of clarity, the detailed derivations of the above adaptive learning procedure will not be given here.",
        "Interested readers can contact the authors for details."
      ]
    },
    {
      "heading": "3.3. Robustness Issues",
      "text": [
        "Since it is easy to improve the performance in a training set by adopting a model with a large number of parameters, the en-or rate measured in the training set frequently turns out to be overoptimistic.",
        "Moreover, the parameters estimated from the training corpus may be quite differ from that obtained from the real applications.",
        "These phenomena may occur due to the factors of finite sampling size, style mismatch, or domain mismatch, etc.",
        "To achieve a better performance in the real application, one must deal with the possible mismatch of parameters, or statistical variation between the training corpus and the real application.",
        "One way to achieve this goal is to enlarge the interclass distance to achieve maximum separation [Su 91a] between the correct candidate and the other candidates.",
        "That is, this approach provides a tolerance zone between different candidates for allowing possible data scattering in the real application.",
        "Traditional adaptive learning methods [Amar 67, Kata 90] stop adjusting parameters once the input pattern has been correctly classified.",
        "However, if we stop adjusting parameters under the condition that the observations are correctly classified in the training corpus, the distance between the correct candidate and other ambiguities may still be too small.",
        "Thus, it is vulnerable to deal with possible modeling errors and statistical variations between the training corpus and the real application.",
        "Su [Su 91a] has proposed a robust learning procedure which continues to enlarge the margin between the correct candidate and the top one, even if the syntax tree of the sentence has been correctly selected.",
        "That is, the parameters will not be adjusted only if the distance between the correct candidate and the others has exceeded a given threshold.",
        "The learning rules in Eq.",
        "(13), (14) are then modified as follows.",
        "If d < 6, where 6 is a preset margin, the syntactic score in the (t+1) iteration for the correct candidate is adjusted according to the following formulas:",
        "And, the syntactic score of the top candidate is adjusted as follows:",
        "The syntactic score of the syntax tree in Fig.]",
        "is then defined as S,,, (syn.,) ?-_-' P (L8, LT, L2 I Li)",
        "where syn A is the parse tree, and 1,1 through L8 represent different phrase levels.",
        "Note that the product terms in the last formula correspond to the rightmost derivation sequence in a general LR parser [Su 91c], with left and right contexts taken into account.",
        "Therefore, such a formulation is especially useful for a generalized LR parsing algorithm, in which context-sensitive processing power is desirable.",
        "Although the context-sensitive model in the above equation provides the ability to deal with intra-level context-sensitivity, it fails to catch inter level correlation.",
        "In addition, the formulation of Eq.",
        "(9) gives rise to the normalization problem for ambiguous syntax trees with different number of nodes.",
        "An alternative to relieve this problem is to compact multiple highly correlated phrase levels into one in evaluating the syntactic scores.",
        "The formulation is expressed as follows [Su 91c]:",
        "Table 1 Categories for words and their log word-to-category scores.",
        "In Table 1, the log word-to-category score, log (P (c I w)), for each word is estimated from the training corpus by calculating their relative frequencies.",
        "For example, in the training corpus, the word \"I\" is used as pronoun for 60 times, and 40 times as noun.",
        "Then, the log word-to-category scores can be calculated as follows.",
        "Because the number of shifts, i.e., the number of terms in Eq.",
        "(I0), is always the same for all ambiguous syntax trees, the normalization problem is then resolved.",
        "Moreover, it provides a way to consider both intra-level context-sensitivity and inter-level correlation of the underlying context-free grammar.",
        "With such a score function, the capability of context-sensitive parsing (in probability sense) can be achieved with a context-free grammar.",
        "In this example, there are 2*2*2*1=8 possible different ways to assign lexical categories to the input l',6ntence.",
        "When these 8 possible lexical sequences are parsed, only four of them are accepted by our parser.",
        "They are listed as follows:",
        "1. pron vt art n 2. n vt art n 3. pron vi prep n 4. n vi prep n. The syntactic scores of different parse trees are then calculated according to Eq.(10).",
        "A parse tree corresponding to the lexical sequence \"[pron vt art n]\" is drawn below as an example.",
        "3.",
        "Discrimination and Robustness Oriented Adaptive Learning 3.1.",
        "Concepts of Adaptive Learning",
        "The general idea of adaptive learning is to adjust the model parameters (in this paper, they are lexical scores and syntactic scores) to achieve the desired criterion (in our case, it is to minimize the error rate).",
        "To explain clearly how the adaptive learning works, we take the sentence \"I saw a man.\" as an example.",
        "The lexical category (i.e., part of speech) and its corresponding log score for each word are listed in Table I."
      ]
    },
    {
      "heading": "4. Simulations",
      "text": [
        "The following experiments are conducted to investigate the advantage of the proposed discrimination and robustness oriented adaptive learning procedure.",
        "In the experiments, 4,000 sentences, which are extracted from IBM tecturical manuals are first associated with their corresponding correct category sequences and correct parsed trees by linguists.",
        "The corpus arc then partitioned into a training corpus of 3,200 sentences and a test set of 800 sentences.",
        "Next, the lexical and syntactic probabilities are estimated from the data in the training corpus.",
        "Afterwards, the sentences in the test set are used to evaluate the performance of the proposed algorithm using the estimated lexical and syntactic probabilities.",
        "This integrated score function approach using the estimated probabilities is considered as the baseline system.",
        "Performances of discrimination oriented adaptive leamings with and without robustness enhancement are then evaluated.",
        "The accuracy rate of the syntactic ambiguity resolution for the training corpus and the test set are summarized in",
        "interested in the details about the uniform probabilistic score function please refer [Chen 91, Su 91b, 92a, 92b].",
        "2.",
        "Overview of Uniform Probabilistic Score Function 2.2.",
        "Lexical Score Function Let eko denote the k-th sequence of the lexical category, or pant of speech, corresponding to the word sequence rel'.",
        "The Lexical Score Function can be expressed as follows [Chen 91, Su 92b]: 2.1.",
        "General Definition",
        "where 1111' is the input word sequence, 'nil w2, , wJ, and Lexi, the corresponding lexical string, i.e., part of speech sequence {eil , cj2, ,cj,,}, By applying the multiplication theorem of probability, P IS'yn,, Lee, I wn can be restated as follows.",
        "The two components, s., (Sy,,,) and s, (Lee,), in the above formula are called syntactic Score Function and Lexical Score Function, respectively.",
        "The original score function, i.e., P(Sy,,,, Lex,114), is then called Integrated Score Function.",
        "Next, we assume the information, from the word sequence re71`, required for syntactic ambiguity resolution, has percolated to the lexical interpretation Lex j.",
        "Also, only little additional information can be provided from relz for the task of disambiguating syntactic interpretation Synj after the lexical interpretation Lexi is given.",
        "Thus, the syntactic score can be approximated as shown in Eq.",
        "(3):",
        "(3) The integrated score function P (Syn,, Lex, 114) is then approximated as follows.",
        "As (Pfrk.",
        "w.))+A/9 (ek, I Cs._,), where A is the lexical weight (A = 0.6 is used in the current setup), and g is a transform function (log).)",
        "is used inn this paper).",
        "Hence, given both Eq.",
        "(6) and (7), the following formula is derived:",
        "[pen vt art nl(-0.7)+(-0.3)+(-0.3)+(-0.2) =-1.5 [n start n](-0.2)+(-0.3)+(-0.3)+(-0.2) =-1.0 [pan vi prep n](-0.7)+(-0.7)+(-0.4)+(-0.3) =-2.1 [n vi prep n](-0.2) +(-0.7)+(-0.4)+(-0.3) =-1.6 Table 2 log syntactic scores or the grammatical Input lexical sequences.",
        "According to Eq.",
        "(5), the total log integrated score (log Siez+logS390) for each parsed sentence hypothesis is calculated.",
        "For example, the log lexical score for \"I/[pron] saw/[vt] a/[art] man/[n]\" = (- 0.22-0.16-0.02-0).",
        "-0.4.",
        "Finally, the log integrated scores for the above grammatical inputs are listed as follows: log integrated score = (-0.40-1.5 = -1.90) 1/1pronj saw/[vt] a/fart] man/In) log integrated score = (-0.57-1.0 = -1.57) : 11[n] saw/(vt) Mart] man/In] log integrated score = (-2.04-2.1 = -4.71) Mpron] saw/(vi] Mprep] man/(n] log integrated score = (-2.21-1.6 = -3.81) : 1/[n] saw/[vi] Mprep] man/1n] Among these four candidates, the candidate 1 is regarded as the desired selection by linguists.",
        "Since our decision criterion will select the candidate which has the highest integrated score, i.e., the second one; 1/[n] saw/Ivt] a/[art] man/[n], it results in a decision error in this case.",
        "To remedy this error, adaptive learning procedure is adopted to adjust the score values iteratively, including lexical and syntactic scores, until the integrated score of the correct candidate (i.e., candidate 1) raises to the highest rank.",
        "In this paper, parameters which are adjusted by adaptive learning procedure are those log scores, including log P (ck, I w1), log P (ck, I ckri) and log P (Li I 121-1).",
        "The amount of adjustment in each iteration depends on the misclassification distance.",
        "Misclassification distance is defined as the difference between the score of the top candidate and that of the correct one.",
        "(In the above example, distance = (score of correct candidate)-(score of top candidate) = (-1.90)- (-1.57)= -0.33).",
        "From iteration to iteration, the parameters (both lexical and syntactic scores) are adjusted so that the integrated score of the correct candidate is increased, and the integrated score of the wrong candidate is decreased at the same time.",
        "The learning procedure for a sentence is stopped when the candidate of this sentence is correctly selected.",
        "To make the explanation of this adaptive learning procedure clear, we assume lexical scores are unchanged during learning.",
        "That is, only the parameters of the syntactic scores are adjusted.",
        "The details of adaptive learning for adjusting syntactic scores are listed as follows: Initialization candidate I A syntactic score =1-0.7 -0.3 -0.3 -0.2] = -1.5, log integrated score = -1.9; candidate -2. syntactic score = [-0.2 -0.3 -0.3 -0.2] = -1.0, log integrated score = -1.57; candidate -3.syntactic score = 1-0.7 -0.7 -0.4 -0.3] = -2.1, log integrated score = -4.71; candidate -4.syntactic score = (-0.2 -0.7 -0.4 -0.3] = -1.6, log integrated score = -3.81; Iteration I candidate -I.",
        "A syntactic score = [-0.5 -0.3 -0.3 -0.21 = -1.3, log integrated score = -1.7; candidate -2. syntactic score =1-0.3 -0.3 -0.3 -0.2] = -1.1, log integrated score = -1.67; candidate -3.syntactic score =1-0.5 -0.7 -0.4 -0.3] = -1.9, log integrated score = -3.94; candidate -4.syntactic score = [-0.3 -0.7 -0.4 -0.3] = -1.7, log integrated score = -3.91; Iteration 2 candidate I A syntactic score = [-0.2 -0.3 -0.3 -0.2] -= -1.0, log integrated score = -1.4; (stop learning) candidate -2,syntactic score = 1-0.6 -0.3 -0.3 -0.2] = -1.4, log integrated score = -1.97; candidate -3.syntactic score [-0.2 -0.7 -0.4 -0.3] = -1.6, log integrated score = -3.64; candidate .4.syntactic score = [0.6 -0.7 -0.4 -0.3] = -2.0, log integrated score = -4.21; (where 4' denotes the top candidate, and A denotes the desired candidate) It is clear that after the second iteration, parameters have been adjusted so that the desired candidate (i.e., candidate 1) would be selected."
      ]
    }
  ]
}
