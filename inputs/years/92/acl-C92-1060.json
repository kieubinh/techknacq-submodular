{
  "info": {
    "authors": [
      "Julian Kupiec"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C92-1060",
    "title": "An Algorithm for Estimating the Parameters of Unrestricted Hidden Stochastic Context-Free Grammars",
    "url": "https://aclweb.org/anthology/C92-1060",
    "year": 1992
  },
  "references": [
    "acl-H89-2014",
    "acl-H91-1046"
  ],
  "sections": [
    {
      "heading": "ABSTRACT",
      "text": [
        "A new algorithm is presented for estimating the parameters of a stochastic context-free grammar (SCFC) from ordinary unparsed text.",
        "Unlike the Inside/Outside (I/0) algorithm which requires a grammar to be specified in Chomsky normal form, the new algorithm can estimate an arbitrary SCFG without any need for transformation.",
        "The algorithm has worst-case cubic complexity in the length of a sentence and the number of nonterminals in the grammar.",
        "Instead of the binary branching tree structure used by the I/O algorithm, the new algorithm makes use of a trellis structure for computation.",
        "The trellis is a generalization of that used by the Baum-Welch algorithm which is used for estimating hidden stochastic regular grammars.",
        "The paper describes the relationship between the trellis and the more typical parse tree representation."
      ]
    },
    {
      "heading": "INTRODUCTION",
      "text": [
        "This paper describes an iterative method for estimating the parameters of a hidden stochastic context free grammar (SCFG).",
        "The \"hidden\" aspect arises from the fact that some information is not available when the grammar is trained.",
        "When a parsed corpus is used for training, production probabilities can be estimated by counting the number of times each production is used in the parsed corpus.",
        "In the case of a hidden SCFG, the characteristic grammar is defined but the parse trees associated with the training corpus are not available.",
        "To proceed in this circumstance, some initial probabilities are assigned which are iteratively re estimated from their current values, and the training corpus.",
        "They are adjusted to (locally) maximize the likelihood of generating the training corpus.",
        "The EM algorithm (Dempster, 1977) embodies the approach just mentioned; the new algorithm can be viewed as its application to arbitrary SCFG's.",
        "The use of unparsed training corpora is desirable because changes in the grammar rules could conceivably require manually re parsing the training corpus several titne,s during grammar development.",
        "Stochastic grammars enable ambiguity resolution to performed on the rational basis of most likely interpretation.",
        "They also accommodate the development of more robust grammars having high coverage where the attendant ambiguity is generally higher.",
        "Previous approaches to the problem of estimating hidden SCFG's include parsing schemes in which all derivations of all sentences in the training corpus are enumerated (Fujisaki et al., 1989; Chitrao Az Grishrnan, 1990)).",
        "An efficient alternative is the Inside/Outside (I/O) algorithm (Baker, 1979) which like the new algorithm, is limited to cubic complexity in both the number of nonterrninals and length of a sentence.",
        "The I/0 algorithm requires that the grammar be in Clionisky normal form (CNF).",
        "The new algorithm has the same complexity, but does not have this restriction, dispensing with the need to transform to and from CM,."
      ]
    },
    {
      "heading": "TERMINOLOGY",
      "text": [
        "The training corpus can be conveniently segmented into sentences for puposes of training; each sentence comprising a sequence of words.",
        "A typical one may consist of 'Y 1 words, indexed from 0 to Y: ( wow tca wr) The lookup function W(y) returns the index k of the vocabulary entry sk matching the word tvi, at position y in the sentence.",
        "The algorithm uses a extension of the representation and terminology used for \"hidden Markov models\"(hidden stochastic regular grammars) for which the Baum-Welch algorithm (Baum, 1972) is applicable (and which is also called the Forward/Backward (F/B) algorithm).",
        "grammar rules are represented as networks and illustrated graphically, maintaining a correspondence",
        "Given a constituent is spanning x...y, Paboot(z, y, n) indicates how the constituents spanning v...y and labeled in that immediately dominate is relate to the constituents that are in turn external to in via [30,(v, y, cm).",
        "This situation is shown in Figure 6, where for simplicity f30ft(v, y, rorn) has not been graphically indicated.",
        "Note that in can dominate x...y as well as left subtrees.",
        "y, 1, n) defines another recursion for a constituent labeled is that spans x...y, and is in state I at time y.",
        "The recursion relates the addition of right sub trees (spanning y+1...w) to the remaining external tree, via fint(x,ts, q, n).",
        "This is depicted in Figure 7 (again the external trees represented by A(x, w,q, n) are not shown).",
        "Also omitted from the figure is the first term of Equation 8 which relates the addition of a single terminal at position y + 1 to an external tree defined by 13i(x , 4 1,i, n).",
        "at and the various other probabilities for a beta trellis are defined next:",
        "The first term in Equation 9 describes the relationship of the tree external to x...y + I to the tree external to x...y, with respect to state j generating the terminal 11)5+1 at time y I I.",
        "If the constituent it spanning x...y is complete, the second term describes the probability of the external tree via constituents that immediately dominate 7i."
      ]
    },
    {
      "text": [
        "volving the root symbol of a grammar.",
        "An equivalent network for the same set of rules is shown in Figure 2.",
        "The lexical rules can be written compactly as networks, with fewer states.",
        "The transitions from the determiner state each have probability 0.5 (i.e a(1, 2) = a(1, 3) = 0.5).",
        "It should be noted that the algorithm can operate on either network."
      ]
    },
    {
      "heading": "TRELLIS DIAGRAM",
      "text": [
        "Trellis diagrams conveniently relate computational quantities to the network structure and a training sentence.",
        "Each network n has a set of Y 1 trellises for subsequences of a sentence tss...wy, starting at each different position and ending at subsequent ones.",
        "A single trellis spanning positions 0...2 is shown in Figure 4 for network NP.",
        "Nonterminal states are associated with a row of start nodes indicating where daughter constituents may start, and a row of end nodes that indicate where they end.",
        "A pair of start/end nodes thus refer to a daughter nonterminal constituent.",
        "In Figure 4, the ADJP network is referenced via the start state at position 0.",
        "An adjective is then generated by a terminal state in the trellis for the ADJP network, followed by a transition and another adjective.",
        "The ADJP network is left at position 1, and a transition is made to the noun state where the word \"cat\" is generated.",
        "Terminal states are associated with a single row of nodes in the trellis (they represent terminal productions that span only a single position).",
        "The path taken through the trellis is shown with broken a line.",
        "A path through different trellises has a corresponding unique tree representation, as exemplified in Figure 5.",
        "In cases where parses are ambiguous, several paths exist corresponding to the alternative derivations.",
        "We shall next consider the computation of the probabilities of the paths.",
        "Two basic quantities are involved, namely alpha and beta probabilities.",
        "Loosely speaking, the alphas represent probabilities of subtrees associated with nonterminals, while the betas refer to the rest of the tree structure external to the subtrees.",
        "Subsequently, products of these quantities will be formed, which represent the probabilities of productions being used in generating a sentence.",
        "These are summed over all sentences in the training corpus to obtain the expected number of times each production is used, based on the current production probabilities and the training corpus.",
        "These are used like frequency counts would be for a parsed corpus, to form ratios that represent new estimates of the production probabilities.",
        "The procedure is iterated several times until the estimates do not change between iterations (i.e. the overall likelihood of producing the training corpus no longer increases).",
        "The algorithm makes use of one set of trellis diagrams to compute alpha probabilities, and another for beta probabilities.",
        "These are both split into terminal, nonterminal-start and nonterminal-end probabilities, corresponding to the three different types of nodes in the trellis diagram.",
        "The alpha set are labeled ori, eii, and oni, respectively.",
        "The algorithm was originally formulated using solely the trellis representation (Kupiec, 1991) however the definitions that follow will",
        "y,p, ii): The probability of generating the prefuc wo...wk_k and suffix wr+1...wy given that network n generated wk _toy and the end node of state p is reached at position y.sentence is found from the top-level network nT,,, p(16) i3oik(x, 9, n)thiak(x, y, p, n)P =nr op) Top(nrop) There are four different kinds of transition:",
        "y, p, n) has the same form as the previous formula for /3,, but is used with nonterminal states.",
        "Via 'C.a.., and Ofid, it relates the tree external to the constituent n (spanning x...y) to the trees external to v...y and x...w. During the recursion, the trees shown in Figures 6 and 7 are substituted into each other (at the positions shown with shaded areas).",
        "Thus the external trees are successively extended to the left and right, until the root of the outermost tree is reached.",
        "It can be seen that the values for ,8(x,y,p,n) are defined in terms of those in other networks which reference n via As a result this computation has a top-down order.",
        "In contrast, the cv(x,y,p, n) probabilities involve other networks that are referred to by network n and so assigned in a bottom-up order.",
        "If the network topology for Chorasky normal form is substituted in equation (12), the recursion for the \"Outer\" probabilities of the I/O algorithm can be derived after further substitutions.",
        "The lime probabilities for final states then correspond to the outer probabilities."
      ]
    },
    {
      "heading": "RE-ESTIMATION FORMULAE",
      "text": [
        "Once the alpha and beta probabilities are available, it is a straightforward matter to obtain new parameter estimates (A, B, I).",
        "The total probability P of a",
        "1.",
        "Terminal node i to terminal node j.",
        "2.",
        "Terminal node i to nontermina/ start node p. 3.",
        "Nontermsna/ end node p to nonterminal start q.",
        "4.",
        "Nonterminal end node p to terminal node i.",
        "The expected total number of times a transition is made from state i to state j conditioned on the observed sentence is E(1,14,1).",
        "The following formulae give E(0) for each of the above cases:",
        "A new estimatej) for a typical transition is then:",
        "Only B matrix elements for terminal states are used, and are re-estimated as follows.",
        "The expected total number of times the k'th vocabulary entry vk is generated in state i conditioned on the observed sentence is (15)E(m,k).",
        "A new estimate for b(i, k) can then be found:"
      ]
    },
    {
      "heading": "DISCUSSION",
      "text": [
        "The preceding equations have been implemented as a computer program and this section describes some practical issues with regard to a robust implementation.",
        "The first concerns the size of the B matrix.",
        "For pedagogical reasons individual words were shown as elements of this matrix.",
        "A vocabulary exceeding 220,000 words is actually used by the program BO it is not practical to try to reliably estimate the B matrix probabilities for each word individually.",
        "Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categories are given the same label.",
        "Thus \"type\", \"store\" and \"dog\" would all be labelled as singular-noun-or-uninflected-verb.",
        "For the category set that is used, only 250 different equivalence classes are necessary to cover the dictionary.",
        "It is important that the initial guesses for parameter values are well-informed.",
        "All productions for any given nonterminal were intially assumed to be equally likely, but the B matrix values were conveniently copied from a trained hidden Markov model (HMM) used for text-tagging.",
        "The HMM was also found very useful for verifying correct program operation.",
        "The algorithm has worst-case cubic complexity in both the length of a sentence and the number of nonterminal states in the grammar.",
        "An index can be used to efficiently update terminal states.",
        "For any word (or equivalence class) the index determines which terminal states require updating.",
        "Also when all probabilities in a column of any trellis become zero, no further computation is required for any other columns in the trellis.",
        "Grammars are currently being developed, and initial experiments have typically used eight training iterations, on training corpora comprising 10,000 sentences or more (having an average of about 22 words per sentence).",
        "To complement the training algorithm, a parser has also been constructed which is a corresponding analogue of the Cocke-Younger-Kasami parser.",
        "The parser is quite similar to the training algorithm, except that maximum probability paths are propagated instead of sums of probabilities.",
        "Trained grammars are used by the parser to predict the most likely syntactic structure of new sentences.",
        "The applications for which the parser was developed make use of incomplete parses if a sentence is not covered by the grammar, thus top-down filtering is not used."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": [
        "also be related to the consituent structures used in the equivalent parse trees.",
        "In the following equations, three sets will be mentioned: and whose next extension will involve trees dominated by N(p, n), the nonterrninal referred to by state p.",
        "1.",
        "Term(n) The set of terminal states in network n. 2.",
        "Nonterm(n) This is the set of nonterminal states in network n. 3.",
        "Final(n) The set F of final states in network n. at(x, y, j, n): The probability that network n generates the words to....wy inclusive and is at the node for terminal state j at position y.",
        "ast.",
        "(x, y,p, n) represents the probability of a constituent for n that spans x...y, formed by extending the various constituents am,(x , v , p, n) (ending at v 1) with corresponding completed constituents starting at v, ending at y and dominated by N(p, n).",
        "The quantity a toi.i(v, y, n) refers to the probability that network n generates the words toy ...toy inclusive and is in a final state of is at position y. Equivalently it is the probability that nonterminal n dominates all derivations that span positions v...y.",
        "The atm./ probabilities correspond to the \"Inner\" (bottom-up) probabilities of the I/O algorithm.",
        "If a network corresponding to Chomsky normal form is substituted in equation (6), the recursion for the inner probabilities of the I/O alrrithm will be produced after further substitutions using equations (1)-(6).",
        "In the previous equations (5) and (6) it can be seen that the a,, probabilities for a network are defined recursively.",
        "They will never be self-referential if the grammar is cycle-free, (i.e. there are no derivations AA for any nonterminal production A).",
        "Although only cycle-free grammars are of interest here, it is worth mention that if cycles do exist (with associated probabilities less than unity), the recursions form a geometric series which has a finite sum.",
        "The alpha probabilities are all computed first because the beta probabilities make use of them.",
        "The latter are defined recursively in terms of trees that are external to a given constituent, and as a result the recursions are less obvious than those for the alpha probabilities.",
        "The basic recursion rests on the quantity fit, which involves the following functions fists.",
        "and Pside:"
      ]
    }
  ]
}
