{
  "info": {
    "authors": [
      "Ralph Grishman",
      "John Sterling"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C92-2099",
    "title": "Acquisition of Selectional Patterns",
    "url": "https://aclweb.org/anthology/C92-2099",
    "year": 1992
  },
  "references": [
    "acl-A92-1022",
    "acl-C90-3010",
    "acl-H90-1053",
    "acl-H90-1056",
    "acl-H91-1059",
    "acl-H91-1060",
    "acl-J91-2002",
    "acl-P91-1030",
    "acl-P91-1036"
  ],
  "sections": [
    {
      "heading": "1 The Problem",
      "text": [
        "For most natural language analysis systems, one of the major hurdles in porting the system to a new domain is the development of an appropriate set of semantic patterns.",
        "Such patterns are typically needed to guide syntactic analysis (as selectional constraints) and to control the translation into a predicate-argument representation.",
        "As systems are ported to more complex domains, the set of patterns grows and the task of accumulating them manually becomes more formidable.",
        "There has therefore been increasing interest in acquiring such patterns automatically from a sample of text in the domain, through an analysis of word co-occurrence patterns either in raw text (word sequences) or in parsed text.",
        "We briefly review some of this work later in the article.",
        "We have been specifically concerned about the practicality of using such techniques in place of manual encoding to develop the selectional patterns for new domains.",
        "In the experiments reported here, we have therefore been particularly concerned with the evaluation of our automatically generated patterns, in terms of their completeness and accuracy and in terms of their efficacy in performing selection during parsing."
      ]
    },
    {
      "heading": "2 Patterns and Word Classes",
      "text": [
        "In principle, the semantic patterns could be stated in terms of individual words - this verb can meaningfully occur with this subject, etc.",
        "In practice, however, this would produce an unmanageable number of patterns for even a small domain.",
        "We therefore need to define semantic word classes for the domain and state our patterns in terms of these classes.",
        "Ideally, then, a discovery procedure for semantic patterns would acquire both the word classes and the patterns from an analysis of the word co-occurrence patterns.",
        "In order to simplify the task, however, while we are exploring different strategies, we have divided it into separate tasks, that of acquiring word classes and that of acquiring semantic patterns (given a set of word classes).",
        "We have previously described [1] some experiments in which the principal word classes for a sublanguge were obtained through the clustering of words based on the contexts in which they occurred, and we expect to renew such experiments using the larger corpora now available.",
        "However, the experiments we report below are limited to the acquisition of semantic patterns given a set of manually prepared word classes."
      ]
    },
    {
      "heading": "3 Pattern Acquisition",
      "text": [
        "The basic mechanism of pattern acquisition is straightforward.",
        "A sample of text in a new domain is parsed using a broad-coverage grammar (but without any semantic constraints).",
        "The resulting parse trees are then transformed into a regularized syntactic structure (similar to the f-structure of Lexical-Functional Grammar).",
        "This regularization in particular reduces all different clausal forms (active, passive, questions, extra-posed forms, relative clauses, reduced relatives, etc.)",
        "into a uniform structure with the 'logical' subject and object explicitly marked.",
        "For example, the sentence Fred ate fresh cheese from France.",
        "would produce the regularized syntactic structure (s eat (subject (np Fred)) (object (np cheese (a-pos fresh) (from (np France))))) We then extract from this regularized structure a series of triples of the form ACTES DE COLING-92, NANTES, 23-28 AGUT 1992 6 5 8 PROC.",
        "OF COUNG-92, NANTES, Auo.",
        "23-28, 1992 head syntactic-function value where - if the value is another NP or S - only the head is recorded.",
        "For example, for the above sentence we would get the triples eat subject Fred eat object cheese cheese a-pos fresh cheese from France Finally, we generalize these triples by replacing words by word classes.",
        "We had previously prepared, by a purely manual analysis of the corpus, a hierarchy of word classes and a set of semantic patterns for the corpus we were using.",
        "From this hierarchy we identified the classes which were most frequently referred to in the manually prepared patterns.",
        "The generalization process replaces a word by the most specific class to which it belongs (since we have a hierarchy with nested classes, a word will typically belong to several classes).",
        "As we explain in our experiment Section below, we made sonic runs generalizing just the value and others generalizing both the head and the value.",
        "As we process the corpus, we keep a count of the frequency of each head-function-value triple.",
        "In addition, we keep separate counts of the number of times each word appears as a head, and the number of times each head-function pair appears (independent of value)."
      ]
    },
    {
      "heading": "4 Coping with Multiple Parses",
      "text": [
        "The procedure described above is sufficient if we are able to obtain the correct parse for each sentence.",
        "However, if we are porting to a new domain and have no semantic constraints, we must rely entirely upon syntactic constraints and so will be confronted with a large number of incorrect parses for each sentence, along with (hopefully) the correct one.",
        "We have experimented with several approaches to dealing with this problem:",
        "1.",
        "If a sentence has N parses, we can generate triples from all the parses and then include each triple with a weight of 1/N.",
        "2.",
        "We can generate a stochastic grammar through unsupervised training on a portion of the corpus [2].",
        "We can then parse the corpus with this stochastic grammar and take",
        "only the most probable parse for each sentence.",
        "For sentences which still generated N > 1 equally-probable parses, we would use a 1/N weight as before.",
        "3.",
        "In place of a 1/N weighting, we can refine the weights for alternative parse trees using an iterative procedure analogous to the inside-outside algorithm [3].",
        "We begin by generating all parses, as in approach I.",
        "Then, based on tile counts obtained initially (using 1/N weighting), we can compute the probability for the various triples and from these the probabilities of the alternative parse trees.",
        "We can then repeat the process, recomputing the counts with weightings based on these probabilities.",
        "All of these approaches rely on the expectation that correct patterns arising from correct parses will occur repeatedly, while the distribution of incorrect patterns from incorrect parses will be more scattered, and so over a sufficiently large corpus---we can distinguish correct from incor- rect patterns on the basis of frequency."
      ]
    },
    {
      "heading": "5 Evaluation Methods",
      "text": [
        "To gather patterns, we analyzed a series of articles on terrorism which were obtained from the Foreign Broadcast Information Service and used as the development corpus for the Third Message Understanding Conference (held in San Diego, CA, May 1991) [4].",
        "For pattern collection, we used 1000 such articles with a total of 14,196 sentences and 330,769 words.",
        "Not all sentences parsed, both because of limitations in our grammar and because we impose a limit on the search which the parser can perform for each sentence.",
        "Within these limits, we were able to parse a total of 7,455 sentences.' The most clearly definable function of the triples we collect is to act as a selectional constraint: to differentiate between meaningful and meaningless triples in new text, and thus identify the correct analysis.",
        "We used two methods to evaluate the effectiveness of the triples we generated.",
        "The first 'For these runs we disabled several heuristics in our system which increase the number of sentences which can be parsed at sonic cost in the average quality of parses; hence the relatively low percentage of sentences which obtained parses.",
        "ACCES DE COLING-92, NAMES, 23-28 A01:7 1992 6 5 9 l'uou.",
        "or COLING-92, NANTEs, AUG. 23-28, 1992 method involved a comparison with manually-classified triples.",
        "We took 10 articles (not in the training corpus), generated all parses, and produced the triples from each parse.",
        "These triples were stated in terms of words, and were not generalized to word classes.",
        "We classified each triple as semantically valid or invalid (a triple was counted as valid if we believed that this pair of words could meaningfully occur in this relationship, even if this was not the intended relationship in this particular text).",
        "This produced a test set containing a total of 1169 distinct triples, of which 716 were valid and 453 were invalid.",
        "We then established a threshold T for the weighted triples counts in our training set, and defined V+ number of triples in test set which were classified as valid and which appeared in training set with count > T v_ number of triples in test set which were classified as valid and which appeared in training set with count < T i+ number of triples in test set which were classified as invalid and which appeared in training set with count > T s_ number of triples in test set which were classified as invalid and which appeared in training set with count < T and then defined By varying the threshold, we can plot graphs of recall vs. precision or recall vs. error-rate.",
        "These plots can then be compared among different strategies for collecting triples and for generalizing triples.",
        "The precision figures are somewhat misleading because of the relatively small number of invalid triples in the test set: since only 39% of the triples are invalid, a filter which accepted all the triples in the test set would still be accounted as having 61% precision.",
        "We have therefore used the error rate in the figures below (plotting recall against 1-error-rate).",
        "The second evaluation method involves the use of the triples in selection and a comparison of the parses produced against a set of known correct parses.",
        "In this case the known correct parses were prepared manually by the University of Pennsylvania as part of their \"Tree Bank\" project.",
        "For this evaluation, we used a set of 317 sentences, again distinct from the training set.",
        "In comparing the parser output against the standard trees, we measured the degree to which the tree structures coincide, stated as recall, precision, and number of crossings.",
        "These measures have been defined in earlier papers [5,6,7]."
      ]
    },
    {
      "heading": "6 Results",
      "text": [
        "Our first set of experiments were conducted to compare three methods of coping with multiple parses.",
        "These methods, as described in section 4, are (1) generating all N parses of a sentence, and weighting each by 1/N; (2) selecting the N most likely parses as determined by a stochastic grammar, and weighting those each by 1/N; (3) generating all parses, but assigning weights to alternative parses using a form of the inside-outside procedure.",
        "These experiments were conducted using a smaller training set, a set of 727 sentences drawn from 90 articles.",
        "We generated a set of triples using each of the three methods and then evaluated them against our hand-classified triples, as described in section 5.",
        "We show in Figure 1 the threshold vs. recall curves for the three methods; in Figure 2 the recall vs. 1-error rate curves.",
        "These experiments showed only very small differences between the three methods (the inside-outside method showed slightly better accuracy at some levels of recall).",
        "Based on this, we decided to use method 2 (statistical grammar) for subsequent experiments.",
        "Other things being equal, method 2 has the virtue of generating far fewer parses (an average of 1.5 per sentence, vs. 37 per sentence when all parses are produced), and hence a far smaller file of regularized parses (about 10 MB for our entire training corpus of 1000 articles, vs. somewhat over 200 MB which would have been required if all parses were generated).",
        "Using method 2, therefore, we generated the triples for our 1000-article training corpus.",
        "Our second series of experiments compared three different ways of accumulating data from the triples:",
        "1. generalizing the value in a head-function-value triple to a word class, but not generalizing the head",
        "with multiple parses in pattern collection, using training corpus of 90 articles.",
        "Recall vs. 1-error rate for o all parses; o = all parses + inside-outside; • = most probable parses from stochastic grammar.",
        "3. ignoring the value field entirely in a head-function-value triple, and accumulating counts of head-function pairs (with no generalization applied to the head); a match with the hand-marked triples is therefore recorded if the head and function fields match Again, we evaluated the patterns produced by each method against the hand-marked triples.",
        "Figure 3 shows the threshold vs. recall curves for each method; Figure 4 the recall vs. 1-error rate curves.",
        "Figure 3 indicates that using pairs yields the highest recall for a given threshold, triples with generalized heads an intermediate value, and triples without generalized heads the lowest recall.",
        "The error rate vs. recall curves of figure 4 do not show a great difference between methods, but they do indicate that, over the range of recalls for which they overlap, using triples without generalized heads produces the lowest error rate.",
        "Finally, we conducted a series of experiments to compare the effectiveness of the triples in selecting the correct parse.",
        "In effect, the selection procedure works as follows.",
        "For each sentence in the test corpus, the system generates all possible",
        "parses and then generates a set of triples from each parse.",
        "Each triple is assigned a score; the score for the parse is the product of the scores of the triples obtained from the parse (the use of products is consistent with the idea that the score for a triple to some degree reflects the probability that this triple is semantically valid).",
        "The parse or parses with the highest total score are then selected for evaluation.",
        "We tested three approaches to assigning a score to a triple:",
        "1.",
        "We used the frequency of head-function-value triples relative to the frequency of the head as an estimate of the probability that this head would appear with this function-value combination.",
        "We used the \"expected likelihood estimate\" [8] in order to assure that triples which do not appear in the training corpus are still assigned non-zero probability; this simple estimator adds 1/2 to each observed frequency: freq.",
        "of triple + 0.5 score – freq.",
        "of head + 0.5 2.",
        "We applied a threshold to our set of collected triples: if a triple appeared with a frequency above the threshold it was assigned one score; if at or below the threshold, a lower score.",
        "We selected a threshold of 0.9, so that any triple which appeared unambiguously in at least one sentence of",
        "the training corpus was included.",
        "For our scores, we used the results of our previous set of experiments.",
        "These experiments showed that at a threshold of 0.9, 82% of the triples above the threshold were semantically valid, while 47% of the triples below the threshold were valid.2 Thus we used score = 0.82 if freq.",
        "of triple > 0.9 0.47 if freq.",
        "of triple < 0.9",
        "3.",
        "We expanded on method 2 by using both triples and pairs information.",
        "To assign a score to a head-function-value triple, we first ascertain whether this triple appears with frequency > T in the collected patterns; if so, we assign a high score to the triple.",
        "If not, we determine whether the head-function pair appears with frequency > T in the collected patterns.",
        "If so, we assign an intermediate score to the triple;",
        "if not, we assign a low score to the triple.",
        "Again, we chose a threshold of 0.9 for both triples and pairs.",
        "Our earlier experiments indicated that, of those head-function-value triples for which the triple was below the threshold for triples frequency but the head-function pair was above the threshold for pair frequency, 52% were semantically valid.",
        "Of those for which the head-function pair was below the threshold for pair frequency, 40% were semantically valid.",
        "Thus we used score = 0.82 if freq.",
        "of triple > 0.9, else 0.52 if freq.",
        "of pair > 0.9, else 0.40 if freq.",
        "of pair < 0.9 Using these three scoring functions for selection, we parsed our test set of sentences and then scored the resulting parses against our \"standard parses\".",
        "As a further comparison, we also parsed the same set using selectional constraints which had been previously manually prepared for this domain.",
        "The parses were scored against the standard in terms of average recall, precision, and number of crossings; the results are shown in Table 1.3 A better match to the correct parses 'The actual value of the scores only matters in cases where one parse generates more triples than another.",
        "'These averages are calculated only over the subset of test sentences which yielded a parse with our grammar within the edge limit alloted.",
        "is reflected in higher recall and precision and lower number of crossings.",
        "These results indicate that the frequency-based scores performed better than either the threshold-based scores or the manually-prepared selection."
      ]
    },
    {
      "heading": "7 Related Work",
      "text": [
        "At NYU we have long been interested in the possibilities of automatically acquiring sublanguage (semantic) word classes and patterns from text corpora.",
        "In 1975 we reported on experiments using a few hundred manually prepared regularized parses for clustering words based on their co-occurrence patterns and thus generating the principal sublanguage word classes for a domain [1].",
        "In the early 1980's we performed experiments, again with relatively small corpora and machine-generated (but manually selected) parses, for collecting sublanguage patterns, similar to the work reported here [9].",
        "fly studying the growth curves of size of text sample vs. number of patterns, we attempted to estimate at that time the completeness of the sublanguage patterns we obtained.",
        "More recently there has been a surge of interest in such corpus-based studies of lexical co-occurrence patterns (e.g., [10,11,12,13]).",
        "The recent volume edited by Zernik [14] reviews many of these efforts.",
        "We mention only two of these here, one seeking a similar range of patterns, the other using several evaluation methods.",
        "Velardi et al.",
        "[11] are using co-occurence data to build a \"semantic lexicon\" with information about the conceptual classes of the arguments and modifiers of lexical items.",
        "This information is closely related to our selectional patterns, although the functional relations are semantic or conceptual whereas ours are syntactic.",
        "They use manually-encoded coarse-grained selectional constraints to limit the patterns which are generated.",
        "No evaluation results are yet reported.",
        "Hindle and Rooth [10] have used co-occurrence data to determine whether prepositional phrases should be attached to a preceding noun or verb.",
        "Unambiguous cases in the corpus are identified first; co-occurrence statistics based on these are then used iteratively to resolve ambiguous cases.",
        "A detailed evaluation of the predictive power of the resulting patterns is provided, comparing the patterns against human judgements over a set of 1000 sentences, and analyzing the error rate in terms of the type of verb and noun association."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "We have described two different approaches to evaluating automatically collected selectional patterns: by comparison to a set of manually-classified patterns and in terms of their effectiveness in selecting correct parses.",
        "We have shown that, without any manual selection of the parses or patterns in our training set, we are able to obtain selectional patterns of quite satisfactory recall and precision, and which perform better than a set of manual selectional patterns in selecting correct parses.",
        "We are not aware of any comparable efforts to evaluate a full range of automatically acquired selectional patterns.",
        "Further studies are clearly needed, particularly of the best way in which the collected triples can be used for selection.",
        "The expected likelihood estimator is quite crude and more robust estimators should be tried, particularly given the sparse nature of the data.",
        "We should experiment with better ways of combining of triples and pairs data to give estimates of semantic validity.",
        "Finally, we need to explore ways of combining these automatically collected patterns with manually generated selectional patterns, which will probably have narrower coverage but may be more precise and complete for the verbs covered."
      ]
    },
    {
      "heading": "9 Acknowledgements",
      "text": [
        "This report is based upon work supported by the Defense Advanced Research Projects Agency under Grant N00014-90-J-1851 from the Office of Naval Research and by the National Science Foundation under Grant IRI-89-02304.",
        "Acrrs DE COLING-92, NAArivs, 23-28 Aoirr 1992 6 6 3 PRoc.",
        "or COLING-92, NANTES, AUG. 23-28, 1992"
      ]
    }
  ]
}
