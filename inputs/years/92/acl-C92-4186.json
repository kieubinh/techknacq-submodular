{
  "info": {
    "authors": [
      "Lars Asker",
      "Björn Gambäck",
      "Christer Samuelsson"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C92-4186",
    "title": "EBL2: An Approach to Automatic Lexical Acquisition",
    "url": "https://aclweb.org/anthology/C92-4186",
    "year": 1992
  },
  "references": [
    "acl-C86-1091",
    "acl-C88-2111",
    "acl-C90-3010",
    "acl-E89-1019",
    "acl-H90-1071",
    "acl-P85-1035",
    "acl-P87-1027",
    "acl-P89-1004",
    "acl-P91-1021"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "A method for automatic lexical acquisition is outlined.",
        "An existing lexicon that, in addition to ordinary lexical entries, contains prototypical entries for various non-exclusive paradigms of open-class words, is extended by inferring new lexical entries from texts containing unknown words.",
        "This is done by comparing the constraints placed on the unknown words by the natural language system's grammar with the prototypes and a number of hand-coded phrase templates specific for each paradigm.",
        "Once a sufficient number of observations of the word in different contexts have been made, a lexical entry is constructed for the word by assigning it to one or several paradigm(s).",
        "Parsing sentences with unknown words is normally very time-consuming due to the large number of grammatically possible analyses.",
        "To circumvent this problem, other phrase templates are extracted automatically from the grammar and domain-specific texts using an explanation-based learning method.",
        "These templates represent grammatically correct sentence patterns.",
        "When a sentence matches a template, the original parsing component can be bypassed, reducing parsing times dramatically."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "A persisting trend in unification-based approaches to natural language processing is to incorporate large quantities of information in the lexicon, information that has traditionally resided in the gramniar rules.",
        "Acquiring a lexicon has thus become a difficult and time consuming task, even for moderately sized lex-ica.",
        "In addition to this, any natural language processing system intended for serious applications lutist include a large lexicon -- several thousands of words or more is commonly considered a minimum size which adds even more to the complexity of the problem.",
        "In view of this, tools for lexical acqusit ion are not only desirable – they become a necessity.",
        "Most approaches to this problem have been",
        "to construct a range of tools that require various degrees of interactive support when new lexical entries are created, either from raw text material (as in e.g., [Trost & Buchberger 86, Grosz et al. 87, Wilensky 90] and the early work by Zernik [Zernik k Dyer 85, Zernik 87]), or from machine readable dictionaries (see e.g., [Boguraev et al. 87, Calzolari k Hindi 90]).",
        "Although interactive tools for lexical acquisition greatly simplifies the task of constructing a lexicon, it is desirable to go one step further and fully remove the need for user interaction.",
        "One of the first.",
        "systems that aimed at constructing lexical entries automatically from raw text was Granger's FOUL-UP system [Granger 77].",
        "FOUL-UI' extended a lexicon by inferring restrictions placed on unknown words by instantiating scripts that matched the sentences containing the unknown words.",
        "This built on a number of assumptions which in general do not, hold, in particular: that all the information needed to create an entry is contained in one text; that no morphological information is needed; that specific (hand-coded) scripts covering the domain can be made available in advance.",
        "In One of the later approaches to automatic lexical acquisition from raw text, [Jacobs U Zernik 88] have shown the need to consult a variety of knowledge sources such as morphological, syntactic, semantic, and contextual knowledge when determining a new lexical entry.",
        "This paper describes an automatic method to acquire new lexical entries by using analytical learning in combination with strategies used in an existing interactive tool for lexical acquisition (VEX [Carter 89]).",
        "In the process of constructing a lexical entry, the system combines several different sources of information: the underlying NI, system (CLE, [Al-shawl (ed.)",
        "92]) will contribute information on syntactically and semantically permissible phrases and on the rules for inflectional morphology.",
        "The corpus will contribute information on which of these constructions actually occur.",
        "This information is combined with the the linguistic knowledge encoded in the interactive lexical acquisition tool to infer lexical entries for unknown words in the text.",
        "The rest of the paper is laid out as follows: Section 2 contains information about the various de-inents on which the method is based.",
        "Section 3 describes the method itself and Section 4 reports on For these \"paradigm words\" only, the complete set the current state of the implementation.",
        "of feature values is explicitly specified.",
        "2 The elements of the scheme"
      ]
    },
    {
      "heading": "2.1 The Core Language Engine, CLE",
      "text": [
        "The Core Language Engine is a general purpose natural language processing system for English developed by SRI Cambridge.",
        "It is intended to be used as a building block in a broad range of applications, e.g. database query systems, machine translation systems, text-to-speech/speech-to-text systems, etc.",
        "The object of the CLE is to map certain natural language expressions into appropriate predicates in logical form (or Quasi-Logical Form [Alshawi k van Eijck 89]).",
        "The system is based completely on unification and facilitates a reversible phrase-structure type grammar.",
        "The Swedish Institute of Computer Science has with support from SRI generalized the framework and developed an equivalent system for Swedish (the S-CLE, [Gamback Rayner 92]).",
        "The two copies of the CLE have been used together to form a machine translation system [Alshawi et a191].",
        "The S-CLE has a fairly large grammar covering most of the common constructions in Swedish.",
        "There is a good treatment of inflectional morphology, covering all main inflectional classes of nouns, verbs and adjectives.",
        "The wide range of possible applications have put severe restrictions on the type of lexicon that can he used.",
        "The S-CLE has a function-word lexicon containing about 400 words, including most Swedish pronouns, conjunctions, prepositions, determiners, particles and \"special\" verbs.",
        "In addition, there is a \"core\" content-word lexicon (with common nouns, verbs and adjectives) and domain specific lexica.",
        "This part of the system is still under development and all these content-word lexica together have about 750 entries.",
        "The lexical entries contain information about inflectional morphology, syntactic and semantic sub-categorization, and sortal (selectional) restrictions.",
        "Information about the linguistic properties of an entry is represented by complex categories that include a principal category symbol and specifications of constraints on the values of syntactic/semantic features.",
        "Such categories also appear in the CLE's grammar and matching and merging of the information encoded in them is carried out by unification during parsing.",
        "Two categories can be unified if the constraints on their feature values are compatible.",
        "In the actual \"core\" and domain lexica, this information is kept implicit and represented as pointers to entries in a \"paradigm\" lexicon with a number of words representing basic word usages and inflections."
      ]
    },
    {
      "heading": "2.2 The Vocabulary EXpander, VEX",
      "text": [
        "In the English CLE, new lexicon entries can be added by the users with a tool developed for the purpose.",
        "This lexicon acquisition tool, the Vocabulary EXpander, is fully described in [Carter 89].",
        "In parallel with the development of the S-CLE, a Swedish version of the VEX system was designed [Gamback 92].",
        "VEX allows for the creation of lexical entries by users with knowledge both of a natural language and of a specific application domain, but not of linguistic theory or of the way lexical entries are represented in the CLE.",
        "It presents example sentences to the user and asks for information on the grammaticality of the sentences, and for selectional restrictions on arguments of predicates.",
        "VEX adopts a copy and edit strategy in constructing lexical entries.",
        "It builds on the -paradigm\" lexicon and sentence patterns, that is, declarative knowledge of the range of sentential contexts in which the word usages in that lexicon can occur.",
        "III the present work we want to investigate to what extent such creation of lexicon entries can be performed with a minimum of user interaction.",
        "Instead of presenting example sentences to the user we are allowing the program to use a very large text where hopefully unknown words will occur in several different sentence patterns.",
        "This strategy will be further described in the following sections.",
        "First, however, we will define what we mean by the notion of (subcategorization) \"paradigm\".",
        "The definition we adopt here is based on the one used in [Carter 89], namely that"
      ]
    },
    {
      "heading": "Definition 1",
      "text": [
        "a paradigm is any Minimal non-empty intersection of lexical entries.",
        "Every category in a paradigm will occur in exactly the sante set of entries in the lexicon as every other category (if any) in that paradigm.",
        "Every entry consists of a disjoint union of paradigms.",
        "Here, we assume that a lexicon can be described in terms of (a small set of) such paradigms, relying on the fact that the open-class words exhibit at least approximate regularities.'"
      ]
    },
    {
      "heading": "2.3 The Lexicon Learning system, L2",
      "text": [
        "Previous experiments in automatic lexical acquistition at SICS (L2 - Lexicon Learning) used a set of",
        "sentences and a formal grammar to infer the lexical categories of the words in the sentences.",
        "The original idea was to start with an empty lexicon, assuming that the grammar would place restrictions on the words in the sentences sufficient to determine an assignment of lexical categories to them [Rayner ei al 88].",
        "This can be viewed as solving a set of equations where the words are variables that are to be assigned lexical categories and the constraints that all sentences parse with respect to the grammar are the equations.",
        "Unfortunately, it proved almost impossible to parse sentences containing several unknown words.",
        "For this reason the scheme was revised in several ways [iThrmander 88]; instead of starting with an empty lexicon, the starting point became a lexicon containing closed-class words such 11.9 pronouns, prepositions and determiners.",
        "The system would then at each stage only process sentences that contained exactly one unknown word, the hope being that the words learned from these sentences would reduce the number of unknown words in the other ones.",
        "In addition to this, a morphological component was included-to guide the assignments.",
        "Although the project proved the feasibility of the scheme, it also revealed some of its inherent problems, especially the need for faster parsing methods."
      ]
    },
    {
      "heading": "2.4 Explanation-based learning, EEL",
      "text": [
        "A problem with all natural language grammars is that they allow a vast number of possible constructions that very rarely, if ever, occur in real sentences.",
        "The application of explanation-based learning2 (EBL) to natural language processing allows us to reduce the set of possible analyses and provides a solution to the parsing inefficiency problem mentioned above (Subsection 2.3).",
        "The original idea [Rayner 88] was to bypass normal processing and instead use a set of learned rules that performed the tasks of the normal parsing component.",
        "fly indexing the learned rules efficiently, analysing an input sentence using the learned rules is very much faster than normal processing [Samuels-son & Rayner 91].",
        "'the learned rules can be viewed as templates for grammatically correct phrases which are extracted from the grammar and a set of training sentences using explanation-based learning.",
        "Here, we assume the following definition: Definition 2",
        "(at arbitrary, but pre-defined, deep levels of nesting) as leaves The fact that the templates are derived from the original grammar guarantees that they represent correct phrases and the fact that they are extracted from real sentences ensures that they represent constructions that actually occur."
      ]
    },
    {
      "heading": "3 Explanation-based lexical learning, EBL2",
      "text": [
        "The basic algorithm goes as follows:",
        "1.",
        "Using a large corpus from the domain, extract templates from the sentences containing no unknown words.",
        "2.",
        "Analyse the remaining sentences (the ones containing unknown words) using the templates, while maintaining an interim lexicon for the unknown words.",
        "3.",
        "Compare the restrictions placed on the unknown words by the analyses obtained with other hand-coded phrase templates specific for the paradigms in the lexicon.",
        "4.",
        "Create \"real\" lexical entries from the information in the interim lexicon when a full set of such templates (covering a paradigm) has been found.",
        "In the following subsections, we will address these issues in turn."
      ]
    },
    {
      "heading": "3.1 Extracting templates from a domain-specific corpus",
      "text": [
        "A typical situation where we think that this method is well suited is when a general purpose NL system with a core lexicon (such as the S-CLE) is to be customized to a specific application domain.",
        "The vocabulary used in the domain will include e.g. technical terms that are not present in the core lexicon.",
        "Also, the use of the words in the core lexicon may differ between domains.",
        "In addition to this, some types of grammatical constructs may be more common in one domain than in another.",
        "We will try to \"get the flavour of the language\" in a particular application environment.",
        "from domain-specific texts.",
        "The corpus is divided into two parts: one with sentences containing unknown words, and another where all the words are known.",
        "The latter group is used to extract phrase templates that capture the grammatical constructions occurring in the domain.",
        "The process of extracting phrase templates from training sentences is outlined in Subsection 2.4.",
        "Acres no COLING-92, Niorrns, 23-28 /Loin 1992 1 1 7 4 PROC.",
        "or COLING-92, NAMES, Alio.",
        "23-28, 1992"
      ]
    },
    {
      "heading": "3.2 Analysing the remaining sentences",
      "text": [
        "Assuming that a particular set of phrase ternplates is applicable to a sentence containing an unknown word will associate a set of constraints with the word.",
        "Naturally, the constraints on the known words of the sentence should be satisfied if this template is to be considered.' This will correspond to a particular parse or analysis of the sentence.",
        "Thus a set of constraints is associated with each different pilrtie.",
        "A number of entries in the prototype lexicon will match the set of constraints associated with a sentence.",
        "Each prototype is an incarnation of a paradigm.",
        "Thus we can associate a word with a set of paradigms.",
        "(Note that the paradigms may be non-exclusive.)",
        "All such associations (corresponding to different, parses of the same sentence) are collected, and used to update the interim lexicon.",
        "The most obvious constraints come front syntactic considerations.",
        "If, in the sentence John loves a cal the word loves were unknown, while the other words did indeed have the obvious lexical entries, the grain-mar will require loves to be a transitive verb of third person singular agreement.",
        "Since the prototypes of verbs are in the imperative form, we must associate a finite verb form with the imperative.",
        "This is clone by applying a morphological rule that, strips the '-s' from the word loves, reinforcing the hypothesis and gaining the tense information in the process.",
        "Now, this morphological information may seers unimportant in English, but it definitely is not in Swedish: a word with more than one syllable ending with '-or' has to be an indefinite common gender noun.",
        "If it is not of latin origin it 'oust be a plural form and thus its entire morphology is known.",
        "The odds that it is a countable noun (like dark), as opposed to a mass noun (such as water), are overwhelming."
      ]
    },
    {
      "heading": "3.3 Constructing lexical entries",
      "text": [
        "During the analysis•of the set of sentences containing unknown words, an interim lexicon for these unknown words is kept.",
        "The interim lexicon is indexed on word sterns and updated each tone a urw sentence is processed.",
        "For each word stein, two pieces of information are retained in this lexicon: a hypothesis about which paradigm or set.",
        "of paradigms the word is assumed to belong to, and a justification that encodes all evidence relevant to the word.",
        "The justification is used to make the hypothesis mid is maintained so that, the entry may be updated when new information about.",
        "the word arrives.",
        "When :ill the phrase templates (sentence patterns) for fulfillment",
        "of a specific paradigm have been found, an entry for the word is made in the domain-specific lexicon that is being constructed.",
        "This is done while still keeping the justification information, since this might contain evidence indicating other word-senses or homographs."
      ]
    },
    {
      "heading": "4 Implementation status",
      "text": [
        "A preliminary version of the lexical acquisition system has been implemented in Prolog.",
        "The module extracting templates from sentences with known words is fully operational.",
        "'the parser for sentences with unknown words has also been tested, while the interim lexicon still is subject to experimentation.",
        "Presently, a very simple strategy for the interim lexicon has been tested.",
        "This version uses the set of all hypotheses as the justification and use their disjunction as the current hypothesis.",
        "We are currently working on extending this scheme to one incorporating the full algorithm described above.",
        "Unknown words are matched with the subcategorization paradigms of the s_cLE.",
        "In total 62 different syntactic/semantic paradigms are known by the present systein; 5 for Swedish nouns, 10 for adjectives, and all the others for verbs.",
        "The morphological inflections are subdivided into 14 different inflectional classes of nouns, 3 classes of adjectives, and 24 classes of verbs."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "We have outlined a method for automatic lexical acquisition.",
        "Au existing lexicon built on the usage of prototypical entries for paradigms of open-class words, is extended by inferring new lexical entries front texts containing unknown words.",
        "The constraints placed on these words by the grammar are compared with the prototypes and a hypothesis is wade shout what paradigm the word is most likely to belong to.",
        "The hypotheses about the unknown words are kept in an interim lexicon until a sufficient level of confidence is reached.",
        "Phrase templates are both hand-coded and extracted from the grammar and domain-specific texts using an explanation-based learning method."
      ]
    },
    {
      "heading": "6 Acknowledgements",
      "text": [
        "The work reported here was funded by the Foundation for the Swedish Institute of Computer Science and the Swedish National Board for Industrial and Technical Development.",
        "(NUTEK).",
        "Acres DE COLING-92, NANMS, 23-28 Hour 1992 1 1 7 5 1420C.",
        "or COL1NG-92, NforrEs, Auc.",
        "23-28, 1992 We would like to thank Manny Rayner and David Carter (SRI Cambridge) and Seif Ilaridi (SICS) for helpful discussions and suggestions, and Pierre Gander (Stockholm University) for valuable support."
      ]
    }
  ]
}
