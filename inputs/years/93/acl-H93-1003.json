{
  "info": {
    "authors": [
      "David S. Pallett",
      "Jonathan G. Fiscus",
      "William M. Fisher",
      "John S. Garofolo"
    ],
    "book": "Human Language Technology Conference",
    "id": "acl-H93-1003",
    "title": "Benchmark Tests for the DARPA Spoken Language Program",
    "url": "https://aclweb.org/anthology/H93-1003",
    "year": 1993
  },
  "references": [
    "acl-H92-1004",
    "acl-H92-1078",
    "acl-H93-1004",
    "acl-H93-1006",
    "acl-H93-1008",
    "acl-H93-1014",
    "acl-H93-1015"
  ],
  "sections": [
    {
      "heading": "1. INTRODUCTION",
      "text": [
        "This paper documents benchmark tests implemented within the DARPA Spoken Language Program during the period November, 1992 - January, 1993.",
        "Tests were conducted using the Wall Street Journal-based Continuous Speech Recognition (WSJ-CSR) 'corpus and the Air Travel Information System (ATIS) corpus collected by the Multi-site ATIS Data C011ection Working (MADCOW) Group.",
        "The WSJ-CSR tests consist of tests of large vocabulary (lexicons of 5,000 to more than 20,000 words) continuous speech recognition systems.",
        "The ATIS tests consist of tests of (1) ATIS-domain spontaneous speech (lexicons typically less than 2,000 words), (2) natural language understanding, and (3) spoken language understanding.",
        "These tests were reported on and discussed in detail at the Spoken Language Systems Technology Workshop held at the Massachusetts Institute of Technology, January 20-22, 1993.",
        "Tests implemented during this period also included experimental or \"dry run\" implementation of two new tests.",
        "In the WSJ-CSR domain, a \"stress test\" was implemented, using test material that was drawn from unidentified sub-corpora.",
        "In the ATIS domain, an experimental \"end-to-end\" evaluation was conducted that included examination of the subject-session \"logfile\".",
        "Following precedents established previously, the results of these dry-run tests are not included as part of the \"official\" NIST test results and are not discussed at length in this paper.",
        "Prior benchmark tests conducted within the DARPA Spoken Language Program are described in papers by Pallett, et al.",
        "in the several proceedings of the DARPA Speech and Natural Language Workshops from 1989 to 1992.",
        "Papers in the Proceedings of the February 1992 Speech and Natural Language Workshop describe the development of the WSJ-CSR corpus, collection procedures and initial experience in building systems for this domain.",
        "Initial use of the Pilot Corpus for a \"dry run\" of benchmark test procedures prior to the February 1992 Speech and Natural Language Workshop is reported in [1].",
        "ATIS-domain tests that were reported at the February 1992 meeting are documented in [2].",
        "System descriptions were submitted to NET by the benchmark test participants and distributed at the Spoken Language Systems Technology Workshop.",
        "Additional information describing these systems can be found references 5-23.",
        "Detailed information is not available (in published papers) for some systems."
      ]
    },
    {
      "heading": "2. WSJ-CSR TESTS: NEW CONDITIONS",
      "text": []
    },
    {
      "heading": "2.1. Stress Test",
      "text": [
        "The established benchmark test protocols for speech recognition systems are such that system developers have prior knowledge of the nature of the test material, based on access to similar development test sets.",
        "Some developers have consistently declined to report results for material of particular interest to DARPA program management (e.g., for secondary microphone data).",
        "Concern has been expressed that the sensitivity or \"robustness\" of some DARPA-sponsored recognition algorithms has not been adequately probed or the systems \"stressed\".",
        "DARPA program management requested that NIST implement, in early December, 1992, a \"dry run\" of a \"stress test\" in which the nature of the test material was unspecified.",
        "Participating DARPA contractors were required to document and freeze the system configuration used to process the test material prior to implementing the test, and to provide data for a baseline test of this system using the 20K NVP test subset of the Nov.'92 test material, as well as for the stress test set.",
        "Test hypotheses were scored by NIST using \"conditional scoring\" -- partitioning and reporting test results for individual test subsets.",
        "The stress test material consisted of a set of 320 utterance files, chosen from three components: (1) read 20K sentences, for 4 female speakers, (2) read 5K sentences, for 4 female speakers, and (3) spontaneously dictated news articles, for 2 male and 2 female speakers.",
        "The read speech included both primary and secondary microphones, so that there were 5 test subsets in all, each consisting of either 60 or 80 utterances.",
        "Reactions to the stress test, as well as to the test results, were mixed.",
        "In general, as would be expected, systems with trigram language models did better than those with bi-grams.",
        "Degradations in performance for the secondary microphone data were relatively smaller for some systems than others -- particularly for those sites that had devoted special effort to the issue of \"noise robustness\".",
        "However, because the individual test subsets and the number of speakers were small, the results of many of the paired comparison significance tests were inconclusive, suggesting that future applications of such a test procedure must involve larger test subsets."
      ]
    },
    {
      "heading": "2.2. New Significance Tests",
      "text": [
        "For several years, NIST has implemented two tests of statistical significance for the results of benchmark tests of speech recognition systems: the McNemar sentence error test (MN) and a Matched-Pair-Sentence-Segment-Word-Error (MAPSSWE) test, on the word error rate found in sentence segments.",
        "In niore recent tests, NIST has also implemented two additional tests: a Signed-pair (SI) test, and the Wllcoxon signed rank (WI) test.",
        "These additional tests are relevant to the word error rates found for individual speakers, and as such are particularly sensitive to the number of speakers in the test set.",
        "References to these tests can be found in the literature on nonparametric or distribution-free statistics."
      ]
    },
    {
      "heading": "2.3. Uncertainty of Performance Measurement Results",
      "text": [
        "Increasing attention is being paid, at NIST, to evaluating and expressing the uncertainty of measurement results.",
        "This attention is motivated, in part, by the realization that \"in general, it is not possible to know in detail all of the uses to which a particular NIST measurement result will be put.",
        "\"[3] Current NIST policy is that \"all NIST measurement results are to be accompanied by quantitative measurements of uncertainty\".",
        "In substance, the recommended approach to expressing measurement uncertainty is that recommended by the International Committee for Weights and Measures (CIPM).",
        "The CIPM-recommended approach includes: (1) determining and reporting the \"standard uncertainty\" or positive square root of the estimated variance for each component of uncertainty that contributes to the uncertainty of the measurement result, (2) combining the individual standard uncertainties into a determination of the \"combined standard uncertainty\", (3) multiplying the combined standard uncertainty by a factor of 2 (a \"coverage factor\", that for normally distributed data corresponds to the 95% confidence interval), and specifying this quantity as the \"expanded uncertainty\".",
        "The expanded uncertainty, along with the coverage factor, or else the combined standard uncertainty, is to be reported.",
        "The paired-comparison significance tests outlined in the previous section represent specific instantiations of tests that evaluate the validity of null hypotheses regarding differences (in measured performance) between systems.",
        "In many cases, however, sufficiently detailed data is not available to implement these tests.",
        "In these cases it is important to refer to explicit estimates of uncertainties.",
        "The case of evaluating the uncertainties associated with performance measurements for spoken language technology is particularly complex because of the number of known complicating factors.",
        "These factors include properties of the speaker population (e.g., gender, dialect region, speaking rate, vocal effort, etc.",
        "), properties of the training and test sets (e.g., vocabulary size, syntactic and semantic properties, microphone/channel, etc.)",
        "and other factors [4].",
        "Performance measures used to date within the DARPA spoken language research community (and included in this paper) do not conform to the recommended approach, since the scoring software, in general, generates a single measurement for the ensemble of test data (e.g., one datum indicating word or utterance error rate for the entire multi-speaker, multi-utterance, test subset, rather than the mean error rate for the ensemble of speakers).",
        "These single-measurement performance evaluation procedures do not yield estimates of the variances \"for each component of uncertainty that contributes to the uncertainty of the measurement result\" that are required in order to implement the CIPM-recommended practice.",
        "In future tests, revisions to the scoring software that would permit estimates of the variance across the speaker population (at the least) are in order.",
        "However, it would seem to be the case that identifying and obtaining quantitative estimates of \"each component of uncertainty that contributes to the uncertainty of the measurement\" will be difficult."
      ]
    },
    {
      "heading": "3. WSJ-CSR NOVEMBER 1992 TEST MATERIAL",
      "text": [
        "The test material, as distributed, included a total of 16 identified test subsets.",
        "In general, these can be sub-categorized five ways: speaker dependent/independent (SD/SI), 5K/ 20K reference vocabularies, the use of verbalized/non-verbalized punctuation (VP/NVP), read/spontaneous speech, and primary (Sennheiser, close-talking)/secondary microphone.",
        "No one participant reported results on all subsets most reported results on only one or two, corresponding to conditions of particular local interest and/or algorithmic strength.",
        "All of the test material was drawn from the WSJ-CSR Pilot Corpus that was collected at MIT/LCS, SRI International, and TI.",
        "The \"spontaneous dictation\" data was collected only at SRI.",
        "Individual test set sizes varied from 72 utterances to (more typically) approximately 320 utterances.",
        "The number of speakers in each subset varied from 3 to 12 speakers.",
        "The actual number of sentence utterances per speaker varied somewhat, because the material was selected in paragraph blocks.",
        "A total of 8 secondary microphones was included in the various test subsets, including one speakerphone, a telephone handset, 3 boundary effect microphones (Crown PCC-160, PZM-6FS, and Shure SM91), two lavalier microphones, and a desk-stand mounted microphone."
      ]
    },
    {
      "heading": "4. WSJ-CSR TEST PROTOCOLS",
      "text": [
        "Test protocols were similar to prior speech recognition benchmark tests.",
        "Test material was shipped to the participating sites on October 20th, results were reported on Nov. 23rd, and NIST reported scored results via ftp to the participants on Dec. 2nd.",
        "The stress test was conducted between Nov. 30th and Dec. 15th.",
        "A \"required baseline\" test was defined for all participants.",
        "It consisted of processing the 5K word speaker independent, non-verbalized punctuation test set using a (common) bi-gram grammar.",
        "Six sites reported 5K baseline test results."
      ]
    },
    {
      "heading": "5. WSJ-CSR TEST SCORING",
      "text": [
        "As for the test protocols, much of the scoring was routine, except for one new additional factor.",
        "Since previous \"official\" CSR benchmark tests had not included spontaneous speech, the community had not reviewed the adequacy of the transcription convention used for spontaneous speech, and several inconsistencies in the transcriptions were noted following release of the preliminary results.",
        "Some of these inconsistencies were resolved prior to releasing \"official\" results."
      ]
    },
    {
      "heading": "6. WSJ-CSR TEST PARTICIPANTS",
      "text": [
        "Participants in these WSJ-CSR tests included the following DARPA contractors: BBN, CMU, Dragon Systems, MIT Lincoln Laboratory, and SRI International.",
        "A \"volunteer\" participant was the French CNRS LIMSI.",
        "LIMSI declined to participate in the \"stress test\"."
      ]
    },
    {
      "heading": "7. WSJ-CSR BENCHMARK TEST RESULTS AND DISCUSSION",
      "text": []
    },
    {
      "heading": "7.1. Test Results: Word and Utterance (Sentence) Error Rates",
      "text": [
        "Table 1 presents the results for the several test sets on which results were reported.",
        "Section I of that table includes results reported by Paul at MIT Lincoln Laboratory [5] for Longitudinal Speaker Dependent (LSD) technology.",
        "Section II includes results reported by BBN for Speaker Dependent (SD) technology.",
        "Section III includes the results of Speaker Independent (SI) technology, for a number of sites for (a) the 20K NVP test set for both baseline and non-baseline SI systems, (b) the 5K NVP test set for both baseline and non-baseline SI systems, (c) the 5K NVP test set \"other micro-phone\" test set data, and (d) the 5K VP test set (on which only LIMSI reported results [6]).",
        "Section IV of Table 1 includes the results reported by BBN for the Spontaneous Dictation test set.",
        "For the test set on which the largest number of results were reported -- the 5K NVP set, using the close-talking microphone -- the lowest word error rates were reported by CMU [7-9]: 6.9% for the baseline, bigram language model, and 5.3% using a trigram language model.",
        "The range of word error rates for the baseline condition for all systems tested was 6.9% to 15.0%, while for non-baseline conditions, the range was from 5.3% to 16.8%.",
        "For the 5K NVP test set's secondary microphone data, as reported by CMU [8] and SRI [10,11], word error rates ranged from 17.7% to 38.9%.",
        "For the 20K NVP test set, on which other baseline data were reported, the word error rates range from 15.2% to 27.8%.",
        "The lowest error rate, reported by CMU, can be shown to be significantly different for all 4 significance tests when compared with the Dragon [13] and MIT Lincoln systems, but shown to be significantly different only for the MAPSSWE test when compared with the BBN system [14].",
        "Thus the performance differences between the CMU and BBN systems for this baseline condition test are very small."
      ]
    },
    {
      "heading": "7.2. Significance Test Results",
      "text": [
        "Table 2 presents the results, in a matrix form, of 4 paired-comparison significance tests for the baseline tests for the 5K NVP test set.",
        "The convention in this form of results tabulation is that if the result of a null-hypothesis test is valid, the word \"same\" is printed in the appropriate matrix element.",
        "If the null hypothesis is not valid, the identifier for the system with the lower (and significantly different) error rate is printed.",
        "For this test set, recall that the CMU system (here identified as cmul-a) had a word error rate of 6.9%.",
        "By comparing the results for the CMU system with the other 5 systems reporting baseline results, note that the significance test results all indicate that the null hypothesis is not valid.",
        "In other words, the error rates for the CMU system are significantly different (lower) than those for the other 5 systems for this test set and baseline conditions.",
        "In general, for this test set, with 12 speakers and 310 utterances, the Wilcoxon signed rank test (WI) is more sensitive than the (ordinary) sign test (SI).",
        "As noted in previous tests, the McNemar test (MN), operating on the sentence error rate, is in general less sensitive than the matched-pair-sentence segment word error rate test (MAPSSWE)."
      ]
    },
    {
      "heading": "8. ATIS TESTS: NEW CONDITIONS",
      "text": [
        "Within the community of ATIS system developers, there is a continuing search for evaluation methodologies to complement the current evaluation methodology.",
        "In particular there is a recognized need for evaluation methodologies that can be shown to correlate well with expected performance of the technology in applications.",
        "Toward the end of 1992, several sites participated in an experimental \"end-toend\" evaluation to assess systems in an interactive form.",
        "The end-to-end evaluation included (1) objective measures such as timing information and time to task completion, (2) human-derived judgements on correctness of system answers and user solutions, and (3) a user satisfaction questionnaire.",
        "The results of this \"dry run\" complementary evaluation experiment are reported by Hirschman et al.",
        "in [15]."
      ]
    },
    {
      "heading": "9. ATIS TEST MATERIAL",
      "text": [
        "Test material for the ATIS benchmark tests consisted of 1002 queries, for 118 subject-scenarios, involving 37 subjects.",
        "It was selected by NIST from set-aside material drawn from data previously collected within the MADCOW community at AT&T, BBN, CMU, MIT/LCS, and SRI.",
        "The selection and composition of this test material is described in more detail in [15].",
        "As in previous years, queries were categorized into two categories of \"answerable\" queries, Class A, which are context-independent, and Class D, which are context-dependent; and \"unanswerable\", or Class X queries.",
        "In the final adjudicated test set, there were a total of 427 Class A queries, 247 Class D queries, and 328 Class X queries."
      ]
    },
    {
      "heading": "10. ATIS TEST PROTOCOLS",
      "text": [
        "As was the case for the speech recognition benchmark tests, ATIS test protocols were similar to prior ATIS benchmark tests.",
        "The test material was shipped to the participating sites on October 20th, results were reported on Nov. 16th, and NIST reported preliminary scored results via ftp to the participants on Nov. 20th.",
        "After the process of formal \"adjudication\" had taken place, official results were reported on Dec. 20th."
      ]
    },
    {
      "heading": "11. ATIS SCORING AND ADJUDICATION",
      "text": [
        "After the preliminary scoring results were distributed, the participating sites were invited to send requests for adjudication (\"bug reports\") to NIST, asking for changes in the scoring of specific queries.",
        "A total of 146 of these bug reports were adjudicated by NIST and SRI jointly.",
        "Since many of these requests for adjudication were duplicates, the number of distinct problems reported was less than 100.",
        "A decision was made on each request for adjudication and the corrected reference material or procedure was used in a final adjudicated rerun of the evaluation.",
        "The judgment was in favor of the plaintiff in approximately 2/3 of the cases.",
        "A number of problems uncovered by this procedure were systematic, in that the same root problem affected several different queries.",
        "Most of these were simply human error, which can be made less likely in the future by working less hectically and making software to double-check the test material.",
        "The major problem that cannot be attributed to just human error is that of transcribing and scoring correctly speech that is difficult to hear and understand.",
        "Some of this speech was \"sotto voce\"; some was mispronounced; some was truncated; and in some cases the phonetic transcription would have been unproblematical but division into lexical words was unclear, as in some contractions and compound words.",
        "The short-term solution adopted was just to make our best judgement on orthographic transcription, considering both acoustics and higher-level language modeling.",
        "But a better long-term cure is to make and use transcriptions that can indicate alternatives when the word spoken is uncertain; proposals to this effect are being considered by relevant committees."
      ]
    },
    {
      "heading": "12. ATIS TEST PARTICIPANTS",
      "text": [
        "Participants in these ATIS tests included the following DARPA contractors: BBN, CMU, MIT Laboratory for Computer Science (MIT/LCS), and SRI.",
        "There were several \"volunteers\": AT&T Bell Laboratories [16], who have participated in previous years; Paramax [17], not a DARPA contractor at the time of these tests, but who have also participated in prior years' tests; and two participants from Canada, CRIM and INRS.",
        "A total of 8 system developers participated in some of the tests (i.e., the NL tests)."
      ]
    },
    {
      "heading": "13. ATIS BENCHMARK TEST RESULTS 13.1. ATIS SPontaneous speech RECognition Tests (SPREC)",
      "text": [
        "Table 3 presents the results for the SPREC tests for all systems and all subsets of the data.",
        "For the interesting case of the subset of all answerable queries, Class A+D, the word error rate ranged from 4.3% to 100%.",
        "The lowest value was reported by BBN [18,19], and the value of 100% was reported by INRS, for an incomplete ATIS system that (in effect) rejected every utterance, resulting in a scored word deletion error of 100%.",
        "Table 4 presents a matrix tabulation of ATIS SPREC results for the set of answerable queries, Class A+D.",
        "This form of matrix tabulation is discussed in [2] for the February 1992 test results.",
        "Considerable variability can be noted for the performance of some systems on \"local data\", and there are indications of varying degree of difficulty for the subsets collected at different sites.",
        "As in the Feb.'92 test set, participants noted the presence of more disfluencies in the AT&T data than for other originating sites.",
        "Word error rates for the \"volunteers\" in these tests (AT&T, CRIM and INRS) are in general higher than for DARPA contractors, perhaps reflecting a reduced level-of-effort, relative to \"funded\" efforts.",
        "Table 5 presents the results, in a matrix form, of 4 paired-comparison significance tests for the 7 SPREC systems for the Class A+D subset.",
        "For this test set, recall that the BBN system (here identified as bbn2a_d) had a word error rate of 4.3%.",
        "By comparing the results for this BBN system with the other 6 ATIS SPREC systems, note that the null hypothesis is not valid for all 4 significance tests for the comparisons with the AT&T, CRIM, INRS, MIT/LCS and SRI systems.",
        "In other words, the differences in performance are significant.",
        "However, when comparing the BBN and CMU SPREC systems, the null hypothesis is valid for 3 of the 4 tests.",
        "Thus, as was the case for the WSJ-CSR data, the performance differences, in this case for ATIS spontaneous speech, between the CMU and BBN speech recognition systems are very small.",
        "13.2.",
        "Natural Language Understanding Tests (NL) Table 6 presents a tabulation of the results for the NL tests for all systems and the \"answerable\" ATIS queries, Class A+D, as well as the subsets, Class A and Class D. For the set of answerable queries, Class A+D, the weighted error ranges from 101.5% to 12.3%.",
        "For the Class A queries, the range is from 79.9% to 12.2%.",
        "And for the Class D queries, the range is from 138.9% to 12.6%.",
        "In each case, the lowest weighted error rate was reported by the CMU system [20].",
        "Note that in general performance is considerably worse for Class D than for Class A.",
        "However, for the CMU and MIT/ LCS [21] systems, performance for the Class D test material is comparable to that for Class A.",
        "These systems would appear to have superior procedures for handling context.",
        "Table 7 presents a matrix tabulation of the NL results for the several subsets of test material.",
        "Note, however, that since the differences in performance between DARPA-contractor-developed systems and those of \"volunteers\", in general, are significant, the column averages presented in this table are not very informative.",
        "Of the 3 CRIM systems, the best performing one (crim3) is one using neural networks to classify each query into 1 of 10 classes based on relation names in the underlying ATIS relational database, with subsequent use of specific parsers built for each class and another parser that determines the constraints [22].",
        "There are two SRI NL systems [23].",
        "The SRI NL-TM system, here designated sril, uses template matching to generate database queries.",
        "The other SRI system, termed the \"Gemini+TM ATIS System\" by SRI, and here designated sri2, is an integration of SRI's unification-based natural-language processing system and the Template Matcher.",
        "Differences in performance do not appear to be pronounced.",
        "As in previous ATIS NL tests, it is important to note that appropriate tests of statistical significance have not yet been developed for ATIS NL tests.",
        "Small differences in weighted error rate are probably of no significance.",
        "However, large, systematic, differences are noteworthy, even if of unknown statistical significance.",
        "The weighted error rates for the CMU NL system, which are in many cases approximately one-half those of the next best systems, are certainly noteworthy."
      ]
    },
    {
      "heading": "13.3. Spoken Language System Understanding (SLS)",
      "text": [
        "Table 8 presents a tabulation of the results for the SLS tests for all systems and the \"answerable\" ATIS queries, Class A+D, as well as the subsets, Class A and Class D. For the set of answerable queries, Class A+D, the weighted error ranges from 100% to 21.6%.",
        "For the Class A queries, the range is from 100% to 19.7%.",
        "And for the Class D queries, the range is from 140.1% to 23.9%.",
        "As in the case of the NL test results, and in each case, the lowest weighted error rate was reported for the CMU system.",
        "The INRS data signify 100% usage of the No_Answer option, since the INRS SPREC system provided null hypothesis strings, causing the NL component to return the No_Answer response.",
        "Note again that the CMU and MIT/LCS systems both handle context sensitivity well.",
        "Table 9 presents a matrix tabulation of the SLS results for the several subsets of test material.",
        "For the ATIS SLS with lowest overall weighted error rate (21.6%), the cmul system, there is an almost tenfold range in error rate over the several test subsets: from 37.1%, for the AT&T subset, to 3.9% for the SRI subset.",
        "The CMU SLS weighted error rates for Class A+D are approximately two-thirds those of the next-best-performing systems, although for the Class A subset, differences in performance between the CMU system and the BBN and SRI systems are less pronounced."
      ]
    },
    {
      "heading": "14. ACKNOWLEDGEMENT",
      "text": [
        "At NIST, our colleague Nancy Dahlgren contributed significantly to the DARPA ATIS community and had a major role in annotating data and implementing \"bug fixes\" in collaboration with the SRI annotation group and others.",
        "Nancy was severely injured in an automobile accident in November, 1992, and is undergoing rehabilitation therapy for treat",
        "ment of head trauma.",
        "It is an understatement to say that we miss her very much.",
        "Brett Tjaden also assisted us at NIST in preparing test material and other ways.",
        "The cooperation of the many participants in the DARPA data and test infrastructure -- typically several individuals at each site -- is gratefully acknowledged."
      ]
    }
  ]
}
