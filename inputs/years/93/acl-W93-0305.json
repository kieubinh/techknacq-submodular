{
  "info": {
    "authors": [
      "Chao-Huang Chang",
      "Cheng-Der Chen"
    ],
    "book": "Workshop on Very Large Corpora: Academic and Industrial Perspectives",
    "id": "acl-W93-0305",
    "title": "HMM-Based Part-Of-Speech Tagging for Chinese Corpora",
    "url": "https://aclweb.org/anthology/W93-0305",
    "year": 1993
  },
  "references": [
    "acl-A92-1018",
    "acl-J88-1003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Chinese part-of-speech tagging is more difficult than its English counterpart because it needs to be solved together wgh the problem of word identification.",
        "In this paper, we present our work on Chinese part-of-speech tagging based on a first-order, fully-connected hsdden Markov model.",
        "Part of the 1991 United Daily corpus of approzimately 10 million Chinese characters is used for training and testing.",
        "A news article is first segmented into clauses, then into words by a Viterbi-based word identification system.",
        "The (untagged} segmented corpus is then used to train the HMM for tagging using the Baum-Welch reestimation procedure.",
        "We also adopt Kupiec's concept of word equivalence classes in the tagger.",
        "Modeling higher order local constraints, a pattern-driven tag corrector is designed to postprocess the tag output of the Viterbi decoder based on trained HMM parameters.",
        "Experimental results for various testing conditions are reported: The system is able to correctly tag approximately 96% of all words in the testing data."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Part-of-speech tagged corpora are very useful for natural language processing (NLP) applications such as speech recognition, text-to-speech, information retrieval, and machine translation systems.",
        "Automatic part-of-speech tagging has been intensively studied and practiced for European languages [1-4,7,8,10].",
        "However, the technology of automatic Chinese part-of-speech tagging is still in its infancy, due to the following reasons: I.",
        "Definition of words in Chinese is not clear; there are not breaks between two adjacent words.",
        "For example, the string M – tiiit contains four characters, but it can be divided into one, two, three, or four words by different linguists.",
        "Other difficult cases include compound words (e.g., faM), split words (e.g., iii).",
        "acronyms (e.g.,JOM ), and literay words.",
        "2.",
        "Word segmentation can not be fully automatic.",
        "3.",
        "Well-defined tag set for Chinese part-of-speech is not available.",
        "9.",
        "A Chinese lexicon with complete parts-of-speech is hard to find.",
        "5.",
        "Chinese part-of-speech tagging is difficult even for human, i.e., the parts-of-speech for many words are either arguable or difficult to decide.",
        "6.",
        "Manually tagged Chinese corpora, counterparts of Brown corpus and LOB corpus in Chinese, are not, available.",
        "These intertwined problems make Chinese part-of-speech tagging an especially difficult task.",
        "Lee and Chang Chien [5,6] used a 'Fri-POS Markov language model and a bootstrap training process for Lagging a small Chinese corpus (1714 sentences for training and 233 sentences for testing).",
        "They reported a tagging accuracy 81.13% for all words and 87.60% for known words.",
        "In this paper, we present our work on part-of-speech tagging a large Chinese corpus based on a. hidden Markov model (HMM).",
        "This is among the first reports on automatic Chinese part-of-speech tagging in the literature [5,6]."
      ]
    },
    {
      "heading": "2 The HMM-based Part-of-Speech Tagger",
      "text": [
        "Kupiec [9] describes a HMM-based tagging system which can be trained with a corpus of untagged text.",
        "There are two new features in Kupiec's tagger: (1) word equivalence classes and (2) predefined networks.",
        "Words with the same set of parts-of-speech are defined as an equivalence class.",
        "For example, “type\" and ''store\" belong to the equivalence class noan-orverb.",
        "This not only reduces the number of parameters effectively and also makes the tagging system robust.",
        "The first-order model is extended with predefined networks based on error analysis and linguistic considerations.",
        "Their experimental results show that the predefined networks reduced the overall error rate by only 0.2%.",
        "Thus, we adopt the concept of equivalence classes but consider that predefined networks are not worthwhile.",
        "Let us briefly review the formulation of 11ls,11%1 for part-of-speech tagging: A first-order EIMM of N states and fs,1 possible observations has three sets of parameters: state transition probability distribution A (N by N), observation probability distribution B (N by M), and initial state distribution P (N).",
        "For an observation sequence 0 of length T, there are algorithms, e.g., Viterbi, to uncover the hidden state sequence I.",
        "For tagging, N is the number of parts-of-speech in the language, AI can be the number of words or the number of equivalence classes (as Kupiec defined).",
        "In Chinese.",
        "the number of words is more than 100,000 while the number of equivalence classes is less than 1.000.",
        "The use of equivalence classes reduces the size of B by 100 times.",
        "The problem of tagging is: Given a word sequence (observations), find out the correct part-of-speech sequence (states)."
      ]
    },
    {
      "heading": "2.1 The Part-of-Speech Tag Set",
      "text": [
        "The tag set contains 46 regular tags plus 11 special tags.",
        "Regular tags include AO (adjective), CO-C1 (conjunctions), DO-D2 (pronouns), 10 (interjection), MO (measure), N0-N9 (nouns), PO (preposition), RO-R6 (particles), TO (mood), UO-U4 (numbers), VOV4 (verbs), XO (onomatope), YO-Y4 (compounds), Z0.22 (adverbs).",
        "Special tags are for punctuations (PAR, SEN. PCT, DUN, COM, SEM, COL), unknown words (UNK), foreign words (ABC), and composed numbers (NUM, ARA).",
        "It is simplified and reorganized from the classification of Chinese Knowledge Information Processing Group (CKIP), Academia Sillies, Taipei.",
        "The original Ch:IP classification is a five-level system, too complicated even for human to use.",
        "Sun 1121 designed a three-level tag set TUCWS of 120 tags for Chinese word segmentation.",
        "However, they tag the corpus by hand without an automatic tagger.",
        "Thus, it is difficult to decide if the set is good for automatic tagging.",
        "Other Chinese tag sets can be found in the literature: 33 tags in Su [11], 30 tags in Lee and Chang Chien [5], and 34 tags in Lee et al.",
        "[6].",
        "These three tag sets are of two origins, CKIP [5] and NTHU [6,11].",
        "The numbers of tags in them are considered too small."
      ]
    },
    {
      "heading": "2.2 Corpus Preparation",
      "text": [
        "The 1991 United Daily corpus contains more than 10 million Chinese characters, about twenty days of news articles published by United Informatics, Inc. during January through March 1991.",
        "Basically, it is a collection of articles in the form of raw text (i.e., character stream).",
        "Thus, we have to segment the character stream into a word stream before it can be used for training or testing the model.",
        "The corpus preparation process consists of the following steps: Preprocessing Clean up inappropriate parts, such as titles, parenthesized texts, reporter information, figures, etc., in the input article.",
        "Articles mostly composed of inappropriate parts are deleted.",
        "Clause identification Divide up the article into clauses delimited by clause-ending punctuations such as periods, commas, question marks.",
        "Automatic word segmentation Segment the characters in a clause into words using a dictionary-based, Viterbi decoding word identification system.",
        "Manual correction (optional) Check the segmented text to correct segmentation errors due to unregistered words or inaccuracy of the segmentation algorithm.",
        "This step is optional but helpful especially for training.",
        "Equivalence class lookup Words in the clause are then converted to identifiers of equivalence class (EQC-ids) via dictionary lookup.",
        "After the above steps, an article is converted into a series of sequences of EQC-ids.",
        "Manual tagging of the whole corpus would take several man-years.",
        "However, tagged corpus is necessary",
        "for evaluation of the model and helpful for initialization of the HMM parameters as Merialdo [8] pointed",
        "out.",
        "Thus, we also tag part of the corpus by the steps below: (1) Train the IIMM using the articles to be tagged; (2) Tag the articles using the trained HMAl; (3) Correct the erroneous tags by hand."
      ]
    },
    {
      "heading": "2.3 Training the Model",
      "text": [
        "The untagged corpus of EQC-ids is then used for training the HMM for Lagging using the Baum-Welch reestimation procedure with multiple observation sequences [9] Before training, the model parameters, A, B, P, can be initialized with a tagged corpus.",
        "A The tag bigrains in the tagged corpus are counted to initialize A, the state transition matrix.",
        "All counts are incremented by one then normalized.",
        "B The EQC-id to tag correspondences are counted to set up B, the observation matrix.",
        "All possible states for an EQC are then incremented by one.",
        "P The initial state matrix P is initialized by counting the tags of first words in the clause.",
        "All counts are incremented by one then normalized.",
        "After training, the model parameters are adjusted to bestly predict the most probable tag sequence for the training data."
      ]
    },
    {
      "heading": "2.4 Automatic Tagging",
      "text": [
        "Having the trained model parameters, we can automatically tag an unseen text based on an HMM decoding algorithm such as Viterbi's. For a given clause, the tagging process is: Automatic word segmentation Segment the characters in the clause into words using the above-mentioned word identification system.",
        "Equivalence class lookup Words in the clause are then converted to EQC-ids via dictionary lookup.",
        "Viterbi decoding The sequence of EQC-ids.",
        "as observations, is then fed to the Viterbi decoder in order to find out the most probable hidden state sequence, namely, the tag sequence"
      ]
    },
    {
      "heading": "Pattern-driven Tag Correction",
      "text": [
        "First-order models are not enough to describe cal constraints for predicting part-of-speech tags.",
        "Higher-order models have much more parameters to estimate and need a lot more training data and resources (memory, CPU time).",
        "Kupiec [4] proposed using networks to model higher-order context based on error analysis and linguistic considerations.",
        "However, using networks is considered not elegant and had only very limited success.",
        "We use a simple pattern-driven Lag corrector to postprocess the tag output: The EQC-id sequence is matched against predefined patterns; when a match is found, the corresponding tag corrections are made.",
        "These patterns are designed according to analysis of error patterns."
      ]
    },
    {
      "heading": "2.5 The Dictionary",
      "text": [
        "The general dictionary has some 80,000 lexical entries each of which contains the Chinese characters and its EQC-id.",
        "The original dictionary is a collaborated work of CCL/ITRI with Academia Sinica, Taipei: ITRI collected the words, their pronunciations and word frequencies, while Academia Sinica provided syntactic and semantic markers.",
        "For our purpose, only the words and their syntactic information (parts-of-speech) are useful.",
        "As mentioned, we restructured the general dictionary based on our newly designed compact tag set.",
        "For purpose of comparison, we also constructed a closed dictionary in which the words and their tags in the training and testing corpora are collected."
      ]
    },
    {
      "heading": "2.6 An Example",
      "text": [
        "In the following, we use a real-world example to illustrate the tagging process.",
        "• A News Paragraph"
      ]
    },
    {
      "heading": "41\".Mtrit --P-FA DIlltittV",
      "text": [
        "IllifttflittEMT22=-K17.",
        "• Clause Identification 1 *Ipttfuftn+ – )1+:\":136flittAM.",
        "2.",
        "*F.17M – EliA_LIM."
      ]
    },
    {
      "heading": "3 ftValtMiirgIAtlialEnnIfifflitZglit",
      "text": [
        "• Word Segmentation 1.",
        "\"=\"7* ithVg Ei I )4 I 7: El RI",
        "A tagged corpus, called corpus 1, was prepared through the steps described in the subsection Corpus Preparation.",
        "The corpus is composed of 1,918 clauses or 12,289 word tokens.",
        "A larger corpus, called corpus3, contains 3,784 clauses.",
        "corpus3 is segmented but untagged, useful only for training.",
        "There are totally 338 word equivalence classes: Each of the 100 most frequently used ambiguous words is assigned a unique EQC-id; the rest 238 EQC-ids are assigned to sets of words with the same set of possible tags.",
        "• Equivalence Classes I. NI UNK PO NUM MO NUM MON3 VU N8 COM 2.",
        "D1 MON3 ZO POV2 NUM AOMOVO DON8 NO COM 3.",
        "POVO N3 MON4 UNK NOVO D220 NOVO C1N3P0 UNK NO NO D1N5U0 U2 U0 UNK COM • Tagging Results 1.",
        "Ni UNK PO NUM MO NUM MO VO N8 COM 2.",
        "DI MO ZO V2 NUM MO N8 NO COM 3.",
        "PO N3 N4 UNK VO D2 NO PO UNK NO NO UO 1)2 1)0 UNK COM • Correct Tags 1.",
        "N1 NO PO NUM MO NUM MO VO N8 COM 2.",
        "DI MO 20 V2 NUM MO N8 NO COM 3 PO N3 N4 N2 VU D2 VU PO NO NO NO UO U2 110 VU COM"
      ]
    },
    {
      "heading": "3 Experimental Results",
      "text": [
        "The whole tagging system, including word segmentation module, equivalence class mapper, HMM trainer, and Viterbi decoder, is implemented in C on a Sun Sparcstation.",
        "Table 1 shows the experimental results for an inside test on corpusi.",
        "The 80,000-word general dictionary was used and the model parameters are uniformly initialized, i.e., the tags in the corpus are not used to initialize the parameters.",
        "The accuracy rate for all words is 86.37% (1,674 errors out of 12,284 words).",
        "Excluding unknown words (words not in the dictionary), the accuracy rate is 93.16% (779 errors).",
        "in other words, approximately half of the errors can be attributed to unknown words.",
        "If we only consider ambiguous (multi-POS) words, the accuracy is 80.26% (771 errors).",
        "We can also observe that only about 35% of the words are ambiguous.",
        "(The difference between the latter two numbers of error is due to special usage of some registered words, e.g., 'everyday' is ZO (adverb) in the dictionary but is used as a company name N2 in it 'Everyday Department Store'.)"
      ]
    },
    {
      "heading": "3.2 Inside Test, Initialized with Tagged Text, General Dictionary",
      "text": [
        "Tagged texts are useful for initializing the model parameters before training.",
        "Table 2 shows that the accuracy for ambiguous words was improved by about three percent (from 80.26% to 83.21%).",
        "The accuracy",
        "In the last row, corpus3 (3,789 clauses, 35,899 words, translated AP news) was used for training while corpust (1,418 clauses, 12,284 words, domestic news) for testing.",
        "Due to difference of text type, accuracy rates are degraded by about 3 percent for ambiguous words.",
        "However, the system is still able to assign correct tags to 91.83 percent of all words.",
        "This shows the robustness of the model, due to the concept of equivalence classes."
      ]
    },
    {
      "heading": "3.5 Outside Test, Closed Dictionary",
      "text": []
    },
    {
      "heading": "3.3 Inside Test, Closed Dictionary",
      "text": [
        "All words and their used tags in corpus I are collected to form an ideal dictionary, so-called closed dictionary, for tagging the corpus.",
        "The 11MA1-based tagger is able to correctly tag 96.83% of all words or 84.00% of ambiguous words (Table 3).",
        "The accuracy rate is comparable to that of Kupiec's IIMM-based English tagger for the well-known Brown corpus."
      ]
    },
    {
      "heading": "3.4 Outside Test, General Dictionary",
      "text": [
        "Table 9 shows the results for outside tests.",
        "The corpus is divided into two parts: one for training, the other for testing.",
        "The first two columns (Train and Test) are the numbers of clauses (not words) used for training and testing, respectively.",
        "The accuracy rates are not, as good as those for inside tests: degraded by shout '2 percent for known words, by 5 percent for ambiguous words.",
        "In general, the system is able to tag approximately 80 percent of ambiguous words correctly.",
        "Table 5 summarizes the results for outside tests on closed dictionary, Approximately 96% of all words and 80% of ambiguous words are tagged correctly."
      ]
    },
    {
      "heading": "4 Error Analysis",
      "text": []
    },
    {
      "heading": "4.1 Confusion Matrix",
      "text": [
        "Table 6 shows part of the confusion matrix for the test described in subsection 3.2; only the confusing parts-of-speech are shown.",
        "The ANVZ problem: Due to lack of inflections in Chinese, a Chinese word can have many different parts-of-speech, yet only one form.",
        "It is sometimes very difficult even for human to identify the correct tag.",
        "For example, Chinese does not have -ing ending for nominalization of verbs, -ly for adverbs, Lion for verbal nouns, en for past participles.",
        "Thus, a word such as 3Alit can be a verb (VO) 'distribute', a noun (NO) 'distribution', an adjective (AO) 'distributive', 'distributing' or 'distributed', and an adverb (ZO) 'distributively' in different contexts.",
        "Nouns and verbs are especially hard to distinguish.",
        "That is why the VU-NO (180), NO-VU (47) confusions are common.",
        "The RP problem: Open classes, such as nouns and verbs, have large population, while closed classed, such as prepositions and particles have small",
        "population.",
        "In general.",
        "this is not a problem for tagging.",
        "However, in our tag set, R5 (aspect prefix) has only three members ri (PO R5 VU), 44, and M. The former two words are also common prepositions (PO).",
        "From the experiments, we observed that while is a preposition in most instances, it is always tagged as R5 (aspect).",
        "After studying the trained model parameters A, B, P, we found (Figure 1) that R5 was assigned large probabilities in B matrix (0.683 for ti , 0.227 for n ) since R5 has only three words while PO was assigned much smaller probabilities (Due to the probabilistic characteristic, sum of the observation probabilities for a state, such as PO, R5, must be one.)",
        "In addition.",
        "R5 and PO have not significant difference in the incoming or °incoming entries of A matrix because of the characteristic of unsupervised learning: all instances of 7)\", are considered as possible candidates for R5.",
        "We consider this as a weakness of HMM for tagging."
      ]
    },
    {
      "heading": "4.2 Error Patterns",
      "text": [
        "Tagging errors usually occur in clusters; that is, an error may cause further mistagging of its neighbors if they are also ambiguous Common patterns of mistagging include VO-VO (as NO-NO), ZO-VO (as AO-NO), VO-NO (as C1-Z2), VO-PO (as N0-R5), PO-NO (as R5-V0), P0-N1 (as R5-V4), and NO-VO-NO (as Ul-C]-Z2).",
        "They can be classified into three types: ANVZ type These error patterns are due to the above-mentioned ANVZ problem.",
        "This type of error is reasonable.",
        "R.?",
        "type Those error patterns involving R5 are due to the RP problem.",
        "The type of error should be eliminated by model improvement or post-processing.",
        "idiomatic type Some idiomatic expressions are composed of highly ambiguous words.",
        "For example, in \"V, ... , all the three words El (Cl N3 PO), Zi (Cl PO VO), (AO NO Z2), are 3-way ambiguous words.",
        "That is why the VO-NO sequence is frequently mistagged as C1-Z2.",
        "If we consider the mistagging of unknown words, more long tagging error clusters would appear.",
        "Actually, an unknown word not only causes mistagging of the word itself but also affects the tagging of its neighbors."
      ]
    },
    {
      "heading": "4.3 Without Equivalence Classes",
      "text": [
        "To verify feasibility of the concept of equivalence classes, we implemented a version of the HMM tagger considering each word as a unique observation (without EQC).",
        "Table 7 compares the results for inside/outside tests on closed dictionary.",
        "To our surprise, the concept of equivalence classes not only has the advantages of saving space/time and making the tagger robust but also achieve higher tagging accuracy, especially in case of outside tests.",
        "This might be due to insufficient training data for the much larger number of parameters to estimate.",
        "Nevertheless, it also proves that the concept is valid and useful."
      ]
    },
    {
      "heading": "5 Concluding Remarks",
      "text": [
        "We have presented our initial effort for Chinese part-of-speech tagging using a first-order fully-connected hidden Markov model and Kupiec's concept or equivalence classes.",
        "The experimental results show that the tagging model is promising.",
        "We have also discussed our observations on some imperfections of the current model.",
        "In the near future, we will (1) use the whole UD corpus to further validate and verify the system, (2) try to implement.",
        "a second-order HMM, (3) attempt to solve part of the unknown word tagging problem, (9) attempt to solve part of the compound word problem, (5) use heuristic rules for postprocessing the tagging output, (6) perform word identification and part-of-speech tagging concurrently, and (7) integrate the tagging HMM with the linguistic decoder of a Chinese speech recognition system."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This paper is a partial result of the project no.",
        "37112100 conducted by the ITRI under sponsorship of the Minister of Economic Affairs, R.O.C."
      ]
    }
  ]
}
