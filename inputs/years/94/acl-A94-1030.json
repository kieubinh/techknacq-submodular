{
  "info": {
    "authors": [
      "Dekai Wu",
      "Pascale Fung"
    ],
    "book": "Applied Natural Language Processing Conference",
    "id": "acl-A94-1030",
    "title": "Improving Chinese Tokenization With Linguistic Filters on Statistical Lexical Acquisition",
    "url": "https://aclweb.org/anthology/A94-1030",
    "year": 1994
  },
  "references": [
    "acl-P94-1010",
    "acl-P94-1012",
    "acl-W93-0305"
  ],
  "sections": [
    {
      "text": [
        "in the dictionary, leaving 4,610 previously unknown new entries.",
        "The same tokenization experiment was then run, using the augmented dictionary instead.",
        "The results shown in Figure 1 bear out our hypothesis that augmenting the lexicon with CXtract's statistically generated lexical entries would improve the overall precision, reducing error rates as much as 32.0% for k = 2."
      ]
    },
    {
      "heading": "EXPERIMENT III",
      "text": [
        "Morphosyntactic filters for lexicon candidates.",
        "CXtract produces excellent recall but we wished to improve precision further.",
        "Ideally, the false candidates should be rejected by some automatic means, without eliminating valid lexical entries.",
        "To this end, we investigated a set of 34 simple filters based on linguistic principles.",
        "Space precludes a full listing; selected filters are discussed below.",
        "Our filters can be extremely inexpensive because CXtract's statistical criteria are already tuned for high precision.",
        "The filtering process first segments the candidate using the original dictionary, to identify the component words.",
        "It then applies morphological and syntactic constraints to eliminate (a) sequences that should remain multiple segments and (b) ill-formed sequences.",
        "Morphological constraints.",
        "The morphologically-based filters reject a hypothesized lexical entry if it matches any filtering pattern.",
        "The particular characters in these filters are usually classified either as morphological affixes, or as individual words.",
        "We reject any sequence with the affix on the wrong end (the special case of the genitive n (de) is considered below).",
        "Because morphemes such as the plural marker IN (men) or the instance marker t (ci) are suffixes, we can eliminate candidate sequences that begin with them.",
        "Similarly, we can reject sequences that end with the ordinal prefix M (di) or the preverbial durative (zhi).",
        "Filtering characters cannot be used if they are polysemous or homonymous and can participate in legitimate words in other uses.",
        "For example, the durative 4 (zhe) is not a good filter because the same character (with varying pronunciations) can be used to mean \"apply\", \"trick\", or \"touch\", among others.",
        "Any candidate lexical entry is filtered if it contains the genitive/associative (Si (de).",
        "This includes, for example, both ill-formed boundary-crossing patterns like trifd; ;:-r (de wei xian, danger of), and phrases like 4CItiffilt (xiang gang de qian tii, Hong Kong's future) which should properly be segmented 4it fit] hilt.",
        "In addition, because the compounding process does not involve two double-character words as frequently as other patterns, such sequences were rejected.",
        "Closed-class syntactic constraints.",
        "The closed-class filters operate on two distinct principles.",
        "Sequences ending with strongly prenominal or preverbial words are rejected, as are sequences beginning with postnominals and postverbials.",
        "A majority of the filtering patterns match correct syntactic units, including prepositional, conjunctive, modal, adverbial, and verb phrases.",
        "The rationale for rejecting such sequences is that these closed-class words do not satisfy the criteria for being bound into compounds, and just co-occur with some sequences by chance because of their high frequency.",
        "Results.",
        "The same tokenization experiment was run using the filtered augmented dictionary.",
        "The filters left 5,506 candidate lexical entries out of the original 6,650, of which 3,467 were previously unknown.",
        "Figure 1 shows significantly improved precision in every measurement except for a very slight drop with k = 8, with an error rate reduction of 49.4% at k = 2.",
        "Thus any loss in token recall due to the filters is outweighed by the gain in precision.",
        "This may be taken as indirect evidence that the loss in recall is not large."
      ]
    },
    {
      "heading": "CONCLUSION",
      "text": [
        "We have introduced a blind evaluation method that accommodates multiple standards and gives some indication of how well algorithms' outputs match human preferences.",
        "We have demonstrated that pure statistically-based lexical acquisition on the same corpus being tokenized can significantly reduce error rates due to unknown words.",
        "We also demonstrated empirically the effectiveness of simple mor-phosyntactic filters in improving the precision of a hybrid statistical/linguistic method for generating new lexical entries.",
        "Using linguistic knowledge to construct filters rather than generators has the advantage that applicability conditions do not need to be closely checked, since the training corpus presumably already adheres to any applicability conditions."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": []
    }
  ]
}
