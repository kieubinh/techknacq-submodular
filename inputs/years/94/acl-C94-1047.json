{
  "info": {
    "authors": [
      "Boubaker Meddeb-Hamrouni"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C94-1047",
    "title": "Logic Compression of Dictionaries for Multilingual Spelling Checkers",
    "url": "https://aclweb.org/anthology/C94-1047",
    "year": 1994
  },
  "references": [
    "acl-C86-1068",
    "acl-C92-1010"
  ],
  "sections": [
    {
      "text": [
        "1.2.",
        "Lexical-based approach: Lexical-based approach appear after the first methods described above, when storage space become less expensive.",
        "The first step is to build complete list of surface forms belonging to the language using morphological generators, SLLP (Specialized Languages for Linguistic Programs), etc.",
        "and then compresses the large word-dictionary.",
        "They are generally used for office applications such as word processors, desktop presentation, etc.",
        "Their main advantage is that they cover a complete language since all the forms can be found in the initial list.",
        "Also, they allow efficient retrieval and guessing of misspelled words [4].",
        "However, some limits exist in such systems: - Multilinguism: The compression process give a good ratio for languages with a weak inflexion factor (English,...) where the compression mechanism give up to 150 KB of storage from around 3 MB of a full list [41.",
        "The compression technologies are still powerful for languages with a medium inflexion factor (Russian,...).",
        "For example, a list of all surface Russian words of between 10 and 15 MB of size can be reduced to 700 KB [41.",
        "For languages with a high inflexion factor (Arabic, Finnish, Hungarian,...), it won't be easy to find compression technologies that give practical results [41.",
        "For instance, a full list of completed vocalized words in Arabic has 300 MB in size and the current compression methods are impractical.",
        "- No morphological knowledge : These methods are neutral with respect to the text language, the efficiency of compression techniques may be improved by using specific properties of the language [4].",
        "II.",
        "A FIRST APPROACH: ADAPTING AN EXISTING METHOD FOR ARABIC",
        "11.1.",
        "Using an existing method As a first step, we take an efficient method used to compress dictionaries for European (I Orglisli, French,...) spelling checkers [41 and try to apply it to Arabic.",
        "The first step of our work consists in building a full list of surface forms using a morphological generator [51 and completed by all irregular forms and existing corpus.",
        "The final large word dictionary which covers non-vocalized Arabic has a size of 75 MB.",
        "The compression process yields 1/i MB in a compressed format.",
        "I:or an idea of the compression process readers can refer to [101.",
        "Table 1 gives some results of the compression process for a few European languages to see the efficiency of the method and its inadequacy for the Arabic language.",
        "word formssizesize uncom wessedcoin ressed",
        "The result for Arabic is impractical for small computers.",
        "We must then find other techniques that produce a smaller dictionary or extend this method; to get an exploitable solution.",
        "1L2.",
        "Extension of the method: The initial idea is applied to the morphological system of Arabic.",
        "While most of the fully inflected forms words in Arabic are built by adding to a stem prefixes and suffixes we propose replacing some words with only one form beginning by a special code that represents a family of prefixes and finishing by another special code which represents a family of suffixes.",
        "For this purpose, we wrote a program in MPW-C that processes a full list of inflected forms and using an existing decomposition of affixes into subsets already established, give the reduced lexicon where many forms are replaced by only one representation (PSi_stem_SSi) where PS; (with respect to SS) is the set i (with respect to j) of prefixes (with respect to suffixes).",
        "Note that the reduced lexicon represents Out the initial list without any silence (missing words) or noise (incorrect words).",
        "Only compressed words are replaced, and the rest remain in the reduced list.",
        "The figure 1 gives an example of words, an example of a decompositions and the obtained result.",
        "Fig.",
        "I: I xample of the compression process The next crucial problem to resolve is to find the best decomposition that provide the best reduced lexicon.",
        "The method must be automatic.",
        "It must process the large word dictionary, and regarding an initial list of prefixes and suffixes, must give as output the best decomposition and the optimal reduced dictionary.",
        "But, before studying the implementation of such an algorithm, we began, to see how much space we could gain by this technique starting from a manual decomposition.",
        "Manual method], Starting from a different full lists for each category of words (transitive verbs, nouns,...), we choose different decompositions and processed the full list with the compression tool.",
        "The best decomposition kept lbr each category was the decomposition which eliminated the maximum forms.",
        "This method gave many candidate decompositions depending on the gratnmatical category of the word.",
        "To choose the best global one we took into account the frequency of dictionary entries.",
        "This method was tested on different Arabic word lists and some results are described here.",
        "Readers can refer to 1.101 or till for more information.",
        "To see some decomposition, consider the following sets: = (wa, fa), I.,/E2= (la, sa), / E3an), / 6 / = (tom, tomna, ta, tona), / iF(ya, aan, yin), ouna), /6\\ 6\\/ 1,6(ha, haL ya, Er, Um, kommt, lama, horn, houma, hona, naa), F7=\\ (ya, nail) + (nil, Ei (with respect to 1) is a set of prefixes (with respect to suffixes).",
        "We note the quantity (with respect to 1.1;1) all strings built by a concatenation of each element of Ei (with respect to 1:1) with each element of Ej (with respect to 19.",
        "1;xample, of 3 class (from 6) of the prefix class: xyAXI lw weAXIllw wIAXI),ts fie lw weAXiSis xyAXI (FAXI",
        "Results are described in table 2.",
        "111.4.",
        "Interpretations: The most interesting thing observed on (his tab le is the improvement obtained when we combine Our method with a previous one.",
        "These results show that the existing methods are not optimal and can be improved by our logical compression in its first step.",
        "These important results in storage space should not hide others aspects of spelt checker systems (retrieval and guessing).",
        "It would be interesting if the results given in the table were followed by other results showing improvements in the retrieval and guessing of words.",
        "tV.",
        "A PROPOSED ARCHITECTURE OF A UNIVERSAL SPELLING CHECKER: Figure 3 shows the architecture of our proposed universal spelling checker.",
        "Our method is inspired from previous methods (.11 1.2), but presents some new original aspects that allow it to be considered a truly multilingual solution.",
        "In summary, our system has the following features: Multilinguism: this method will insure the multilingual constraint.",
        "By using different tools, specific to each language, to create a list of all surface forms.",
        "Storage space: by introducing the logic compression into the compression process, we will be able to get a reduced lexicon for whatever language we have to use.",
        "One task that still remains is to improve the logic compression by making the task of finding the best decomposition more an 1010:0 This problem is combinatorial; we must discover how to apply the optimization algorithms (genetic algorithm, stochastic algorithm,...) in each case to find an optimal reduced lexicon starting from the large word-dictionary and primitive morphological knowledge (list of affixes and vowels).",
        "Retrieval/guessing: even though we haven't any concrete results now, the first experiments show that the process of checking words in an FSM formalism is faster than other existing methods.",
        "Furthermore, we are exploring paths to introduce functions (similarity key,...) into the final obtained lexicon to make a rapid guessing of replacements for misspelled words."
      ]
    },
    {
      "heading": "CONCLUSION",
      "text": [
        "Our approach to spell checking differs from previous methods by taking into account a new parameter which is"
      ]
    },
    {
      "heading": "INTRODUCTION",
      "text": [
        "Since the first work in 1957 by Glantz [61, a great deal of theorizing and research has taken place on the subject of spelling verification and correction.",
        "Many commercial products (word processors, desktop presentation,...) include efficient spelling checkers on microcomputers.",
        "The classical methods, used are generally based on a morphological analyzer.",
        "This is sufficient to provide a robust monolingual spelling checker, but using morphological analyzers can become unrealistic when we want to develop an universal solution.",
        "In fact, the analyzers built for each language use various linguistic models and engines, and it is impossible to convert a morphological analyzer from one formalism to another.",
        "Furthermore, using these classical methods would lead to combining into the host application as many of grammars and parsers as languages, which would increase the code size and the maintenance problem of rules and data.",
        "The method presented in this paper is based on building a dictionary of all surface forms for each language, which is sufficient for spelling checkers applications.",
        "The dictionary built with the existing generators can be easily updated manually but may be huge, especially for some agglutinative language (Arabic, Turkish,...).",
        "A compression process on the multilingual dictionaries is necessary to obtain a reduced size.",
        "The existing compression methods generally used are physical and provide good results for indo-Furopean languages.",
        "Applying the same techniques to other languages (Arabic, Turkish,...) shows their limits.",
        "For this reason we introduce a new kind of compression techniques that we called \"logic compression\".",
        "This new technique requires a primitive morphological knowledge during the compression process and requires less storage space than previous methods.",
        "It also has the advantage of being an universal method applicable to all languages.",
        "Section 1 contains an overview of existing methods for building spell checkers and the limits of such system when we take into account new constraints such as multilingualism.",
        "Section 2 outlines the first two steps of our work: we adapt an existing method to Arabic, then make a first extension by introducing a new kind of compression called \"logic compression\".",
        "Section 3 introduces in detail the logic compression with its application to other languages, and shows the improvements obtained when using logic compression in conjunction with existing methods.",
        "Section 4 outlines the architecture of our multilingual spelling checker system and sonic future projects.",
        "I. OVERVIEW OF EXISTING METHODS ra untoar-based approach These methods were used in the beginning on early computers when storage space was expensive.",
        "It consists in building a small lexicon containing roots and affixes, it grammar of rules that express the morphographemic alternations, and an engine that uses the grammar and the lexicon to see if an input word belongs to the language or not.",
        "If the process of recognition fails, some operations (substitution, insertion,...) are performed on the misspelled word to provide a list of candidate words that helps the user to select the correct form.",
        "Even though, it is a great accomplishment to design a powerful engine [31 181 and to express rules in a pseudo natural way 191 even for different languages [11 [21 [III, these systems present some limits:",
        "- This methods does not support all languages.",
        "To offer a multilingual solution for n languages you have to store n grammars and n lexicons, and generally 11 different engines into the host application.",
        "- Cost of retrieval: 12or some languages, the retrieval of words may be long.",
        "For instance, a vocalized Arabic spell checker must accept non-vocalized or partially vocalized words which require more time to be accepted than fully vocalized words.",
        "- Cost of guessing alternatives for a misspelled word: To guess a correct word when a misspelled word is found, we have to modify the misspelled word by all possible operations (substitution, insertion, suppression,...) for 1 or 2 characters and then try to check them.",
        "This matter can take a lot of time before displaying the correct forms for end users.",
        "- Maintaining the grammars and data: The grammars and lexicon require continuous updating.",
        "You need to find a multilingual computational linguist who knows the linguistic theory and the formalism to easily update data and rules [81.",
        "- Ergonomic features: In some languages, end users want to have some options that let them choose how the spell checker will accept words.",
        "In Arabic, for example, different regions have slightly different orthographical conventions.",
        "the multilinguism., The system proposed tries to give solutions for the three main problems: Multilinguism, detection/guessing and storage size.",
        "The first results, although using a manual method to find the decomposition in this first step, show that the previous methods to store dictionaries are not optimal and can be improved by exploring other techniques from the language itself.",
        "Another interesting experiment is to find an original optimization algorithm to find the optimal reduced lexicon that represents faithfully the initial list without any silence (missing words) or noise (incorrect words).",
        "Yet another project is to build a more robust method for the two other problems (detection and guessing) from the reduced lexicon."
      ]
    },
    {
      "heading": "ACKNOWLEDGMENTS",
      "text": [
        "The author would like to thank Prof. Christian BOITET for his constant support and encouragement.",
        "I am also very grateful to Mr. Kenneth BEESLEY (Rank Xerox, Grenoble) for his fruitful discussions and Mr. Lauri KARTTUNEN (Rank Xerox, Grenoble) for his help to realize some experiments."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": []
    },
    {
      "heading": "III LOGIC COMPRESSION:",
      "text": [
        "111.1.",
        "Theoretical aspects: Let V be a finite set and V* the set of words built on V including null strings noted 0.",
        "W e V*.",
        "W = WiW2...Wn.",
        "Wi V. ic [1..n].",
        "Let V+ = V* - {0).",
        "Let Y be a subset of V that contain vowels.",
        "1.",
        "Prefix(W).",
        "V W c V.",
        "We call order i prefix the quantity:",
        "if Vk= (0).",
        "111.2.",
        "Logic Compression: What is it ?",
        "Lees take the following automata that represent some surface vocalized words (fig 2) is a prefix.",
        "1jn.",
        "is a suffix.",
        "1 5 in.",
        "C, are the consonants of the vocabulary.",
        "1ik.",
        "is the vowel attached to the consonant Ci.",
        "1 5_ i 5 qandl j k. 0 is the null string.",
        "This automata recognizes all words beginning from an initial state (marked by *) and finishing in a final state (marked by a double circle) The number of arcs of such an automata is:",
        "If we consider, for example, that affixes have a single character, the number of arcs is equal to 2(n+1) + 2q(k-1).",
        "The logic compression consist in supplying the class of prefixes, suffixes and vowels and replaces each set by only one arc that represent a family of prefixes, suffixes or vowels.",
        "Starting from the following sets already established: Pi= (0, Pi1,110,...Pi) a class of prefixes stored as x. Si = (0, S, Si2,...Si) a class of suffixes stored as y.",
        "The number of arcs kept in the automata is equal to 3 + k. The set Vk contains a subset of k vowels which must be applied to the last k characters.",
        "111.3.",
        "Experiments: The logic compression with only an affix decomposition, built by the manual method explained above, has been tested on various list of words that represent collections of multilingual dictionaries (a list of inflected forms).",
        "Three languages are tested: non-vocalized Arabic which has a great inflexion factor, French which has a"
      ]
    }
  ]
}
