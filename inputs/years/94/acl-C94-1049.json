{
  "info": {
    "authors": [
      "Yoshiki Niwa",
      "Yoshihiko Nitta"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C94-1049",
    "title": "Co-Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries",
    "url": "https://aclweb.org/anthology/C94-1049",
    "year": 1994
  },
  "references": [
    "acl-C90-2067",
    "acl-C92-2070",
    "acl-E93-1028",
    "acl-H92-1046",
    "acl-P89-1010",
    "acl-P93-1022",
    "acl-P93-1024"
  ],
  "sections": [
    {
      "heading": "CO-OCCURRENCE VECTORS FROM CORPORA VS. DISTANCE VECTORS FROM DICTIONARIES",
      "text": []
    },
    {
      "heading": "Abstract",
      "text": [
        "A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions.",
        "The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was higher than that by using distance vectors from the Collins English Dictionary (60K head words + 1.6M definition words).",
        "However, other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors."
      ]
    },
    {
      "heading": "Introduction",
      "text": [
        "Word vectors reflecting word meanings arc expected to enable numerical approaches to semantics.",
        "Some early attempts at vector representation in psycholinguistics were the semantic differential approach (Osgood et al.",
        "1957) and the associative distribution approach (Deese 1962).",
        "However, they were derived manually through psychological experiments.",
        "An early attempt at automation was made by Wilks et al.",
        "(1990) using co-occurrence statistics.",
        "Since then, there have been some promising results from using co-occurrence vectors, such as word sense disambiguation (Schiltye 1993), and word clustering (Pereira et al.",
        "1993).",
        "However, using the co-occurrence statistics requires a huge corpus that covers even most rare words.",
        "We recently developed word vectors that are derived from an ordinary dictionary by measuring the inter-word distances in the word definitions (Niwa and Nitta 1993).",
        "This method, by its nature, has no problem handling rare words.",
        "In this paper we examine the usefulness of these distance vectors as semantic representations by comparing them with co-occurrence vectors."
      ]
    },
    {
      "heading": "2 Distance Vectors",
      "text": [
        "A reference network of the words in a dictionary (Fig.",
        "1) is used to measure the distance between words.",
        "The network is a graph that shows which words are used in the definition of each word (Nitta 1988).",
        "The network shown in Eig.",
        "1 is for a very small portion of the reference network for the Collins English Dictionary ( [979 edition) in the CD-ROM I (Liberman 1991), with 60K head words + 1.6M definition words.",
        "For example, the definition for dictionary is \"a book in which the words of a language are listed alphabetically ...",
        ".\"",
        "'The word dictionary is thus linked to tlie words book, word, language, and alphabetical.",
        "A word vector is defined as the list of distances from a word to a certain set of selected words, which we call origins.",
        "The words in Fig. 1 marked with Oi (unit, book, and people) arc assumed to be origin words.",
        "In principle, origin words can be freely chosen.",
        "In our experiments we used middle frequency words: the 01st to 1050th most frequent words in the reference Collins English Dictionary (cED).",
        "The distance vector for dictionary is derived as lows:",
        "The i••th element is the distance (the length of the shortest path) between dictionary and the i-th origin, 0,:.",
        "To begin, we assume every link has a constant length of 1.",
        "The actual definition for link length will be given later.",
        "If word A is used in the definition of word fl, these words are expected to be strongly related.",
        "This is the basis of our hypothesis that the distances in the reference network reflect the associative distances between words (Nitta 1993).",
        "alphabetical",
        "Use of Reference Networks Reference networks have been successfully used as neural networks (by Veronis and Ide (1990) for word sense disambiguation) and as fields for artificial association, such as spreading activation (by Kojima and l'urugori (199:3) for context-coherence measurement).",
        "The distance vector of a word can be considered to be a list, of the activation strengths at the origin nodes when the word node is activated.",
        "Therefore, distance vectors can be expected to convey almost the same information as the entire network, and clearly they are much easier to handle.",
        "Dependence on Dictionaries As a semantic representation of words, distance vectors arc expected to depend very weakly on the particular source dictionary.",
        "We compared two sets of distance vectors, one from 1,DOCE (Procter 1978) and the other from COBUILI) (Sinclair 1987), and verified that their difference is at least smaller than the difference of the word definitions themselves (Niwa and Nitta 1993).",
        "We will now describe some technical details about the derivation of distance vectors.",
        "Link Length Distance measurement in a reference network depends on the definition of link length.",
        "Previously, we aSSII Med for simplicity that every link has a constant length.",
        "However, this simple definition seems unnatural because it does not, reflect word frequency.",
        "Because a path through low-frequency words (rare words) implies a strong relation, it should be measured as a shorter path.",
        "Therefore, we use the following definition of link length, which takes account, of word frequency.",
        "This shows the length of the links between words 1,2) in Fig. 2, where N1 denotes the total number of links from and to W1 and n denotes the number of direct links between these two words.",
        "Fig.",
        "2 Links between two words.",
        "Normalization Distance vectors are normalized by first changing each coordinate into its deviation in the coordinate: v (10 1)j – a (Ti where a; and ai are the average and the standard deviation of the distances from the i-th origin.",
        "Next, each coordinate is changed into its deviation in the vector:",
        "where n'and re are the average and the standard deviation of (i = I,...)."
      ]
    },
    {
      "heading": "3 Co-occurrence -Vectors",
      "text": [
        "We use ordinary co-occurrence statistics and measure the co-occurrence likelihood between two words, X and Y, by the mutual information estimate (Church and Hanks 1989):",
        "where P(X) is the occurrence density of word X ill a whole corpus, and the conditional probability P(X Y) is the density of X in a neighborhood of word Y.",
        "Here the neighborhood is defined as 50 words before or after any appearance of word Y.",
        "(There is a variety of neighborhood definitions such as \"100 surrounding words\" (Yarowsky 1992) and \"within a distance of no more than :3 words ignoring function words\" (Dagan et al.",
        "1993).)",
        "The logarithm with `-1-' is defined to be 0 for an argument less than 1.",
        "Negative estimates were neglected because they arc mostly accidental except when X and Y are frequent enough (Church and Hanks 1989).",
        "A co-occurence vector of a word is defined as the list of co-occurrence likelihood of the word with a certain set of origin words.",
        "We used the same set of origin words as for the distance vectors.",
        "When the frequency of X or Y is zero, we can not measure their co-occurence likelihood, and such cases are not exceptional.",
        "This sparseness problem is well-known and serious in the co-occurrence statistics.",
        "We used as a corpus the 1987 Wall Street :Journal in the CD-ROM t (1991), which has a total of 20M words.",
        "he number of words which appeared at, least once was about 50% of the total 0210 head words of CIE), mid the percentage of the word-origin pairs which appeared at least once was about 16% of total 62K x 1K (=QM) pairs.",
        "When the co-occurrence likelihood can not be measured, the value I(X, V) was set to 0.",
        "(v;)"
      ]
    },
    {
      "heading": "4 Experimental Results",
      "text": [
        "We compared the two vector representations by using them for the following two semantic tasks.",
        "The first is word sense disambiguation (WSD) based on the similarity of context vectors; the second is the learning of positive or negative meanings from example words.",
        "With WSD, the precision by using co-occurrence vectors from a 20M words corpus was higher than by using distance vectors from the CED."
      ]
    },
    {
      "heading": "4.1 Word Sense Disambiguation",
      "text": [
        "Word sense disambiguation is a serious semantic problem.",
        "A variety of approaches have been proposed for solving it.",
        "For example, Veronis and Ide (1990) used reference networks as neural networks, Ilearst (1991) used (shallow) syntactic similarity between contexts, Cowie et al.",
        "(1992) used simulated annealing for quick parallel disambiguation, and Yarowsky (1992) used co-occurrence statistics between words and thesaurus categories.",
        "Our disambiguation method is based on the similarity of context vectors, which was originated by Wilks el a!.",
        "(1990).",
        "In this method, a context vector is the sum of its constituent word vectors (except the target word itself).",
        "That is, the context vector for context,",
        "The similarity of contexts is measured by the angle of their vectors (or actually the inner product of their normalized vectors).",
        "Let word w have senses S2, s,„, and each sense have the following context examples.",
        "Sense Context Examples",
        "We infer that the sense of word win an arbitrary context C is si if for some j the similarity, sim(C,C, j), is maximum among all the context examples.",
        "Another possible way to infer the sense is to choose sense si such that the average of sim(C, Cii) over j = 1,2, is maximum.",
        "We selected the first method because a peculiarly similar example is more important than the average similarity.",
        "Figure 3 (next page) shows the disambiguation precision for 9 words.",
        "For each word, we selected two senses shown over each graph.",
        "These senses were chosen because they are clearly different and we could collect sufficient number (more than 20) of context examples.",
        "The names of senses were chosen from the category names in Roget's International Thesaurus, except organ's. The results using distance vectors are shown by dots (• • •), and using co-occurrence vectors from the 1987 WSJ (20M words) by circles (o o o).",
        "A context size (x-axis) of, for example, 10 means 10 words before the target word and 10 words after the target word.",
        "We used 20 examples per sense; they were taken from the 1988 WSJ.",
        "The test contexts were from the 1987 WSJ: The number of test contexts varies from word to word (100 to 1000).",
        "The precision is the simple average of the respective precisions for the two senses.",
        "The results of Fig. 3 show that the precision by using co-occurrence vectors are higher than that by using distance vectors except two cases, interest and customs.",
        "And we have not yet found a case where the distance vectors give higher precision.",
        "Therefore we conclude that co-occurrence vectors are advantageous over distance vectors to WSD based on the context similarity.",
        "The sparseness problem for co-occurrence vectors is not serious in this case because each context consists of plural words."
      ]
    },
    {
      "heading": "4.2 Learning of positive-or-negative",
      "text": [
        "Another experiment using the same two vector representations was done to measure the learning of positive or negative meanings.",
        "Figure shows the changes in the precision (the percentage of agreement with the authors' combined judgement).",
        "The x-axis indicates the number of example words for each positive or negative pair.",
        "Judgement was again done by using the nearest example.",
        "The example and test words are shown in Tables 1 and 2, respectively.",
        "In this case, the distance vectors were advantageous.",
        "The precision by using distance vectors increased to about 80% and then leveled off, while the precision by using co-occurrence vectors stayed around 60%.",
        "We can therefore conclude that the property of po.sitive-or-negative is reflected in distance vectors more strongly than in co-occurrence vectors.",
        "The sparseness problem is supposed to be a major factor in this case.",
        "positive (20 words) balanced elaborate elation eligible enjoy fluent honorary honourable hopeful hopefully influential interested legible lustre normal recreation replete resilient restorative sincere negative (30 words) confusion cuckold dolly dmmiation ferocious flaw hesitate hostage huddle inattentive liverish lowly mock neglect queer rape ridiculous savage scanty sceptical schizophrenia scoff scruffy shipwreck superstition sycophant trouble wicked worthless"
      ]
    },
    {
      "heading": "4.3 Supplementary Data",
      "text": [
        "In the experiments discussed above, the corpus size for co-occurrence vectors was set to 20M words ('87 WSJ) and the vector dimension for both co-occurrence and distance vectors was set to 1000.",
        "Here we show some supplementary data that support these parameter settings.",
        "a. Corpus size (for co-occurrence vectors) Figure 5 shows the change in disambiguation precision as the corpus size for co-occurrence statistics increases from 200 words to 20M words.",
        "(The words are suit, issue and race, the context size is 10, and the number of examples per sense is 10.)",
        "These three graphs level off after around 1M words.",
        "Therefore, a corpus size of 20M words is not too small.",
        "number of examples: 10/sense, vector dimension: 1000. b. Vector Dimension Figure 6 (next page) shows the dependence of disambiguation precision on the vector dimension for (i) co-occurrence and (ii) distance vectors.",
        "As for co-occurrence vectors, the precision levels off near a dimension of 100.",
        "Therefore, a dimension size of 1000 is sufficient or even redundant.",
        "However, in the distance vector's case, it is not clear whether the precision is leveling or still increasing around 1000 dimension."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "• A comparison was made of co-occurrence vectors from large text corpora and of distance vectors from dictionary definitions.",
        "• For the word sense disambiguation based on the context similarity, co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was advantageous over distance vectors from the Collins English Dictionary (60K head words -I-1.6M definition words).",
        "• For learning positive or negative meanings from example words, distance vectors gave remarkably higher precision than co-occurrence vectors.",
        "This suggests, though further investigation is required, that distance vectors contain some different semantic information from co-occurrence vectors."
      ]
    }
  ]
}
