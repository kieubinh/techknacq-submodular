{
  "info": {
    "authors": [
      "Timo Jarvinen"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C94-1092",
    "title": "Annotating 200 Million Words: The Bank of English Project",
    "url": "https://aclweb.org/anthology/C94-1092",
    "year": 1994
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The Bank of English is an international English language project sponsored by Harper-Collins Publishers, Glasgow, and conducted by the COBUILD team at the University of Birmingham, UK.",
        "The text hank will comprise some 200 million words of both written and spoken English.",
        "The whole 200 million word corpus is being annotated morphologically and syntactically during 1993-94 at the Research Unit for Computational Linguistics (ItUCL), University of Helsinki, using the English morphological analyser (ENGTWOL) and English Constraint Grammar (ENGCG) parser.",
        "The first half of the texts (103 million words) has already been processed in 1993.",
        "The project is lead by Prof. John Sinclair in Birmingham, and Prof. Fred Karlsson in Helsinki.",
        "The present author is responsible for conducting the annotation.",
        "In the introduction of this paper the routines for dealing with large text corpora are presented and our analysing system outlined.",
        "Chapter 2 gives an overlook how the texts are preprocessed.",
        "Chapter 3 describes the lexicon updating, which is a preliminary step to the analysis.",
        "The last.",
        "part presents the ENGCG parser and the ongoing development of its syntactic component."
      ]
    },
    {
      "heading": "1 INTRODUCTION",
      "text": [
        "Each month the COBUILD team supplies an approximately 10 million word batch of markup coded running text (see Appendix A) in ASCII format,.",
        "Every new batch is first scanned by the ENGTWOI, lexical and morphological analyser [Koskennierni, 1983] in filtering mode for the purpose of detecting words not included in the present lexicon.",
        "This is followed by a semi-automatic updating of the lexicon.",
        "After these adjustments, the whole system is used for annotating the data.",
        "Our analysing system, wIdch is presented in detail in [Karlsson, 1994], consists of the following successive stages:",
        "• preprocessing • ENGTWOL lexical analysis • ENGCG morphological disambiguation • ENGCG syntactic mapping and disambiguation",
        "The main routines performed on the monthly data, including constant monitoring of both incorning texts and analysed output and managendent (documentation, backups) are closely linked to the updating of the preprocessing module and tie ENGTWOL lexicon."
      ]
    },
    {
      "heading": "2 PREPROCESSOR",
      "text": [
        "The preprocessing modules standardise the running text and tokenise it into a form suitable lOr the.",
        "ENGTWOL lexical analyser.",
        "ENCICG has been developed so that it.",
        "takes into account various textual coding conventions [Earls-son, 1994].",
        "We have developed preprocessing procedures further to cater for the dilierent, types of markup codes systematically.",
        "Since texts usually come from various sources, there may be undocumented idiosyncracies or systematic errors in some samples, The information conveyed by the markup codes is utilised in the parsing process.",
        "Updating the preprocessing module to achieve the highest possible systematisation is therefore considered worthwhile.",
        "fire present system can deal with any code properly if it is used unambiguously in either a sentence-delimiting function (e.g, codes indicating headings, paragraph markers), sentence-internal function (e.g. font change codes) or word-internal (e.g. accent, codes) function.",
        "Since preprocessing is the first step before lexical littering, it, indicates the kinds of dilliculties we are likely to encounter.",
        "If error messages are produced at.",
        "this stagy, I do the necessary adjustrrtents to the preprocessor until it seems to produce the output, smoothly.",
        "Errors in preprocessing may occasionally result in a truncation of lengthy passages of text, or even a crash.",
        "It is important for the utilisation of the corpus that no information is lost during standardisation.",
        "Therefore, WO aim to mark all corrections inade to the text,.",
        "For example, the preprocessor inserts a code marking the correction when it separates strings such as of/he and andthe.",
        "Most errors are not, corrected, such as confusion of sentence boundaries, truncation of sentences due to running headings or page numbers, misplacement or doubling of blocks of text, etc."
      ]
    },
    {
      "heading": "3 THE LEXICON",
      "text": [
        "Filtering produces a list of all tokenised word-forms in the input text which are not included in the current ENO'[ 'WOL lexicon.",
        "The roost common types are taken under closer scrutiny.",
        "It has to be decided whether these are genuine word forms or non-words (e.g. misspellings).",
        "At the beginning, I used several days to update the lexical module for a new batch of text but experience and increased coverage of the lexicon have diminished the time needed for this task considerably.",
        "I have added words above a certain frequency routinely to the F,NGTWOL lexicon.",
        "The frequency is not fixed but determined by practical considerations.",
        "For instance, when the data contain a great deal of duplication (as in the BBC material owing to the repetitive nature of daily broadcasting), simple token frequency is a poor indicator of what is a suitable item to add to the lexicon.",
        "However, sampling methods have not been developed to optimise the size of the lexicon, because it is not crucial for the present purpose.",
        "My lexical practices differ somewhat from the updating procedure documented in [Voutilainen, 1994].",
        "If our aim is to supply every word in running text with all proper morphological and syntactic readings, we cannot deprive frequent non-standard words (e.g. tarn, veggie, manna) of their obvious morphological readings because this might cause the whole sentence to be misztnalysed.",
        "Since prescriptive considerations were not taken into account in the design of ENGTwoL, many entries marked as informal' or long' in conventional dictionaries were added to the lexicon.",
        "I have also included highly domain-specific entries into the lexicon if they were frequent enough in certain types of data, especially when heuristics might produce erroneous or incomplete analyses for the word in question (e.g. species of fish which have the same form in singular and plural: brill, chub, garfish)I.",
        "One advantage of including all frequent graphical words to the lexicon is that ENCTWOL filtering of incoming texts produces output which can be niece reliably dealt with by automatic means.",
        "When all frequent nonstandard and even foreign words are listed in the lexicon, the output can be used in a straightforward way for generating new entries.",
        "The procedure of adding new entries to the lexicon goes as follows: first, all words are classified according to the part-of-speech they belong to.",
        "Second, new entries in the ENGTwoI, format are generated automatically from these word-lists using ready-made tools presented in [Vontilainen, 1994].",
        "Lists of new entries are carefully checked up, and additional features (such as transitivity and complementation 'The default category of morphological heuristics is a singular noun.",
        "In the case of a potential plural form (sending), au underspecified tag SC/01, is given.",
        "features for verbs) are supplied manually.",
        "In describing the items, 1 have relied mainly on Collins COMM) Dictionary (1987) and Collins English Dictionary (1991) which have been available for us in electronic form.",
        "But when the usage and distribution seems to be unclear, I have generated an on-line concordance directly from the corpus.",
        "Since I have dealt, with words which have a frequency of, say, at least 10 tokens in the corpus, this method seems to be quite reliable.",
        "We cannot detect errors in the lexicon during the initial filtering phase.",
        "Once a certain string has had one or more entries in the lexicon, it is not present in the output, of the filtering, and other potential uses might.",
        "not be added to the lexicon'.",
        "And frequent.",
        "errors tend to get corrected since all incorrect.",
        "analyses detected during the manual inspection are corrected directly in the lexicon.",
        "The EN1GTWOL lexicon which is used in the Bank analyses contains approximately 75,00(1 entries.",
        "Morphological analysis caters for all inflected forms of the lexical items.",
        "The coverage of the lexicon before updating is between 97% – 98% of all word-form tokens in running text.",
        "Appendix A presents the number of additional lexical entries generated from each batch of data.",
        "The cumulative trend shows that a very small mirriber or new entries is needed when analysing the latter half the corpus.",
        "Morphological heuristics is applied alter ENC-TWO', analysis as a separate module (by Voutilainen Tapanainen).",
        "IL assigns reliable analyses to words which were not included in the lexicon.",
        "This also contributes to the fact.",
        "that lexicon updating will be a minor task in the future."
      ]
    },
    {
      "heading": "4 ENCCG DISAMBIGUATION A.ND SYNTAX",
      "text": [
        "English Constraint.",
        "Crarrimar is a ride-based morphological and dependency-oriented surface syntactic analyser of rimming English text.",
        "Morphological disambiguation of multiple part-of-speech and other inllectionid tags is carried out be-rme syntactic analysis.",
        "Morphological disarnbignaLion reached a mature level well before the beginning of this project, (see evaluation in [Voutilainen, 1992]).",
        "The morphological disambiguation rules (some 1100 in the present gran-mm.0 were written by Alan Voutilainen.",
        "The [tank data is analysed using both granular-based' and heuristic' disambiguation rules.",
        "This leaves less morphological ambiguity (below :1%), although the error rate is still extremely low (below 0.5%).",
        "'Although missing entries are possible to find indirectly, e.g. lug and -cd forms in the filtering output indicates that the base form is not, described in the lexicon as a verb"
      ]
    },
    {
      "heading": "4.1 Current state of IINOCC syntax",
      "text": [
        "The first version of ENGCG syntax was written by Arto Anttila [Anttila, 1994].",
        "At the beginning of the flank project,, new Constraint, Grammar Parser implementations for syntactic mapping and disambiguation were written by Pasi Tapanainen.",
        "These have been tested during the first months of this project.",
        "Some adjustment to the syntax was needed to cater for new specifications, e.g. in rule application order.",
        "I have tested all constraints extensively with different types of text from the Bank.",
        "I have revised almost all syntactic rules and written new ones.",
        "The current ENGCG parser uses 282 syntactic mapping rules, 492 syntactic constraints and 204 heuristic syntactic constraints.",
        "The mapping rules should be the most reliable, since they attach all possible syntactic alternatives to the morphologically disain-biguated oul.put.",
        "Syntactic rules prune contextually inappropriate syntactic tags, or accept, just one contextually appropriate tag.",
        "Syntactic.",
        "and heuristic rule components are formally similar but they diner in reliability.",
        "It is possible not to use heuristic rules at all if one aims at maximally error-free output, but the cost is all increase in ambiguity.",
        "During the project, the quality of syntax has improved considerably.",
        "The current, error rate, when parsing new unrestricted running text, is approximately 2%, i.e., 2 words out of 100 get the wrong syntactic code.",
        "But the ambiguity rate is still fairly high, 10.4% in a 0.5m word sample, which means that 10 words out.",
        "of 100 still have more than one morphological or syntactic alternative.",
        "Much or the remaining ambiguity is of the prepositional attachment type.",
        "This particular type of ambiguity accounts for approximately 20% of all remaining ambiguity.",
        "More heuristic rules are needed for pruning the remaining ambiguities.",
        "Of course, many of the remaining ambiguities (especially PI' attachment) are genuine and should be retained.",
        "'nue speed of the whole system used in morphological and syntactic annotation is about 400 words per second on a SUN SparcStation 10/30."
      ]
    },
    {
      "heading": "4.2 Developing the syntax",
      "text": [
        "Facilities for the fast compilation of a parser with a new rule file and the speed of the analysis makes a very good environment for the linguist to lest mete constraints.",
        "A special debugging version of the parser can be used for testing purposes.",
        "The debugging version takes fully disambignated ENGCG texts as input.",
        "Ideally, every rule is tested against a representative sample from a corpus.",
        "This would set the requirement, that the test, corpus should be made of large random samples.",
        "However, it is time-consuming to prepare manually large amounts of corrected and disarribignated data, even from ENGCG output.",
        "Therefore, a very large test corpus is beyond the scope of this project.",
        "The current syntactic test corpus contains approximately 30,000 words.",
        "It is large enough for testing reliable syntactic rules, but if we want to rate the acceptability of heuristic syntactic.",
        "rules, a larger syntactic corpus would be necessary.",
        "The test corpus consists of 10 individual text samples From the Hank of English data.",
        "The texts have been chosen so that they take text type variation into account.",
        "All sainples but one are continuous, unedited sub-parts or the corpus.",
        "IL seems worthwhile to continue preparing a disambiguated corpus from selected pieces or text.",
        "Once new data is received, it is expedient to add a representative sample Iron.' it, to the test, corpus.",
        "A manually disambiguated test corpus constitutes a very straightforward documentation of the applied parsing scheme (as described in [Sampson, 1987]).",
        "0 CONC.EMS:ION The analysing system has reached a mature stage, where all technical problems Seelll to lie solved.",
        "We have developed methods dealing with the data, with it considerable degree or automatisation.",
        "ENGCG has proved to be a fast, and accurate rule-I:insed system for analysing unrestricted text.",
        "Writing and documenting ENCCC syntax will be the main concern during the following months.",
        "Our part, of the project will lie completed by March, 1995.",
        "It is possible that the whole 200 million corpus will he analysed afresh near the end oldie project.",
        "This would put to Ilse ;ill the improvettwas made (luring; the two-year period and would icirtraiitee a I I laximal degree of uniformity and the overall accuracy of the annotated corpus."
      ]
    },
    {
      "heading": "ACKNOWLEDGEMENTS",
      "text": [
        "Special thailli.s tie (hie to Harper Collins Publishers, Glasgow, for permission to use both Collins C(.",
        ")1iU11,11) and Collins English Dictionary in electronic Form.",
        "Personally, I am greatly iralelited to Pasi Tapatiainen for solutions to all incalculalde number of technical problems and to Atro Von-tilaineri for v;itticIfince and supervision during this project.",
        "I wish to tliatilc also ['rot'.",
        "fired 1<artssoil, gala.",
        "Ileilacil5, K.ari l'iltiinen and Sari Salinistio for reviewing; earlier drafts of this paper.",
        "The table above shows the size of the 11 batches annotated so far in words and the number of new lexical entries' derived from them.",
        "Syntactic tags, listed in [Tapanainen, 1094; Voutilainen, 1992] are marked with an at-sign (M. The shallow syntax distinguishes four verb chain labels and nominal head and modifier functions.",
        "Modifier functions have a pointer (> or <) to the head to the right or to the left, respectively.",
        "PP and adverbial attachment is solved when it can be done reliably."
      ]
    }
  ]
}
