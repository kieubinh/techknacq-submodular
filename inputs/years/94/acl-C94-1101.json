{
  "info": {
    "authors": [
      "Makoto Nagao",
      "Shinsuke Mori"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C94-1101",
    "title": "A New Method of N-Gram Statistics for Large Number of N and Automatic Extraction of Words and Phrases from Large Text Data of Japanese",
    "url": "https://aclweb.org/anthology/C94-1101",
    "year": 1994
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In tile process of establishing the information theory, C. E. Shannon proposed the Markov process as a good model to characterize a natural language.",
        "The core of this idea is to calculate the frequencies of strings composed of n characters (n-graces), but this statistical analysis of large text data and for a large is has never been carried out because of the memory limitation of computer and the shortage of text data.",
        "Taking advantage of the recent powerful computers we developed a new algorithm of n-grams of large text data for arbitrary large c. and calculated successfully, within relatively short time, n-grams of some Japanese text data containing between two and thirty million characters.",
        "From this experiment it became clear that the automatic extraction or determination of words, compound words and collocations is possible by mutually comparing n-gram statistics for different values of n. category: topical paper, quantitative linguistics, large text corpora, text processing"
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Claude E. Shannon established the information theory in 1948 [1].",
        "His theory included the concept that a language could be approximated by an nth order Markov model by n to be extended to infinity.",
        "Since his proposal there were many trials to calculate a-grams (statistics of a character strings of a language) for a big text data of a. language.",
        "however computers up to the present could not calculate them for a large a because the calculation required huge amount of memory space and time.",
        "For example the frequency calculation of 10- grams of English requires at least 2610 ti 10' giga word memory space.",
        "Therefore the calenlation was done at most for a == 4 5 with modest text.",
        "quantity.",
        "We developed a, new method of calculating a-grams for large 71:S .",
        "We do not prepare a table for an n-gram.",
        "Our methods consists of two stages.",
        "The first stage performs the sorting of substrings of a text and finds out the length of the prefix parts which are the same for the adjacent substrings in the sorted table.",
        "The second stage is the calculation of an 'n-gram when it is asked for a specific n. Only the existing n character combinations require the table entries for the frequency count, so that we need not reserve a big space for to-gram table.",
        "The program we have developed requires 71 bytes for an 1 character text of two byte code such as Japanese and Chinese texts and 6/ bytes for an 1 character text of English and other European languages.",
        "fay the present program a can be extended up to 255.",
        "The program can he changed very easily for larger.",
        "71 if it is required.",
        "We performed n-gram frequency calculations for three different text data.",
        "We were not so much interested in the entropy value of a. language but were interested in the extraction of varieties of language properties, such as words, compound words, collocations and so on.",
        "The calculation of frequency of occurrences of character strings is particularly important to determine what is a word in such languages as Japanese and Chinese where there is no spaces between words and the determination of word boundaries is not so easy.",
        "In this paper we wits explain some of our results on these problems."
      ]
    },
    {
      "heading": "2 Calculation of n-grams for an arbitrary large number of n",
      "text": [
        "It was very difficult to calculate n-grams for a large number of 1a because of the memory limitation of a computer.",
        "For example, Japanese language has mom than 4000 different characters and if we want",
        "to have 10-gram frequencies of a Japanese text, we must reserve 400010 entries, which exceed 10'.",
        "Therefore only 3 or 4-grams were calculated so far.",
        "A new method we developed can calculate n-grams for an arbitrary large number of n with a reasonable memory size in a reasonable calculation time.",
        "It consists of two stages.",
        "The first stage is to get a table of alphabetically sorted substrings of a text string and to get the value of coincidence number of prefix characters of adjacently sorted strings.",
        "The second stage is to calculate the frequency of n-grams for all the existing 71 character strings from the sorted strings for a specific number of n."
      ]
    },
    {
      "heading": "2.1 First stage",
      "text": [
        "(1) When a text is given it is stored in a computer as one long character string.",
        "it may include sentence boundaries, paragraph boundaries and so on if they are regarded as components of text.",
        "When a text is composed of 1 characters it occupies 21 byte memory because a Japanese character is encoded by 16 bit code.",
        "We prepare another table of the same size (1), each entry of which keeps the pointer to a substring of the text string.",
        "This is illustrated in Figure 1.",
        "A substring pointed by is defined as composed of the characters from the i-th position to the end of the text string (see Figure 1).",
        "We call this substring a word.",
        "The first word is the text string itself, and the second word is the string which starts from the second character and ends at the final character of the text string.",
        "Similarly the last word is the final character of the text string.",
        "As the text size is 1 characters a pointer must have at least p bits where 21' > 1.",
        "In our program we set p 32 bits so that we can accept the text size up to 232 4 giga characters.",
        "The pointer table represents a set of 1 words.",
        "We apply the dictionary sorting operation to this set of 1 words.",
        "It is performed by utilizing the pointers in the pointer table.",
        "We used comb sort[2] which is an improved version of bubble sort.",
        "The sorting time is the order of 0(/log 1).",
        "When the sorting is completed the result is the change of pointer positions in the pointer table, and there is no replacement of actual words.",
        "As we are interested in n-grams of n less than 255, actual sorting of words is performed for the leftmost 255 or less characters of words.",
        "(2) Next we compare two adjacent words in the pointer table, and count the length of the prefix parts which are the same in the two words.",
        "For example when \"extension to the left side ...\" and \"extension to the right side ...\" are two words placed adjacent, the number is 17.",
        "This is stored in the table of coincidence number of prefix characters.",
        "This is shown in Figure 2.",
        "As we are interested in 1 < n < 255, one byte is given to an entry of this table.",
        "The total memory space required to this first stage operation is 21+ 41+ 1 = 71 bytes.",
        "For example when a text size is 10 mega Japanese characters, 70 mega.",
        "byte memory must be reserved.",
        "This is not difficult by the present-day computers.",
        "table of coincidence number of characters",
        "We developed two software versions, one by using main memory alone, and the other by using a disc memory where the software has the additional operations of disc merge sort.",
        "I-3y the disc version we can handle a text of more than 100 mega character Japanese text.",
        "The software was implemented on a"
      ]
    },
    {
      "heading": "SUN SPARC Station.",
      "text": []
    },
    {
      "heading": "2.2 Second stage",
      "text": [
        "The second stage is the calculation of n-gram frequency .table.",
        "This is clone by using the pointer table and the table of coincidence number of prefix characters.",
        "Let us fix n to a certain number.",
        "We first read out the first n characters of the first word in the pointer table, and see the number in the table of coincidence number of prefix characters.",
        "If this is equal to or larger than n it means that the second word has at least the same n prefix characters with the first word.",
        "Then we see the next entry of the coincidence number of prefix characters arid check whether it is equal to or larger than n or not.",
        "We continue this operation until we meet the condition that the number is smaller than n. The number of words checked up to this is the frequency of the n prefix characters of the first word.",
        "At this stage the first a prefix characters of the next word is different, and so the same operation as the first a characters is performed from here, that is, to check the number in the coincidence number of prefix characters to see whether it is equal to or larger than n or not, and so on.",
        "In this way we get the frequency of the second a prefix characters.",
        "We perform this process until the last entry of the table.",
        "These operations give the n-gram table of the given text.",
        "We do not need any extra memory space in this operation when we print out every n-gram string and its frequency when they are obtained.",
        "We calculated n-grams for some different Japanese texts which were available in electronic form in our laboratory.",
        "These were the followings.",
        "1.",
        "Encyclopedic Dictionary of Computer Science (3.7 M bytes) 2.",
        "Journalistic essays from Asahi Newspaper (8 M bytes) 3.",
        "Miscellaneous texts available in our laboratory (59 M bytes)",
        "The first two texts were not large and could be managed in the main memory.",
        "The third one was processed by using a disc memory by applying a merge sort program three times.",
        "The first two texts were processed within one and two hours by a standard SUN SPARC Station for the first stage mentioned above.",
        "The third text required about twenty four hours.",
        "Calculation of n-gram frequency (tire second stage) took less than an hour including printout.",
        "3 Extraction of useful linguistic information from n-gram frequency data"
      ]
    },
    {
      "heading": "3.1 Entropy",
      "text": [
        "Everybody is interested in the entropy value of a language.",
        "Shannon's theory says that the entropy is calculated by the formula [3]",
        "where P(w) is the probability of occurrence of and the summation is for all the different strings w of a characters appearing in a language.",
        "The entropy Of a language",
        "We calculated .II„(L) for the texts mentioned in Section 2 for It 2, 3, .,.",
        "Tire results is shown in Figure 3.",
        "Unlike our initial expectation that the entropy will converge to a certain constant value between 0.6 and 1.3 which C. E. Shannon estimated for English, it continued to decrease to zero.",
        "We checked in detail whether our method had something wrong, but there was nothing doubtful.",
        "Our conclusion for this strange phenomenon was that the text quantity of a few mega characters were too small to get a meaningful statistics for a large a because we have more than 4000 different characters in the Japanese language.",
        "For English and many other European languages which have alphabetic sets of less than fifty characters the situation may be better.",
        "But still the text quantity of a few giga.",
        "bytes or more will be necessary to get a meaningful entropy value for a iO or more."
      ]
    },
    {
      "heading": "3.2 Obtaining the longest compound frequencies",
      "text": [
        "word 101 From the n-gram frequency table we can get many 1689 interesting information.",
        "When we have a string w 1310 (length n) of high frequency as shown in Figure 4, 784 we can try to find out the longest string w' which 784 includes w by the following process by using the 770 n-gram frequency table.",
        "147",
        "(1) extension to the left: We cut off the last character of w and add a character a: to the left of w. We call this a cut-and-pasted word.",
        "We look for the character a which will give the maximum frequency to the cut-and-pasted word.",
        "Repeat the same operation step by step to the left and draw a frequency curve for these words.",
        "This operation will be stopped when the frequency curve drops to a certain value.",
        "This process is performed by seeing the n-grain frequency table alone.",
        "(2) extension to the right: The same operation as (1) is performed by cutting the left character and adding a character to the right.",
        "( 3 ) extraction of high frequency part: From the frequency curve as shown in Figure 1 we can easily extract a high frequency part as the longest string.",
        "An example is shown in Figure 5 The strings extracted in this way are very often compound words of postpositions in Japanese.",
        "Postpositional phrases are usually composed of one to three words, and are used as if they are compound postpositions.",
        "Some extracted.",
        "exam pies are, After getting high frequency character strings by the above method we can make consultations with dictionaries for these strings.",
        "Then we find out many strings which are not included in the dictionaries, Some are ph rases(collocations, idiomatic expressions), some others are terminology words, and unknown (new) words.",
        "Front the text data of Encyclopedic Dictionary of Computer Science we extracted many terminological words.",
        "In general the frequencies of n-grams become smaller as 11 becomes larger.",
        "But we had sometimes relatively high frequency values in n-gramS of large n's.",
        "These were very often terminological words or terminological phrases.",
        "We extracted such terminological phrases as,",
        "• ) -;lily 7.",
        "12Y 7 (programs written by (... ) language) • AirAilMlfc ?.).",
        "(problem solving in artificial intelligence) • :inn :-.1* 9 :‹"
      ]
    },
    {
      "heading": "3.4 Compound word",
      "text": [
        "We can get more interesting information when we compare data of different n's.",
        "When we have a character string (length n) of high frequency, which we may be able to define as a ‘vord (w), we are recommended to check whether two substrings (w1 and w2) of the length wi and n2 (nt + n2 n) as",
        "shown in Figure 6 have high frequency appearance in nl-gram and n2-gram tables.",
        "If we can find out such a situation by changing ni (aud n2) we can conclude that the original character string w is a. compound word of nil and w2.",
        "Some examples are shown in Table t."
      ]
    },
    {
      "heading": "3.5 Collocation",
      "text": [
        "We can see whether a particular word w has strong collocational relations with some other words front the n-gram frequency results.",
        "We can get an n-gram table where n is sufficiently large, w is the prefix of these n-grams, and some words (ut', w\", ...) may appear in relatively high frequency.",
        "This is shown in Figure 7.",
        "We can find out easily that --- qv' and qv -- qv\" are two allocation:0 expres- sions from this figure.",
        "For example we have MI (effect) and find out that 6 _I (receive effect) and flMiki-.1 _I (give effect) have relatively high frequencies and there are no other significant combinations in the n-gram table with 1-0211 as the prefix.",
        "(in and on t hospital) have almost all the time Nti (repeat) as the following phrase, and so we Will be able to judge that (-Aign-ten!",
        "h 'Et.]",
        "ist.n idiomatic expression."
      ]
    },
    {
      "heading": "4 Conclusions",
      "text": [
        "We developed a new method and software for a-gram frequency calculation for n up to 255, and calculated n-grams for some large text data, of Japanese.",
        "From these data we could derive words, compound words and collocations automatically.",
        "We think a at this method is equally useful for languages like Chinese where there is no word spaces in a. sentence, and for Furopean languages as well, and also for speech phoneme sequences to get more detailed I models.",
        "Another possibility is that when we get a large text data with part-speech tags, we can extract high frequency part-of-speech sequences by this n-grant calculation over tine part-of-speech data.",
        "These may he regarded as grammar rules of the primary level.",
        "I3y replacing these part-of-speech sequences by single non-terminal symbols we can calculate new rt.-grains, and will be able to get higher level grammar rules.",
        "These examples indicate that large text data with varieties of annotations are very important and valuable for the extraction of linguistic information by calculating n-grains for larger value of n."
      ]
    }
  ]
}
