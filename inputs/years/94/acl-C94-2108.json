{
  "info": {
    "authors": [
      "Penelope Sibun",
      "David S. Farrar"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C94-2108",
    "title": "Content Characterization Using Word Shape Tokens",
    "url": "https://aclweb.org/anthology/C94-2108",
    "year": 1994
  },
  "references": [
    "acl-A92-1018"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "By quickly classifying character images into character shape categories, it is possible to automatically extract syntactic information from the text of document images without optical character recognition.",
        "Using word shape tokens composed of these charactershape codes, a properly trained text tagger can extract part-of-speech information from scanned document images.",
        "Later components of a document processing system can then use this information to locate topics, characterize document style, and assist in information retrieval."
      ]
    },
    {
      "heading": "I INTRODUCTION",
      "text": [
        "There are many text processing tasks that we would like to accomplish, such as document classification, text database structuring, matching documents with queries, and topic characterization.",
        "The field of computational linguistics has developed a variety of techniques for accomplishing these tasks for text documents represented by character codes (e.g., ASCII).",
        "However, many documents for which we would like to use our automated techniques arc not stored online in character-coded format, but instead exist only on paper.",
        "Optical character recognition (OCR) is a technique for converting scanned document images into character codes.",
        "By using OCR, document images can be converted into a form amenable to existing text processing techniques.",
        "I lowever, OCR is expensive, slow, and often inaccurate.",
        "Because of these drawbacks, we would like to avoid OCR if we can, or at the least, postpone using OCR until we arc confident that a document warrants detailed processing.",
        "In other words, we would like a high-bandwidth document processing system that is sensitive enough to detect desired document features.",
        "Our document understanding goals at the Fuji Xerox Palo Alto Laboratory include language determination (Nakayama and Spitz, 1993; Sibun and Spitz, forthcoming), con tent characterization, and style characterization.",
        "Toward these goals, we are developing a set of methods for extracting information from document images which do not depend on OCR.",
        "We have been working toward our goal of inexpensive content characterization by adapting a panafxpeech logger to process word shape tokens rather than character coded words.",
        "Part-of-speech tagging is a technique that has been developed and refined over the past several years, and it provides an inexpensive, fast, and reliable source of information for recognizing noun phrases and other syntax-related text features which help characterize a document's content.",
        "In this paper, we describe how we combine our technology for determining word shape tokens with text-tagging technology.",
        "We are developing systems that can extract noun phrases and other content characteristics using only word shape tokens that have been tagged with their parts of speech.",
        "Using this approach, we can process document images quickly to determine whether OCR is warranted, for example, when a text is a likely match for keywords in a database query.",
        "In the next two sections, we describe how word shape tokens arc derived; in section four, we discuss part-of-speech tagging; in the following four sections, we describe in detail part-of-speech tagging using word shape tokens; in sections nine and ten we discuss our results."
      ]
    },
    {
      "heading": "2 WORD SHAPE TOKEN CREATION",
      "text": [
        "In this section we briefly describe our system that constructs character shape codes and word shape tokens from a document image (for more detail, see Nakayama and Spitz, 1993; Sibun and Spitz, forthcoming).",
        "To recognize character shape codes from an image, sonic transformations arc first made to correct for various scanning artifacts such as skew angle and text line curvature.",
        "On each text line, four horizontal lines define three significant zones: the area between the baseline and the top of characters such as \"x\" is the x zone; the area above the x-hcight level is the ascender zone; the area below the x-zone is the descender zone (figure 1).",
        "The text line is further divided into charactercells by vertical boundaries which delineate the connected components of each character image.",
        "baseline bottom",
        "The majority of characters can easily be mapped to a small number of distinct codes (figure 2).1 Characters which arc contained entirely in the x-zone map to shape codex; characters which extend from the baseline to alxve the x-height line map to shape code A; and those which extend from below the baseline to the x-height line map to shape code g. Characters which map to A, x, or g arc composed of a single connected component.",
        "Sortie characters contain more than one connected component: an x-height character with a single diacritical mark in the ascender zone maps to i ; a character with a descender and a single diacritical mark maps to j.",
        "Most common punctuation marks map to unique shape codes; however, If â€¢ It this mapping can he done front document images, it can more trivially he accomplished from character-coded documents, such as ASCII text (providing, of course, that the method of encoding is known).",
        "top x height",
        "some are mapped into shape codes shared with alphabetic characters (e.g., \"&\" maps to shape code A)."
      ]
    },
    {
      "heading": "3 SHAPE CONVERSION",
      "text": [
        "In general, our approach to document processing finesses the problems inherent in mapping from an image to a character-coded representation: we map instead from the image to a shape-based representation.",
        "This technique can transform even a degraded document image into a representation which provides useful abstractions about the text of a document.",
        "The shape-based representation that we construct is proving to he a remarkably rich source of information.",
        "While our initial goal has been to use it for language identification in support of downstream OCR processes, we arc finding that this representation may be a sufficient source of information for document content characterization, such as that supported by part-of-speech tagging.",
        "In our tagging work, we have used character shape coded text derived from normal character-coded text.",
        "'this is simply because we do not have access to enough image documents on which to train a tagger.",
        "We call the process of creating a shape-based version of the document front the character code based version shape conversion.",
        "For the purpose of text tagging, then, we can think of the word shape token representation as an approximation of the representation composed of words.",
        "We can think about the relationship between words and :word shape tokens as a mapping from a word to its corresponding word shape token.",
        "For example, the word \"apple\" maps to the word shape token xggAx, and the word \"apples\" maps to the word shape token xggA xx.",
        "In documents, words exist as surface prim, not as morphological systems; thus \"apple\" and \"apples\" are different words.",
        "Therefore, it is of no use to us to have a lexicon organized in terms of stems and suffixes; instead, our lexicon is composed of surface forms like \"apple\" and \"apples\".",
        "Throughout the rest of this paper, when we say \"words\", we mean words as surface forms."
      ]
    },
    {
      "heading": "4 PART-OF-SPEECH TAGGING",
      "text": [
        "A part-of speech nigger is a system that uses context to assign parts of speech to words.",
        "Part-of-speech information facilitates higher-level analysis, such as recognizing noun phrases and other patterns in text.",
        "Several different approaches have been used for building text taggers.",
        "A particular form of Markov model has been widely used that assumes that a word depends probabilistically on just i LS part-of-speech category, which in turn depends solely on the categories of the preceding two words.",
        "Training the model is sometimes done by means of a large tagged corpus, but this is not necessary.",
        "The Baum-Welch algorithm (Baum, 1972), also known as the Forward-Backward algorithm, can be used.",
        "I 11 this case, the model is called a hidden Markov model (I IMM), since state transitions (i.e., part-ol speech categories) are assumed to be unobservable.",
        "For this work, we use an FIMM-based text tagger that is publicly available from Xerox PARC.",
        "As described in Cutting et al.",
        "(1992), the PAR(' tagger is efficient and highly flexible.",
        "It is particularly important that the tagger can be trained on any corpus of text, using any lexicon.",
        "This flexibility allows us to shape-convert our training corpus and lexicon, as described in section 5, without needing to modify the taggcr itself.",
        "Below we outline the basic operation of the PAR(' tagger; please refer to Cutting et al.",
        "(1992) for further detail.",
        "I.",
        "Text destined for the bigger first encounters a tokenizer, whose duty is to convert text (a sequence of characters) into a sequence of tokens.",
        "Each sentence boundary is also identified by the tokenizer, and is passed as a special token.",
        "2.",
        "The tokenizer passes tokens to the lexicon, where tokens are matched with a set of surface forms, each annotated with a part-of-speech tag.",
        "The set of tags constitutes an ambiguity class.",
        "The lexicon passes along a stream of (surface form, ambiguity class) pairs.",
        "3a.",
        "In training mode, the tagger takes long sequences of ambiguity classes as input.",
        "It uses the Baum-Welch algorithm to produce a trained 11MM, which is used as input in tagging mode.",
        "Training is performed on some corpus of interest; this corpus may be of broad coverage or may be genre-specific.",
        "3b.",
        "In tagging mode, the tagger buffers sequences of ambiguity classes between sentence boundaries.",
        "These sequences are disambiguated by computing the maximal path through the 11MM with the Viterbi algorithm (1%7).",
        "Operating at sentence granularity does not sacrifice accuracy, since sentence boundaries are unambiguous.",
        "Output consists of pairs of surface forms and tags."
      ]
    },
    {
      "heading": "5 THE LEXICON",
      "text": [
        "The word shape tagging in our work follows the IMM-based process described above.",
        "Both word shape tagging and standard word tagging require a lexicon."
      ]
    },
    {
      "heading": "5.1 Constructing the Lexicon",
      "text": [
        "A word shape lexicon can be derived from a standard lexicon of words.",
        "The lexicon used with the standard text tagger contains a list of all the distinct surface forms likely to be encountered in the language.",
        "Associated with each surface form is a list of the possible parts of speech that the surface Form can have.",
        "For example: apple noun macs plural noun eat verb eats third person singular verb tut noun, adjective the determiner Once we have a lexicon which consists of surface forms, we can use it to create a lexicon of word shape tokens for",
        "word shape tagging.",
        "In particular, this transformat: consists of the following steps:",
        "I.",
        "Shape convert the surface forms to their corresponding word shape tokens.",
        "2.",
        "Sort the lexicon by surface form word shape.",
        "At this stage there may be duplicate word shape tokens.",
        "3.",
        "Eliminate duplicate entries in the lexicon: collect all parts of speech behind one word shape token (combine their ambiguity classes).",
        "At this stage each word shape token should be unique.",
        "4.",
        "Eliminate duplicate parts of speech behind each word shape token.",
        "At this stage each part of speech should be unique within each ambiguity class.",
        "The lexicon fragment above would be converted to: xggAx noun xggAxx plural noun x x A verb, noun, adjective xxAx third person singular verb"
      ]
    },
    {
      "heading": "A A x determiner",
      "text": []
    },
    {
      "heading": "5.2 Analysis of the Lexicon",
      "text": [
        "For this work, we use a lexicon provided by Xerox PARC.",
        "This lexicon is organized so that there is an entry for each of roughly 150,000 surface forms.",
        "For word shape tagging, we shape converted this lexicon.",
        "As can be seen in the table, shape conversion results in about 50,000 distinct word shape surface forms.",
        "This suggests that, on average, each word shape token is a mapping of three surface forms.",
        "However, about 30,000 of the word shape tokens arc unique, that is, correspond to a single surface form.",
        "Thus, the word shape lexicon is apprc ximately )ne-third the size of the standard lexicon.",
        "Clearly, infortm tion has been lost, but not as much as one might think.",
        "In fact, the 20% of the word shape tokens that arc unique carry exactly as much information as their corresponding character-coded words.",
        "While some surface Dorms that map to unique word shape tokens are long and infrequent (like \"flibbertigibbet\", AAiAAxxAigiAAxA), many arc short, common words:",
        "While word shape tokens that arc unique have the same parts of speech as their corresponding surface forms, the others will tend on average to have many more parts of speech than an average surface form.",
        "This depends somewhat on the tagset (see section 6).",
        "In general, word shape tokens frequently have as many as 10 to 15 parts of speech, whereas standard surface forms rarely have more than 4 or 5."
      ]
    },
    {
      "heading": "6 DEVISING THE TAGSET",
      "text": [
        "The tagset is implicit in the lexicon: it includes all parts of speech listed in any entry of the lexicon; it also includes a small set of tags fm punctuation, such as comma, hyphen, and sentence boundary.",
        "Although the tagset is not explicitly defined, we can modify it by mapping from selected tags found in the lexicon to other tags of our choosing.",
        "For example, the lexicon distinguishes between verb tenses and has separate tags for different combinations of verb tense, person, and number: present tense verb, past tense verb, third person singular present verb, etc.",
        "If we preferred, we could map all these different verb forms to a single verb tag.",
        "However, we typically prefer to maintain such distinctions, as the text tagger can take advantage of differences in the surface forms of verbs with different tenses in order to uniquely identify their parts of speech.",
        "Shape conversion collapses different surface forms onto one word shape and merges their ambiguity classes.",
        "The result is that there tend to be fewer distinct surface forms, and that each surface Corm has, on average, a larger ambiguity class.",
        "If this ambiguity is problematic, one way to reduce it may be to reduce the site of the tagset.",
        "For example, we may choose to have one undifferentiated verb tag rather than a set which differentiates tense, person, and number.",
        "With fewer possible parts of speech to choose from, the HMM may find the part-or speech selection more constrained.",
        "This in turn may improve its accuracy at selecting one of the tags that are available.",
        "The uninteresting case, of course, is where every word shape has the same tag, that is, a tag set of one.",
        "This situation yields no useful syntactic information from the document.",
        "Since the use of word shape tokens does reduce the amount of information that is available to the tagger, it may reduce the number of different tags it can accurately assign.",
        "The proper size of the tagset becomes constrained on one hand by the amount of syntactic information we wish to extract (more information with a larger tagset) and on the other by the size of the ambiguity classes of the word shape tokens (more ambiguity with a larger tagset).",
        "Its proper size is thus an empirical question.",
        "For our tests we used tagsets with approximately 30 parts of speech."
      ]
    },
    {
      "heading": "7 THE TRAINING PROCESS",
      "text": [
        "Just as the hidden Markov model for standard text tagging requires a large corpus of text to train on, the word shape HMM requires a large corpus of text that has been converted to word shape tokens.",
        "We used at least 3.5 megabytes of ASCII text for our standard text tagger's corpus; we then shape converted this text to create the corpus for the word shape tagger.",
        "This corpus consisted of a variety of different writing styles (from colloquial to professional) and difficulty levels (from casual to erudite).",
        "Examples include essays by humorists, proposals lor new government policies, and classic works of literature."
      ]
    },
    {
      "heading": "8 THE TAGGING PROCESS",
      "text": [
        "With the word shape lexicon in place and an adequately trained 1-IMM, word shape tagging works just as standard text tagging :does.",
        "In particular, word shape tagging consists of the following steps: I.",
        "A stream of lext is tokenized into a stream of word shape tokens segmented into sentences.",
        "2.",
        "The shape-converted lexicon assigns an ambiguity class to each word shape token.",
        "The result is a stream or sentences composed of (word shape token, ambiguity class) pairs.",
        "3.",
        "The tagger uses the trained hidden Markov model to compute the highest probability,: part of speech for each word shape token in a sentence.",
        "The result is a stream of (word shape token, part o/ speech) pairs, which arc grouped according to sentence boundaries.",
        "We can now MC the resulting parts of speech to inform other segments of a document understanding system.",
        "The word shape part-of-speech tagger thus accepts word shape tokens grouped by sentence boundaries; within those boundaries, it assigns the most likely part of speech to each word shape token."
      ]
    },
    {
      "heading": "9 RESULTS",
      "text": [
        "In this section, we introduce a tool which can recognize noun phrases in sentences, and we use this tool to compare the performance of the standard tagger and the word shape tagger.",
        "We exemplify the comparison with two texts: one on which the standard tagger performs very well, and one on which it does relatively poorly.",
        "While the word shape tagger does less well in each case, its behavior tracks that of the standard tagger, exhibiting similar successes and failures.",
        "For the particular task of finding simple noun phrases, the word shape tagger's performance is less than that of the standard tapper's, but a large fraction of the noun phrases still arc found.",
        "WC have a system that can recognize simple noun phrases when given as input the sequence of tags for a sentence.",
        "1:.,ach of these phrases comprises a contiguous sequence of tags that satisfies a simple grammar.",
        "For example, a Howl phrase can be simply a pronoun tag or an arbitrary sequence of noun and adjective tags, possibly preceded by a determiner lag and possibly with an embedded possessi tag.",
        "The longest possible such sequences are found.",
        "Conjunctions ate not recognized as part of a noun phrase, nor is prepositional phrase attachment performed.",
        "We can be confident of finding many simple noun phrases because the word \"the\" has the unique word shape A A x.3 Recognition of noun phrases is a first step in topic identification: the topic of a document is likely to he indicated by its most himuent inilln phrases.",
        "In evaluating the tapper error rate, we use several measures (see tables).",
        "We calculate the percentage of total errors, the percentage of trivial errors, and the percentage",
        "he possessive tag is used for \"'s \" or \" as in \"the cat's pajamas' stripes\" 3 :knottier lainlish word, \"Flu,\" also inap to AAx:, lorinnately, in most contexts this word is rare.",
        "of pernicionserrors (there arc a few errors that do not fall in either of the latter categories).",
        "Tagging \"alarming\" in \"what the advocates are finding alarming\" as a present participle rather than as an adjective is an example of a trivial error.",
        "Pernicious errors typically involve mistagging nouns as verbs or verbs as nouns (in I 'Aiglish, there are many surface limns that can be either nominal or verbal).",
        "These latter errors cause problems in later processing, such as detecting simple noun phrases, since they may obscure noun phrases or introduce spurious ones.",
        "We compare the standard tagger and the word shape tagger by counting the matches in the streams of output tags.",
        "We do not demand strict matches, but instead allow the tags to belong to pertinent equivalence classes.",
        "For example, the standard tagger labels the noun \"monitors\" as a plural noun, and the word shape tagger labels xxxiAxxx simply as a noun.",
        "We consider this a match, since a noun and a plural noun are equally well recognized as part of a noun phrase.",
        "Almost all instances of mismatches result from the standard tagger being right and the word shape taggcr being wrong.",
        "Very occasionally the situation is the reverse, but this is to be expected as within the normal range of probabilities.",
        "More interesting is the observation that almost every pernicious error made by the standard tagger is repeated by the word shape logger.",
        "We take this as confirmation of the word shape tagger's ability to approximate the standard tagger's performance.",
        "The first comparison of tagger performance involves a 394-word excerpt from a government document.",
        "The standard logger's performance is better titan 95% correct, or better than 97% if trivial errors Mc disregarded.",
        "The word shape tagger's performance is a 59% match of the standard tagger's (or 51% if only exact matches are considered).",
        "The noun phrase recognize': found 113 simple noun phrases in the standard tagger's output and 77 (68%) 01 these in the word shape tagger's output."
      ]
    },
    {
      "heading": "Standard Tagger Errors",
      "text": [
        "'I'he second comparison is of tagging a 144-word piece of nonsense verse.",
        "The standard tagger's performance is",
        "89% correct, or 94u/0 disregarding trivial errors.",
        "The word shape tagger's performance is a 47% match (or 38% considering only exact matches).",
        "The noun phrase recognizer found 45 simple noun phrases in the standard tagger's output and 17 (38%) of these in the word shape lagger's output.",
        "Further study is needed to determine exactly how reliable word shape part-of-speech tagging and simple noun phrase recognition will he in finding the topic or topics in a document image.",
        "One means of improving this reliability may be our technique for grammatical function assignment which uses only the output of the part-of-speech tagger and phrase recognizers (Sibun 1991).",
        "However, we can already use part-of-speech lagging and simple noun phrase recognition as a tool for discerning something about the content of the document by discovering at least some of its noun phrases.",
        "Since our document recognition technology allows us to use word shape tokens to index directly into the document image, we can also identify parts of the image as promising candidates for OCR."
      ]
    },
    {
      "heading": "10 DISCUSSION",
      "text": [
        "Although the word shape tagger deals with greater ambiguity, it can still extract significant information from a text.",
        "The increase in ambiguity is not as high as might be expected: a large number of word shapes remain unambiguous after the lexicon has been shape converted.",
        "As noted above, the creation of the word shape lexicon from the standard lexicon reduces the number of distinct entries to approximately one-third.",
        "For example, distinct words such as \"cat\" and \"rat\" map onto the same word shape token xx A.",
        "Nevertheless, the complexity of English spelling still allows a large proportion of surface forms to be distinguished merely by their word shapes.",
        "Several improvements on our technique remain to he fully implemented.",
        "We do not yet have a principled way to determine the optimal tagset for a given corpus of text.",
        "As noted above, there is a tension between the size of the tagset and the amount of syntactic information that is available in the word shape tokens.",
        "We are also investigating computationally inexpensive ways of making finer distinctions between characters that map to the character shape codes x and A.",
        "Initially, parentheses and brackets were always classified as A and distorted any word shape they were adjacent to: for example, \"(USA)\" would be shape converted to AAAA A.",
        "Recently we have made progress in recognizing these non-alphabetic characters as word shape token delimiters, rather than parts of the word shape tokens themselves.",
        "It may also be useful to distinguish more alphabetic character classes by mapping scanned character images to a larger set of character shape codes.",
        "We can extract more useful information by distinguishing upper case letters from lower case letters, such as \"h\" and \"k\", which map to the character shape code A.",
        "A larger number of character shape codes gives us more information about the word shape tokens, and helps to reduce ambiguity.",
        "However, we must be careful to choose character shape features which can he easily detected in the image and quickly classified by a character shape code.",
        "In keeping with Fuji Xerox's multilingual document emphasis, we are also exploring ways in which this method may be applied to other Roman-alphabet languages, such as French, German, Dutch, and Spanish.",
        "The technique will need to be evaluated separately for each language, however, to better understand how each language's typographic conventions may be reflected in its word shape."
      ]
    },
    {
      "heading": "11 CONCLUSION",
      "text": [
        "We have presented a new technique for the understanding of English document images without optical character recognition.",
        "By scanning and categorizing character shapes, it is possible to extract word shapes from the document text; these word shapes tokens can then be used as input to a tagger which determines part-of-speech information.",
        "This part-of-speech information can then be used to inform other document understanding techniques, including noun phrase recognition and topic identification.",
        "The lack of OCR means we cannot extract all of the information contained in the scanned document's image; nevertheless, the information from the word shape tokens allows us to characterize the document's content with significant accuracy, and more quickly than if we had performed OCR."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank Larry Spitz and Masa Ozaki for their useful comments."
      ]
    }
  ]
}
