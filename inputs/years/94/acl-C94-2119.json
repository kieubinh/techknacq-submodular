{
  "info": {
    "authors": [
      "Ralph Grishman",
      "John Sterling"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C94-2119",
    "title": "Generalizing Automatically Generated Selectional Patterns",
    "url": "https://aclweb.org/anthology/C94-2119",
    "year": 1994
  },
  "references": [
    "acl-C92-2085",
    "acl-C92-2099",
    "acl-H93-1050",
    "acl-J86-3002",
    "acl-J91-2002",
    "acl-P90-1034",
    "acl-P92-1023",
    "acl-P92-1053",
    "acl-P93-1022",
    "acl-P93-1024"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Frequency information on co-occurrence patterns can be automatically collected from a syntactically analyzed corpus; tins information can then serve as the basis for selectional constraints when analyzing new text from the same domain.",
        "This information, however, is necessarily incomplete.",
        "We report on measurements of the degree of selectional coverage obtained with different sizes of corpora.",
        "We then describe a technique for using the corpus to identify selectionally similar terms, and for using this similarity to broaden the selectional coverage for a fixed corpus size."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Selectional constraints specify what combinations of words are acceptable or meaningful in particular syntactic relations, such as subject-verb-object or head-modifier relations.",
        "Such constraints are necessary for the accurate analysis of natural language text.",
        "Accordingly, the acquisition of these constraints is an essential yet time-consuming part of porting a natural language system to a new domain.",
        "Several research groups have attempted to automate this process by collecting co-occurrence patterns (e.g., subject-verb-object patterns) from a large training corpus.",
        "These patterns are then used as the source of selectional constraints in analyzing new text.",
        "The initial successes of this approach raise the question of how large a training corpus is required.",
        "Any answer to this question must of course be relative to the degree of coverage required; the set of selectional patterns will never be 100% complete, so a large corpus will always provide greater coverage.",
        "We attempt to shed to some light on this question by processing a large corpus of text from a broad dornain (business news) and observing how selectional coverage increases with domain size.",
        "In many cases, there are limits on the amount of training text available.",
        "We therefore also consider how coverage can be increased using a fixed amount of text.",
        "The most straightforward acquisition procedures build selectional patterns containing only the specific word combinations found in the training corpus.",
        "Greater coverage can be obtained by generalizing from the patterns collected so that patterns with semantically related words will also be considered acceptable.",
        "In most cases this has been clone using manually-created word classes, generalizing from specific words to their classes [12,1,10].",
        "If a preexisting set of classes is used (as in [10]), there is a risk that the classes available may not match the needs of the task.",
        "If classes are created specifically to capture selectional constraints, there may be a substantial manual burden in moving to a new domain, since at least some of the semantic word classes will be domain-specific.",
        "We wish to avoid this manual component by automatically identifying semantically related words.",
        "This can be done using the co-occurrence data, i.e., by identifying words which occur in the same contexts (for example, verbs which occur with the same subjects and objects).",
        "From the co-occurrence data one can compute a similarity relation between words [8,7].",
        "This similarity information can then be used in several ways.",
        "One approach is to form word clusters based on this similarity relation [8].",
        "This approach was taken by Sekinc et al.",
        "at U MIST, who then used these clusters to generalize the semantic patterns [11].",
        "Pereira et al.",
        "[9] used a variant of this approach, \"soft clusters\", in which words can be members of different clusters to different degrees.",
        "An alternative approach is to use the word similarity information directly, to infer information about the likelihood of a co-occurrence pattern from information about patterns involving similar words.",
        "This is the approach we have adopted for our current experiments [6], and which has also been employed by Dagan et al.",
        "[2].",
        "We compute from the co-occurrence data a \"confusion matrix\", which measures the interchangeability of words in particular contexts.",
        "We then use the confusion matrix directly to generalize the semantic patterns."
      ]
    },
    {
      "heading": "2 Acquiring Semantic Patterns",
      "text": [
        "Based on a series of experiments over the past two years [5,6] we have developed the following procedure",
        "for acquiring semantic patterns from a text corpus:",
        "1.",
        "Parse the training corpus using a broad-coverage grammar, and regularize the parses to produce something akin to an f-structure, with explic-• illy labeled syntactic relations such as SU13.11i1(:'1' and 0 Eli E(3'1'.",
        "I 2.",
        "Extract from the regularized parse a series of triples or the form",
        "head syntactic-relation head-of-argument, /modifier We will use the notation r wj > for such a triple, and < r :tit) > for a relation-argument pair.",
        "3.",
        "Compute the frequency of each head and each",
        "triple in the corpus.",
        "a sentence produces N parses, a triple generated front a single parse has weight 1/N in the total.",
        "We will use the notation lq< wirwj >) for the fro.-quency of a triple, and rh,,,,d(wi) for the frequency with which mi appears as a head in a parse t,ree.2"
      ]
    },
    {
      "heading": "Lor example, the sentence",
      "text": [
        "Mary likes young linguists from Limerick.",
        "would produce the regularized syntactic structure (s like (subject, (up Mary)) (object, (up linguist (a.-pos young) (from (np Itimerick))))) from which the following four triples are generated: like subject Mary like object linguist linguist a-pos young linguist from Limerick Given the frequency information I'', we can then estimate the probability that a particular head wi appears with a particular argument or modifier < r ruj>:",
        "'fins probability information would then be used in scoring alternative parse trees.",
        "hoc the evaluation below, however, we will use the frequency data L' directly.",
        "(a) if a verb has a separable particle (e.g., \"out\" in \"carry out\"), this is attached to the head (to create the head carry-oat) and not treated as a separate relation.",
        "Different particles often correspond to very different senses of a verb, so this avoids conflating the subject told object, distributions of these different senses, if the verb is \"be\", we generate a relation be-complement between the subject and the predicate complement.",
        "(e) triples in which either the head or the argument is a pronoun are discarded (d) triples in which the argument is a subordinate clause are discarded (this includes subordinate conjunctions and verbs taking clausal arguments) (e) triples indicating negation (with an argument, of \"not\" or \"never\") arc ignored"
      ]
    },
    {
      "heading": "3 Generalizing Semantic Patterns",
      "text": [
        "The procedure described above produces a. set of frequencies and probability estimates based on specific words.",
        "The \"traditional\" approach to generalizing this information has been to assign the words to a set, of semantic classes, and then to collect the frequency information on combinations of semantic classes [L2,1].",
        "Since at least some of these classes will be domain specific, there has been interest in automating the acquisition of these classes as well.",
        "'ibis can be done by clustering together words which appear in the same context.",
        "Starting from the file of triples, this involves: I. collecting for each word the Frequency with which it occurs in each possible context; for example, for a noun we would collect, the frequency with which it occurs as the subject, and the object °l each verb 2. defining a similarity measure between words, winch reflects the number of common contexts in which they appear forming clusters based on this similarity: measure Such a procedure was performed by Sekine et al.",
        "at UMIST [lib these clusters were then manually reviewed and the resulting clusters were used to generalize selectional patterns.",
        "A similar approach to word cluster formation was described by Hirschman et al.",
        "in 1975 [81.",
        "More recently, Pereira et al.",
        "[91 have described ;t word clustering method using \"soft clusters\", in which a word can belong to several clusters, with different, cluster membership probabilities, Cluster creation has the advantage that the clusters are amenable to manual review and correction.",
        "On the other hand, our experience indicates that successful cluster generation depends on rather delicate adjustment of the clustering criteria.",
        "We have therefore",
        "elected to try an approach which directly uses a form of similarity measure to smooth (generalize) the probabilities.",
        "Co-occurrence smoothing is a method which has been recently proposed for smoothing n-gram models [3].3 The core of this method involves the computation of a co-occurrence matrix (a matrix of confusion probabilities) Pc(willw), which indicates the probability of word wj occurring in contexts in which word Wi occurs, averaged over these contexts.",
        "where the sum is over the set of all possible contexts s. In applying this technique to the triples we have collected, we have initially chosen to generalize (smooth over) the first element of triple.",
        "Thus, in triples of the form worn relation wore we focus on wordl, treating relation and wore as the context:",
        "Informally, we can say that a large value of Pc(wilv4) indicates that Wi is selectionally (semantically) acceptable in the syntactic contexts where word u4 appears.",
        "For example, looking at the verb \"convict\", we see that the largest values of Pc(convict, x) are for x = \"acquit\" and a; = \"indict\", indicating that \"convict\" is selectionally acceptable in contexts where words \"acquit\" or \"indict\" appear (see Figure 4 for a larger example).",
        "How do we use this information to generalize the triples obtained from the corpus?",
        "Suppose we are interested in determining the acceptability of the pattern convict-object-owner, even though this triple does not appear in our training corpus.",
        "Since \"convict\" can appear in contexts in which \"acquit\" or \"indict\" appear, and the patterns acquit-object-owner and indict-object-owner appear in the corpus, we can conclude that the pattern convict-object-owner is acceptable too.",
        "More formally, we compute a smoothed triples frequency Fs 'from the observed frequency P' by averaging over all words u4, incorporating frequency information for wa to the extent that its contexts are also suitable contexts for Fs(< w wi >) Pc(wiltt/i) • F(< 114 wi >) In order to avoid the generation of confusion table entries from a single shared context (which quite often",
        "is the result of an incorrect parse), we apply a filter in generating Pc: for i j, we generate a non-zero Pc(wilwi) only if the wi and wj appear in at least two common contexts, and there is some common context in which both words occur at least twice.",
        "Furthermore, if the value computed by the formula for Pc is less than some threshold rc, the value is taken to be zero; we have used rc; = 0.001 in the experiments reported below.",
        "(These filters are not applied for the case i = j; the diagonal elements of the confusion matrix are always computed exactly.)",
        "Because these filters may yeild an unnormalized confusion matrix (i.e., Ew, Pc(wilwi) < 1), we renormalize the matrix so that Pc(wjliw) = 1.",
        "A similar approach to pattern generalization, using a similarity measure derived from co-occurrence data, has been recently described by Dagan et al.",
        "[2].",
        "Their approach differs from the one described here in two significant regards: their co-occurrence data is based on linear distance within the sentence, rather than on syntactic relations, and they use a different similarity measure, based on mutual information.",
        "The relative merits of the two similarity measures may need to be resolved empirically; however, we believe that there is a virtue to our non-symmetric measure, because sub-stitutibility in selectional contexts is not a symmetric relation .4"
      ]
    },
    {
      "heading": "4 Evaluation",
      "text": []
    },
    {
      "heading": "4.1 Evaluation Metric",
      "text": [
        "We have previously [5] described two methods for the evaluation of semantic constraints.",
        "For the current experiments, we have used one of these methods, where the constraints are evaluated against a set of manually classified semantic triples.'",
        "For this evaluation, we select a small test corpus separate from the training corpus.",
        "We parse the corpus, regularize the parses, and extract, triples just as we did for the semantic acquisition phase.",
        "We then manually classify each triple as valid or invalid, depending on whether or not it arises from the correct parse for the sentence.'",
        "We then establish a threshold 7' for the weighted triples counts in our training set, and define 4 If trq allows a broader range of arguments than a2, then we can replace tn2 by to1, but not vice versa.",
        "For example, We can replace \"speak\" (which takes a human subject) by \"sleep\" (which takes an animate subject), and still have a selectionally valid pattern, but not the other way around.",
        "`\"Phis is similar to tests conducted by Pereira et al.",
        "[9] and Pagan et al.",
        "[2].",
        "The cited tests, however, were based on selected words or word pairs of high frequency, whereas our test sets involve a representative set of high and low frequency triples.",
        "Tb i s is a different criterion from the one used in our earlier papers.",
        "In our earlier work, we marked a triple as valid if it could be valid in sonic sentence in the domain.",
        "We found that it was very difficult to apply such a standard consistently, and have therefore changed to a criterion based on an individual sentence.",
        "number of triples in test set, which were classified as valid and which appeared in training set with count > v_ number of triples in test set which were classified as valid and which appeared in training set with count.",
        "7' number of triples in test set which were classified as invalid and which appeared in training set with count > 7' ail(' then define recall precision liy varying the threshold, we can select different trade-offs of recall and precision (at high threshold, we select only a small number of triples which appeared frequently and in which we therefore have high confidence, thus obtaining a high precision but., low recall; conversely, at a low threshold we admit a much larger number of triples, obtaining a high recall but lower precision)."
      ]
    },
    {
      "heading": "4.2 Test Data",
      "text": [
        "The training and test corpora were taken from the Wall Street Journal.",
        "In order to get higher-quality parses ()I' these sentences, we disabled sonic of the recovery mechanisms normally used irk our parser.",
        "Of the 57,366 sentences in our training corpus, we obtained complete parses for 31,414 and parses of initial substrings for an additional 12,441 sentences.",
        "These parses were then regularized and reduced to triples.",
        "We generated a total of 279,233 distinct triples from the corpus.",
        "'rho test corpus used to generate the triples which were manually classified consisted of 10 articles, also",
        "(percentage of total corpus used).",
        "o = at 72% precision; • = maximum recall, regardless of precision; x predicted values for maximum recall front the Wall Street Journal, distinct, front those in the training set.",
        "These articles produced a test set containing a total of 1932 triples, of which I 107 were valid and 825 were invalid."
      ]
    },
    {
      "heading": "4.3 Results",
      "text": []
    },
    {
      "heading": "4.3.1 Growth with Corpus Size",
      "text": [
        "We began by generating triples from the entire corpus and evaluating the selectional patterns as described above; the resulting recall/precision curve generated by varying the threshold is shown in Figure 1.",
        "To see how pattern coverage improves with corpus size, we divided our training corpus into 8 segments and computed sets of triples based on the first segment, the first two segments, etc.",
        "We show in Figure 2 a plot of recall vs. corpus size, both at a constant precision of 72% and for maximum recall regardless of precision.",
        "The rate of growth of the maximum recall can be understood in terms of the frequency distribution of triples.",
        "In our earlier work [4] we lit the growth data to curves of the form 1 - exp(-fix), on the assumption that all selectional patterns are equally likely.",
        "This may have been a roughly accurate assumption for that application, involving semantic-class based patterns (rather than word-based patterns), and a rather sharply circumscribed sublanguage (medical reports).",
        "Vol) the (word-level) patterns described here, however, the distribution is quite skewed, with a small number of very-high-frequency patterns,' which results in dill",
        "ferent growth curves.",
        "Figure 3 plots the number of distinct triples per unit frequency, as a function of frequency, for the entire training corpus.",
        "This data can he very closely approximated by a function of the form N(11 = al?-n, where a = 2.2.9 To derive a growth curve for maximum recall, we will assume that the frequency distribution for triples selected at random follows the same form.",
        "Let p(7') represent the probability that a triple chosen at random is a particular triple 7'.",
        "Let P(p) be the density of triples with a given probability; i.e., the number of triples with probabilities between p and p is (P(p) (for small e).",
        "Then we are assuming that P(p) = Kira , for p ranging from some minimum probability prni„ to 1.",
        "For a triple T, the probability that we would find at, least one instance of it in a corpus of T triples is approximately I -- e-1147').",
        "The maximum recall for a corpus of T triples is the probability of a given triple (the \"test triple\") being selected at random, multiplied by the probability that that triple was found in the training corpus, summed over all triples:",
        "Hy selecting an appropriate value of k. (and corresponding limi„ so that, the total probability is 1), we can get a the fact that our lexical scanner replaces all identifiable company names by the token a compang, all currency values by a-citTrency, etc.",
        "Many of the highest frequency triples involve such tokens.",
        "'This is quite similar to a Zipf 's law distribution, for which = 2.",
        "good match to the actual maximum recall values; these computed values are shown as x in Figure 2.",
        "Except for the smallest data set, the agreement is quite good considering the very simple assumptions made."
      ]
    },
    {
      "heading": "4.3.2 Smoothing",
      "text": [
        "In order to increase our coverage (recall), we then applied the smoothing procedure to the triples from our training corpus.",
        "In testing our procedure, we first generated the confusion matrix Pc and examined some of the entries.",
        "Figure 4 shows the largest entries in 1% for the noun \"bond\", a c.onamon word in the Wall Street Journal.",
        "It is clear that (with some odd exceptions) roost of the words with high Pc values are semantically related to the original word.",
        "To evaluate the effectiveness of our smoothing proce.dure, we have plotted recall vs. precision graphs for both unsmoothed and smoothed frequency data.",
        "The results are shown in Figure 5.",
        "Over the range of precisions where the two curves overlap, the smoothed data performs better at low precision/high recall, whereas the unsmoothed data is better at high precision/low recall.",
        "In addition, smoothing substantially extends the level of recall which can be achieved for a given corpus size, although at some sacrifice in precision.",
        "Intuitively we can understand why these curves should cross as they do.",
        "Smoothing introduces a certain degree of additional error.",
        "As is evident from Figure 4, some of the confusion matrix entries are spurious, arising from such sources as incorrect parses and the conflation of word senses.",
        "In addition, some of the triples being generalized are themselves incorrect (note that even at high threshold the precision is below 90%).",
        "The net result is that a portion (roughly 1/3 to 1/5) of",
        "the triples added by smoothing are incorrect.",
        "At low levels of precision, this produces a net gain on the precision/recall curve; at higher levels of precision, there is a net loss.",
        "In any event, smoothing does allow for substantially higher levels of recall than are possible without smoothing."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We have demonstrated how selectional patterns can be automatically acquired front a corpus, and how selectional coverage gradually increases with the size of the training corpus.",
        "We have also demonstrated that --- - for a given corpus size - - coverage can be significantly improved by using the corpus to identify selectionally related terms, and using these similarities to generalize the patterns observed in the training corpus.",
        "'Plus is consistent with other recent, results using related tech-.",
        "niques [2,9].",
        "We believe that these techniques can be further unproved in several ways.",
        "The experiments reported above have only generalized over the lust (head) position of the triples; we need to measure the effect of generalizing over the argument position as well.",
        "With larger corpora it may also be feasible to use larger patterns, including in particular subject-verb-object patterns, and thus reduce the confusion due to treating different words senses as common contexts."
      ]
    }
  ]
}
