{
  "info": {
    "authors": [
      "Young S. Han",
      "Key-Sun Choi"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C94-2138",
    "title": "A Reestimation Algorithm for Probabilistic Ttecursive Transition Network",
    "url": "https://aclweb.org/anthology/C94-2138",
    "year": 1994
  },
  "references": [
    "acl-H91-1046",
    "acl-J93-1002"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Probabilistic Recursive Transition Network (PRTN) is an elevated version of RTN to model and process languages in stochastic parameters.",
        "The representation is a direct derivation from the RTN and keeps much the spirit of Hidden Markov Model at the same time.",
        "We present a reestimation algorithm for PRTN that is a variation of Inside-Outside algorithm that computes the values of the probabilistic parameters from sample sentences (parsed or unparsed)."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In this paper, we introduce a network representation, Probabilistic Recursive Transition Network that is directly derived from WIN and IIMM, and present an estimation algorithm for the probabilistic parameters.",
        "PRTN is a ETN augmented with probabilities in the transitions and states and with the lexical distributions in the transitions, or is the Ridden Markov Model augmented with a stack that makes some transitions deterministic:.",
        "The parameter estimation of PliTN is developed as a variation of Inside-Outside algorithm.",
        "The Inside-Outside algorithm has been applied to SCFG recently by Jelinek (1990) and Licari (1991).",
        "The algorithm was first introduced by Baker in 1979 and is the context free language version of Forward-Backward algorithm in Hidden Markov Models.",
        "Its theoretical foundation is laid by Baum and Welch in the late 60's, which in turn is a type of the EM algorithm in statistics (Rabiner, 1989).",
        "Kupiec (1991) introduced a trellis based estimation algorithm of Hidden SCFG that accomodates both Inside-Outside algorithm and Forward-Backward algorithm."
      ]
    },
    {
      "heading": "2. Probabilistic Recursive Transition Network",
      "text": [
        "A probabilistic 10EN (PIFFN, hereafter) denoted by A is a 4 tuple.",
        "A (A, B , E).",
        "A is a transition matrix containing transition probabilities, and X3 is an observation matrix containing probability distribution of the words observable at each terminal transition where row and column correspond to terminal transitions and a list of words respectively.",
        "t specifies the types of transitions, and E denotes a stack.",
        "'the first two model parameters are the same as that of IIMM, thus typed transitions and the existence of a stack are what distinguishes PRTN from IIMM.",
        "The stack operations are associated with transitions.",
        "According to the stack operation, transitions are classified into three types.",
        "The first type is push transition in which state identification is pushed into the stack.",
        "The second type is pop transition which is selected by the content of stack.",
        "Transitions of the third type are not committed to stack operation.",
        "The three types are also accompanied by different grammatical implication, hence grammatical categories are assigned to transitions except pop transitions.",
        "Push transitions are associated with nonterminal categories, and will be called nonterminal transition when it is more transparent in later discussions.",
        "In general, the grammar expressed in PRTN consists of layers.",
        "A layer is a fragment of network that corresponds to a nonterminal.",
        "The third type of transition is linked to the category of terminals (words), thus is named terminal transition.",
        "Also a table of probability distribution of words is defined on each terminal transition.",
        "In the context of IIMMs, the words in the terminal transition are observations to be generated.",
        "Pop transitions represent returning of a layer to one of its possibly multiple higher layers.",
        "The network topology of PRTN is not different from that of RTN.",
        "In a conceptual drawing of a grammar, each layer looks like an independent network.",
        "Compared with conceptual drawing of the network, an operational view provides more vivid representation in which actual paths or parses are composed.",
        "The only difference between the two is that in operational view a nonterminal transition is connected directly to the first state of the corresponding layer.",
        "In this paper, the parses or paths are assumed to be sequences of dark-headed transitions (see Fig. 1 for example).",
        "Before we start explaining the algorithms let us define some notations.",
        "There is one start state denoted by S, and one final state denoted by T. Also let us call states immediately following a terminal transition terminal state, and states at which pop transitions are defined pop state.",
        "Some more notations are as follows.",
        "• first(1) returns the first state of layer 1.",
        "• last(1) returns the last state of layer 1.",
        "• layers) returns the layer state s belongs to.",
        "• bout(1) returns the states from which layer 1 branches out.",
        "• bin(1) returns the states to which layer 1 re turns.",
        "• terminal(1) returns a set of terminal edges in layer 1.",
        "• monterminal(1) returns a set of nonterminal edges in layer 1.",
        "• ij denotes the edge between states i and j.",
        "• [i, j] denotes the network segment between states i and j.",
        "• Wa„,b is an observation sequence covering from ath to bth observations."
      ]
    },
    {
      "heading": "3. Reestimation Algorithm",
      "text": [
        "PRTN is a RTN with probabilistic transitions and words' that can be estimated from sample sentences by means of statistical techniques.",
        "we present a reestimation algorithm for obtaining the probabilities of transitions and the observation symbols (words) defined at each terminal transition.",
        "Inside-Outside algorithm provides a formal basis for estimating parameters of context free languages such that the probabilities of the observation sequences (sample sentences) are maximized.",
        "The reestimation algorithm iteratively estimates the probabilistic parameters until the probability of sample sentence(s) reaches a certain stability.",
        "The reestimation algorithm for PRTN is a variation of Inside-Outside algorithm customized for the representation.",
        "The algorithm to be discussed is defined only for well formed observation sequences.",
        "Definition 1 An observation sequence is well formed if there exists at least a path that generates the sequence in the network and starts at S and ends at Let an observation sequence of length N denoted by",
        "We start explaining the reestimation algorithm by defining Inside-probability.",
        "The Inside probability denoted by Pi(i),,t of state i is the probability that a portion of layer(i) 'we do not consider probabilistic states in this paper.",
        "(from state i to the last state of the layer) generates Ws,,,t.",
        "That is, it is the probability that a certain fragment of a layer generates a certain segment of an input sentence, and this can be computed by summing the probabilities of all the possible paths in the layer segment that generate the given input segment.",
        "raj, el – IA) where e last(layer(i)) More constructive representation of Inside probability is then",
        "The paths starting at state i are classified into two cases according to the type of immediate transition from i: it can be of terminal or nonterminal type, In case of terminal, after the probability of the terminal transition is taken into account, the rest of the layer segment is responsible for the input segment short of one word just generated by the terminal transition.",
        "In case of nonterminal, first the transition probabilities (push and respective pop transitions) are multiplied, then depending on the coverage of the nonterminal transition (sublayer) the rest of the current layer is responsible for the remaining input sequence after done by the sublayer.",
        "After the last observation is made, the last state (pop state) of /aye•(i) should be reached.",
        "Fig.",
        "2 is the pictorial view of the Inside probability.",
        "A well formed sequence can begin only at state S, thus to be strict, 1-'3(8) has additional product term P(S) that can be computed also using Inside-Outside algorithm.",
        "Now define the Outside probability.",
        "The Outside probability denoted by .P0 (i, is the probability that partial sequences, W1„,,_4 and Wto.,„,N , are generated provided that the partial sequence, Ws_t, is generated by [i, j] given a model, A.",
        "This is a complementary point of Inside-probability.",
        "This time, we look at the outside of given layer segment and input segment.",
        "Assuming a given layer segment generates a given input segment, we want to compute the probability that the surrounding portion of the whole PETN generates the rest of the input sequence.",
        "The Outside probability is computed first by considering the current layer consisting of two parts after excluding [i, j] that are captured in Inside-probability.",
        "Beyond the current layer is simply an Outside probability with respect to the current layer.",
        "And by definition, where x E bout(layer(i)),",
        "x represents a state from which layer(i) branches out, and y represents a state to which layer(j) returns to.",
        "Every time a different combination of left and right sequences with respect to W„„t is tried in the layer states i and j belong to, the rest of remaining sequences is the Outside probability at the layer above layer(i).",
        "Fig.",
        "3 shows the network configuration in computing the Outside probability.",
        "Bi9(f,i)„,_1 is the probability that sequence, 147„ – ,q, is generated by layer(i) left to state i. Pi(j)t,fi – b is the probability that sequence WH-1,..b is generated by layer(i) right to state j.",
        "The portions of W not covered by Wa_b is then left to the parent layers of layer(i).",
        "PI (f, i):,„4 is a slight variation of Inside probability in which Bi(f),b's in the Inside probability formula are replaced by Pl(f,i),b.",
        "Its actual computation is done as follows:",
        "It is basically the same as Inside probability except that it carries a state identification i to check the validity of stop state.",
        "If there are observations left for generation (s < t), things are done just as in computing Inside probability, ignoring i.",
        "When boundary point is reached (s>t), if the last state is i, it returns 1, and 0, otherwise.",
        "The probability of an observation sequence can be computed using Inside probability as",
        "Now we can derive the reestimation algorithm for A and B using the Inside and Outside probabilities.",
        "As the result of constrained maximization of Baum's auxiliary function, we have the following form of reestimation for each transition (Rabiner 1989).",
        "expected no.",
        "of transitions from i • The expected frequency is defined for each of the three types of transition.",
        "For a terminal transition,",
        "uv is a nonterminal transition .",
        "Considering that transitions of terminal and nonterminal types can occur together at a state, the reestimation for terminal transitions is done as follows: The reestimation process continues until the probability of the observation sequences reaches a certain stability.",
        "It is not unusual to assume that the training set can be very large, and even grow indefinitely in non trivial applications in which case additive training can be tried using a smoothing technique as in (Jane and Pieraccini 1987).",
        "The complexity of Inside-Outside algorithm is 0(N3) both in the number of states and input length (Lari :1.990).",
        "The efficiency comes from the fact that the algorithm successfully exploits the context-freeness.",
        "for instance, the generation of substrings by a nonterminal is independent of the surroundings of the nontermimd, and this is how the product of the Inside and Outside probabilities works and the complexity is derived."
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "Recently several probabilistic parsing approaches have been suggested such as SCFG, probabilistic GLR, and probabilistic link grammar (Lafferty, 1992).",
        "Kupiec extended the reestimation algorithm for SCFG to cover non-Chomsky normal forms (Carroll, 1993).",
        "This paper further advances the trend by implanting the Inside-Outside algorithm on the plain topology of RTN which distinguishes itself from Kupiec's work."
      ]
    }
  ]
}
