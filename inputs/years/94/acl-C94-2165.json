{
  "info": {
    "authors": [
      "Christer Johansson"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C94-2165",
    "title": "Catching the Cheshire Cat",
    "url": "https://aclweb.org/anthology/C94-2165",
    "year": 1994
  },
  "references": [
    "acl-C92-3141"
  ],
  "sections": [
    {
      "heading": "ABSTRACT",
      "text": [
        "Finding useful phrases is important in applications like information retrieval, and text-to-speech systems.",
        "One of the currently most used statistics is the mutual information ratio.",
        "This paper compares the mutual information ratio and a measure that takes temporal ordering into account.",
        "Using this modified measure, some local syntactic constraints as well as phrases are captured."
      ]
    },
    {
      "heading": "INTRODUCTION",
      "text": [
        "In Alice's Adventures in Wonderland by Lewis Carrot many of Alice's friends have names that consists of two words, for example: the March Hare, the Mock Turtle, and the Cheshire Cat.",
        "The individual words in these combinations, if we ignore capitalisation, might be quite common.",
        "Individual words usually mean different things when they are free.",
        "I or example, in \"The March against Apartheid\", and \"The March Hare\", \"march\" means totally different things.",
        "There is obviously a strong link between \"the\" and \"march\", but the link between \"march\" and \"hare\" is definitely stronger, at least in Lewis Carrot's text.",
        "The goal of this paper is to propose a statistic that measures the strength of such glue between words in a sampled text.",
        "Finding the names of Alice's friends can he done by searching for two adjacent words with initial capital letters.",
        "One use of statistical associations could he to find translatable concepts and phrases, that might be expressed with a different number of words in another language.",
        "Another possibly interesting use of statistical associations is to predict whether words constitute new or given information in speech.",
        "It has been proposed (e.g. Horne & Johansson, 1993) that the stress of words in speech is highly dependent on the informational content of the word.",
        "Also, statistical associations are not incompatible with the first stages of the \"hypothesis space\" proposed by Processability Theory (personal communication with Manfred Pienemann of Sydney University, see also Meisel & al., 1981).",
        "There are different methods of calculating statistical associations.",
        "Yang & Chute (1992) showed that a linear least square mapping of natural language to canonical terms is both feasible, and a way of detecting synonyms.",
        "Their method does not seem to detect dependencies in the order of words however.",
        "To do this we need a measure that is sensitive to the order between words.",
        "in this paper we will use a variant of mutual information that derives from Shannon's theory of information.",
        "(as discussed in e.g., Salton & McGill, 1983)"
      ]
    },
    {
      "heading": "Definitions and assumptions",
      "text": [
        "The definition of a word in a meaningful way is far from easy, but a working definition, for technical purposes, is to assume that a word equals a string of letters.",
        "These 'words' are separated by non-letters.",
        "The case 01' letters is ignored, i.e. converted into lower case.",
        "For example: \"there's\" are two 'words': \"there\" and Its , A collocation consists of a word and the word that immediately follows.",
        "Index I will re fer to the first word and 2 to the second word.",
        "Index 12 will refer to word I followed by word2, and similarly for 21.",
        "Another assumption is that natural language is more predictive in the (left-to-right) temporal order, than in the reversed order.",
        "This is motivated by the simple observation that speech comes into the system through the ears serially.",
        "For example: consider the French phrase \"un bon vin blanc\" (Lit.",
        "\"a good wine white\").",
        "\"Bon\" can (relatively often) he followed by \"yin\", but usually not \"yin\" by \"bon\".",
        "The same kind of link exists between \"yin\" and \"Filmic\", but not between \"blanc\" and \"vin\".",
        "This linking affects the intonation of French phrases, and also that intonation supports these kinds of links.",
        "Note, that this is not an explanation of either intonation or syntax: we most.",
        "likely have to consider massive interaction between different modalities of language."
      ]
    },
    {
      "heading": "Deriving the measure",
      "text": [
        "The mutual information ratio, [t, provides a rough estimation on the glue between words.",
        "It measures, roughly, how much more common a collocation is in a text than can be accounted for by chance.",
        "This measure does not assume any ordering between the words making up a collocation, in the sense that the [I-measure of [wl...w2[ and [w2...wl] are calculated as if they were unrelated collocations.",
        "The mutual information ratio (in Steier & Belew, 1991) is expressed:",
        "where 'p' defines the probability function, paw 1 ...w2D is read as \"the probability of finding word w2 after word w1\"."
      ]
    },
    {
      "heading": "Adjusting for order between words",
      "text": [
        "We have experimented with the difference in mutual information, Ap,, between the two different orderings of two words making up a collocation.",
        "The results indicate that As captures some of the local constraints in a sampled text.",
        "AR can be expressed:",
        "Formula 2: The difference in mutual information where F([wx...wyil) denotes the frequency of which wx and wy co-occur in the sample.",
        "F(wx) is the frequency of word wx.",
        "Note that the size of the sample cancels in this equation.",
        "Note also that this measure is not sensitive to the individual probabilities of the words.",
        "A problem is when there is no Faw2...wi]).",
        "In these cases, we have chosen to arbitrarily set Fqw2...w lip to 0.1, with the justification that if the sample was ten times larger we might have found at least one such pair."
      ]
    },
    {
      "heading": "MATERIAL",
      "text": [
        "The material is Alice's Adventures in Wonderland by Lewis Carrol, available in electronic format via email from the Gutenberg Project.",
        "The text contains 27332 words of which 2576 are unique, making up a total of 14509 unique word pairs.",
        "Alice in Wonderland was chosen because it is a well-known text, it contains some phrases that we know are in there (e.g. March Hare), and it contains a sufficient number of words, and variations of words, to be interesting for the experiment.",
        "Studies could be done for other collections of texts, e.g. medical abstracts.",
        "As more documents are available, comparisons between documents can be done (Steier & Belew, 1991).",
        "This experiment only contains within comparisons of phrases for one specific text."
      ]
    },
    {
      "heading": "METHOD",
      "text": [
        "For each of the unique words in the text the frequencies of all immediately following words were collected.",
        "In this text, no filtering of the text was performed.",
        "Some initial experiments were performed, with a stoplist, to remove function words and some other common words (see Fox, 1992, for details).",
        "Some simple stemming was also tried, e.g. removing 's' and 'ed' from the end of words.",
        "Stemming may lead to difficulties in distinguishing compounds from noun-verb complexes.",
        "It is not clear if the pros of using stemming outweighs the cons, consequently we decided to work with the raw text.",
        "Stoplists and stemming might be more important when the ordinary [I-measure is used."
      ]
    },
    {
      "heading": "RESULTS",
      "text": [
        "The collocations were ordered differently by the two measures.",
        "The s was sensitive to individual frequencies, and favoured very low frequency collocations.",
        "The As was sensitive to the ordering of the words, and favoured high frequency collocations that only occurred in one order.",
        "The quality of the different measures can be seen by comparing the top and last ten collocations between the measures.",
        "Table 1.1 and 2.1 refer to An, and Table 1.2 and 2.2 refer to s. The N column tells the rank-number of the collocation.",
        "Note that the frequencies of the individual words, Fl and F2, are not used to compute An, they are only provided for comparison with the [1-measure.",
        "Note that the numerical values of the [t-measure and the An-measure cannot be directly compared since they measure slightly different phenomena.",
        "Au gives a measure of local links between words.",
        "As can he seen from Table 1.1, AÂµ captures local constraints: that prepositions are usually followed by a noun phrase, that 'and' usually is used as a noun coordinator (indicated by the high value for 'and->the').",
        "Mitjushin (1992) has proposed similar links on a higher syntactic level, using a rule-based approach.",
        "We have deliberately tried to avoid talking about word-classes since it is misleading at this level of analysis.",
        "However, we get many examples of good representatives for word-classes that form collocations.",
        "The flavour of the collocations that u rate highly is different.",
        "As can he seen from Table 1.2, low individual frequencies result in a high n-value, even if the collocation is unique.",
        "This gives an illusion of a semantic relation, which is due to the fact that low frequency words are usually high in content.",
        "The n-measure is useful.",
        "when we are interested in the correlation between words within and between documents (Steier & Belew, 1991).",
        "This notion could he expanded upon to incorporate correlation between any two words in general, and it seems to work well for the n-measure (Wader and Rapp, 1989).",
        "The last ten collocations.",
        "An is sensitive to deviation from an expected ordering in the sample.",
        "The negative valued link between these words makes a phrase boundary between the two words probable.",
        "The n-measure, in contrast, gives some collocations that are intuitively unlikely phrases consisting of high frequency words.",
        "In the case of \"the- the\" there exists 1641 pairs that speak against that pairing, but it is hard to explain this in terms of local syntactic constraints.",
        "The negative scores seems to capture possible typographic errors."
      ]
    },
    {
      "heading": "Particle verbs",
      "text": [
        "Particle verbs are hard to rank high for the n-measure, because the individual frequencies of the particles are usually devastatingly high, and the frequency of the main verb in particle verb constructions are usually higher than average.",
        "The An are, in general, good at finding such combinations if the order between the two words is fixed (Table 3.1)."
      ]
    },
    {
      "heading": "Finding thematic phrases",
      "text": [
        "But what about finding Alice's friends?",
        "Does the Au find the phrases that the text is about thematic phrases)?",
        "To test this we chose some of the names of Alice's friends (Table 3.2).",
        "We found that the rank number that Ali delivers is higher than the rank number for the I\"measure for all the checked friends.",
        "This is due to the frequency effects discussed above."
      ]
    },
    {
      "heading": "What is lost",
      "text": [
        "There are obviously good phrases that u rates higher than A.",
        "These usually consists of two words that are uncommon in the sample.",
        "Some idioms are of this kind.",
        "The Au needs to find more examples of collocations with the exact ordering between the constituents to rate the collocation high ( Table 3.3)."
      ]
    },
    {
      "heading": "Adding memory",
      "text": [
        "We have also done some experiments with adding memory to the method.",
        "A 'memory' could, for example, extend 10 words after each word.",
        "All words following within a distance equal to the size of the memory were collected.",
        "Adding a memory allowed the model to detect shared information of words that was further apart (for example \"pack of cards\" or \"boots and shoes\".",
        "The memory introduced false collocations: e.g., \"grammar-> mouse\".",
        "The context was: \"Alice thought this must be the right way of speaking to a mouse: she had never done such a thing before, but she remembered having seen in her brother's Latin Grammar, 'A mouse--of a mouse--to a mouse--a mouse--O mouse!\"'",
        "This context gave up to 5 collocations for \"grammar\" followed by \"mouse\", and therefore rated \"grammar-> mouse\" very high.",
        "Otherwise, words that happened to be near a word without being statistically related to the word were usually rated low.",
        "The u gave clearly better results on finding related phrases than the Au, with the model with the 'memory'.",
        "With the memory, the Ala ordered the pairs closer to the original raw-frequency ordering the more 'memory' was present.",
        "The experiment with the memory was useful because it showed that this was not worth doing for Au, but likely worth doing for u."
      ]
    },
    {
      "heading": "CONCLUSIONS",
      "text": []
    },
    {
      "heading": "Possible usefulness",
      "text": [
        "The higher sensitivity to local constraints in the temporal ordering could be used in a parser for finding local phrases.",
        "This might also have its implications for language acquisition.",
        "It could be tested if language learners make mistakes that could be explained by the statistical connectivity between words.",
        "Further research is needed on how the measure of connectivity behaves on phrase boundaries.",
        "Areas where phrase finding could be useful include: text-to-speech (phrase intonation), machine translation (translation of compounds), and in information retrieval: phrase transformation of high frequency terms into medium frequency terms with a better discrimination value (Salton & McGill, 1983)."
      ]
    },
    {
      "heading": "Characteristics",
      "text": [
        "The u-measure is good at estimating global correlations in a document or collection of documents (Wettler & Rapp, 1989).",
        "This could he used for capturing contextual and pragmatic constraints in a text.",
        "Other methods exist that are good, perhaps even better, at capturing for example synonymy.",
        "Linear least square mapping (Yang & Chute [992) is one method that has shown to he promising on capturing very good mappings between, in their case, symptoms and diagnosis.",
        "The same technique could he used for mapping a text to its abstract.",
        "The draw-hack of these methods is their inherent parallel structure which makes it hard to account for the ordering that natural language requires.",
        "The Art measure, on the other hand, is a local measure, that seems to capture dependencies in the temporal ordering of the language.",
        "It is hard to draw any definite conclusions from the analysis of only one text, but we have seen how the two proposed measures react to the frequencies of individual words, as well as the frequencies of word pairs.",
        "Taking into account the ability of An to find dependencies in the temporal ordering, we think it is a more relevant measure than ft for several aspects of natural language processing, but not all."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "Thanks to the people at my department: especially Barbara Gawronska."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": []
    },
    {
      "heading": "Information Retrieval & Extraction",
      "text": []
    }
  ]
}
