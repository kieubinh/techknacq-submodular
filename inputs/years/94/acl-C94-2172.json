{
  "info": {
    "authors": [
      "Louise Guthrie",
      "Elbert Walker"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C94-2172",
    "title": "Document Classification by Machine: Theory and Practice",
    "url": "https://aclweb.org/anthology/C94-2172",
    "year": 1994
  },
  "references": [
    "acl-H92-1041"
  ],
  "sections": [
    {
      "heading": "DOCUMENT CLASSIFICATION BY MACHINE:Theory and Practice",
      "text": [
        "Louise Guthrie Elbert Walker New Mexico State University Las Cruces, New Mexico 88001 Joe Guthrie University of Texas at El Paso El Paso, Texas 79968"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "In this note, we present results concerning the theory and practice of determining for a given document which of several categories it best fits.",
        "We describe a mathematical model of classification schemes and the one scheme which can be proved optimal among all those based on word frequencies.",
        "Finally, we report the results of an experiment which illustrates the efficacy of this classification method."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "A problem of considerable interest in Computational Linguistics is that of classifying documents via computer processing [Hayes, 1992; Lewis 1992; Walker and Arnsler, 1986].",
        "Simply put, it is this: a document is one of several types, and a machine processing of the document is to determine of which type.",
        "In this note, we present results concerning the theory and practice of classification schemes based on word frequencies.",
        "The theoretical results arc about mathematical models of classification schemes, and apply to any document classification problem to the extent: that the model represents faithfully that problem.",
        "One must choose a model that not only provides a mathematical description of the problem at hand, but one in which the desired calculations can be made.",
        "For example, in document classification, it would be nice to be able to calculate the probability that a document on subject i will be classified as on subject i.",
        "Further, it would be comforting to know that there is no better scheme than the one being used.",
        "Our models have these characteristics.",
        "They are simple, the calculations of probabilities of correct document classification are straightforward, and we have proved that there are no schemes using the same information that have better success rates.",
        "In an experiment the scheme was used to classify two types of documents, and was found to work very well indeed."
      ]
    },
    {
      "heading": "2 The Description of a Classification Scheme",
      "text": [
        "Suppose that we must classify a document into one of k types.",
        "These types are known.",
        "Here, k is any positive integer at least 2, and a typical value might he anywhere from 2 to 10.",
        "Denote these types 7', • .. , Tk.",
        "The set of words in the language is broken into rn disjoint subsets W1, W2, , Wi„.",
        "Now from a host of documents, or a large body of literature, on subject the frequencies pij of words in Wj are determined.",
        "So with subject T1 we have associated the vector of frequencies (Po , Piz, • • • Pim), and of course pit 1-Pi2+• = 1.",
        "Now, given a document on one of the possible k subjects, it is classified as follows.",
        "The document has n words in it, ni words from W1, n2 words from W2, • • , and tin, words from Wr„.",
        "Based on this information, a calculation is made to determine from which subject the document is most likely to have come, and is so classified.",
        "This calculation is key: there are many possible calculations on which a classification can be made, but some are better than others.",
        "We will prove that in this situation, there is a best one.",
        "We elaborate on a specific case which seems to hold promise.",
        "The idea is that the frequencies (pit, Po, • 'Pips) will be different enough from i to i to distinguish between types of documents.",
        "From a document of word length n, let a1 be the number of words in Wj.",
        "Thus the vector of word frequencies for that particular document is (ni/n, n2/n, nm/n).",
        "The word frequencies ;from a document of type i should resemble the frequencies (pit, po, ,pin,), and indeed, the classification scheme is to declare the document to be of type Ti if its frequencies \"most closely resemble\" the frequencies (pi], pi2, • • • )n/71).",
        "Intuitively, if two of the vectors are (pii , pia, .",
        ".",
        ".",
        ", pin\") very nearly equal, then it will he difficult to distinguish documents of those two types.",
        "Thus the success of classification depends critically on the vectors (pit, Pi2,•Vim) of frequencies.",
        "Equivalently, the sets Wj are critical, and must be chosen with great care.",
        "The particular situation we have in mind is this.",
        "Each of the types of documents is",
        "on a rather special topic, calling for a somewhat specialized vocabulary.",
        "The language is broken into k disjoint sets W1iW2, • • • , Wk+1 of words.",
        "For i < k, the words in W1 are \"specific\" to subject i, and Wk+1 consists of the remaining words in the language.",
        "Now from a host of documents, or a large body of literature, on the subject we determine the frequencies pij of words in Wi .",
        "But first, the word sets Wi are needed, and it is also from such bodies of text that they will be determined.",
        "Doing this in a manner that is optimal for our models is a difficult problem, but doing it in such a way that our models are very effective seems quite routine.",
        "So with subject Ti we have associated the vector of frequencies (p11, p12, ..,pi,3), the vector being of length one more than the number of types of documents.",
        "Since the words in W1 are specific to documents of type T1, these vectors of frequencies should be quite dissimilar and allow a sharp demarkation between document types.",
        "This particular scheme has the added advantage that m is small, being k+1, only one more than the number of document types.",
        "Further, our scheme will involve only a few hundred words, those that appear in W1, W2, • • • Wk, with the remainder appearing in Wk+1.",
        "This makes is possible to calculate the probabilities of correct classification of documents of each particular type.",
        "Such calculations are intractable for large m, even on fast machines.",
        "There are classification schemes being used with tn in the thousands, making an exact mathematical calculation of probabilities of correct classification next to impossible.",
        "But with k and m small, say no more than 10, such calculations are possible."
      ]
    },
    {
      "heading": "3 The Mathematical Model",
      "text": [
        "A mathematical description of the situation just described is this.",
        "We are given k multinomial populations, with the i-th having frequencies (P11, P12, • • • 'Pim).",
        "The i-th population may be envisioned to be an infinite set consisting of in types of elements, with the proportion of type j being P. We are given a random sample of size n from one of the populations, and are asked to determine from which of the populations it came.",
        "If the sample came from population i, then the probability that it has nj elements of type j is given by the formula",
        "This is an elementary probabilistic fact.",
        "If a sample to be classified has nj elements of type j, we simply make this calculation for each i, and judge the sample to be from population i if the largest of the results was for the i-th population.",
        "Thus, the sample is judged to be from the i-th population if the probability of getting the particular nj's that were gotten is the largest for that population.",
        "To determine which of (n !in' rt2 !",
        "• • • !)",
        "(P711 P722 \" gas\" ) is the largest, it is only necessary to determine which of the Kipn,2 • • •p7,7) is largest, and that is an easy machine calculation.",
        "All numbers are known beforehand except the nj 'a, which are counted from the sample.",
        "Before illustrating success rates with some calculations, some comments on our modeling of this document classification scheme are in order.",
        "The i-th multinomial population represents text of type This text consists of rn types of things, namely words from each of the M. The frequencies (p11, p12, ,pin,) give the proportion of words from the classes W1, W2, • • .,Wm in text of type A random sample of size n represents a document* of word length n. This last representation is arguable: a document of length n is not a random sample of n words from its type of text.",
        "It is a structured sequence of such words.",
        "The validity of the model proposed depends on a document reflecting the properties of a random sample in the frequencies of its words of each type.",
        "Intuitively, long documents will do that.",
        "Short ones may not.",
        "The success of any implementation will hinge on the frequencies (pit, p12, These frequencies must differ enough from document type to document type so that documents can be distinguished on the basis of them."
      ]
    },
    {
      "heading": "4 Some Calculations",
      "text": [
        "We now illustrate with some calculations for a simple case: there are two kinds of documents, T1 and 72, and three kinds of words.",
        "We have in mind here that W1 consists of words specific to documents of type Tt, W2 specific to T2, and that W3 consists of the remaining words in the language.",
        "So we have the frequencies (P11, P12, P13) and (P21, P22, P23).",
        "Of course pia 1 - P11 -pie.",
        "Now we are given a document that we know is either of type T1 or of type T2, and we must discern which type it is on the basis of its word frequencies.",
        "Suppose it has nj words of type j, j 1, 2, 3.",
        "We calculate the numbers",
        "for 1, 2, and declare the document to be of type Ti if is the larger of the two.",
        "Now what is the probability of success?",
        "Here is the calculation.",
        "If a document of size n is drawn from a trinomial population with parameters (p11, P12, p13), the probability of getting n1 words of type 1, n2 words of type 2, and n3 words of type 3 is (nYni!n2!rt3!)(41p1pn).",
        "Thus to calculate the probability of classifying successfully a document of type T1 as being of that type, we must add these expressions over all those triples (ni , 712, 713) for which t1 is larger than t2.",
        "This is a",
        "fairly easy computation, and we have carried it out for a host of different p's and n's.",
        "'fable 1 contains results of some of these calculations.",
        "Table 1 gives the probability of classifying a document of type T1 as of type '11, and of classifying a, document of type 12 as of type T2.",
        "These probabilities are labeled Prob(1) and Prob(2), respectively.",
        "Of course, here we get for free the probability that a document of type 7.1, will be classified as of type 72, namely 1 Prob(1).",
        "Similarly, 1,-- Prob(2) is the probability that a document of type 12 will be classified as of type 71.",
        "The Ai are the frequencies of words from Wj for documents of type 7;, and n is the number of words in the document.",
        "There are several things worth noting in Table 1.",
        "The frequencies used in the table were chosen to illustrate the behavior of the scheme, and not necessarily to reflect document classification reality.",
        "However, consider the first set of frequencies (.08, .04, .88) and (.03, .06, .91).",
        "This represents a circumstance where documents of type 11 have eight percent of their words specific to that subject, and four percent specific to the other subject.",
        "Documents of type 22 have six percent of their words specific to its subject, and three percent specific to the other subject.",
        "These percentages seem to be easily attainable.",
        "Our scheme correctly classifies a document of length 200 and of type T1 95.1 percent of the time, and a document of length 400 99.1 percent of the time.",
        "The last set of frequencies, (.08, .04, .88) and (.07, .04, .89) are almost alike, and as the table shows, do not serve to classify documents correctly with high probability.",
        "In general, the probabilities of success arc remarkably high, even for relatively small n, and in the experiment reported on in Section 6, it was easy to find word sets with satisfatory frequencies.",
        "It is a fact that the probability of success can be made as close to 1 as desired by taking n large enough, assuming that (Thi, Pie, Pia) is not identical to (p21, P22, p23).",
        "however, since for reasonable frequencies, the probabilities of success are high for n just a few hundred, this suggests that long documents would not have to be completely tabulated in order to be classified correctly with high probability.",
        "One could just use a random sample of appropriate size from the document.",
        "The following table give some success rates for the case where there are three kinds of documents and four word classes.",
        "The rates are surprisingly high."
      ]
    },
    {
      "heading": "5 Theoretical Results",
      "text": [
        "In this section, we prove our optimality result.",
        "But first we roust give it a precise mathematical formulation.",
        "To say that there is no better classification scheme than some given one, we must know not only what \"better\" means, we must know precisely what a classification scheme is.",
        "The setup is as in Section 3.",
        "We have k naultinomial populations with frequencies (pit, p121 Pim), i -= 1, 2, k. We are given a random sample of size n from one of the populations and are forced to assert from which one it came.",
        "The information at our disposal, besides the set of frequencies (ph ,220, Pi„,), is, for each j, the number ni of elements of type j in the sample.",
        "So tire information ifrom the sample is the tuple (ni , n2, , n„,).",
        "Our scheme for specifying from which population it came is to say that it came from population i if (nVnikt2!",
        "• • • nm NKIIP:'22 • • • 1):,Z) is maximum over the i's.",
        "This then, determines which (n1, n2, ..., nm) results in which classification.",
        "Our scheme partitions the sample space, that is, the set of all the tupls (ni, n2, • • • , rim), into k pieces,",
        "the i-th piece being those tuples (ni, n2, , for which (nlindn2!",
        "• • nm!",
        ")(9711pn2 • •p7,;;') is maximum.",
        "For a given sample (or document) size n, this leads to the definition of a scheme as any partition (A1, A2, .",
        "Ak} of the set of tuples (ni, n2, ..., for which Ei ni n into k pieces.",
        "The procedure then is to classify a sample as coming from the i-th population if the tuple (rtl, n2, ..., nm) gotten from the sample is in Ai.",
        "It doesn't matter how this partition is arrived at.",
        "Our method is via the probabilities gi(ni , n2, .",
        "• • , nm) = (nlini!n2!",
        "• • • nm!",
        ")(1/711K22 ' • • F7,•;; )• There are many ways we could define optimality.",
        "A definition that has particular charm is to define a scheme to be optimal if no other scheme has an higher overall probability of correct classification.",
        "But in this setup, we have no way of knowing the overall rate of correct classification because we do not know what proportion of samples come from what populations.",
        "So we cannot use that definition.",
        "An alternate definition that makes sense is to define a scheme to be optimal if no other scheme has, for each population, a higher probability of correct classification of samples from that population.",
        "But our scheme is optimal in a much stronger sense.",
        "We define a scheme Ai, A2, • .., Ak to be optimal if for any other scheme B1, .1321 • • • 1Bk,",
        "Proofs of the theorems in this note will be given elsewhere.",
        "Theorem 1 Let T1,T2, ,Tk be multinomial populations with the i – th population having frequencies (P11, po, , Aria).",
        "For a random sample of size n from one of these populations, let ni be the number of elements of type j.",
        "Let qi(ni n2, ••• , nm) = (nYni k12!",
        "• • • nm!",
        ")(KilK22 • • • P:`,,•7 )• Then the partition of the sample, space ((ni, n2, • • nk) ni > 0, Ei ni = n} given by",
        "is an optimal scheme for determining from which of the populations a sample of size n came.",
        "An interesting feature of Table 1 is that for all frequencies Prob(1) + Prob(2) is greater for sample size 100 than for sample size 50.",
        "This supports our intuition that larger sample sizes should yield better results.",
        "This is indeed a fact.",
        "Theorem 2 The following inequality holds, with equality only in the trivial case that Pik = pik for all i, j, and k,",
        "where in+i means to sum over those tuples (ni, n2, .",
        ".",
        ".",
        ", nr„) whose sum is n+ 1, and En means to sum over those tuples (ni, n2, .",
        ".",
        ", rt,,n) whose sum is n."
      ]
    },
    {
      "heading": "6 Practical Results",
      "text": [
        "Our theoretical results assure us that documents can be classified correctly if we have appropriate sets of words.",
        "We have algorithms which compute the probability of classifying document types correctly given the document size and the probability of some specialized sets of words appearing in the two document types.",
        "Tables 1 and 2 show some sample outputs from that program.",
        "Intuitively, we need sets of words which appear much more often in one text type than the other, but the words do not need to appear in either text type very often.",
        "Below we describe an experiment with two document collections that indicates that appropriate word sets can be chosen easily.",
        "Moreover, in our sample experiment, the word sets were chosen automatically and the classification scheme worked perfectly, as predicted by our theoretical results.",
        "Two appropriate collections of text were available at the Computing Research Laboratory.",
        "The first was made up of 1000 texts on business (joint ventures) Lfrom the DARPA TIPSTER project and the second collection consisted of 1100 texts from the Message Understanding Conference (MUC) [Sundheim, 1991] describing terrorist incidents in South America.",
        "The business texts were all newspaper articles, whereas the MUC texts were transmitted by teletype and came from various sources, such as excerpts from newspaper articles, radio reports, or tape recorded messages.",
        "The collections were prepared by human analysts who judged the relevance of the documents in the collections.",
        "Each collection contained about half a million words.",
        "We removed any dates, annotations, or header information from the documents which uniquely identified it as being of one text type or another.",
        "We divided each collection of texts in half to form two training sets and two test sets of documents, yielding four collections of about a quarter of a million words each.",
        "We treated each of the training sets as one huge text and obtained frequency counts for each of the words in the text.",
        "Words were not stemmed and no stop list was used.",
        "The result was two lists of words with their corresponding frequencies, one for the TIPSTER training set and one for the MUC training set.",
        "Our goal at this point was to choose two sets of words, which we call TIP-SET and MUC-SET, that could be used to distinguish the documents.",
        "We knew from the results of TABLE 1 that if we could identify one set of words (TIP-SET) that appeared in the TIP",
        "STER documents with probability .1 and in the MUC documents with low probability (say .03 or less) and another set (MUC-SET) that appeared with probability .1 in the MUG documents and a low probability (say .03 or less) in the TIPSTER documents, that we could achieve perfect or nearly perfect classification.",
        "We used a simple heuristic in our initial tests: choose the TIP-SET by choosing words which were among the 300 most frequent in the TIPSTER training set and not in the 500 most frequent in the MUC training set.",
        "We intended to vary the 300 and 500 to see if we could choose good sets.",
        "However, this algorithm yielded a set of words that appeared with probability .13 in the TIPSTER training set and with probability .01 in the MUG training set.",
        "Note that even though no stop list was used when the frequency counts were taken, this procedure effectively creates a stop list automatically.",
        "The same algorithm was used to create the MUG-SET: choose words from among the 300 most, frequent in the MUG training set if they did not appear in the 500 most frequent in the TIPSTER training set.",
        "Our theoretical results implied that we could classify each document type correctly 99.99% or the time if we had documents with at least 200 words.",
        "Our average document size in the two collections was 500 words.",
        "We then tested the classification scheme on the remaining half (those not used for training) of each document set.",
        "Only one document was classified differently from the human classification.",
        "When we read the text in question, it was our opinion that the original document classification by a human was incorrect.",
        "If we change the classification of this text, then our document classification scheme worked perfectly on 700 documents.",
        "It should be noted that the two document collections that were available to us were on very different subject matter, so the choice of the word sets was extremely easy.",
        "We expect that differentiating texts which are on related subject areas will be much more difficult and we are developing refinements for this task."
      ]
    },
    {
      "heading": "7 References",
      "text": []
    }
  ]
}
