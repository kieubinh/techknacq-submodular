{
  "info": {
    "authors": [
      "Bernard Merialdo"
    ],
    "book": "Computational Linguistics",
    "id": "acl-J94-2001",
    "title": "Tagging English Text With a Probabilistic Model",
    "url": "https://aclweb.org/anthology/J94-2001",
    "year": 1994
  },
  "references": [
    "acl-A92-1018",
    "acl-A92-1019",
    "acl-E85-1023",
    "acl-E85-1024",
    "acl-H90-1055",
    "acl-J88-1003",
    "acl-P88-1026",
    "acl-P90-1031"
  ],
  "sections": [
    {
      "text": [
        "In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence.",
        "The main novelty of these experiments is the use of untagged text in the training of the model.",
        "We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided.",
        "Two approaches in particular are compared and combined:",
        "• using text that has been tagged by hand and computing relative frequency counts, • using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.",
        "Experiments show that the best training is obtained by using as much tagged text as possible.",
        "They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.",
        "In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence.",
        "Two main approaches have generally been considered:",
        "• rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al.",
        "1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al.",
        "1992).",
        "More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989).",
        "Through these different approaches, some common points have emerged:",
        "• For any given word, only a few tags are possible, a list of which can be found either in the dictionary or through a morphological analysis of the word.",
        "• When a word has several possible tags, the correct tag can generally be chosen from the local context, using contextual rules that define the valid sequences of tags.",
        "These rules may be given priorities so that a selection can be made even when several rules apply.",
        "These kinds of considerations fit nicely inside a probabilistic formulation of the problem (Beale 1985; Garside and Leech 1985), which offers the following advantages: • a sound theoretical framework is provided • the approximations are clear • the probabilities provide a straightforward way to disambiguate • the probabilities can be estimated automatically from data.",
        "In this paper we present a particular probabilistic model, the triclass model, and results from experiments involving different ways to estimate its parameters, with the intention of maximizing the ability of the model to tag text accurately.",
        "In particular, we are interested in a way to make the best use of untagged text in the training of the model."
      ]
    },
    {
      "heading": "2. The Problem of Tagging",
      "text": [
        "We suppose that the user has defined a set of tags (attached to words).",
        "Consider a sentence W = wiw2 wn, and a sequence of tags T =-- tlt2 tn, of the same length.",
        "We call the pair (W, T) an alignment.",
        "We say that word w, has been assigned the tag t, in this alignment.",
        "We assume that the tags have some linguistic meaning for the user, so that among all possible alignments for a sentence there is a single one that is correct from a grammatical point of view.",
        "A tagging procedure is a procedure 0 that selects a sequence of tags (and so defines an alignment) for each sentence.",
        "0:W--+T=0(W) There are (at least) two measures for the quality of a tagging procedure:",
        "• at sentence level perfs(0) percentage of sentences correctly tagged • at word level perfw(0) = percentage of words correctly tagged",
        "In practice, performance at sentence level is generally lower than performance at word level, since all the words have to be tagged correctly for the sentence to be tagged correctly.",
        "The standard measure used in the literature is performance at word level, and this is the one considered here."
      ]
    },
    {
      "heading": "3. Probabilistic Formulation",
      "text": [
        "In the probabilistic formulation of the tagging problem we assume that the alignments are generated by a probabilistic model according to a probability distribution:",
        "In this case, depending on the criterion that we choose for evaluation, the optimal tagging procedure is as follows:",
        "• for evaluation at sentence level, choose the most probable sequence of tags for the sentence",
        "We call this procedure Viterbi tagging.",
        "It is achieved using a dynamic programming scheme.",
        "• for evaluation at word level, choose the most probable tag for each word in the sentence",
        "where 0(W), is the tag assigned to word w, by the tagging procedure in the context of the sentence W. We call this procedure Maximum Likelihood (ML) tagging.",
        "It is interesting to note that the most commonly used method is Viterbi tagging (see DeRose 1988; Church 1989) although it is not the optimal method for evaluation at word level.",
        "The reasons for this preference are presumably that:",
        "• Viterbi tagging is simpler to implement than ML tagging and requires less computation (although they both have the same asymptotic complexity) • Viterbi tagging provides the best interpretation for the sentence, which is linguistically appealing • ML tagging may produce sequences of tags that are linguistically impossible (because the choice of a tag depends on all contexts taken together).",
        "However, in our experiments, we will show that Viterbi and ML tagging result in very similar performance.",
        "Of course, the real tags have not been generated by a probabilistic model and, even if they had been, we would not be able to determine this model exactly because of practical limitations.",
        "Therefore the models that we construct will only be approximations of an ideal model that does not exist.",
        "It so happens that despite these assumptions and approximations, these models are still able to perform reasonably well."
      ]
    },
    {
      "heading": "4. The Triclass Model",
      "text": [
        "We have the mathematical expression:",
        "The triclass (or tri-POS [Derouault 1986], or tri-Ggram [Codogno et al.",
        "1987], or HK) model is based on the following approximations:",
        "• The probability of the tag given the past depends only on the last two tags p(ti/witi • • • Wi-lti-1) = h(ti/ti-2ti-1) • The probability of the word given the past depends only on its tag",
        "(the name HK model comes from the notation chosen for these probabilities).",
        "In order to define the model completely we have to specify the values of all h and k probabilities.",
        "If Nw is the size of the vocabulary and NT the number of different tags, then there are:",
        "• NT.NT.NT values for the h probabilities • Nw.NT values for the k probabilities.",
        "Also, since all probability distributions have to sum to one, there are: • NT.NT equations to constrain the values for the h probabilities • NT equations to constrain the values for the k probabilities.",
        "The total number of free parameters is then:",
        "Note that this number grows only linearly with respect to the size of the vocabulary, which makes this model attractive for vocabularies of a very large size.",
        "The triclass model by itself allows any word to have any tag.",
        "However, if we have a dictionary that specifies the list of possible tags for each word, we can use this information to constrain the model: if t is not a valid tag for the word w, then we are sure that",
        "There are thus at most as many nonzero values for the k probabilities as there are possible pairs (word, tag) allowed in the dictionary."
      ]
    },
    {
      "heading": "5. Training the Triclass Model",
      "text": [
        "We consider two different types of training:",
        "• Relative Frequency (RF) training • Maximum Likelihood (ML) training which is done via the Forward-Backward (FB) algorithm."
      ]
    },
    {
      "heading": "5.1 Relative Frequency Training",
      "text": [
        "If we have some tagged text available we can compute the number of times N(w, t) a given word w appears with the tag t, and the number of times N(ti, t2, t3) the sequence (t1, t2, t3) appears in this text.",
        "We can then estimate the probabilities h and k by computing the relative frequencies of the corresponding events on this data:",
        "These estimates assign a probability of zero to any sequence of tags that did not occur in the training data.",
        "But such sequences may occur if we consider other texts.",
        "A probability of zero for a sequence creates problems because any alignment that contains this sequence will get a probability of zero.",
        "Therefore, it may happen that, for some sequences of words, all alignments get a probability of zero and the model becomes useless for such sentences.",
        "To avoid this, we interpolate these distributions with uniform distributions, i.e. :onsider the interpolated model defined by:",
        "where",
        "number of words that have the tag t The interpolation coefficient A is computed using the deleted interpolation algorithm (Jelinek and Mercer 1980) (it would also be possible to use two coefficients, one for the interpolation on h, one for the interpolation on k).",
        "The value of this coefficient is expected to increase if we increase the size of the training text, since the relative frequencies should be more reliable.",
        "This interpolation procedure is also called \"smoothing.\" Smoothing is performed as follows:",
        "• Some quantity of tagged text from the training data is not used in the computation of the relative frequencies.",
        "It is called the \"held-out\" data.",
        "• The coefficient A is chosen to maximize the probability of emission of the held-out data by the interpolated model.",
        "• This maximization can be performed by the standard Forward-Backward",
        "(FB) or Baum – Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer 1983; Poritz 1988), by considering A and 1 – A as the transition probabilities of a Markov model.",
        "It can be noted that more complicated interpolation schemes are possible.",
        "For example, different coefficients can be used depending on the count of (t1, t2) , with the intuition that relative frequencies can be trusted more when this count is high.",
        "Another possibilitity is to interpolate also with models of different orders, such as hrf (t3/t2) or kJ- (t3).",
        "Smoothing can also be achieved with procedures other than interpolation.",
        "One example is the \"backing-off\" strategy proposed by Katz (1987)."
      ]
    },
    {
      "heading": "5.2 Maximum Likelihood Training",
      "text": [
        "Using a triclass model M it is possible to compute the probability of any sequence of words W according to this model:",
        "where the sum is taken over all possible alignments.",
        "The Maximum Likelihood (ML) training finds the model M that maximizes the probability of the training text:",
        "where the product is taken over all the sentences W in the training text.",
        "This is the problem of training a hidden Markov model (it is hidden because the sequence of tags is hidden).",
        "A well-known solution to this problem is the Forward-Backward (FB) or Baum – Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer 1983), which iteratively constructs a sequence of models that improve the probability of the training data.",
        "The advantage of this approach is that it does not require any tagging of the text, but makes the assumption that the correct model is the one in which tags are used to best predict the word sequence."
      ]
    },
    {
      "heading": "6. Tagging Algorithms",
      "text": [
        "The Viterbi algorithm is easily implemented using a dynamic programming scheme (Bellman 1957).",
        "The Maximum Likelihood algorithm appears more complex at first glance, because it involves computing the sum of the probabilities of a large number of alignments.",
        "However, in the case of a hidden Markov model, these computations can be arranged in a way similar to the one used during the FB algorithm, so that the overall amount of computation needed becomes linear in the length of the sentence (Baum and Eagon 1967)."
      ]
    },
    {
      "heading": "7. Experiments",
      "text": [
        "The main objective of this paper is to compare RF and ML training.",
        "This is done in Section 7.2.",
        "We also take advantage of the environment that we have set up to perform other experiments, described in Section 7.3, that have some theoretical interest, but did",
        "not bring any improvement in practice.",
        "One concerns the difference between Viterbi and ML tagging, and the other concerns the use of constraints during training.",
        "We shall begin by describing the textual data that we are using, before presenting the different tagging experiments using these various training and tagging methods."
      ]
    },
    {
      "heading": "7.1 Text Data",
      "text": [
        "We use the \"treebank\" data described in Beale (1988).",
        "It contains 42,186 sentences (about one million words) from the Associated Press.",
        "These sentences have been tagged manually at the Unit for Computer Research on the English Language (University of Lancaster, U.K.), in collaboration with IBM U.K. (Winchester) and the IBM Speech Recognition group in Yorktown Heights (USA).",
        "In fact, these sentences are not only tagged but also parsed.",
        "However, we do not use the information contained in the parse.",
        "In the treebank 159 different tags are used.",
        "These tags were projected on a smaller system of 76 tags designed by Evelyne Tzoukermann and Peter Brown (see Appendix).",
        "The results quoted in this paper all refer to this smaller system.",
        "We built a dictionary that indicates the list of possible tags for each word, by taking all the words that occur in this text and, for each word, all the tags that are assigned to it somewhere in the text.",
        "In some sense, this is an optimal dictionary for this data, since a word will not have all its possible tags (in the language), but only the tags that it actually had within the text.",
        "We separated this data into two parts:",
        "• a set of 40,186 tagged sentences, the training data, which is used to build the models • a set of 2,000 tagged sentences (45,583 words), the test data, which is used to test the quality of the models."
      ]
    },
    {
      "heading": "7.2 Basic Experiments",
      "text": [
        "RF training, Viterbi tagging In this experiment, we extracted N tagged sentences from the training data.",
        "We then computed the relative frequencies on these sentences and built a \"smoothed\" model using the procedure previously described.",
        "This model was then used to tag the 2,000 test sentences.",
        "We experimented with different values of N, for each of which we indicate the value of the interpolation coefficient and the number and percentage of correctly tagged words.",
        "Results are indicated in Table 1.",
        "As expected, as the size of the training increases, the interpolation coefficient increases and the quality of the tagging improves.",
        "When N = 0, the model is made up of uniform distributions.",
        "In this case, all alignments for a sentence are equally probable, so that the choice of the correct tag is just a choice at random.",
        "However, the percentage of correct tags is relatively high (more than three out of four) because:",
        "• almost half of the words of the text have a single possible tag, so that no mistake can be made on these words • about a quarter of the words of the text have only two possible tags so that, on the average, a random choice is correct every other time.",
        "Note that this behavior is obviously very dependent on the system of tags that is used.",
        "It can be noted that reasonable results are obtained quite rapidly.",
        "Using 2,000 tagged sentences (less than 50,000 words), the tagging error rate is already less than 5%.",
        "Using 10 times as much data (20,000 tagged sentences) provides an improvement of only 1.5%."
      ]
    },
    {
      "heading": "ML training, Viterbi tagging",
      "text": [
        "In ML training we take all the training data available (40,186 sentences) but we only use the word sequences, not the associated tags (except to compute the initial model, as will be described later).",
        "This is possible since the FB algorithm is able to train the model using the word sequence only.",
        "In the first experiment we took the model made up of uniform distributions as the initial one.",
        "The only constraints in this model came from the values k(w It) that were set to zero when the tag t was not possible for the word w (as found in the dictionary).",
        "We then ran the FB algorithm and evaluated the quality of the tagging.",
        "The results are shown in Figure 1.",
        "(Perplexity is a measure of the average branching factor for probabilistic models.)",
        "This figure shows that ML training both improves the perplexity of the model and reduces the tagging error rate.",
        "However, this error rate remains at a relatively high level – higher than that obtained with a RF training on 100 tagged sentences.",
        "Having shown that ML training is able to improve the uniform model, we then wanted to know if it was also able to improve more accurate models.",
        "We therefore took as the initial model each of the models obtained previously by RF training and, for each one, performed ML training using all of the training word sequences.",
        "The results are shown graphically in Figure 2 and numerically in Table 2.",
        "These results show that, when we use few tagged data, the model obtained by relative frequency is not very good and Maximum Likelihood training is able to improve it.",
        "However, as the amount of tagged data increases, the models obtained by Relative Frequency are more accurate and Maximum Likelihood training improves on the initial iterations only, but after deteriorates.",
        "If we use more than 5,000 tagged sentences, even the first iteration of ML training degrades the tagging.",
        "(This number is of course dependent on both the particular system of tags and the kind of text used in this experiment).",
        "These results call for some comments.",
        "ML training is a theoretically sound procedure, and one that is routinely and successfully used in speech recognition to estimate the parameters of hidden Markov models that describe the relations between sequences of phonemes and the speech signal.",
        "Although ML training is guaranteed to improve perplexity, perplexity is not necessarily related to tagging accuracy, and it is possible to improve one while degrading the other.",
        "Also, in the case of tagging,",
        "ML training from various initial points (top line corresponds to N=100, bottom line to N=a11).",
        "the relations between words and tags are much more precise than the relations between phonemes and speech signals (where the correct correspondence is harder to define precisely).",
        "Some characteristics of ML training, such as the effect of smoothing probabilities, are probably more suited to speech than to tagging."
      ]
    },
    {
      "heading": "7.3 Extra Experiments Viterbi versus ML tagging",
      "text": [
        "For this experiment we considered the initial model built by RF training over the whole training data and all the successive models created by the iterations of ML training.",
        "For each of these models we performed Viterbi tagging and ML tagging on the same test data, then evaluated and compared the number of tagging errors produced by these two methods.",
        "The results are shown in Table 3.",
        "The models obtained at different iterations are related, so one should not draw strong conclusions about the definite superiority of one tagging procedure.",
        "However, the difference in error rate is very small, and shows that the choice of the tagging procedure is not as critical as the kind of training material.",
        "Constrained ML training Following a suggestion made by F. Jelinek, we investigated the effect of constraining the ML training by imposing constraints on the probabilities.",
        "This idea comes from the observation that the amount of training data needed to properly estimate the model increases with the number of free parameters of the model.",
        "In the case of little training data, adding reasonable constraints on the shape of the models that are looked for reduces the number of free parameters and should improve the quality of the estimates.",
        "We tried two different constraints:",
        "• The first one keeps p(t/w) fixed if w is a frequent word, in our case one of the 1,000 most frequent words.",
        "We call it tw-constraint.",
        "The rationale is that if w is frequent, the relative frequency provides a good estimate for p(t/w) and the training should not change it.",
        "• The second one keeps the marginal distribution p(t) constant and is based on a similar reasoning.",
        "We call it t-constraint."
      ]
    },
    {
      "heading": "tw-constraint",
      "text": [
        "The tw-constrained ML training is similar to the standard ML training, except that the probabilities p(t/w) are not changed at the end of an iteration.",
        "The results in Table 4 show the number of tagging errors when the model is trained with the standard or tw-constrained ML training.",
        "They show that the tw-constrained ML training still degrades the RF training, but not as quickly as the standard ML.",
        "We",
        "have not tested what happens when smaller training data is used to build the initial model.",
        "t-constraint This constraint is more difficult to implement than the previous one because the probabilities p(t) are not the parameters of the model, but a combination of these parameters.",
        "With the help of R. Polyak we have designed an iterative procedure that allows the likelihood to be improved while preserving the values of p(t).",
        "We do not have sufficient space to describe this procedure here.",
        "Because of its greater computational complexity, we have only applied it to a biclass model, i.e. a model where",
        "As in the previous experiment, the results in Table 5 show the number of tagging errors when the model is trained with the standard or t-constrained ML training.",
        "They show that the t-constrained ML training still degrades the RF training, but not as quickly as the standard ML.",
        "Again, we have not tested what happens when smaller training data is used to build the initial model."
      ]
    },
    {
      "heading": "8. Conclusion",
      "text": [
        "The results presented in this paper show that estimating the parameters of the model by counting relative frequencies over a very large amount of hand-tagged text lead to the best tagging accuracy.",
        "Maximum Likelihood training is guaranteed to improve perplexity, but will not necessarily improve tagging accuracy.",
        "In our experiments, ML training degrades the performance unless the initial model is already very bad.",
        "The preceding results suggest that the optimal strategy to build the best possible model for tagging is the following:",
        "• get as much tagged (by hand) text as you can afford",
        "• compute the relative frequencies from this data to build an initial model Mo • get as much untagged text as you can afford • starting from Mo, perform the Forward-Backward iterations.",
        "At each iteration, evaluate the tagging quality of the new model M, on some held-out tagged text.",
        "Stop either when you have reached a preset number of iterations or the model M, performs worse than whichever occurs first."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "I would like to thank Peter Brown, Fred Jelinek, John Lafferty, Robert Mercer, Salim Roukos, and other members of the Continuous Speech Recognition group for the fruitful discussions I had with them throughout this work.",
        "I also want to thank one of the referees for his judicious comments."
      ]
    },
    {
      "heading": "Appendix A: List of Tags Used",
      "text": [
        "$* possessive marker ('s, ') APP$* possessive adjectives (my, your, our) AT* article (the, a, no)",
        "ICS* preposition that can also be used as a conjunction (since, after) IF* the preposition for I0* the preposition of J* adjective (small, pretty) J*R comparative adjective (smaller, prettier) J*T superlative adjective (prettiest, nicest) LE* leading coordinator (both, either, neither) M* cardinal number MD* ordinal number (first, second) N* noun without number (english) N*1 singular noun (cat, man) N*2 plural noun (cats, men) NPR* proper noun (paris, fred) NR* noun/adverb of direction (south, west) or time (now, tomorrow, tuesday) P* non-nominative pronoun (none, anyone, oneself) P*() who, whom, whoever, whomever PNX1* personal pronoun reflexive (himself)",
        "VDN* past participial form of do (did) VDPAST* past form of do (did) VDO* do as a conjugated form and infinitive VDOZ* does as a conjugated form VHG* having VHN* past participial form of have (had) VHPAST* past form of have (had)",
        "Bernard Merialdo Tagging English Text with a Probabilistic Model VHO* have as a conjugated form VHOZ* has as a conjugated form VM* modals (can, would, ought, used) VVG* non-aux verb in -ing VVN* past participial form of non-aux verb VVPAST* preterit of non-aux verb VVO* non-third-person-singular form of non-aux verb and infinitive VVOZ* third-person-singular form of non-aux verb XX* not"
      ]
    }
  ]
}
