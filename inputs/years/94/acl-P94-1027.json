{
  "info": {
    "authors": [
      "Christian Jacquemin"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P94-1027",
    "title": "Optimizing the Computational Lexicalization of Large Grammars",
    "url": "https://aclweb.org/anthology/P94-1027",
    "year": 1994
  },
  "references": [
    "acl-C88-2121",
    "acl-E89-1001",
    "acl-P93-1017"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The computational lexicalization of a grammar is the optimization of the links between lexicalized rules and lexical items in order to improve the quality of the bottom-up filtering during parsing.",
        "This problem is NP complete and untractable on large grammars.",
        "An approximation algorithm is presented.",
        "The quality of the suboptimal solution is evaluated on real-world grammars as well as on randomly generated ones."
      ]
    },
    {
      "heading": "Introduction",
      "text": [
        "Lexicalized grammar formalisms and more specifically Lexicalized Tree Adjoining Grammars (LTAGs) give a lexical account of phenomena which cannot be considered as purely syntactic (Schabes et al., 1990).",
        "A formalism is said to be lexicalized if it is composed of structures or rules associated with each lexical item and operations to derive new structures from these elementary ones.",
        "The choice of the lexical anchor of a rule is supposed to be determined on purely linguistic grounds.",
        "This is the linguistic side of lexicalization which links to each lexical head a set of minimal and complete structures.",
        "But lexicalization also has a computational aspect because parsing algorithms for lexicalized grammars can take advantage of lexical links through a two-step strategy (Schabes and Joshi, 1990).",
        "The first step is the selection of the set of rules or elementary structures associated with the lexical items in the input sentence'.",
        "In the second step, the parser uses the rules filtered by the first step.",
        "The two kinds of anchors corresponding to these two aspects of lexicalization can be considered separately :",
        "• The linguistic anchors are used to access the grammar, update the data, gather together items with similar structures, organize the grammar into a hierarchy... • The computational anchors are used to select the relevant rules during the first step of parsing and to improve computational and conceptual tractability of the parsing algorithm.",
        "Unlike linguistic lexicalization, computational anchoring concerns any of the lexical items found in a rule and is only motivated by the quality of the induced filtering.",
        "For example, the systematic linguistic anchoring of the rules describing \"Nmetal alloy\" to their head noun \"alloy\" should be avoided and replaced by a more distributed lexicalization.",
        "Then, only a few rules \"Nmetat alloy\" will be activated when encountering the word \"alloy\" in the input.",
        "In this paper, we investigate the problem of the optimization of computational lexicalization.",
        "We study how to choose the computational anchors of a lexicalized grammar so that the distribution of the rules on to the lexical items is the most uniform possible",
        "with respect to rule weights.",
        "Although introduced with reference to LTAGs, this optimization concerns any portion of a grammar where rules include one or more potential lexical anchors such as Head Driven Phrase Structure Grammar (Pollard and Sag, 1987) or Lexicalized Context-Free Grammar (Schabes and Waters, 1993).",
        "This algorithm is currently used to good effect in FASTR a unification-based parser for terminology extraction from large corpora (Jacquemin, 1994).",
        "In this framework, terms are represented by rules in a lexicalized constraint-based formalism.",
        "Due to the large size of the grammar, the quality of the lexicalization is a determining factor for the computational tractability of the application.",
        "FASTR is applied to automatic indexing on industrial data and lays a strong emphasis on the handling of term variations (Jacquemin and Royaute, 1994).",
        "The remainder of this paper is organized as follows.",
        "In the following part, we prove that the problem of the Lexicalization of a Grammar is NP-complete and hence that there is no better algorithm known to solve it than an exponential exhaustive search.",
        "As this solution is untractable on large data, an approximation algorithm is presented which has a computational-time complexity proportional to the cubic size of the grammar.",
        "In the last part, an evaluation of this algorithm on real-world grammars of 6,622 and 71,623 rules as well as on randomly generated ones confirms its computational tractability and the quality of the lexicalization."
      ]
    },
    {
      "heading": "The Problem of the Lexicalization of a Grammar",
      "text": [
        "Given a lexicalized grammar, this part describes the problem of the optimization of the computational lexicalization.",
        "The solution to this problem is a lexicalization function (henceforth a lexicalization) which associates to each grammar rule one of the lexical items it includes (its lexical anchor).",
        "A lexicalization is optimized to our sense if it induces an optimal preprocessing of the grammar.",
        "Preprocessing is intended to activate the rules whose lexical anchors are in the input and make all the possible filtering of these rules before the proper parsing algorithm.",
        "Mainly, preprocessing discards the rules selected through lexicalization including at least one lexical item which is not found in the input.",
        "The first step of the optimization of the lexicalization is to assign a weight to each rule.",
        "The weight is assumed to represent the cost of the corresponding rule during the preprocessing.",
        "For a given lexicalization, the weight of a lexical item is the sum of the weights of the rules linked to it.",
        "The weights are chosen so that a uniform distribution of the rules on to the lexical items ensures an optimal preprocessing.",
        "Thus, the problem is to find an anchoring which achieves such a uniform distribution.",
        "The weights depend on the physical constraints of the system.",
        "For example, the weight is the number of nodes if the memory size is the critical point.",
        "In this case, a uniform distribution ensures that the rules linked to an item will not require more than a given memory space.",
        "The weight is the number of terminal or non-terminal nodes if the computational cost has to be minimized.",
        "Experimental measures can be performed on a test set of rules in order to determine the most accurate weight assignment.",
        "Two simplifying assumptions are made :",
        "• The weight of a rule does not depend on the lexical item to which it is anchored.",
        "• The weight of a rule does not depend on the other rules simultaneously activated.",
        "The second assumption is essential for settling a tractable problem.",
        "The first assumption can be avoided at the cost of a more complex representation.",
        "In this case, instead of having a unique weight, a rule must have as many weights as potential lexical anchors.",
        "Apart from this modification, the algorithm that will be presented in the next part remains much the same than in the case of a single weight.",
        "If the first assumption is removed, data about the frequency of the items in corpora can be accounted for.",
        "Assigning smaller weights to rules when they are anchored to rare items will",
        "make the algorithm favor the anchoring to these items.",
        "Thus, due to their rareness, the corresponding rules will be rarely selected.",
        "Illustration Terms, compounds and more generally idioms require a lexicalized syntactic representation such as LTAGs to account for the syntax of these lexical entries (Abellle and Schabes, 1989).",
        "The grammars chosen to illustrate the problem of the optimization of the lexicalization and to evaluate the algorithm consist of idiom rules such as 9: 9 = {from time to time, high time, high grade, high grade steel} Each rule is represented by a pair (wi, Ai) where wi is the weight and Ai the set of potential anchors.",
        "If we choose the total number of words in an idiom as its weight and its non-empty words as its potential anchors, 9 is represented by the following grammar",
        "We call vocabulary, the union V of all the sets of potential anchors Ai.",
        "Here, V = {grade, high, steel, time}.",
        "A lexicalization is a function A associating a lexical anchor to each rule.",
        "Given a threshold 0, the membership problem called the Lexicalization of a Grammar (LG) is to find a lexicalization so that the weight of any lexical item in V is less than or equal to 0.",
        "If 0 4 in the preceding example, LG has a solution A:",
        "If 0 5.",
        "3, LG has no solution.",
        "Definition of the LG Problem",
        "total function anchoring the rules so that (V(w, A)E G) A((w, , A))E A and (VvE V) w 9) A((w, A)).",
        "v The associated optimization problem is to determine the lowest value 0opt of the threshold so that there exists a solution (V, G, 0opt, A) to LG.",
        "The solution of the optimization problem for the preceding example is 000= 4.",
        "It is evident that checking whether a given lexicalization is indeed a solution to LG can be done in polynomial time.",
        "The relation R defined by (2) is polynomially decidable :",
        "(2) R(V, G, 0, A) = [if A: V-+G and (V vE V)",
        "w Othen true else false] A((w, A)) = v The weights of the items can be computed through matrix products : a matrix for the grammar and a matrix for the lexicalization.",
        "The size of any lexicalization A is linear in the size of the grammar.",
        "As (V, G, 0, A)ELG if and only if [R(V, G, 0, A)] is true, LG is in NP.",
        "•",
        "Bin Packing (BP) which is NP-complete is polynomial-time Karp reducible to LG.",
        "BP (Baase, 1986) is the problem defined by (3) :",
        "(3) BP -= (R, {R1, , Rk)) I where",
        "rational numbers less than or equal to 1 and (R1, , Rk} is a partition of R (k bins in which the ris are packed) such that",
        "TER' First, any instance of BP can be represented as an instance of LG.",
        "Let (R, {R1, , Rk}) be an instance of BP it is transformed into the instance (V. G, 0, A) of LG as follows :",
        "and (ViE {1, , k}) (V je {1, , n})",
        "For all 1E {1, , k} and j€ {1, , n 1, we consider the assignment of ri to the bin R. of BP as the anchoring of the rule (ri, V) to the item vi of LG.",
        "If (R, (R1, , RkDE BP then :",
        "(5) (ViE {1, ,k}) r 1",
        "{R1, , Rk} is a partition of R because the lexicalization is a total function and the preceding formula ensures that each bin is correctly loaded.",
        "Thus (R, {R1, , Rk})e BP.",
        "It is also simple to verify that the transformation from BP to LG can be performed in polynomial time.",
        "The optimization of an NP-complete problem is NP-complete (Sommerhalder and van Westrhenen, 1988), then the optimization version of LG is NP-complete."
      ]
    },
    {
      "heading": "An Approximation Algorithm for L G",
      "text": [
        "This part presents and evaluates an n3-time approximation algorithm for the LG problem which yields a suboptimal solution close to the optimal one.",
        "The first step is the 'easy' anchoring of rules including at least one rare lexical item to one of these items.",
        "The second step handles the `hard' lexicalization of the remaining rules including only common items found in several other rules and for which the decision is not straightforward.",
        "The discrimination between these two kinds of items is made on the basis of their global weight GW",
        "(6) which is the sum of the weights of the rules which are not yet anchored and which have this lemma as potential anchor.",
        "VA and GA are subsets of V and G which denote the items and the rules not yet anchored.",
        "The ws and 0 are assumed to be integers by multiplying them by their lowest common denominator if necessary.",
        "(6) (VvE VA) GW(v) = (w, A) e GA, V E A",
        "Step 1 : 'Easy' Lexicalization of Rare Items This first step of the optimization algorithm is also the first step of the exhaustive search.",
        "The value of the minimal threshold 0„,in given by (7) is computed by dividing the sum of the rule weights by the number of lemmas (1x1 stands for the smallest integer greater than or equal to x and I VA I stands for the size of the set VA) :",
        "All the rules which include a lemma with a global weight less than or equal to 0„,i,, are anchored to this lemma.",
        "When this linking is achieved in a non-deterministic manner, emin is recomputed.",
        "The algorithm loops on this lexicalization, starting it from scratch every time, until emin remains unchanged or until all the rules are anchored.",
        "The output value of emin is the minimal threshold such that LG has a solution and therefore is less than or equal to 0.",
        "After Step 1, either each rule is anchored opt or all the remaining items in VA, have a global weight strictly greater than emin.",
        "The algorithm is shown in Figure 1.",
        "Step 2 : 'Hard' Lexicalization of Common Items During this step, the algorithm repeatedly removes an item from the remaining vocabulary and yields the anchoring of this item.",
        "The item with the lowest global weight is handled first because it has the smallest combination of anchorings and hence the probability of making a wrong choice for the lexicalization is low.",
        "Given an item, the candidate rules with this item as potential anchor are ranked according to 1 The highest priority is given to the rules whose set of potential anchors only includes the current item as non-anchored item.",
        "2 The remaining candidate rules taken first are the ones whose potential anchors have the highest global weights (items found in several other non-anchored rules).",
        "The algorithm is shown in Figure 2.",
        "The output of Step 2 is the suboptimal computational lexicalization A. of the whole grammar and the associated threshold 9„bopt.",
        "Both steps can be optimized.",
        "Useless computation is avoided by watching the capital",
        "of weight C defined by (8) with 9 0„,in during Step 1 and 0 - Osubopt during Step 2:",
        "C corresponds to the weight which can be lost by giving a weight W(w) which is strictly less than the current threshold O.",
        "Every time an anchoring to a unit to is completed, C is reduced from 8 W(0).",
        "If C becomes negative in either of both steps, the algorithm will fail to make the lexicalization of the grammar and must be started again from Step 1 with a higher value for O.",
        ";; anchoring the rules with only w as ;; free potential anchor (tii EVA with ;; the lowest global weight)",
        "Example3 The algorithm has been applied to a test grammar G2 obtained from 41 terms with 11 potential anchors.",
        "The algorithm fails in making the lexicalization of G2 with the minimal threshold Omin = 12, but achieves it with esubopt 13.",
        "This value of Osubopt can be compared with the optimal one by running the exhaustive search.",
        "There are 232 (:4: 4 109) possible lexicalizations among which 35,336 are optimal ones with a threshold of 13.",
        "This result shows that the approximation algorithm brings forth one of the optimal solutions which only represent a proportion of 8 10-6 of the possible lexicalizations.",
        "In this case the optimal and the suboptimal threshold coincide.",
        "Time-Complexity of the Approximation Algorithm A grammar G on a vocabulary V can be represented by a IGIx vi matrix of Boolean values for the set of potential anchors and a lx IGI matrix for the weights.",
        "In order to evaluate the complexity of the algorithms as a function of the size of the grammar, we assume that I VI and I G I are of the same order of magnitude n. Step 1 of the algorithm corresponds to products and sums on the preceding matrixes and takes 0(n3) time.",
        "The worst-case time-complexity for Step 2 of the algorithm is also 0(n3) when using a naive 0(n2) algorithm to sort the items and the rules by decreasing priority.",
        "In all, the time required by the approximation algorithm is proportional to the cubic size of the grammar.",
        "This order of magnitude ensures that the algorithm can be applied to large real-world grammars such as terminological grammars.",
        "On a Sparc 2, the lexicalization of a terminological grammar composed of 6,622 rules and 3,256 words requires 3 seconds (real time) and the lexicalization of a very large terminological grammar of 71,623 rules and 38,536 single words takes 196 seconds.",
        "The two grammars used for these experiment were generated from two lists of terms provided by the documentation center INIST/CNRS."
      ]
    },
    {
      "heading": "Evaluation of the Approximation Algorithm",
      "text": [
        "Bench Marks on Artificial Grammars In order to check the quality of the lexicalization on different kinds of grammars, the algorithm has been tested on eight randomly generated grammars of 4,000 rules having from 2 to 10 potential anchors (Table 1).",
        "The lexicon of the first four grammars is 40 times smaller than the grammar while the lexicon of the last four ones is 4 times smaller than the grammar (this proportion is close to the one of the real-world grammar studied in the next subsection).",
        "The eight grammars differ in their distribution of the items on to the rules.",
        "The uniform distribution corresponds to a uniform random choice of the items which build the set of potential anchors while the Gaussian one corresponds to a choice taking more frequently some items.",
        "The higher the parameter s, the flatter the Gaussian distribution.",
        "The last two columns of Table 1 give the minimal threshold Omin after Step 1 and the suboptimal threshold 0„bopt found by the approximation algorithm.",
        "As mentioned when presenting Step 1, the optimal threshold °opt is necessarily greater than or equal to 0„,i, after Step 1.",
        "Table 1 reports that the suboptimal threshold 0„bopt is not over 2 units greater than 0„,i, after Step 1.",
        "The suboptimal threshold yielded by the approximation algorithm on these examples has a high quality because it is at worst 2 units greater than the optimal one.",
        "A Comparison with Linguistic Lexicalization on a Real-World Grammar This evaluation consists in applying the algorithm to a natural language grammar composed of 6,622 rules (terms from the domain of metallurgy provided by INIST/CNRS) and a lexicon of 3,256 items.",
        "Figure 3 depicts the distribution of the weights with the natural linguistic lexicalization.",
        "The frequent head words such as alloy are heavily loaded because of the numerous terms in N – alloy with N being a name of metal.",
        "Conversely, in Figure 4 the distribution of the weights from the approximation algorithm is much more",
        "uniform.",
        "The maximal weight of an item is 241 threshold after Step 1 being 34, the suboptimal with the linguistic lexicalization while it is only threshold yielded by the approximation 34 with the optimized lexicalization.",
        "The algorithm is equal to the optimal one."
      ]
    },
    {
      "heading": "Conclusion",
      "text": [
        "As mentioned in the introduction, the improvement of the lexicalization through an optimization algorithm is currently used in FASTR a parser for terminological extraction through NLP techniques where terms are represented by lexicalized rules.",
        "In this framework as in top-down parsing with LTAGs (Schabes and Joshi, 1990), the first phase of parsing is a filtering of the rules with their anchors in the input sentence.",
        "An unbalanced distribution of the rules on to the lexical items has the major computational drawback of selecting an excessive number of rules when the input sentence includes a common head word such as \"alloy\" (127 rules have \"alloy\" as head).",
        "The use of the optimized lexicalization allows us to filter 57% of the rules selected by the linguistic lexicalization.",
        "This reduction is comparable to the filtering induced by linguistic lexicalization which is around 85% (Schabes and Joshi, 1990).",
        "Correlatively the parsing speed is multiplied by 2.6 confirming the computational saving of the optimization reported in this study.",
        "There are many directions in which this work could be refined and extended.",
        "In particular, an optimization of this optimization could be achieved by testing different weight assignments in correlation with the parsing algorithm.",
        "Thus, the computational lexicalization would fasten both the preprocessing and the parsing algorithm."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "I would like to thank Alain Colmerauer for his valuable comments and a long discussion on a draft version of my PhD dissertation.",
        "I also gratefully acknowledge Chantal Enguehard and two anonymous reviewers for their remarks on earlier drafts.",
        "The experiments on industrial data were done with term lists from the documentation center INIST/CNRS."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": []
    }
  ]
}
