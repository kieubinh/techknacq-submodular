{
  "info": {
    "authors": [
      "Rens Bod"
    ],
    "book": "Conference of the European Association for Computational Linguistics",
    "id": "acl-E95-1015",
    "title": "The Problem of Computing the Most Probable Tree in Data-Oriented Parsing and Stochastic Tree Grammars",
    "url": "https://aclweb.org/anthology/E95-1015",
    "year": 1995
  },
  "references": [
    "acl-C92-2066",
    "acl-C92-3126",
    "acl-E93-1006",
    "acl-H90-1021",
    "acl-J93-1002",
    "acl-J93-2004",
    "acl-P92-1006"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We deal with the question as to whether there exists a polynomial time algorithm for computing the most probable parse tree of a sentence generated by a data-oriented parsing (DOP) model.",
        "(Scha, 1990; Bod, 1992, 1993a).",
        "Therefore we describe DOP as a stochastic tree-substitution grammar (STSG).",
        "In STSG, a tree can be generated by exponentially many derivations involving different elementary trees.",
        "The probability of a tree is equal to the sum of the probabilities of all its derivations.",
        "We show that in STSG, in contrast with stochastic context-free grammar, the Viterbi algorithm cannot be used for computing a most probable tree of a string.",
        "We propose a simple modification of Viterbi which allows by means of a \"select-random\" search to estimate the most probable tree of a string in polynomial time.",
        "Experiments with DOP on ATIS show that only in 68% of the cases, the most probable derivation of a string generates the most probable tree of that string.",
        "Therefore, the parse accuracy obtained by the most probable trees (96%) is dramatically higher than the parse accuracy obtained by the most probable derivations (65%).",
        "It is still an open question whether the most probable tree of a string can be deterministically computed in polynomial time."
      ]
    },
    {
      "heading": "1 Data-Oriented Parsing",
      "text": [
        "A Data-Oriented Parsing model (Scha, 1990; Bod, 1992, 1993a) is characterized by a corpus of analyzed language utterances, together with a set of operations that combine sub-analyses from the corpus into new analyses.",
        "We will limit ourselves in this paper to corpora with purely syntactic annotations.",
        "For the semantic dimension of DOP, the reader is referred to (van den Berg et al., 1994).",
        "Consider the imaginary example corpus consisting of only two trees in figure 1.",
        "We will assume one operation for combining subtrees.",
        "This operation is called \"composition\", and is indicated by the infix operator The composition of t and u, tou, yields a copy of t in which its leftmost nontenninal leaf node has been identified with the root nodeof u (i.e., u is substituted on the leftmost nontenninal leaf node of t).",
        "For reasons of simplicity we will write in the following (tou)ov as: to u ov.",
        "As the reader may easily ascertain, a different derivation may yield a different parse tree.",
        "However, a different derivation may also very well yield the same parse tree; for instance:",
        "tree for \"She displayed the dress on the table\" Thus, a parse tree can have several derivations involving different subtrees.",
        "Using the corpus for our stochastic estimations, we estimate the probability of substituting a certain subtree on a specific node as the probability of selecting this subtree among all subtrees in the corpus that could be substituted on that node.'",
        "The probability of a derivation can be computed as the product of the probabilities of the substitutions that it involves.",
        "The probability of a parse tree is equal to the probability that any of its derivations occurs, which is the sum of the probabilities of all derivations of that parse tree.",
        "Finally, the probability of a word string is equal to the sum of the probabilities of all its parse trees.",
        "substitution-probabilities of the corresponding subtrees of DOP (Bod, 1993c).",
        "A Stochastic Tree-Substitution Grammar G is a five-tuple <VN, VT, S. R, P> where VN is a finite set of nonterminal symbols.",
        "VT is a finite set of terminal symbols.",
        "S E VN is the distinguished symbol.",
        "R is a finite set of elementary trees whose top nodes and interior nodes are labeled by nonterminal symbols and whose yield nodes are labeled by terminal or nonterminal symbols.",
        "P is a function which assigns to every elementary tree te R a probability p(t).",
        "For a tree t with a root a, p(t) is interpreted as the probability of substituting t on a.",
        "We require, therefore, that 0 < p(t) 1 and Itroot(t).--a p(t) = 1.",
        "If t/ and t2 are trees such that the leftmost nonterminal yield node of t/ is equal to the root of t2, then t/ ot2 is the tree that results from substituting t2 for this leftmost nonterminal yield node in t1.",
        "The partial function o is called leftmost substitution.",
        "For reasons of conciseness we will use the term substitution for leftmost substitution.",
        "A leftmost derivation generated by an STSG G is a tuple of trees <//,...,tn> such that ti,...,tn are elements of R, the root of t/ is labeled by S and the yield of tio...ot, is labeled by terminal symbols.",
        "The set of leftmost derivations generated by G is thus given by Derivations(G) = I E R",
        "convenience we will use the term derivation for leftmost derivation.",
        "A derivation <ti,...,tn> is called a derivation of tree T, iff t/....otn = T. A derivation <t/.....t> is called a derivation of string s, iff = s. The probability of a derivation tn> is defined as p(t 1) ... p(tn).",
        "A parse tree generated by an STSG G is a tree T such that there is a derivation < t1,...,tn > E Derivations(G) for which t/.... .t, = T. The set of parse trees, or tree language, generated by G is given by Parses(G) = {TI 3 <t1,...,tn> e Derivations(G) tio....t, = T).",
        "For reasons of conciseness we will often use the terms parse or tree for a parse tree.",
        "A parse whose yield is equal to string s, is called a parse of S. The probability of a parse is defined as the sum of the probabilities of all its derivations.",
        "A string generated by an STSG G is an element of VT+ such that there is a parse generated by",
        "3Computing a most probableFor the input string abcd, the following derivation parse tree in STSG forest is then obtained: In order to deal with the problem of computing the most probable parse tree of a sentence, we will distinguish between parsing and disambiguation.",
        "By parsing we mean the creation of a parse forest for an input sentence.",
        "By disambiguation we mean the selection of the most probable parse2 from the forest.",
        "The creation of a parse forest is an intermediate step for computing the most probable parse."
      ]
    },
    {
      "heading": "3.1 Parsing",
      "text": [
        "From the way STSG combines elementary trees by means of substitution, it follows that an input sentence can be parsed by the same algorithms as (S)CFGs.",
        "Every elementary tree t is used as a context-free rewrite rule root(t) --> yield(t).",
        "Given a chart parsing algorithm, an input sentence of length n can be parsed in n3 time.",
        "In order to obtain a chart-like forest for a sentence parsed in STSG, we need to label the well formed substrings in the chart not only with the syntactic categories of that substring but with the full elementary trees t that correspond to the use of the derived rules root(t) -->yield(t).",
        "Note that in a chart like forest generated by an STSG, different derivations that generate a same tree do not collapse.",
        "We will therefore talk about a derivation forest generated by an STSG (cf. Sima'an et al., 1994).",
        "The following formal example illustrates what a derivation forest of a string may look like.",
        "In the example, we leave out the probabilities, which are needed only in the disambiguation process.",
        "The visual representation comes from (Kay, 1980): every entry (ij) in the chart is indicated by an edge and spans the words between the i-th and the j-th position of a sentence.",
        "Every edge is labeled with the elementary trees that denote the underlying phrase.",
        "The exampleSTSG consists of the following elementary trees: 2 Although theoretically there can be more than one most probable parse for a sentence, in practice a system that employs a non-trivial treebank tends to generate exactly one most probable parse for a given input sentence.",
        "Note that different derivations in the forest generate the same tree.",
        "By exhaustively unpacking the forest, four different derivations generating two different trees are obtained.",
        "We may ask whether we can pack the forest by collapsing spurious derivations.",
        "Unfortunately, no efficient procedure is known that accomplishes this (remember that there can be exponentially many derivations for one tree)."
      ]
    },
    {
      "heading": "3.2 Disambiguation",
      "text": [
        "Cubic time parsing does not guarantee cubic time disambiguation, as a sentence may have exponentially many parses and any such parse may have exponentially many derivations.",
        "Therefore, in order to find the most probable parse of a sentence, it is not efficient to compare the probabilities of the parses by exhaustively unpacking the chart.",
        "Even for determining the probability of one parse, it is not efficient to add the probabilities of all derivations of that parse.",
        "3.2.1 Viterbi optimization is not feasible for finding the most probable parse There exists a heuristic optimization algorithm, known as Viterbi optimization, which selects on the basis of an SCFG the most probable derivation of a sentence in cubic time (Viterbi, 1967; Fujisaki et al., 1989; Jelinek et al., 1990).",
        "In STSG, however, the most probable derivation does not necessarily generate the most probable parse, as the probability of a parse is defined as the sum of the probabilities of all its derivations.",
        "Thus, there is an important question as to whether we can adapt the Viterbi algorithm for finding the most probable parse.",
        "To understand the difficulty of the problem, we look in more detail at the Viterbi algorithm.",
        "The basic idea of the Viterbi algorithm is the early pruning of low probability subderivations in a bottom-up fashion.",
        "Two different subderivations of the same part of the sentence and whose resulting",
        "This elementary tree may be developed to the same tree that can be developed by d2, but not to the tree that can be developed by dl.",
        "And since the probability of a parse tree is equal to the sum of the probabilities of all its derivations, it is still possible that d2 contributes to the generation of the most probable parse.",
        "Therefore we are not allowed to eliminate d2.",
        "This counterexample does not prove that there is no heuristic optimization that allows polynomial time selection of the most probable parse.",
        "But it makes clear that a \"select-best\" search, as accomplished by Viterbi, is not adequate for finding the most probable parse in STSG.",
        "So far, it is unknown whether the problem of finding the most probable parse in a deterministic way is inherently exponential or not (cf. Sima'an et al., 1994).",
        "One should of course ask how often in practice the most probable derivation produces the most probable parse, but this can only be answered by means of experiments on real life corpora.",
        "Experiments on the ATIS corpus (see session 4) show that only in 68% of the cases the most probable derivation of a sentence generates the most probable parse of that sentence.",
        "Moreover, the parse accuracy obtained by the most probable parse is dramatically higher than the parse accuracy obtained by the parse generated by the most probable derivation.",
        "3.2.2 Estimating the most probable parse by Monte Carlo search We will leave it as an open question whether the most probable parse can be deterministically derived in polynomial time.",
        "Here we will ask whether there exists a polynomial time approximation procedure that estimates the most probable parse with an estimation error that can be made arbitrarily small.",
        "We have seen that a \"select-best\" search, as accomplished by Viterbi, can be used for finding the most probable derivation but not for finding the most probable parse.",
        "If we apply instead of a select-best search, a \"select-random\" search, we can generate a random derivation.",
        "By iteratively generating a large number of random derivations we can estimate the most probable parse as the parse which results most often from these random derivations (since the probability of a parse is the probability that any of its derivations occurs).",
        "The most probable parse can be estimated as accurately as desired by making the number of random samples as large as desired.",
        "According to the Law of Large Numbers, the most often generated parse converges to the most probable parse.",
        "Methods that estimate the probability of an event by taking random samples are known as Monte Carlo methods (Hammersley & Handscomb, 1964).3 The selection of a random derivation is accomplished in a bottom-up fashion analogous to Viterbi.",
        "Instead of selecting the most probable subderivation at each node-sharing in the chart, a random subderivation is selected (i.e. sampled) at each node-sharing (that is, a subderivation that has n times as large a probability as another subderivation should also have n times as large a chance to be chosen as this other subderivation).",
        "Once sampled at the S-node, the random derivation of the whole sentence can be retrieved by tracing back the choices made at each node-sharing.",
        "Of course, we may postpone sampling until the S-node, such that we sample directly from the distribution of all S-derivations.",
        "But this would take exponential time, since there may be exponentially many derivations for the whole sentence.",
        "By sampling bottom-up at every node where ambiguity appears, the maximum number of different subderivations at each node-sharing is bounded to a constant (the total number of rules of that node), and therefore the time complexity of generating a random derivation of an input sentence is equal to the time complexity of finding the most probable derivation, 0(n3).",
        "This is exemplified by the following algorithm.",
        "Sampling a random derivation from a derivation forest Given a derivation forest, of a sentence of n words, consisting of labeled entries (i,j) that span the words between the i-th and the j-th position of the sentence.",
        "Every entry is labeled with linked elementary trees, together with their probabilities, that constitute subderivations of the underlying subsentence.",
        "Sampling a derivation from the chart consists of choosing at every labeled entry (bottom-up, breadth first) a random subderivation of each root-node:",
        "select4 a random subderivation of root X eliminate the other subderivations We now have an algorithm that selects a random derivation from a derivation forest.",
        "Converting this derivation into a parse tree gives a first estimation for the most probable parse.",
        "Since one random sample is not a reliable estimate, we sample a large number of random derivations and see which parse is generated most frequently.",
        "This is exemplified by the following algorithm.",
        "(Note that we might also estimate the most probable derivation by random sampling, namely by counting which derivation is sampled most often; however, the most probable derivation can be more effectively generated by Viterbi.)",
        "Estimating the most probable parse (MPP) Given a derivation forest for an input sentence: repeat until the MPP converges sample a random derivation from the forest store the parse generated by the random derivation MPP := the most frequently occurring parse There is an important question as to how long the convergence of the most probable parse may take.",
        "Is there a tractable upper bound on the number of derivations that have to be sampled from the forest before stability in the top of the parse distribution occurs?",
        "The answer is yes: the worst case time complexity of achieving a maximum estimation error E by means of random sampling is 0(E-2), independently of the probability distribution.",
        "This is a classical result from sampling theory (cf. Hammersley and Handscomb, 1964), and follows directly from Chebyshev's inequality.",
        "In practice, it means that the 4 Let ( (el, pi) , (e2, p2) ,, (en, pn) ) be a probability distribution of events e1, e2, ..., en; an event e i is said to be randomly selected iff its probability of being selected is equal to pi.",
        "In order to allow for \"direct sampling\", one must convert the probability distribution into a corresponding sample space for which holds that the frequency of occurrence fi of each event e i is a positive integer equal to Npi, where N is the size of the sample space.",
        "error e is inversely proportional to the square-root of the number of random samples N and therefore, to reduce e by a factor of k, the number of samples N needs to be increased k2-fold.",
        "In practical experiments (see 4), we will limit the number of samples to a pre-determined, sufficiently large bound N. What is the theoretical worst case time complexity of parsing and disambiguation together?",
        "That is, given an STSG and an input sentence, what is the maximal time cost of finding the most probable parse of a sentence?",
        "If we use a CKY-parser, the creation of a derivation forest for a sentence of n words takes 0(n3) time.",
        "Taking also into account the size G of an STSG (defined as the sum of the lengths of the yields of all its elementary trees), the time complexity of creating a derivation forest is proportional to Gn3 .",
        "The time complexity of disambiguation is both proportional to the cost of sampling a derivation, i.e. Gn3, and to the cost of the convergence by means of iteration, which is C2.",
        "This means that the time complexity of disambiguation is given by 0(Gn3e-2).",
        "The total time complexity of parsing and disambiguation is equal to 0(Gn3) + 0(Gn3 e-2) = 0(Gn3 e-2).",
        "Thus, there exists a tractable procedure that estimates the most probable parse of an input sentence.",
        "Notice that although the Monte Carlo disambiguation algorithm estimates the most probable parse of a sentence in polynomial time, it is not in the class of polynomial time decidable algorithms.",
        "The Monte Carlo algorithm cannot decide in polynomial time what is the most probable parse; it can only make the error-probability of the estimated most probable parse arbitrarily small.",
        "As such, the Monte Carlo algorithm is a probabilistic algorithm belonging to the class of Bounded error Probabilistic Polynomial time (BPP) algorithms.",
        "We hypothesize that Monte Carlo disambiguation is also relevant for other stochastic grammars.",
        "It turns out that all stochastic extensions of CFGs that are stochastically richer than SCFG need exponential time algorithms for finding a most probable parse tree (cf. Briscoe & Carroll, 1992; Black et al., 1993; Magerman & Weir, 1992; Schabes & Waters, 1993).",
        "To our knowledge, it has never been studied whether there exist BPP-algorithms for these models.",
        "Alhough it is beyond the scope of our research, we conjecture that there exists a Monte Carlo disambiguation algorithm for at least Stochastic Tree-Adjoining Grammar (Schabes, 1992)."
      ]
    },
    {
      "heading": "3.2.3 Psychological relevance of Monte Carlo disambiguation",
      "text": [
        "As has been noted, an important difference between the Viterbi algorithm and the Monte Carlo algorithm is, that with the latter we never have 100% confidence.",
        "In our opinion, this should not be seen as a disadvantage.",
        "In fact, absolute confidence about the most probable parse does not have any significance, as the probability assigned to a parse is already an estimation of its actual probability.",
        "One may ask as to whether Monte Carlo is appropriate for modeling",
        "human sentence perception.",
        "The following lists some properties of Monte Carlo disambiguation that may be of psychological interest: 1.",
        "As mentioned above, Monte Carlo never provides 100% confidence about the best analysis.",
        "This corresponds to the psychological observation that people never have absolute confidence about their interpretation of an ambiguous sentence.",
        "2.",
        "Although conceptually Monte Carlo uses the total space of possible analyses, it tends to sample only the most likely ones.",
        "Very unlikely analyses may only be sampled after considerable time, but it is not guaranteed that all analyses are found in finite time.",
        "This matches with experiments on human sentence perception where very implausible analyses are only perceived with great difficulty and after considerable time.",
        "3.",
        "Monte Carlo does not necessarily give the same results for different sequences of samples, especially if different analyses in the top of the distribution are almost equally likely.",
        "In the case there is more than one most probable analysis, Monte Carlo does not converge to one analysis but keeps alternating, however large the number of samples is made.",
        "In experiments with human sentence perception, it has often been shown that different analyses can be perceived for one sentence.",
        "And in case these analyses are equally plausible, people perceive so-called fluctuation effects.",
        "This fluctuation phenomenon is also well-known in the perception of ambiguous visual patterns.",
        "4.",
        "Monte Carlo can be made parallel in a very straightforward way: N samples can be computed by N processing units, where equal outputs are reinforced.",
        "The more processing units are employed, the better the estimation.",
        "However, since the number of processing units is finite, there is never absolute confidence.",
        "This has some similarity with the Parallel Distributed Processing paradigm for human (language) processing (Rumelhart & McClelland, 1986)."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "In this section, we report on experiments with an implementation of DOP that parses and disambiguates part-of-speech strings.",
        "In (Bod, 1995) it is shown how DOP is extended to parse word strings that possibly contain unknown words."
      ]
    },
    {
      "heading": "4.1 The test environment",
      "text": [
        "For our experiments, we used a manually corrected version of the Air Travel Information System (ATIS) spoken language corpus (Hemphill et al., 1990) annotated in the Pennsylvania Treebank (Marcus et al., 1993).",
        "We employed the \"blind testing\" method, dividing the corpus into a 90% training set and a 10% test set by randomly selecting sentences.",
        "The 675 trees from the training set were converted into their subtrees together with their relative frequencies, yielding roughly 4* i05 different subtrees.",
        "The 75 part-of-speech sequences from the test set served as input strings that were parsed and disambiguated using the subtrees from the training set.",
        "As motivated in (Bod, 1993b), we use the notion of parse accuracy as our accuracy metric, defined as the percentage of the test strings for which the most probable parse is identical to the parse in the test set."
      ]
    },
    {
      "heading": "4.2 Accuracy as a function of subtree-depth",
      "text": [
        "It is one of the most essential features of DOP, that arbitrarily large subtrees are taken into consideration to estimate the probability of a parse.",
        "In order to test the usefulness of this feature, we performed different experiments constraining the depth of the subtrees.",
        "The following table shows the results of seven experiments for different maximum depths of the training set subtrees.",
        "The accuracy refers to the parse accuracy at 400 randomly sampled parses, and is rounded off to the nearest integer.",
        "The CPU time refers to the average CPU time per string employed by a Spark II.",
        "depth of subtreesparseCPU time (hours)",
        "The table shows a dramatic increase in parse accuracy when enlarging the maximum depth of the subtrees from 1 to 2.",
        "(Remember that for depth one, DOP is equivalent to a stochastic context-free grammar.)",
        "The accuracy keeps increasing, at a slower rate, when the depth is enlarged further.",
        "The highest accuracy is obtained by using all subtrees from the training set: 72 out of the 75 sentences from the test set are parsed correctly.",
        "Thus, the accuracy increases if larger subtrees are used, though the CPU time increases considerably as well.",
        "4.3Does the most probable derivation generate the most probable parse?",
        "Another important feature of DOP is that the probability of a resulting parse tree is computed as the sum of the probabilities of all its derivations.",
        "Although the most probable parse of a sentence is not necessarily generated by the most probable derivation of that sentence, there is a question as to how often these two coincide.",
        "In order to study this, we also calculated the derivation accuracy, defined as the percentage of the test strings for which the parse generated by the most probable derviation is identical to the parse in the test set.",
        "The following table shows the derivation accuracy against the parse accuracy for the 75 test set strings from the ATIS corpus, using different maximum depths for the corpus subtrees.",
        "The table shows that the derivation accuracy is equal to the parse accuracy if the depth of the subtrees is constrained to 1.",
        "This is not surprising, as for depth 1, DOP is equivalent with SCFG where every parse is generated by exactly one derivation.",
        "What is remarkable, is, that the derivation accuracy decreases if the depth of the subtrees is enlarged to 2.",
        "If the depth is enlarged further, the derivation accuracy increases again.",
        "The highest derivation accuracy is obtained by using all subtrees from the corpus (65%), but remains far behind the highest parse accuracy (96%).",
        "From this table we conclude that if we.are interested in the most probable analysis of a string we must not look at the probability of the process of achieving that analysis but at the probability of the result of that process.",
        "4.4 The significance of once-occurring subtrees There is an important question as to whether we can reduce the \"grammar constant\" of DOP by eliminating very infrequent subtrees, without affecting the parse accuracy.",
        "In order to study this question, we start with a test result.",
        "Consider the test set sentence \"Arrange the flight code of the flight from Denver to Dallas Worth in descending order\",Mdfich has the following parse in the test set:",
        "The corresponding p-o-s sequence of this sentence is the test set string \"VB DT NN NN IN DT NN IN NP TO NP NP IN VBG NN\".",
        "At subtree-depth 2, the following most probable parse was estimated for this string (where for reasons of readability the words are added to the p-o-s tags): In this parse, we see that the prepositional phrase \"in descending order\" is incorrectly attached to the NP \"the flight\" instead of to the verb \"arrange\".",
        "This wrong attachment may be explained by the high relative frequencies of the following subtrees of depth 2 (that appear in structures of sentences like \"Show me the transportation from SFO to downtown San Francisco in August\", where the PP \"in August\" is attached to the NP \"the transportation\", and not to the verb \"show\"):",
        "We have shown that in DOP and STSG the Viterbi algorithm cannot be used for computing a most probable tree of a string.",
        "We developed a modification of Viterbi which allows by means of an iterative Monte Carlo search to estimate the most probable tree of a string in polynomial time.",
        "Experiments on ATIS showed that only in 68% of the cases, the most probable derivation of a string generates the most probable tree of that string, and that the parse accuracy is dramatically higher than the derivation accuracy.",
        "We conjectured that the Monte Carlo algorithm can also be applied to other stochastic grammars for computing the most probable tree of a string.",
        "The question as to whether the most probable tree of a string can also be deterministically derived in polynomial time is still unsolved."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The author is indebted to Remko Scha for valuable comments on an earlier version of this paper, and to Khalil Sima'an for useful discussions."
      ]
    }
  ]
}
