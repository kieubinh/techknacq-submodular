{
  "info": {
    "authors": [
      "Kenneth Ward Church",
      "William A. Gale"
    ],
    "book": "Very Large Corpora",
    "id": "acl-W95-0110",
    "title": "Inverse Document Frequency (IDF): A Measure of Deviations from Poisson",
    "url": "https://aclweb.org/anthology/W95-0110",
    "year": 1995
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Low frequency words tend to be rich in content, and vice versa.",
        "But not all equally frequent words are equally meaningful.",
        "We will use inverse document frequency (IDF), a quantity borrowed from Information Retrieval, to distinguish words like somewhat and boycott.",
        "Both somewhat and boycott appeared approximately 1000 times in a corpus of 1989 Associated Press articles, but boycott is a better keyword because its IDF is farther from what would be expected by chance (Poisson).",
        "1.",
        "Document frequency is similar to word frequency, but different",
        "Word frequency is commonly used in all sorts of natural language applications.",
        "The practice implicitly assumes that words (and ngrams) are distributed by a single parameter distribution such as a Poisson or a Binomial.",
        "But we find that these distributions do not fit the data very well.",
        "Both the Poisson and Binomial assume that the variance over documents is no larger than the mean, and yet, we find that it can be quite a bit larger, especially for interesting words such as boycott where there are hidden variables such as topic that conspire to undermine the independence assumption behind the Poisson and the Binomial.",
        "Much better fits are obtained by introducing a second parameter such as inverse document frequency (IDF).",
        "Inverse document frequency (IDF) is commonly used in Information Retrieval (Sparck Jones, 1972).",
        "IDF is defined as – log2df/D, where D is the number of documents in the collection and dfw is the document frequency, the number of documents that contain w. Obviously, there is a strong relationship between document frequency, df„, and word frequency, f. The relationship is shown in Figure 1, a plot of log of and IDF for 193 words selected from a 50 million word corpus of 1989 Associated Press (AP) Newswire stories (D = 85,432 stories).",
        "Although logiof,„ is highly correlated with IDF (p = – 0.994), it would be a mistake to assume that the two variables are completely predictable from one another.",
        "Indeed, the experience of the Information Retrieval community has indicated that IDF is a very useful quantity.",
        "Attempts to replace IDF with fw (or some simple transform off) have not been very successful.",
        "Figure 2 shows one such attempt.",
        "It compares the observed IDF with InF, an estimate based on f Assume that a document is merely a \"bag of words\" with no interesting structure (content).",
        "Words are randomly generated by a Poisson process, it.",
        "The probability of k instances of a word w is ic(9,k) fw where 0=",
        "In particular, the probability that w will not be found in a document is rc(0,0).",
        "Conversely, the probability of at least one w is 1 – TC(0, 0).",
        "And therefore, IDF ought to be: IDF=-1og2 (1 – 740,0))= – Iog2(1 – e-9) Predicted IDF",
        "The prediction errors are shown in more detail in Figure 3, which plots the residual IDF (difference between predicted and observed) as a function of log iof,„ for the same 193 words shown in Figure 2.",
        "The prediction errors are relatively large in the middle of the frequency range, and smaller at both ends.",
        "Unfortunately, we believe the words in the middle are often the most important words for Information Retrieval purposes."
      ]
    },
    {
      "heading": "2. A Good Keyword is far from Poisson",
      "text": [
        "To get a better look at the crucial differences between IDF and f in the middle frequency range (f =103), we selected a set of 53 words for further investigation with 1000 <f< 1020 in the 1989 AP corpus.",
        "The 53 words are shown in Table 1, sorted by df.",
        "Note that the words near the top of the list tend to be more appropriate for use in an information retrieval system than the words toward the bottom of the list.",
        "Stories that mention the word boycott, for example, are likely to be about boycotts.",
        "In contrast, stories that mention the word somewhat could be about practically anything.1",
        "Why is IDF such a useful quantity?",
        "One might try to answer the question in terms of information theory (Shannon, 1948).",
        "IDF can be thought of as the usefulness in bits of a keyword to a keyword retrieval system.",
        "If we tell you that the document that we are looking for has the keyword boycott, then we have narrowed the search space down to just 6761D documents.",
        "But, this answer doesn't explain the fundamental difference between boycott and somewhat.",
        "boycott has an IDF of – log 2676/D = 7.0 bits, only a little more than somewhat, which has an IDF of – log 2979/D = 6.4.",
        "And yet, boycott is a reasonable keyword and somewhat is not.",
        "A good keyword, like boycott, picks out a very specific set of documents.",
        "The problem with somewhat is that it behaves almost like chance (Poisson).",
        "Under a Poisson, the 1013 instances of somewhat should be found in approximately D(1 – n(0,0))=D(1 – n(1013/85432,0)) =1007 documents.",
        "In fact, somewhat was found in 979 documents, only a little less than what would have been expected by chance.",
        "Good keywords tend to bunch up into many fewer documents.",
        "boycott, for example, bunches up into only 676 documents, much less than chance (D(1 – n(1009/85432,0)) =1003).",
        "Almost all words are more \"interesting\" in this sense than Poisson, but good keywords like boycott are a lot more interesting than Poisson, and crummy ones like somewhat are only a little more interesting than Poisson.",
        "On this account, a good keyword is one that behaves very differently from the null hypothesis (Poisson).",
        "We conjecture that the best keywords tend to be found toward the middle of the frequency range, where there are relatively large deviations from Poisson, as illustrated in Figure 3.",
        "This hypothesis runs counter to the standard practice in Information Retrieval of weighting words by IDF, favoring extremely rare words, no matter how they are distributed.",
        "Of course, IDF is but one of many ways to show deviations from chance.",
        "Figure 4 shows the distributions for boycott and somewhat.",
        "Note that somewhat is much \"closer\" to Poisson in almost any sense of closeness that one might consider.",
        "Three measures of \"closeness\" are presented in Table 2: IDF, variance (o2), and entropy (H).",
        "Table 2 compares the top 10 words in Table 1 (labeled \"better keywords\") with the bottom 10 words in Table 1 (labeled \"worse keywords\").",
        "The better keywords have more IDF, more variance and less entropy than what would be expected under a Poisson with e=f/D=.1000/85,432=0.012.",
        "3.",
        "How robust are these deviations from chance?",
        "We were concerned that the crucial deviations from Poisson behavior might not hold up if we looked at another corpus of similar material.",
        "Figure 5 shows the word boycott in five different years of the AP news.",
        "The \"fat tails\" show up in each of the five years.",
        "Clearly, the non-Poisson phenomenon is robust.",
        "Figures 6 and 7 compare IDF and logi0o2 for the 53 words in Table 1, and find that IDF and logi0a2 are reasonably stable across years.",
        "The correlations of IDF and logi0a2 across years are presented in Tables 3-4.",
        "All of the correlations are quite large.",
        "The correlations for IDF are perhaps somewhat larger than those for log 10 a2, suggesting that IDF may be somewhat more robust, which is not",
        "surprising given that empirical estimates of variance are notoriously subject to outliers.",
        "None of the correlations in Tables 3 and 4 can be attributed to word frequency effects since the 53 words were all chosen with almost the same 1989 frequency.",
        "In general, the correlations in Tables 3-4 are larger near the diagonal, suggesting that estimates degrade over time.",
        "If you want to predict next year's IDF, it is better to use this year's estimate than a ten-year-old estimate.",
        "Another way to confirm that our measurements of IDF, variance and H have consequences across years in the AP data, is to note that measurements of IDF, variance and H in 1989 can be used to predict word frequency in some other year.",
        "The correlations are shown in Table 5.",
        "They may not not be large, but they are too large to be due to chance and they all point in the same direction.",
        "The correlations cannot be attributed to variations in frequency in 1989, since all 53 words have almost the same 1989 frequency.",
        "Clearly, there are some interesting systematic relationships between IDF/variance/H and f that hold up to replication across multiple years in the AP, measurement errors, and other sources of noise."
      ]
    },
    {
      "heading": "4. Katz' K-mixture",
      "text": [
        "Clearly, the Poisson does not fit our data very well, especially for good keywords like boycott.",
        "This is, however, a negative result.",
        "Can we say something more constructive?",
        "Katz (personal communication) proposed the following alternative to the Poisson.",
        "Pr K(k) is the probability of k instances of w in a document.",
        "Poissons.",
        "Suppose that, within documents, boycott is distributed by a Poisson process, but, across documents, the Poisson parameter 0 is allowed to vary from one document to another depending on how much the document is about boycotts.",
        "In other words, Pr K(k) can be expressed as a convolution of Poissons with a density function 4):",
        "In this way, the Os can depend on an infinite number of unknowable hidden variables, e.g., what the documents are about, who wrote them, when they were written, what was going on in the world when they were written, etc., but we don't need to know these dependencies for any particular document.",
        "All we need to know is 0, the density of Os, aggregated over all possible combinations of hidden variables.",
        "a In the case of Katz' K-mixture, OM is assumed to be ( 1 - a) 8(8)+ P -, e. 8(k) is Dirac's delta function, 00 when k = 0, and otherwise, 0.",
        "Katz' K-mixture has two parameters, a and 13.",
        "The a parameter determines the fraction of relevant and irrelevant documents.",
        "1 – a of the documents have no chance of mentioning boycott (0 = 0) because they are totally irrelevant to boycotts.",
        "The 3 parameter determines the average 9 among the relevant documents.",
        "The two parameters, a and pi, can be fit from almost any pair of variables considered thus far, e.g., f, IDF, a2, H. We have found that f and IDF are particularly easy to work with, and are more robust than some others such as a2.",
        "It has been our experience that Katz' K-mixture fits the data much better than the Poisson, as can be seen in Figure 5.",
        "Unlike the Poisson, the K-mixture has two parameters, a and 13, and can therefore account for the fact that IDF and f are not completely predictable from one another.",
        "In related work (Church and Gale, submitted), we looked at a number of different Poisson mixtures, and found that our data can also be fit by a negative binomial, which can be viewed as a Poisson mixture where ONB (8) is a Gamma distribution (Johnson and Kotz, 1969).",
        "See Mosteller and Wallace (1964) for an example of how to use the negative binomial in a Bayesian discrimination task.",
        "It is straightforward to generalize the Mosteller and Wallace approach to use Katz' K-mixture or any other mixture of Poissons."
      ]
    },
    {
      "heading": "5. Conclusions",
      "text": [
        "Documents are much more than just a bag of words.",
        "The Poisson distribution predicts that lightning is unlike to strike twice in a single document.",
        "We shouldn't expect to see two or more instances of boycott in the same document (unless there is some sort of hidden dependency that goes beyond the Poisson).",
        "But when it rains, it pours.",
        "If a document is about boycotts, we shouldn't be surprised to find two boycotts or even a half dozen in a single document.",
        "The standard use of the Poisson in modeling the distribution of words and ngrams fails to fit the data except where there are almost no interesting hidden dependencies as in the case of somewhat.",
        "Why are the deviations from Poisson more salient for \"interesting\" words like boycott than for \"boring\" words like somewhat?",
        "Many applications such as information retrieval, text categorization, author identification and word-sense disambiguation attempt to discriminate documents on the basis of certain hidden variables such as topic, author, genre, style, etc.",
        "The more that a keyword (or ngram) deviates from Poisson, the stronger the dependence on hidden variables, and the more useful the keyword (or ngram) is for discriminating documents on the basis of these hidden dependences.",
        "Similar arguments apply in a host of other important applications such as text compression and language modeling for speech recognition where it is desirable for word and ngram probabilities to adapt appropriately to frequency changes due to various hidden dependencies.",
        "We have used document frequency, df, a concept borrowed from Information Retrieval, to find deviations from Poisson behavior.",
        "Document frequency is similar to word frequency, but different in a subtle but crucial way.",
        "Although inverse document frequency (IDF) and log iof are extremely highly",
        "correlated (p = - 0.994), it would be a mistake to try to model one with a simple transform of the other.",
        "Figure 5 showed one such attempt, where f was transformed into a predicted IDF by introducing a",
        "relatively large for the most important keywords, words with moderate frequencies such as Germans.",
        "To get a better look at the subtle differences between document frequency and word frequency, we focused our attention on a set of 53 words that all had approximately the same word frequency in a corpus of 1989 AP stories.",
        "Table 1 showed that words with larger IDF tend to have more content.",
        "boycott, for example, is a better keyword than somewhat because it bunches up into a relatively small set of documents.",
        "Table 2 showed that variance and entropy can also be used as a measure of content (at least among a set of words with more or less the same word frequency).",
        "A good keyword like boycott is farther from Poisson (chance) than a crummy keyword like somewhat by almost any sense of closeness that one might consider, e.g., IDF, variance, entropy.",
        "These crucial deviations from Poisson are robust.",
        "We showed in section 4 that deviations from Poisson in one year of the AP can be used to predict deviations in another year of the AP."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work benefited considerably from extensive discussions with Slava Katz."
      ]
    }
  ]
}
