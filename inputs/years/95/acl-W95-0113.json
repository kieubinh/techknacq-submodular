{
  "info": {
    "authors": [
      "Hsin-Hsi Chen",
      "Yue-Shi Lee"
    ],
    "book": "Very Large Corpora",
    "id": "acl-W95-0113",
    "title": "Development of a Partially Bracketed Corpus With Part-Of-Speech Information Only",
    "url": "https://aclweb.org/anthology/W95-0113",
    "year": 1995
  },
  "references": [
    "acl-A88-1019",
    "acl-C94-2123",
    "acl-H91-1026",
    "acl-H91-1037",
    "acl-J93-1007",
    "acl-P90-1034",
    "acl-P92-1017",
    "acl-P94-1032"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Research based on a treebank is active for many natural language applications.",
        "However, the work to build a large scale treebank is laborious and tedious.",
        "This paper proposes a probabilistic chunker to help the development of a partially bracketed corpus.",
        "The chunker partitions the part-of-speech sequence into segments called chunks.",
        "Rather than using a treebank as our training corpus, a corpus which is tagged with part-of-speech information only is used.",
        "The experimental results show the probabilistic chunker has more than 92% correct rate in outside test.",
        "The well-formed partially bracketed corpus is a milestone in the development of a treebank.",
        "Besides, the simple but effective chunker can also be applied to many natural language applications."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Research based on a treebank, i.e., a corpus annotated with syntactic structures, is active for many natural language applications [1-5].",
        "Framis [1] proposes a methodology to extract selectional restrictions at a variable level of abstraction from the Penn Treebank.",
        "Chen and Chen [2] propose a probabilistic chunker to decide the implicit boundaries of constituents and utilize the linguistic knowledge to extract the noun phrases by a finite state mechanism.",
        "In their study, Susanne Corpus is used as a training corpus for their chunker.",
        "Pocock and Atwell [3] investigate statistical grammars extracted from Spoken English Corpus (SEC), and apply these grammars to find the grammatically optimal path through a word lattice.",
        "The stochastic parsers are also developed in [4,5].",
        "All these applications employ the syntactic information extracted from different treebanks and show the satisfactory results.",
        "However, the work to build a large scale treebank is laborious and tedious.",
        "Very few large-scale treebanks are currently available especially for languages other than English.",
        "In this paper, we propose a probabilistic chunker to help the development of a partially bracketed corpus, i.e., a simpler version of a treebank.",
        "The chunker partitions the part-of-speech sequence into segments called chunks.",
        "Rather than using a treebank as our training corpus, a corpus which is tagged with part-of-speech information only is used.",
        "In the following sections we first introduce the experimental framework of our model.",
        "Lancaster-Oslo/Bergen (LOB) Corpus and Susanne Corpus are adopted.",
        "Then a tag mapper and a probabilistic chunker are described.",
        "Before concluding the experimental results are demonstrated."
      ]
    },
    {
      "heading": "2. Experimental Framework",
      "text": [
        "Because the probabilistic chunker proposed in this paper is based on syntactic tags (parts of speech), a part-of-speech tagger is needed.",
        "A word sequence W is input to the part-of-speech tagger and a part-of",
        "speech sequence P is generated.",
        "The output of the tagger is the input of the chunker.",
        "The probabilistic chunker partitions P into C, i.e., a sequence of chunks.",
        "Each chunk contains one or more parts of speech.",
        "Consider the example \"Attorneys for the mayor said that an amicable property settlement has been agreed upon .\".",
        "This 15-word sentence is input to the part-of-speech tagger and a part-of-speech sequence \"NNS IN ATI NPT VBD CS AT JJ NN NN HVZ BEN VBN IN .\" is generated.",
        "The probabilistic chunker then partitions this sequence into several chunks.",
        "The chunked result is shown as follows.",
        "[ NNS][ IN ATI NPT ] [ VBD ] [ CS ] [ AT ] [ JJ NN NN ] [ HVZ BEN ] [ VBN ] [ ] [ . ]",
        "However, the performance evaluation of the chunker is a sticky work.",
        "To evaluate the performance of the chunker, Susanne Corpus, which is a modified and condensed version of Brown Corpus, is adopted.",
        "But, the tagging sets [6,7] of LOB Corpus and Susanne Corpus are different.",
        "The latter has finer tags than the former.",
        "Thus, a tag mapper is introduced in the experimental framework shown as Figure 1.",
        "hi our experiments, the test sentence Ps comes from Susanne Corpus.",
        "It is a part-of-speech sequence.",
        "The corresponding syntactic structure T is regarded as an evaluation criterion for the probabilistic chunker.",
        "It is sent to the performance evaluation model.",
        "The tag mapper in this figure is used to transform the Susanne part-of-speech into LOB part-of-speech.",
        "Through the tag mapper, Ps is converted into P1.",
        "Then, PI is input to the probabilistic chunker and a chunk sequence C is produced.",
        "Finally, the performance evaluation model reports the evaluation results according to C and T."
      ]
    },
    {
      "heading": "3. A Tag Mapper",
      "text": [
        "The tagging set of Susanne Corpus is extended and modified from LOB Corpus.",
        "They have 424 and 153 tags, respectively.",
        "To map a Susanne tag into a LOB tag manually is a tedious work.",
        "Thus, an automatic tag mapping algorithm is provided.",
        "By our investigation, we found that words are good clues to relate these two tagging sets.",
        "Therefore, the first step in automatic tag mapping is to collect words from Susanne Corpus for each Susanne tag.",
        "Table 1 lists some examples.",
        "Column three in Table 1 denotes the correct mapping to LOB tags.",
        "The second step is to find the corresponding LOB tags from LOB Corpus for each word collected at the first step.",
        "Table 2 shows the sample results.",
        "Those words which cannot be found in LOB Corpus are removed.",
        "Symbol * denotes that all the words cannot be found in LOB Corpus.",
        "The third step is to find the corresponding LOB tag for each Susanne tag.",
        "For each Susanne tag, the frequency of LOB tags is calculated and the most frequent LOB tag is regarded as the result.",
        "For example, LOB tags NN and NNS in row three of Table 2 appear three and one times, respectively.",
        "Thus, Susanne tag NN1ux is mapped to LOB tag NN.",
        "After examining all the Susanne tags by these three steps, three cases have to be considered:",
        "(1) Unique Tag.",
        "Only one LOB tag remains.",
        "(2) Multiple Tags.",
        "More than one LOB tags remain.",
        "(3) No Match.",
        "When all the words extracted from Susanne Corpus for a Susanne tag cannot be found in LOB Corpus, the Susanne tag is mapped to \"No Match\".",
        "Some of these words are characteristic words such as YTL1.",
        "The experimental results are shown in Table 3.",
        "In Table 3, \"Include\" denotes that the correct tag belongs to the remaining multiple tags and Exclude\" denotes that the correct tag is not included in the remaining tags.",
        "Note that the ditto tags are not considered in this experiment.",
        "This is because the mapping for ditto tags can be obtained by human easily.",
        "Therefore, only 310 Susanne tags are resolved in this experiment.",
        "The experimental results show that the number of multiple tags is large.",
        "Thus, two heuristic rules are introduced to reduce the number of multiple tags.",
        "First, those LOB tags which are similar to Susanne tag are selected.",
        "For example, Susanne tag NNJ2 can be mapped to LOB tags NNS or VBZ in the above experiment.",
        "NNS has two common characters with NNJ2, so that SUsanne tag NNJ2 is mapped to LOB tag NNS.",
        "Under this heuristic rule, the experimental results are shown in Table 4.",
        "Next, let us consider an example.",
        "Susanne tag IW can be mapped to LOB tags IN or RI in the above experiment.",
        "Thus, the first heuristic rule has no effects.",
        "We examine the tag mapping for the preceding and subsequent three tags of IW.",
        "They are listed as follows.",
        "(-1) Susanne Tag lit is mapped to LOB Tag IN.",
        "(-2) Susanne Tag IIx is mapped to LOB Tag IN.",
        "(-3) Susanne Tag 10 is mapped to LOB Tag IN.",
        "(**) Susanne Tag IW is mapped to LOB Tag IN RI.",
        "(+2) Susanne Tag JB is mapped to LOB Tag JJ.",
        "(+3) Susanne Tag JBo is mapped to LOB Tag AP.",
        "Note that only tags which have the same first character as IW are considered, that is, only (-1), (-2) and (-3) are considered.",
        "In these three mappings, LOB tag IN is the most frequent and the only one mapping, and IN is a candidate for IW.",
        "Thus, Susanne tag IW is mapped to LOB tag IN.",
        "The above procedure forms the second heuristic rule.",
        "The experimental results after applying two heuristic rules are shown as follows.",
        "Three tags - say, FA, FB and GO, must be treated in particular.",
        "For example, Susanne Corpus tags genitive case noun as [John NP 's_GG], but LOB Corpus tags it as [Jolm's_PN$].",
        "Two Susanne tags may be mapped into one LOB tag.",
        "Ignoring these three special tags, only nineteen Susanne tags have wrong mapping in Unique-Tag case."
      ]
    },
    {
      "heading": "4. A Probabilistic Chunker",
      "text": [
        "Gale and Church [8] propose 02, a x2-like statistic, to measure the association between two words.",
        "Table 6 illustrates a twO-by-two contingency table for words wi and w2.",
        "Word wi Word w2 a b c d Cell a counts the number of sentences that contain both wi and w2.",
        "Cell b (c) counts the number of sentences that contain w2 (wi) but not w1 (w2).",
        "Cell d counts the number of sentences that does not contain both w1 and w2.",
        "That is, if N is the total number of sentences, d=N-a-b-c. Based on this contingency table, 4)2 is defined as follows:",
        "d=N-a-b-c where pi denotes part-of-speech i, F(p1,p2) is the frequency of which p2 follows pl, F(p 1) and F(p2) are the frequencies of pi and p2, and N is the corpus size in terms of the number of words in training corpus.",
        "Based on this definition and 4)2 measure, consider the sentence \"The Fulton County Grand Jury said Friday an investigation ...\", which has tag sequence \"ATI NP NPL JJ NN VBD NR AT NN ...\".",
        "Its syntactic structure for the first seven words is shown in Figure 2.",
        "The (1)2 distribution for these parts of speech is shown in Figure 3.",
        "Position i (x axis) is the location between parts of speech pi and pi+ i.",
        "Figure 3 shows that there are four local minimal positions, i.e., positions 1, 3, 5 and 6.",
        "They can be regarded as the ,boundaries of chunks.",
        "That is, ATI and NP belong to different chunks.",
        "Similarly, (NPL and II), (NN and VND) and (VND and NR) have the same situation.",
        "Let us discuss these concepts formally.",
        "For aprobabilistic chunker, the generalized contingency table is defined as follows."
      ]
    },
    {
      "heading": "Definition 2: (For Two Chunks)",
      "text": [
        "a=--F(c ,c2) b=F(c2)-F(c ,c2) c=F(c 1)-F(c ,c2) where ci denotes chunk i, F(ci,c2) is the frequency of which c2 follows ci, F(c1) and F(c2) are the frequencies of ci and c2, and N is the corpus size in terms of the number of words in training corpus.",
        "Let the tag sequence P be pi, p2, pn.",
        "Assume there are two possible chunked results.",
        "The first is composed of two chunks, i.e., [PI, P2, •••, Pi] and [1i+1, Pi+2, Pn], and is regarded as a correct result.",
        "The second is also composed of two chunks, i.e., [pi, p2, pi_i] and [pi, p1+1, ..., pa], but is regarded as a wrong result.",
        "Since [pi, p2, •.., pi] is a chunk, [pi, p2, pi_i] is very likely to be followed by pi.",
        "In other words,",
        "The above derivation tells us: the local minimums of the +2 distribution denote plausible boundaries of two chunks.",
        "To simplify Definition 2, Definitions 3 and 4 are formulated.",
        "where pi denotes part-of-speech i, Fapi],[p1+1]) is the frequency of which pi±i follows pi, Fapip and F([pi+1]) are the frequencies of pi and Pi+i, and N is the corpus size in terms of the number of words in training corpus.",
        "It is clear that Definition 3 is the same as Definition 1.",
        "Based on Definitions 3, the probabilistic chunker is presented as follows.",
        "Note that N is the length of the tag sequence and the last chunk is always a one-tag chunk (punctuation).",
        "d=N-a-b-c where pi denotes part-of-speech i, pi+1],[pi+2]) is the frequency of which pi+1,pi+2 follows pi, F([pi, pi+d) and F([pi+21) are the frequencies of (pi, pi+i) and pi+2, and N is the corpus size in terms of the number of words in training corpus."
      ]
    },
    {
      "heading": "Right Chunk",
      "text": [
        "d=N-a-b-c where pi denotes part-of-speech i, Fapillpi+1, pi+2]) is the frequency of which pi+1,pi+2 follows pi, Fapip and F([Pi+1, Pi+21) are the frequencies of pi and (Pi+1, Pi+2), and N is the corpus size in terms of the number of words in training corpus."
      ]
    },
    {
      "heading": "End",
      "text": [
        "Probabilistic chunker based on Definition 3 concerns the 4)2 distribution between two parts of speech.",
        "For each while loop, probabilistic chunker based on Definition 4 processes three parts of speech and concerns the 4)2 distribution between them."
      ]
    },
    {
      "heading": "5. Experimental Results",
      "text": [
        "LOB Corpus, which is a million-word collection of present-day British English texts, is adopted as the source of training data.",
        "Susanne Corpus is adopted as the source of testing data for evaluating the performance of our probabilistic chunker.",
        "This corpus contains one tenth of Brown Corpus, but involves more syntactic and semantic information than Brown Corpus.",
        "For evaluating the performance, a criterion [2], i.e., the content of each chunk should be dominated by one non-terminal node in Susanne parse field, is adopted.",
        "The performance evaluation model compares the chunked result C with the corresponding syntactic structure T. According to this criterion, the experimental results for Definitions 3 and 4 are shown in Table 7 as follows.",
        "One-tag chunks cover about 50%.",
        "We further analyze what grammatical components constitute the one-tag chunks and find that most of the one-tag chunks contain punctuation marks, nouns and verbs.",
        "This is because proper name forms the bare subject or object.",
        "Verb is presented in the form of third person and singular, past tense, or base form.",
        "These three cases form about 62% of one-tag chunks.",
        "By analyzing the error chunked results, we find that many errors result from conjunctions.",
        "Besides, some tags cannot be located at the end of the chunks.",
        "Therefore, the heuristic rule is applied to improve the performance.",
        "The tags that cannot be located at the end of chunks are listed as follows:",
        "Applying this heuristic rule, the experimental results are listed in Table 10.",
        "It shows the usefulness of the heuristic rule.",
        "The performance increases about 10%."
      ]
    },
    {
      "heading": "6. Concluding Remarks",
      "text": [
        "To process real text is indispensable for a practical natural language system.",
        "Probabilistic method provides a robust way to tackle with the unrestricted text.",
        "This paper proposes a probabilistic chunker to help the development of a partially bracketed corpus.",
        "Rather than using a treebank as our training corpus, LOB Corpus which is tagged with part-of-speech information only is used.",
        "The experimental results show the probabilistic chunker has more than 92% correct rate in outside test.",
        "The well-formed partially bracketed corpus is a milestone in the development of a treebank.",
        "In addition, the simple but effective chunker can also be applied to many natural language applications such as extracting the predicate-argument structures [9,10], grouping words [11] and gathering collocation [12].",
        "The evaluation criterion adopted in this paper is not very strict.",
        "Under a strict criterion, the method proposed in this paper may not be suitable for short-fat trees.",
        "That is, it is suitable for tall-thin trees.",
        "To solve this problem, a more general definition which considers more parts of speech in contingency table is needed.",
        "However, that introduces another problem: the more the general definitions we use, the larger the tagged corpus we need.",
        "This paper also presents a tag mapper.",
        "It sets up the mapping between different tagging sets.",
        "Such an algorithm facilitates the development of a large-scale tagged corpus from different sources.",
        "By the way, much more reliable statistic information can be trained from the large-scale tagged corpus, so that the feasibility of the chunker is assured.",
        "Besides the above problem, the critical points for local minimum are not obvious in some cases.",
        "Thus their determination is also demanded in the future."
      ]
    }
  ]
}
