{
  "info": {
    "authors": [
      "Hang Li",
      "Naoki Abe"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C96-1003",
    "title": "Clustering Words With the MDL Principle",
    "url": "https://aclweb.org/anthology/C96-1003",
    "year": 1996
  },
  "references": [
    "acl-H90-1056",
    "acl-J92-4003",
    "acl-J93-2004",
    "acl-P90-1034",
    "acl-P91-1030",
    "acl-P93-1022",
    "acl-P93-1024"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We address the problem of automatically constructing a thesaurus by clustering words based on corpus data.",
        "We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs, and propose a learning algorithm based on the Minimum Description Length (MDL) Principle for such estimation.",
        "We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering, and found that the former outperforms the latter.",
        "We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus.",
        "Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Recently various methods for automatically constructing a thesaurus (hierarchically clustering words) based on corpus data have been proposed (Hindle, 1990; Brown et al., 1992; Pereira et al., 1993; Tokunaga et al., 1995).",
        "The realization of such an automatic construction method would make it possible to a) save the cost of constructing a thesaurus by hand, b) do away with subjectivity inherent in a hand made thesaurus, and c) make it easier to adapt a natural language processing system to a new domain.",
        "In this paper, we propose a new method for automatic construction of thesauri.",
        "Specifically, we view the problem of automatically clustering words as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns (in general, any set of words) and a partition of a set of verbs (in general, any set of words), and propose an estimation"
      ]
    },
    {
      "heading": "*Real World Computing Partership",
      "text": [
        "algorithm using simulated annealing with an energy function based on the Minimum Description Length (MDL) Principle.",
        "The MDL Principle is a well-motivated and theoretically sound principle for data compression and estimation in information theory and statistics.",
        "As a method of statistical estimation MDL is guaranteed to be near optimal.",
        "We empirically evaluated the effectiveness of our method.",
        "In particular, we compared the performance of an MDL-based simulated annealing algorithm in hierarchical word clustering against that of one based on the Maximum Likelihood Estimator (MLE, for short).",
        "We found that the MDL-based method performs better than the MLE-based method.",
        "We also evaluated our method by conducting pp-attachment disambiguation experiments using a thesaurus automatically constructed by it and found that disambiguation results can be improved.",
        "Since some words never occur in a corpus, and thus cannot be reliably classified by a method solely based on corpus data, we propose to combine the use of an automatically constructed thesaurus and a hand made thesaurus in disambiguation.",
        "We conducted some experiments in order to test the effectiveness of this strategy.",
        "Our experimental results indicate that combining an automatically constructed thesaurus and a hand made thesaurus widens the coverage' of our disambiguation method, while maintaining high accuracy'."
      ]
    },
    {
      "heading": "2 The Problem Setting",
      "text": [
        "A method of constructing a thesaurus based on corpus data usually consists of the following three steps: (i) Extract co-occurrence data (e.g. case frame data, adjacency data) from a corpus, (ii) Starting from a single class (or each word composing its own class), divide (or merge) word classes 1`Coverage' refers to the proportion (in percentage) of test data for which the disambiguation method can make a decision.",
        "2`Accuracy' refers to the success rate, given that the disambiguation method makes a decision.",
        "based on the co-occurrence data.",
        "using some similarity (distance) measure.",
        "(The former approach is called 'divisive', the latter 'agglomerative'.)",
        "(iii) Repeat step (ii) until some stopping condition is met, to construct a thesaurus (tree).",
        "The method we propose here consists of the same three steps.",
        "Suppose available to us are frequency data (co-occurrence data) between verbs and their case slot, values extracted from a corpus (step (i)).",
        "We then view the problem of clustering words as that of estimating a probabilistic model (representing a. probability distribution) that generates such data.",
        ".",
        "We assume that the target model can be defined in the following way.",
        "First, we define a noun partition 'PA' over a given set of nouns ,V and a, verb partion 'Pv over a given set of verbs V. A. noun partition is any set PA, satisfying PA, C 21r, UciEp„Ci.",
        "Ar and VC!,:, C!,) E = 0.",
        "A verb partition Pv is defined analogously.",
        "In this paper, we call a member of a noun partition 'a noun cluster', and a member of a verb partition a 'verb cluster'.",
        "We refer to a member of the Cartesian product of a noun partition and a verb partition ( E l'Ar X Tv ) simply as 'a cluster•.",
        "We then define a probabilistic model (a joint distribution), written P(C„, Cu), where random variable assumes a value from a fixed noun partition Pjr, and Cu a. value from a fixed verb partition Pv.",
        "Within a given cluster, we assume that each element is generated with equal probability, i.e.,",
        "In tins paper, we assume that the observed data, are generated by a model belonging to the class of models just described, and select a model which best explains the data.",
        "As a result of this, we obtain both noun clusters and verb clusters.",
        "This problem setting is based on the intuitive assumption that, similar words occur in the same context, with roughly equal likelihood, as is made explicit, in equation ( t ).",
        "Thus selecting a model \\vhich best, explains the given data is equivalent to finding the most appropriate classification of words based on their co-occurrence."
      ]
    },
    {
      "heading": "3 Clustering with MDL",
      "text": [
        "We now turn to the question of what, strategy (or criterion) we should employ for estimating the best model.",
        "Our choice is the MDL (Minimum Description Length) principle (Rissanen, 1989), a well-known principle of data compression and statistical estimation from information theory.",
        "MDL stipulates that the best probability model for given data is that model which requires the least code length for encoding of the model itself, as well as the given data relative to it'.",
        "We refer to the code length for the model We refer the interested reader to (Li and Abe, 1995) for explanation of rationals behind using the as 'the model description length' and that for the data 'the data description length.' We apply MDL to the problem of estimating a model consisting of a pair of partitions as described above.",
        "In tins context, a model with less clusters tends to be simpler (in terms of the number of parameters), but also tends to have a poorer fit to the data.",
        "In contrast, a model with more clusters is more complex, but tends to have a better fit to the data.",
        "Thus, there is a trade-off relationship between the simplicity of a model and the goodness of fit to the data.",
        "The model description length quantifies the simplicity (complexity) of a. model, and the data description length quantifies the fit to the data.",
        "According to MDL, the model winch minimizes the slim total of the two types of description lengths should be selected.",
        "In what follows, we will describe in detail how the description length is to be calculated in our current context, as well as our simulated annealing algorithm based on MDL."
      ]
    },
    {
      "heading": "3.1 Calculating Description Length",
      "text": [
        "We will now describe how the description length for a model is calculated.",
        "Pecan that each model is specified by the (,artesian product of a partition of nouns and a partition of verbs, and a number of parameters for thew.",
        "here we let k„ denote the size of the noun partition, and k,„ the size of the verb partition.",
        "Then, there are k„ • - 1 free parameters in a model.",
        "Given a model Al and data .8', its total description length L(1.1)4 is computed as the sum of the model description length 1,1„d(A1), the description length of its parameters Ti„„.",
        "(Af), and data description length Ldat(Al).",
        "(We often refer to Lmod(-A1) -11 'Par ( ) as the model description",
        "We employ the 'binary noun clustering method', in which k,, is fixed at 11)1 and we are to decide whether k„ = 1 or k„ = 2, which is then to be applied recursively to the clusters thus obtained.",
        "This is as if we view the nouns as entities and the verbs as features and cluster the entities based on their features.",
        "Since there are 2I 'I subsets of the set of nouns and for each 'binary' noun partition we have two different subsets (a special case of which is when one subset is Ar and the other the empty set 0), the number of possible binary noun partitions is 201/2 = 21-^11-1-.",
        "Thus for each binary noun partition we need log 21Ar1-1 IA' – 1 bitsbits''II to describe it.'Thence Lmod(111) is calculated MDL principle in natural language processing.. 1L(ItT) depends on .5', but.",
        "we will leave S implicit.",
        "Throughout the paper 'log' denotes the logarithm to the base 2.",
        "6 For further explanation, see (Quinlan and Ilivest, 1989).",
        "as'",
        "where !SI denotes the input data size, and k„ • k„ – 1 is the number of (free) parameters in the model.",
        "It is known that using log-07 51= l'gr,151 bits to describe each of the parameters will (approximately) minimize the description length (Rissanen, 1.989).",
        "Finally, L dat (AI)",
        "where f (a, v) denotes the observed frequency of the noun verb pair (a., v), and P(11, v) the estimated probability of (a , a), which is calculated as follows.",
        "where f(C,,,C„) denotes the observed frequency of the noun verb pairs belonging to cluster With the description length of a. model defined in the above manner, we wish to select a. model having the minimum description length and output it as the result of clustering.",
        "Since the model description length L„,,d is the same for each model, in practice we only need to calculate and compare OM) = Lp„,.",
        "(11/1 ) Ld„,(M)."
      ]
    },
    {
      "heading": "3.2 A Simulated Annealing-based Algorithm",
      "text": [
        "We could in principle calculate the description length for each model and select a model with the minimum description length, if computation time were of no concern.",
        "However, since the number of probabilistic models under consideration is super exponential, this is not feasible in practice.",
        "We employ the 'simulated annealing technique' to deal with this problem.",
        "Figure 1 shows our (divisive) clustering algorithms."
      ]
    },
    {
      "heading": "4 Advantages of Our Method",
      "text": [
        "In this section, we elaborate on the merits of our method.",
        "In statistical natural language processing, usually the number of parameters in a probabilistic 'The exact formulation of Lmod(M) is subjective, and it depends on the exact coding scheme used for the description of the models.",
        "sAs we noted earlier, an alternative would be to employ an agglomerative algorithm.",
        "model to be estimated is very large, and therefore such a model is difficult to estimate with a reasonable data size that is available in practice.",
        "(This problem is usually referred to as the 'data sparseness problem'.)",
        "We could smooth the estimated probabilities using an existing smoothing technique (e.g., (Dagan et al., 1992; Gale and Church, 1990)), then calculate some similarity measure using the smoothed probabilities, and then cluster words according to it.",
        "There is no guarantee, however, that the employed smoothing method is in any way consistent with the clustering method used subsequently.",
        "Our method based on MDL resolves this issue in a unified fashion.",
        "By employing models that embody the assumption that words belonging to a same class occur in the same context with equal likelihood, our method achieves the smoothing effect as a side effect of the clustering process, where the domains of smoothing coincide with the classes obtained by clustering.",
        "Thus, the coarseness or fineness of clustering also determines the degree of smoothing.",
        "All of these effects fall out naturally as a corollary of the imperative of 'best possible estimation', the original motivation behind the MDL principle.",
        "In our simulated annealing algorithm, we could alternatively employ the Maximum Likelihood Estimator (MLE) as criterion for the best probabilistic model, instead of MDL.",
        "MLE, as its name suggests, selects a. model which maximizes the likelihood of the data, that is, P arg maxp FLE,5 P(x).",
        "This is equivalent to minimizing the 'data description length' as defined in Section 3, i.e. P arg mine „s – log P(x).",
        "We can see easily that MDL generalizes MLE, in that it also takes into account the complexity of the model itself.",
        "In the presence of models with varying complexity, MLE tends to overfit the data, and output a model that is too complex and tailored to fit the specifics of the input data.",
        "If we employ MLE as criterion in our simulated annealing algorithm, it will result in selecting a. very fine model with many small clusters, most of which will have probabilities estimated as zero.",
        "Thus, in contrast to employing MDL, it will not have the effect of smoothing at all.",
        "Purely as a method of estimation as well, the superiority of MDL over MLE is supported by convincing theoretical findings (c.f.",
        "(Barron and Cover, 1991; Yamanishi, 1992)).",
        "For instance, the speed of convergence of the models selected by MDL to the true model is known to be near optimal.",
        "(The models selected by MDL converge to the true model approximately at the rate of 11s where s is the number of parameters in the true model, whereas for MLE the rate is 111, where t is the size of the domain, or in our context, the total number of elements of .AT x V.) 'Consistency' is another desirable property of MDL, which is not shared by MLE.",
        "That is, the number of parame-is calculated by"
      ]
    },
    {
      "heading": "Algorithm: Clustering",
      "text": [
        "1.",
        "Divide the noun set N into two subsets.",
        "Define a probabilistic model consisting of the partition of nouns specified by the two subsets and the entire set of verbs.",
        "2. do { 2.1 Randomly select one nonn, remove it from the subset it belongs to and add it to the other.",
        "2.2 Calculate the description length for the two models (before and after the move) as Li and L9, respectively."
      ]
    },
    {
      "heading": "2.3 Viewing the description length as the energy function for annealing, let AL = L2 – ",
      "text": [
        "If AL < 0, fix the move, otherwise ascertain the move with probability P = exp( – A L 111. }",
        "while (the description length has decreased during the past 10 • trials.)",
        "here T is the annealing temperature whose initial value is 1 and updated to be 0.91' after 10 • trials.",
        "3.",
        "If one of the obtained subset is empty, then return the non-empty subset., otherwise recursively apply Clustering on both of the two subsets.",
        "tors in the models selected by MD.1, converge to that of the true model (Rissanen, 1989).",
        "Both of these properties of MDT, are empirically verified in our present context, as will be shown in the next section.",
        "In particular, we have compared the performance of employing an MDL-based simulated annealing against that of one based on 1l1,1;in hierarchical word clustering."
      ]
    },
    {
      "heading": "5 Experimental Results",
      "text": [
        "it he company they we i one market year investor official hank sale loss rate price stock share billion million",
        "We describe our experimental results in this section."
      ]
    },
    {
      "heading": "5.1 Experiment 1: MDL v.s. MLE",
      "text": [
        "We compared the performance of employing MDI, as a criterion in oar simulated annealing algo-rithni, against that of employing MLE by simulation experiments.",
        "We artificially constructed a true model of word co-occurrence, and then generated data according to its distribution.",
        "We then used the data to estimate a model (clustering words), and measured the KL distance') between",
        "the true model and the estimated model.",
        "(The algorithm used for MI,E was the same as that shown in Figure 1, except the 'data description length' replaces the (total) description length' in Step 2.)",
        "Figure 3(a) plots the number of obtained noun (lust em's (leaf' nodes in the obtained thesaurus tree) versus the input data size, averaged over 10 trials.",
        "(The number of noun clusters in the true model is 4.)",
        "Figure 3(h) plots the KL distance versus the data size, also averaged over the same 10 trials.",
        "The results indicate that M DI, converges to the true model faster than MLE,.",
        "Also, MI,E, tends to select, a model overfitting the data, while MIA, tends to select a model which is simple and yet fits the data reasonably well."
      ]
    },
    {
      "heading": "5.2 Experiment 2: Qualitative Evaluation",
      "text": [
        "We extracted roughly 180,000 case frames from the bracketed WSJ (Wall Street Journal) corpus of the Penn Tree Bank (Marcus et al., 1993) as co-occurrence data, We then constructed a number of thesauri based on these data, using on r method.",
        "Figure 2 shows an example thesaurus for the 20 most frequently occurred nouns in the data, constructed based on their appearances as subject and object of roughly 2000 verbs, The obtained thesaurus seems to agree with human intuition to some degree.",
        "For example, 'million' and 'billion' are classified in one noun cluster, and `stock' and 'share' are classified together.",
        "Not, all of the noun clusters, however, seem to he meaningful in the useful sense.",
        "This is probably because the data size we had was not large enough.",
        "Pragmatically speaking, however, whether the obtained thesaurus agrees with our intuition in itself' is only of secondary concern, since the main pose is to use the constructed thesaurus to help improve on a disambiguation task.",
        "((lover and Thontas, 1991) .",
        "l is always non-negative and is zero Hi the two distributions are identical."
      ]
    },
    {
      "heading": "5.3 Experiment 3: Disambiguation",
      "text": [
        "We also evaluated our method by using a. constructed thesaurus in a pp-attachment disambiguation experiment.",
        "We used as training data.",
        "the same 180, 000 case frames in Experiment 1.",
        "We also extracted as our test data.",
        "172 (verb, no unl, prep, n 11011179 ) patterns from the data.",
        "in the same corpus, which is not used in the training data.",
        "For the 150 words that appear in the position of noun 2 in the test data, we constructed a thesaurus based on the co-occurrences between heads and slot values of the frames in the training data.",
        "This is because in our disambiguation test we only need a. thesaurus consisting of these 150 words.",
        "We then applied the learning method proposed in (Li and Abe, 1995) to learn case frame patterns with the constructed thesaurus as input using the same training data.",
        "That is, we used it to learn the conditional distributions P (C las si verb, pre p) , P(Class2Inouni, prep), where Classi and C las s2 vary over the internal nodes in a certain 'cut' in the thesaurus tree 10.",
        "We then compare",
        "which are estimated based on the case frame patterns, to determine the attachment site of (prep, noun2).",
        "More specifically, if the former is larger than the latter, we attach it to verb, and if the latter is larger than the former, we attach it to noun , and otherwise (including when both are 10Each 'cut' in a thesaurus tree defines a different noun partition.",
        "See (Li and Abe, 1995) for details.",
        "0), we conclude that we cannot make a decision.",
        "Table 1 shows the results of our pp-attachment disambiguation experiment in terms of 'coverage' and 'accuracy.' Here 'coverage' refers to the proportion (in percentage) of the test patterns on which the disambiguation method could make a decision.",
        "'Base Line' refers to the method of always attaching (prep, noun2) to nouns.",
        "'WordBased', `MLE-Thesaurus', and `MDL-Thesaurus' respectively stand for using word-based estimates, using a thesaurus constructed by employing MLE, and using a thesaurus constructed by our method.",
        "Note that the coverage of 'MDL-Thesaurus' significantly outperformed that of 'Word-Based', while basically maintaining high accuracy (though it drops somewhat), indicating that using an automatically constructed thesaurus can improve disambiguation results in terms of coverage.",
        "We also tested the method proposed in (Li and Abe, 1995) of learning case frames patterns using an existing thesaurus.",
        "In particular, we used this method with WordNet (Miller et al., 1993) and using the same training data, and then conducted pp-attachment disambiguation experiment using the obtained case frame patterns.",
        "We show the result of this experiment as `WordNet' in Table 1.",
        "We can see that in terms of `coverage', 'WordNer outperforms `MDL-Thesaurus', but in terms of 'accuracy', 'MDL-Thesaurus' outperforms 'WordNet'.",
        "These results can be interpreted as follows.",
        "An automatically constructed thesaurus is more domain dependent and captures the domain dependent features better, and thus using it achieves high accuracy.",
        "On the other hand, since training data, we had available is insufficient, its coverage is smaller than that of a hand made thesaurus.",
        "In practice, it makes sense to combine both types of thesauri.",
        "More specifically, an automatically constructed thesaurus can be used within its coverage, and outside its coverage, A hand made thesaurus can be used.",
        "Given the current state of the word clustering technique (namely, it requires data size that is usually not available, and it tends to be computationally demanding), this strategy is practical.",
        "We show the result of this combined",
        "method as `M1)1,-Thesaurus WordNet' in Table 2.",
        "Onr experimental result shows that (im-ploying the combined method does increase the coverage of disambiguation.",
        "We also tested AID1,-Thesaurus 4 WordNet 1 LA 4 Default', which stands for using the learned thesaurus and WordNet first, then the lexical association value proposed by (Hindle and Rooth, 1991), and finally the default (i.e. always attaching prep, noun, to nounfl.",
        "Our best disambiguation result obtained using this last combined method somewhat improves the accuracy reported in (Li and Abe, 1.995) (84.3%)."
      ]
    },
    {
      "heading": "6 Concluding Remarks",
      "text": [
        "We have proposed a method of hierarchical clustering of words based on large corpus data.",
        "We conclude with the following remarks.",
        "thesaurus and a hand made thesaurus for disambiguation purpose.",
        "The disambiguation accuracy obtained this way was 85.5%.",
        "In the future, hopefully with larger training data size, we plan to construct larger thesauri as well as to test, other clustering algorithms."
      ]
    }
  ]
}
