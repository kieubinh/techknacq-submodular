{
  "info": {
    "authors": [
      "Chinatsu Aone",
      "Kevin Hausman"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C96-1011",
    "title": "Unsupervised Learning of a Rule-Based Spanish Part of Speech Tagger",
    "url": "https://aclweb.org/anthology/C96-1011",
    "year": 1996
  },
  "references": [
    "acl-A92-1018",
    "acl-E95-1021",
    "acl-J94-2001",
    "acl-W95-0101"
  ],
  "sections": [
    {
      "text": [
        "Systems Research and Applications Corporation (SRA) /1300 rair Lakes Court l'airfax, VA 22033 aonecOsra.com, hatisman(Osra.com Abstract; This paper describes a Spanish fart-of.",
        "Speech (POS) tagger which applies and extends Brill's algorithm for unsupervised learning of rule-based taggers (Brill, 1995).",
        "First, we discuss Our general approach including extensions we made to the algorithm in order to handle unknown words and parameterize learning and tagging options.",
        "Next, we report and analyze our experimental results using different parameters.",
        "Then, We describe our \"hybrid\" approach which was necessary in order to overcome a fundamental limitation in Brill's original algorithm.",
        "Finally, we compare our tagger with Hidden Markov Model (IIMM)based taggers."
      ]
    },
    {
      "heading": "t Introduction",
      "text": [
        "We have developed a Spanish fart-of-Speech (1)0S) Tagger which applies and extends Brill's algorithm for unsupervised learning (Brill, 1995) Lo create a set of rules that reduce the ambiguity of POS tags on words.",
        "We have chosen an unsupervised learning algorithm because it does not require a large 1)0S-tagged training corpus.",
        "Since there was no FOS-tagged Spanish corpus available to us and since creating a large hand-tagged corpus is both costly and prone to inconsistency, the decision was also a practical one.",
        "We have decided to develop a rule-based tagger because such a tagger learns a set of declarative rules and also because we wanted to compare it with Hidden Markov Model (IIMM)-based taggers.",
        "We extended algorithm in several ways.",
        "First, we extended it to handle unknown words in the training and test texts.",
        "Second, we parameterized learning and tagging options.",
        "Finally, we experimented with a \"hybrid\" solution, where we used a very small number of hand-disambiguated texts during training to overcome a fundamental limitation in the learning algorithm."
      ]
    },
    {
      "heading": "2 Coniponents",
      "text": [
        "Our Spanish POS tagger consists of three components: the Initial State Annotator, the Learner, and the Rule Tagger, each of which is described below."
      ]
    },
    {
      "heading": "2.1 Initial State Annotator",
      "text": [
        "'Phis component is used to assign all possible POS tags to a given Spanish word.",
        "It consists of lexicon lookup, morphological analysis, and unknown word handling.",
        "The Spanish l'OS tag s(1, used in this work consists of the following tags: A D.1.",
        "ADV, (forte of ser or rstar), CLOCK-TIME, COLON, COMMA, CONJ, HATE, HET.",
        "HAVE (form of haber) , HYPHEN, I, ETT ER, 1,PAREN, MODEL, MUINIPLIER, N, NUMBH, I', PERIOD, PRO, PROPN, (PAS-MARK, QUOTE, HOMAN, RPAREN, SEMICOLON, SI,ASII, SUHCON.1, SUFFIX, THERE (ltaccr used in \"there\" constructions), WHDET (curd WANT' (que'), WII PP (dOnde), and V (See Table 3)."
      ]
    },
    {
      "heading": "2.1.1 Lexicon Lookup and Morphological Analysis",
      "text": [
        "Unlike Brill's English tagger experiment.",
        "described in (Brill, 1995), no large POS-tagged Spanish corpus was available to us from which a large lexicon can he derived.",
        "As a result, we decided to parse the on-line Collins Spanish-English Dictionary', and derived a large lexicon from it.",
        "(about 15,000 entries).",
        "We used only the open-class entries from this lexicon, and then augmented it.",
        "with irregular verb forms and a number of' closed-class words.",
        "Our morphological analyzer uses a set of rewrite rules to strip off and/or modify word endings to find root forms of words."
      ]
    },
    {
      "heading": "2.1.2 Unknown Word Handling",
      "text": [
        "Since the lexicon and morphological analysis will not cover every single word that can appear in a text, an attempt.",
        "is made at.",
        "this stage to classify Unknown words.",
        "Any word which did not get.",
        "assigned one or more parts-of-speech in I We have obtained a license to the dictionary.",
        "the lookup/morphology phase is examined for certain traits often indicative of particular parts-of-speech.",
        "This task is similar to what was done by the guessers for the HMM-based French and German taggers (Chanod and Tapanainen, 1995; Feldweg, 1995).",
        "For example, words ending in the letters \"mente\" are assigned the tag of ADV (adverb).",
        "Those words ending in \"ando\" or \"endo\" are assigned the tag V-CONTINUOUS-NIL (continuous form of the verb).",
        "Table 1 shows a list of unknown word handling rules.",
        "Performing these simple checks reduces the number of unknowns in our test set of 17,639 words from 737 (4.2%) to 158 (0.9%).",
        "The remaining unknowns are assigned a set of ambiguous open-class tags of N, V, ADJ, and ADV so that they can be disambiguated by the Learner."
      ]
    },
    {
      "heading": "2.2 Learner",
      "text": [
        "The Learner takes as input ambiguously tagged texts produced by the Initial State Annotator, and tries to learn a set of rules that will reduce the ambiguity of the tags.",
        "Output is a file of rules in the following form:",
        "context is one of PREVWORD, NEXTWORD, PREVTAG or NEXTTAG,2 C is a word or tag, , Pn are the ambiguous parts-of-speech to be reduced, Pi is the part-of-speech that replaces",
        "Here are some examples taken from the actual learned rules:",
        "• NEXTWORD = DE : PIN N • PREVWORD .= EN : DETIADV --> DET • PREVTAG = DET : VIN N • NEXTTAG = SUBCONJ : BEIV V",
        "The Learner applies Brill's algorithm for unsupervised learning to try to reduce the ambiguity of the tags in the input corpus.",
        "The following steps are taken:",
        "1.",
        "The Learner examines each ambiguously tagged word and creates a set of contexts for the word.",
        "Two of these contexts will be PREVWORD and NEXTWORD.",
        "The remainder consist of PREVTAG and NEXTTAG contexts as required by the tag(s) on the preceding and following words.",
        "For example, if the word preceding the ambiguously tagged word is ambiguously tagged with two tags, then the Learner must generate two PREVTAG contexts.",
        "2.",
        "An attempt is made to find unambiguously tagged words in the corpus that are tagged with one and only one of the tags on the ambiguously tagged word.",
        "For example, if the word in question has both N and V tags, then the Learner would search for words with only an N tag or only a V tag.",
        "3.",
        "If such a word is found, the contexts of that word are examined to determine if there is an overlap between them and the contexts generated for the ambiguously tagged word.",
        "One issue for this determination is how much ambiguity should be tolerated in the context of the unambiguously tagged word.",
        "For example, if one of the possible contexts is PRE-VTAG=N and the word preceding the unambiguously tagged word has both N and V tags, should the context apply?",
        "To permit various approaches to be tried, we extended the Learner to accept a parameter (i.e., freedom) that determines how much ambiguity will be accepted on the context words for the context to match.",
        "4.",
        "If a context matches for this unambiguously tagged word, the count of unambiguously tagged words with the particular part of speech occurring in that context is incremented.",
        "5.",
        "After the entire corpus is examined, each of these possible reduction rules (of the .form \"Change the tag of a word from x to Y in the context C where Y E x\") is ranked according to the following.",
        "First, for each tag Z E x, Z Y, the Learner computes:",
        "freq(Y)= number of occurrences of words unambiguously tagged with Y, freq(Z)= number of occurrences of words unambiguously tagged with Z, incontext(Z,C), number of times a word unambiguously tagged with Z occurs in context C.",
        "The tag Z that gives the highest score Ront this formula is saved as It.",
        "Then the score for a particular transformation is",
        "incontext(Y,C) fg(cilo) * inco dext(le,,C) 6.",
        "If the highest-ranked transformation is not positive, the Learner is done.",
        "Otherwise, the highest-ranked transformation is appended to the list of transformations learned.",
        "The Learner then searches this list for the transformation that will result in the most reduction of ambiguity (which will always he the latest rule learned) and applies it.",
        "This process continues until no further reduction of ambiguity is possible.",
        "Here, we also extended the Learner to accept a different parameter (i.e., 1-lagfreedom) that determines how much ambiguity will he accepted on a word that is used for context during ambiguilu reduelion, that is, when the Learner has found a rule and is applying it to the training text.",
        "Note that specifying too small a value for this parameter can cause the Learner to go into an endless loop, as restricting the valid contexts may have the effect of nullifying the just, learned rule.",
        "7.",
        "The Learner then returns to step 1 to begin the process again."
      ]
    },
    {
      "heading": "2.3 Rule Tagger",
      "text": [
        "This component reads tagged texts produced by the Initial State Annotator and rules produced by the Learner and applies the learned rules to the tagged texts to reduce the ambiguity of the tags.",
        "We extended the Rule Tagger to have two possible modes of operation (i.e., best-rule-first and learned-sequence modes controlled by the seq parameter) for using the learned rules to reduce ambiguity: -I.",
        "The Rule Tagger can use an algorithm similar to that used in step 7 of the.",
        "Learner.",
        "Each possible reduction rule is examined against the text to determine which rule results in the greatest reduction of ambiguity.",
        "2.",
        "The Rule Tagger can use a sequential application of the learned rules in the order that the rules were learned.",
        "After each rule has been applied in sequence, all of the rules preceding it are reapplied to take advantage of ambiguity reductions made by the latest, rule applied.",
        "The Rule Tagger allows one to specify, as in the Learner, how much ambiguity will be tolerated for a context to match.",
        "For example, one can be very restrictive and require that a tag context; PREVTAG=N) match only an unambiguously tagged word (in this case, a word with only an N tag).",
        "This parameter (i.e., r-lagfreedom) specifies the maximum ambiguity allowed on a context.",
        "word for a context tag to match: I requires that.",
        "the context word be unambiguously tagged, 2 requires that there be no more than two tags on the word, and so on."
      ]
    },
    {
      "heading": "3 Experiments and Results",
      "text": [
        "For training and testing of the tagger, we have randomly picked articles from a large (274M II) \"El Norte\" Mexican newspaper corpus, and separated them into the training and test sets.",
        "'Hie test set (17,639 words) was tagged manually for comparison against; the system-tagged texts.",
        "For training, we partitioned the development set, into several different, sizedsets in order to see the el-feels of training corpus sizes.",
        "The breakdown can be found in Table 2.",
        "If one randomly picks one of the possible tags on each word in the test.",
        "set, the accuracy is 78.9% (78.6% with the simple verb tag set).",
        "The average POS ambiguity per word is 1.52 (1.49) including punctuation tags and 1.58 (1.56) excluding punctuation tags.",
        "For comparison, the accuracy of (frill's unsupervised English tagger was 95.1% using 120,000-word Penn Treebank texts.",
        "his initial state tagging accuracy was 90.7%, which is considerably higher than our Spanish case (78.6%)."
      ]
    },
    {
      "heading": "3.1 Effect of Tag Set",
      "text": [
        "Our first set of experiments tests the effect of the POS tag complexity.",
        "We used both the simple verb tag set (5 tags) and the complex verb tag set, (42 tags), which is shown in Table 3, where * can be either ISO, 250, 350, 1131,, 211,, or :3PL.",
        "In the case of simple verb tag set, tense, person and number information is discarded, leaving only a \"V\" tag and the lower four tags in the table.",
        "'I'he scores with the simple verb tag set for dif, ferent sizes of training sets are found in Table 4, and those with the complex verb tag set in Table 5.",
        "For these two experiments, the Learner was set to have a tight restriction on using context for learning (i.e, the freedom parameter was set, to 1) and a loose restriction on context for applying the learned rules (i.e., 1-lagfreedom 10).",
        "The Rule Tagger was given a moderately-tight restriction on using context for reduction rule application (i.e., r-lagfreedom 2).",
        "In general, the scores are slightly higher using the simple verb tag set over the complex verb",
        "tag set (91.8% vs. 90.3% for the \"Medium\" corpus).",
        "This behavior is most likely clue to the fact that some verb tense/person/number combinations cannot easily be distinguished from context, so the Learner was unable to find a rule that would disambiguate them.",
        "As can be seen from the tables, performance increased as the size of the learning set increased up to the \"Medium\" set, where the score levelled off.",
        "With very small learning sets, the system was unable to find sufficient examples of phenomena to produce reduction rules with good coverage.",
        "One surprising data point in the simple verb tag set experiments was the \"Full\" score, which dropped almost 9% from the \"Medium\" score.",
        "After analyzing the results more closely, it was found that the Learner had learned a very spec:ilie rule regarding the reduction of preposition/subordinate-conjunction combinations late in the learning process.",
        "The learned rule was: PREVTA( = N : PIS U13( ()NJ SUBCONJ 'fable 5: Ambiguously tagged texts, Complex Verbs Set # of rules learned Score Tiny 125 81.7% Small 212 89.6% Medium 323 90.3% Full 564 90.2% (none) 0 78.0% This rule was learned late in the learning process when most P/SUBCON.1 pairs had already been reduced.",
        "However, as one can see from the context of the rule, it will apply in a large number of cases in a text.",
        "The Rule Tagger notes this and applies the rule early, thus incorrectly changing many P/SUBCONJ pairs to SUBCONJ and reducing the accuracy of the tagging.",
        "Since this phenomenon never occurred in any of the other learning runs, one can see that the learning process can be heavily influenced by the choice of input texts."
      ]
    },
    {
      "heading": "3.2 Effect of Rule Application Parameters",
      "text": [
        "The next tests performed involved using rules generated above and changing parameters to the Rule Tagger to see how the scores would be influenced.",
        "In the following test, we used the simple verb tag set rules but varied the r-lagfreedom parameter and the seq parameter.",
        "The results can be found in Table 6.",
        "Although the variations are slight, the best value for the r-lagfrecdom parameter seems to be at an ambiguity level of 2.",
        "It seems that the strategy of reducing the ambiguity as quickly as possible (best-rule-first) is better than following the ordering of the rules by the Learner.",
        "This may well be clue to the fact that the ordering of the rules as produced by the Learner is dependent on the training texts.",
        "Since the test set was a different set of texts, the ordering of the rules was not as applicable to them as to the training texts, and so the tagging performance suffered."
      ]
    },
    {
      "heading": ":3.3 Effect of Hand-tagged Texts",
      "text": [
        "After examining the results front the above experiments, we realized that, sonic of the closed-class words in Spanish are almost always ambiguous (e.g., prepositions are usually ambiguous between PREP and SUBCONJ, and determiners between E'[' and PRO).",
        "Tilts means that the Learner will never learn a rule to disambiguate these closed-class cases because there will rarely be unambiguous contexts in the training texts tagged by the Initial State Annotator.",
        "'that is, unlike open-class words, we will not find new unambiguous closed-class words in texts precisely because there is only a closed set of them.",
        "Thus, we decided to introduce a small number of hand-tagged texts into the training set given to the Learner.",
        "Since the hand-tagged texts have \"correct\" examples of various phenomena, the Learner should be able to find good examples in them to learn from.",
        "Lor our tests, we defined four sets of hand-tagged texts that we added to the \"Small\" (3066 words) set of ambiguously tagged texts.",
        "The breakdown is in Table 7.",
        "Again, the heartier was set to have a. tight restriction on using context for learning (freedom I) and a loose restriction on context for applying the learned rules (tagfreedoin 10).",
        "The Ride Tagger was given a moderately-tight, restriction on using context for reduction rule application (frecdom 2).",
        "The best-rule-first mode of the Rule Tagger was used.",
        "The results, as shown in Table 8, are slightly better than when using only ambiguously tagged texts.",
        "It is interesting to note that the higher accuracy was achieved with fewer rules.",
        "ln fact, all experiments resulted in learning a little over 200 rules.",
        "lit addition to the experiments above, we wanted to know if the introduction of hand-tagged texts into the \"Pull\" ambiguously tagged set, would improve its rather low score (cf. Table 4).",
        "We performed an experiment using sitnple verb tags, the \"Full\" ambiguously tagged texts, and the \"Full\" hand-tagged texts.",
        "The results were 422 rules learned with a score of 92.1%, which tied with the \"Small\" ambiguously tagged set for achieving the highest, accuracy of all of the, learning/tagging runs, a full 13.5% higher than using no learning."
      ]
    },
    {
      "heading": "4 Problems and Possible Improvements",
      "text": [
        "Although our Spanish POS tagger performed reasonably well, achieving an improvement of 13.5% in accuracy over randomly picking tags, there were several problems that prevented the system from reaching an even higher score."
      ]
    },
    {
      "heading": "4.1. Learning Problem.",
      "text": [
        "As discussed in Section :3.3, ambiguous closed-class words (e.g., prepositions, determiners, etc..) cannot be reduced when there are no unambiguous exaniples of them in the training texts.",
        "This is prevalent in Spanish, where most prepositions can also be subordinate conjunctions, determiners can be pronouns, etc.",
        "A few hand-tagged texts are required to learn good rules for reducing the ambiguity on these words.",
        "It is possible, however, that such texts can be disambiguated only 1(.",
        ": their always ambiguous closed-class words but.",
        "not unambiguous closed-class words or open-class words.",
        "Such an experiment similar to selective sampling discussed in I/agan and Engelson (1)agan and Engelson, 1995) would be useful in the future because, if it is true, it will reduce the cost.",
        "()I' manual tagging considerably."
      ]
    },
    {
      "heading": "4.2 Lexicon Problem",
      "text": [
        "Problems that becante apparent as we ran more tests were the incompleteness and rnistakes in the lexicon.",
        "While the lexicon, derived from the Collitis Spanish-English dictionary, was quite rich in words, its tag set did not, always match the tag definitions we employed.",
        "Lot: example, our tag set, distinguishes proper nouns (PROPN) and nouns (N), whereas the Collins dictionary marked both as nouns (N).",
        "We have added our existing proper name.",
        "lists to the lexicon to partially solve this problem, hut the lists arc currently limited to location names ;_uid people's first, names.",
        "We also found several mistakes in the Collins definitions (e.g., several adverbs ending \"-ttiente\" were classified adjectives).",
        "Although we fixed these mistakes as we noticed them, it is difficult to know how many such errors still remain in the lexicon.",
        "It turned out that the incompleteness of the lexicon was another fundamental problem to brill's unsupervised learning algorithm.",
        "That is, when",
        "the lexicon does not list all the possible tags for a word, the tagger is very likely to make a mistake.",
        "This is because the learner is trained to reduce the ambiguity of possible tags of a word (say N, V, ADJ tags), but if the lexicon lists only a subset of the possible tags (say N and V tags), the system will never learn to assign an ADJ tag even when the word is used as an adjective.",
        "This type of problem was observed frequently when words are ambiguous between proper nouns and some other parts-of-speech such as \"Flores (ADJ/PROPN),\" \"Lozano (ADJ/PROPN),\" \"van (V/PROPN)\"3, \"Serra (V/Pl{OPN),\" etc.",
        "because not all the proper nouns are in the lexicon.",
        "The problems described above did not occur in Brill's experiments because he derived the lexicon from a POS-tagged corpus and used the untagged version of the same corpus for training and testing.",
        "Thus, he used an \"optimal\" lexicon which contains all the words with only parts-of-speech which appeared in the corpus.",
        "In addition, in such a corpus, rarely used PUS tags of a word are less likely to occur, and words are less likely to be ambiguous.",
        "Thus, in a sense, his \"unsupervised learning\" experiments did take advantage of a large POS-tagged corpus."
      ]
    },
    {
      "heading": "5 Related Works",
      "text": [
        "It is very difficult to compare performances between taggers when accuracy depends on quality of corpora and lexicons, and maybe on characteristics of languages.",
        "But in this section, we compare our tagger with Hidden Markov Model-based taggers.",
        "A more widely used algorithm for unsupervised learning of a PUS tagger is Hidden Markov Model (11MM).",
        "Cutting el al.",
        "(Cutting et al., 1992) and Melialdo (Merialdo, 1994) used IIMM to learn English POS taggers while Chanod and Tapanainen (Chanod and Tapanainen, 1995) , Feldweg (Feldweg, 1995), and Lean and Serrano (Leon and Serrano, 1995) ported the Xerox tagger (Cutting et al., 1992) to French, German, and Spanish respectively.",
        "One of the drawbacks of an IIMM-based approach is that laborious manual tuning of symbol and transition biases is necessary to achieve high accuracy.",
        "Without tuned biases, the German Xerox tagger achieved 85.89% while the French Xerox tagger achieved 87% accuracy.",
        "After one man-month of tuning biases, the accuracy of the French tagger increased to 96.8%.",
        "One could derive such biases from a corpus, as discussed in (Merialdo, 1994), but it unfortunately requires a tagged corpus.",
        "The best accuracy of the Spanish Xerox tagger was 91.51% for the reduced tag set (174 tags)",
        "with the base accuracy (i.e. no training) of 88.98% while the best accuracy of our tagger is currently 92.1% for the simple tag set (39 tags) with the base accuracy of 78.6%.",
        "The lower base accuracy in our experiment is probably due to the large number of entries in the Collins dictionary."
      ]
    },
    {
      "heading": "6 Summary",
      "text": [
        "Our Spanish Part of Speech Tagger is a successful implementation and extension of Brill's unsupervised learning algorithm that reduces the ambiguity of part-of-speech tags on words in Spanish texts.",
        "The system requires few, if any, hand-tagged texts to bootstrap itself.",
        "Rather, it merely requires a Spanish lexicon and morphological analyzer that can tag words with all their possible parts-of-speech.",
        "Given that the system performs at approximately 92% accuracy even with the aforementioned problems and with the inclusion of unknown words, we would expect that this system could achieve better results, approaching those of similar English-language POS taggers, when these problems arc rectified."
      ]
    }
  ]
}
