{
  "info": {
    "authors": [
      "Hubert Hin-Cheung Law",
      "Chorkin Chan"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C96-1036",
    "title": "N-Th Order Ergodic Multigram HMM for Modeling of Languages Without Marked Word Boundaries",
    "url": "https://aclweb.org/anthology/C96-1036",
    "year": 1996
  },
  "references": [
    "acl-A92-1018",
    "acl-C94-1032",
    "acl-J92-4003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Ergodic IIMMs have been successfully used for modeling sentence production.",
        "However for some oriental languages such as Chinese, a word can consist of multiple characters without word boundary markers between adjacent words in a sentence.",
        "This makes word-segmentation on the training and testing data necessary before ergodic TIMM can be applied as the language model.",
        "This paper introduces the Nth order Ergodic Multigram TIMM for language modeling of such languages.",
        "Each state of the TIMM can generate a variable number of characters corresponding to one word.",
        "The model can be trained without word-segmented and tagged corpus, and both segmentation and tagging are trained in one single model.",
        "Results on its application on a Chinese corpus are reported."
      ]
    },
    {
      "heading": "1 Motivation",
      "text": [
        "Statistical language modeling offers advantages including minimal domain specific knowledge and handwritten rules, trainability and scalability given a language corpus.",
        "Language models, such as N-gram class models (Brown et al., 1992) and Ergodic Hidden Markov Models (Kuhn et al., 1994) were proposed and used in applications such as syntactic class (POS) tagging for English (Cutting et al., 1992), clustering and scoring of recognizer sentence hypotheses.",
        "However, in Chinese and many other oriental languages, there are no boundary markers, such as space, between words.",
        "Therefore preprocessors have to be used to perform word segmentation in order to identify individual words before applying these word-based language models.",
        "As a result current approaches to modeling these languages are separated into two seperated processes.",
        "Word segmentation is by no means a trivial process, since ambiguity often exists.",
        "For proper segmentation of a sentence, some linguistic information of the sentence should he used.",
        "However, commonly used heuristics or statistical based approaches, such as maximal matching, frequency counts or mutual information statistics, have to perform the segmentation without knowledge such as the resulting word categories.",
        "To reduce the impact of erroneous segmentation on the subsequent language model, (Chang and Chan, 1993) used an N-best segmentation interface between them.",
        "However, since this is still a two stage model, the parameters of the whole model cannot be optimized together, and an N-best interface is inadequate for processing outputs from recognizers which can be highly ambiguous.",
        "A better approach is to keep all possible segmentations in a lattice form, score the lattice with a language model, and finally retrieve the best candidate by dynamic programming or some searching algorithms.",
        "N-gram models are usually used for scoring (Cu et al., 1991) (Nagata, 1994), but their training requires the sentences of the corpus to be manually segmented, and even class-tagged if class-based N-gram is used, as in (Nagata, 1994).",
        "A language model winch considers segmentation ambiguities and integrates this with a N-gram model, and able to be trained and tested on a raw, unsegmented and untagged corpus, is highly desirable for processing languages without marked word boundaries."
      ]
    },
    {
      "heading": "2 The Ergodic Multigram HMM Model",
      "text": []
    },
    {
      "heading": "2.1 Overview",
      "text": [
        "Based on the Hidden Markov Model, the Ergodic Multigram Hidden Markov Model (Law and Chan, 1996), when applied as a language model, can process directly on unsegmented input corpus",
        "as it allows a variable number of characters in each word class.",
        "Other than that its properties are similar to Ergodic Hidden Markov Models (Kuhn et al., 1994), that both training and scoring can be done directly on a raw, untagged corpus, given a lexicon with word classes.",
        "Specifically, the Nth order Ergodic Multigrarn MM, as in conventional class-based (N+1)-gram model, assumes a doubly stochastic process in sentence production.",
        "The word-class sequence in a sentence follows the Nth order Markov assumption, i.e. the identity of a class in the sentence depends only on the previous N classes, and the word observed depends only on the class it belongs to.",
        "The difference is that this is a multi-gram model (Deligne and Binibot, 1995) in the sense that each state (i.e. node in the 11MM) can generate a variable number of observed character sequences.",
        "Sentence boundaries are 'nodded as a special class.",
        "This model can be applied to an input, sentence or a character lattice as a language model.",
        "The maximum likelihood state sequence through the model, obtained using the Viterbi or Stack Decoding Algorithm, represents the best particular segmentation and class-tagging for the input sentence or lattice, since transition of states denotes a word boundary and state identity denotes the current word class."
      ]
    },
    {
      "heading": "2.2 Lexicon",
      "text": [
        "A lexicon (CK1P, 1993) of 78,322 words, each containing up to 10 characters, is available for use in this work.",
        "Practically all characters have an entry in the lexicon, so that out-of-vocabulary words are modeled as individual characters.",
        "There is a total of 192 syntactic classes, arranged in a hierarchical way.",
        "For example, the month names are denoted by the class Ndabc, where N denotes Noun, Nd denotes Temporal Nouns, Nda for Time names and Ndab for reusable time names.",
        "There is a total of 8 major categories.",
        "Each word in the dictionary is annotated with one or more syntactic tags, representing different syntactic classes the word can possibly belong to.",
        "Also, a frequency count for each word, based on a certain corpus, is given, but without information on its distribution over different syntactic classes."
      ]
    },
    {
      "heading": "2.3 Terminology",
      "text": [
        "Lel, W be the set of all Chinese words in the lexicon.",
        "A word wk E 1/11 is made up of one or more characters.",
        "Let sf = s2, ...sT) denote a sentence as a '1'-character sequence.",
        "A function fi,„ is defined such that fiw(wk,s111-7-1) is 1 if WA, is a p-character word stand 0 otherwise.",
        "Let II be the upper bound of r, i.e. the maximum number of characters in a word (10 in this paper).",
        "Let C = {el } be the set of syntactic classes, where L is the number of syntactic classes in the lexicon (192 in our case).",
        "Let Cc WaC denote the relation for all syntactic classifications of the lexicon, such that (u/k„ E C ilf ci is one of the syntactic classes for wk.",
        "Each word wk must belong to one or more of the classes.",
        "A path through the inodel represents a particular segmentation and class tagging for the sentence.",
        "Let E = (WA, .",
        ".",
        ".",
        ";WK , et„) be a particular segmentation and class tagging for the sentence sT, where Wk is the kth word and cik denotes the class assigned to WA;, as illustrated below.",
        "['or L to he proper, n fitv(wk,s;kkiiI) = 1 and (wk, cik) E C most be satisfied, where to = 1,1K = + l and to-1 < to for I < k < K."
      ]
    },
    {
      "heading": "2.4 HMM States for the Nth order model",
      "text": [
        "In the first order IIMM (class Ingram) model, each .IIMM state corresponds directly to the word-class of a word.",
        "liut in general, for an Nth order 1.1MM model, since each class depends on N previous classes, each state has to represent the combination of the classes of the most recent N words, including the current word.",
        "Let Q represent a state of the N-tit order Ergodic M ultigram TIMM.. Tints = .. • where is the current word class, is the previous word class, etc.",
        "There is a total of /fly states, which may mean too many parameters (0+1 possible state transitions, each state can transit to L other states) for the model if N is anything greater than one.",
        "To solve this problem.",
        "a reasonable assumption can be made that the detailed class identities of a more distant word have, in general, less influence than the closer ones to the current word class.",
        "'linos instead of using C as the classification relation for all previous words, a set of 'The algoritin to be described assumes that the character identities are known for the sentence sr , but it can also be applied when each character position st becomes a set of possible character candidates by simply letting 6,„(wk, st`+'') = I for all words wk which can be constructed front time character positions st • • • St+r_i of the input character lattice.",
        "This enables the model to be used as the language model component for recognizers and for decoding phonetic input.",
        "classification relations {C(°), C(1), ...C(N-1)} can be used, where C(°) C represents the original, most detailed classification relation for the current word, and C(n) is the less detailed classification scheme for the nth previous word at each state.",
        "Thus the number of states reduces to LQ = L(o)L(1) ...L(N-1) in which L(n) < L. Each state is represented as Qi = (c(o) where C(') = {cn)}, 1 < 1 < L(n) is the class tag set for the nth previous word.",
        "However, if no constraints are imposed on the series of classification relations C(n), the number of possible transitions may increase despite a decrease in the number of states, since state transitions may become possible between every two state, resulting in a total of L(02L(1)2 L(N- 1)2 possible transitions.",
        "A constraint is imposed that, given that a word belongs to the class c(n) in the classification C(\"), we can determine the corresponding word class c(n+1) the given word will belong to in C(n+1), and for every word there is no extra classifications in C(n+l) not corresponding to one in C(\").",
        "Formally, there exist mapping functions Y(n) : C(n) H C(n+1), 0 < n < N – 2, such that if (4n), 4\"'\") E 1\"(n), then ((wk, cr) E C(n)) ((wk, 41+1)) E C(71+1)) for all wk E W, and that Y(') is surjective.",
        "In particular, to model sentence boundaries, we allow $ to be a valid class tag for all C(n), and define .T(n)($) = $.",
        "The above constraint ensures that given a state",
        "where cio(0) is any state in C(°).",
        "Thus reducing to the maximum number of possible transitions to",
        "This constraint is easily satisfied by using a hierarchical word-class scheme, such as the one in the CKIP lexicon or one generated by hierarchical word-clustering, so that the classification for more distant words (higher n in C(\")) uses a higher level, less detail tag set in the scheme."
      ]
    },
    {
      "heading": "2.5 Sentence Likelihood Formulation",
      "text": [
        "Let {E} be the set of all possible segmentations and class taggings of a sentence.",
        "Under the Nth order model ON, the likelihood of each valid segmentation and tagging E of the sentence sr , P(sr,EION), can be derived as follows.",
        "using Nth order Markov assumption and representing the class history as HMM states.",
        "$ denotes the sentence boundary, cik is $ for k < 0, and Qik = (cV ).",
        "Note that Qik can be determined from cik and Qik_, due to the constraint on the classification, and thus P(QiklQik_,) P(clk iQlk-i)• The likelihood of the sentence sr under the model is given by the sum of the likelihoods of its possible segmentations."
      ]
    },
    {
      "heading": "3 The Algorithms",
      "text": []
    },
    {
      "heading": "3.1 The Parameters",
      "text": [
        "As in conventional HMM, the Ergodic Multigram HMM consists of parameters eN = {A, B}, in which A = faijl, 0 < i,j < LQ (Total number of states), denotes the set of state transition probabilities from Qi to Qi, P(QiIQi).",
        "In particular, aoi = P(QiI$N) and aio = P(SIQi) denote the probabilities that the state Qi is the initial and final state in traversing the HMM, respectively.",
        "aoo is left undefined.",
        "B = {bi(wk)}, where 1 < j < LQ, denotes the set of word observation probabilities of wk at the state i.e. P(wki(4)• The B matrix, as shown above, models the probabilities that wk is observed given N most recent classes, and contains LQIWI parameters (recall that .LQ = L(°) L(1) .",
        ".",
        ".",
        "L(N-1)).",
        "Our ,assumption that wk only depends on the current class reduces the number of parameters to L(')I W I for the B matrix.",
        "Thus in the model, bi(wk)",
        "representing are tied together for all states Qj with the same current word-class, i.e.",
        "ber of parameters in the A matrix is only L(°)LQ.",
        "Given the segmentation and class sequence E of a sentence, the state sequence (Qi, ...Q1,) can be derived from the class sequence (ci, Thus the observation probability of the sentence sr given E and the model ON, P(sr ,,CION), can be reformulated as",
        "Given this formulation the training procedure is mostly similar to that of the first order Ergodic Multigram 11MM."
      ]
    },
    {
      "heading": "3.2 Forward and Backward Procedure",
      "text": [
        "The forward variable is defined as",
        "where 62 i(t) is the state of the 11MM when the word containing the character st as the Iasi character is produced.",
        "The recursive equations for at(i) are",
        "Similarly, the backward variable is defined as",
        "starts at the character st+1 in the state Q.",
        "Thus j) can be expressed as",
        "for 1 < t < 7' t, 1 < j < LQ.",
        "Furthermore define 't(i) to be the probability that, given si' and ON, a word ends at the character st in the state Thus",
        "Summation of 6(i, j) over t gives the expected number of times that state Qi transits to state.",
        "Qj in the sentence, and summation of yt(i) over t gives the expected number of state Qi occurring in it.",
        "Thus the quotient of their summation over t gives , the new estimation for aij",
        "The initial and final class probability estimates, rioj and aio can be re-estimated as follows.",
        "To derive bi(wk), first define at k (i) as the probability of the sentence prefix (si _8-0 with wk in state Qi as the last complete word.",
        "Thus aoi aio = As A, .8 arrays and the 6„, function are mostly Os, considerable simplification can be done in implementation.",
        "The likelihood of the sentence given the model can be evaluated as",
        "The Viterbi algorithm for this model can be obtained by replacing the summations of the forward algorithm with maximizations."
      ]
    },
    {
      "heading": "3.3 Re-estimation Algorithm",
      "text": [
        "j) is defined as the probability that given a sentence sT and the model 01V a word ends at the character st in the state Qi and the next word This represents the contribution of wk, occurring as the last word in to ot(j).",
        "Also define -.y.",
        "'t\"(j) to be the probability that, given the sentence sT and the model, wk is observed to end at character st in the state Q.",
        "Let Qj o Qi, denotes the relation that both Qj and Qi, represent the same current word class.",
        "Thus summation of 'y (j) over 1 gives the expected number of times that wk is observed in",
        "state Qj, and summation of ^yt(j) over t gives the total expected number of occurrence of state Qj.",
        "Since states with the same current word class are tied together by our assumption, the required value of bj(wk) is given by"
      ]
    },
    {
      "heading": "4 Experimental Results",
      "text": []
    },
    {
      "heading": "4.1 Setup",
      "text": [
        "A corpus of daily newspaper articles is divided into training and testing sets for the experiments, which is 21M and 4M in size respectively.",
        "The first order (N-=1) algorithms are applied to the training sets, and parameters obtained after different iterations are used for testing.",
        "The initial parameters of the 11MM are set based on the frequency counts from the lexicon.",
        "The class-transition probability aij is initialized as the a priori probability of the state P(Qj), estimated from the relative frequency counts of the lexicon.",
        "bj(wk) is initialized as the relative count of the word wk within the class corresponding to the current word class in Qj .",
        "Words belonging to multiple classes have their counts distributed equally among them.",
        "Smoothing is then applied by adding each word count by 0.5 and normalizing.",
        "After training, the Viterbi algorithm is used to retrieve the best segmentation and tagging ,C* of each sentence of the test corpus, by tracing the best state sequence traversed.",
        "where the summation is taken over all sentences sTi in the testing corpus, and M represents the number of characters in it, is used to measure the performance of the model.",
        "The results for models trained on training corpus subsets of various sizes, and after various iterations are shown (Table 1.).",
        "It is obvious that with small training corpus, over-training occurs with more iterations.",
        "With more training data, the performance improves and over-training is not evident."
      ]
    },
    {
      "heading": "4.3 Phonetic Input Decoding",
      "text": [
        "A further experiment is performed to use the models to decode phonetic inputs (Gu et al., 1991).",
        "This is not trivial since each Chinese syllable can correspond to up to 80 different characters.",
        "Sentences from the testing corpus are first expanded into a lattice, formed by generating all the common homophones of each Chinese character.",
        "Tested on 360K characters, a character recognition rate of 91.24% is obtained for the model trained after 8 iterations with 21M of training text.",
        "The results are satisfactory given that the test corpus contains many personal names and out of vocabulary words, and the highly ambiguous nature of the problem."
      ]
    },
    {
      "heading": "5 Discussion and Conclusion",
      "text": [
        "In this paper the Nth order Ergodic Multigrain IIMM is introduced, whose application enables integrated, iterative language model training on untagged and unsegmented corpus in languages such as Chinese.",
        "The performance on higher order models are expected to be better as the size of training corpus is relatively large.",
        "However some form of smoothing may have to be applied when the training corpus size is small.",
        "With some modification this algorithm would work on phoneme candidate input instead of character candidate input.",
        "This is useful in decoding phonetic strings without character boundaries, such as in continuous Chinese/Japanese/Korean phonetic input, or speech recognizers which output phonemes.",
        "This model also makes a wealth of techniques developed for I1 MM in the speech recognition field available for language modeling in these languages."
      ]
    }
  ]
}
