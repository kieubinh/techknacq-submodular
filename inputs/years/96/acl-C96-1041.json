{
  "info": {
    "authors": [
      "Sung Young Jung",
      "Young C. Park",
      "Key-Sun Choi",
      "Youngwhan Kim"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C96-1041",
    "title": "Markov Random Field Based English Part-Of-Speech Tagging System",
    "url": "https://aclweb.org/anthology/C96-1041",
    "year": 1996
  },
  "references": [
    "acl-A88-1019",
    "acl-A92-1021",
    "acl-C90-3038",
    "acl-J93-2004",
    "acl-J93-2006",
    "acl-J94-2001"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Probabilistic models have been widely used for natural language processing.",
        "Part-of-speech tagging, which assigns the most likely tag to each word in a given sentence, is one of the problems which can be solved by statistical approach.",
        "Many researchers have tried to solve the problem by hidden Markov model (HMM), which is well known as one of the statistical models.",
        "But it has many difficulties: integrating heterogeneous information, coping with data sparseness problem, and adapting to new environments.",
        "In tins paper, we propose a Markov radon field (MIRE) model based approach to the tagging problem.",
        "The MBE provides the base frame to combine various statistical information with maximum entropy (ME) method.",
        "As Gibbs distribution can be used to describe a posteriori probability of tagging, we use it in maximum a posteriori (MAP) estimation of optimizing process.",
        "Besides, several tagging models are developed to show the effect of adding information.",
        "Experimental results show that the performance of the tagger gets improved as we add more statistical information, and that MRF-based tagging model is better than IIMM based tagging model in data sparseness problem."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Part-of-speech tagging is to assign the correct tag to each word in the context of the sentence.",
        "'['here are three main approaches in tagging problem: rule-based approach (Klein and Simmons 1963; Brodda 1982; Paulussen and Martini 1992; Brill et al.",
        "1990), statistical approach (Church 11988; Merialdo 1994; Foster 1991; Weischedel et al.",
        "1993; Kupiec 1992) and connectionist approach (Benello et al.",
        "1989; Nakamura et al.",
        "1989).",
        "In these approaches, statistical approach has the following advantages : a theoretical framework is provided automatic learning facility is provided the probabilities provide a straightforward way to disambiguate Many information sources must be combined to solve tagging problem with statistical approach.",
        "It is a significant assumption that the correct tag can generally be chosen from the local context.",
        "Not only local sequences of words and tags are needed to solve tagging problem, but syntax, semantic and morphological level information is also required in general.",
        "Usually information sources such as bigram, trigram and unigram are used in the tagging systems which are based on statistical method.",
        "Traditionally, linear interpolation and its variants have been used to combine the information sources, but these are shown to be seriously deficient.",
        "ME (Maximum Entropy) estimation method provides the facility to combine several information sources.",
        "Each information source gives rise to a set of constraints, to be imposed on the combined estimate.",
        "The function with the highest entropy within the constraints is the ME solution.",
        "Given consistent statistical evidence, a unique ME solution is guaranteed to exist and an iterative algorithm is provided.",
        "MRE (Markov random field) model is based on ME method and it has the facility to combine many information sources through feature functions.",
        "MBE model has the following advantages: robustness, adaptability, parallelism and the facility of combining information sources.",
        "MBE-based tagging model inherits these advantages.",
        "In this paper, we will present one of the statistical models, namely MRF-based tagging system.",
        "We will show that several information sources including unigram, bigrarn and trigram, can be combined in MBE-based tagging model.",
        "Experimental results show that the M RE-based tagger has very good performance especially when training data size is small.",
        "Section 2 describes the tagging problem , Section 3 describes statistical model already known",
        "and section the research for combining statistical information.",
        "Section 5 provides MRT-based tagging model and section 9 sliowes the experimental results.",
        "Section 10 compares MRF with 1.11\\4.M.",
        "Finally we, conclude in section I I."
      ]
    },
    {
      "heading": "2 The Problem of Tagging",
      "text": [
        "When sentence 6V = 'tut,w.",
        "is given, there exist, corresponding tags 7'...,of the same length.",
        "We call the pair (W,T) an alignment.",
        "We say that wordhas been assigned the tagin this alignment.",
        "We suppose that, a set, of tags is given.",
        "Tagging is assigning correct tag sequence T =.........f for given word sequence I/1711/1,"
      ]
    },
    {
      "heading": "3 Probabilistic",
      "text": [
        "Formulation(IMM) Let us assume that we want to know the most likely tag sequence 0(4/), given a particular word sequence W. 'file tagging problem is defined as finding the most likely tag sequence 7'",
        "arg max P(W P(T) where ITT) is the a priori probability of a tag sequence 'I', P(l'VP') is the conditional probability of word sequence W, given the sequence of tags and P(W) is the unconditioned probability of word sequence W. The probability P(147) in (2) is removed because it, has no effect on 0(117).",
        "Consequently, it is sufficient to find the tag sequence 1' which satisfies (3).",
        "We can rewrite the probability of each sequence as a product of the conditional probabilities of each word or tag given all of the previous tags.",
        "Typically, one makes two simplifying assumptions to cut down on the number of probabilities to be estimated.",
        "First., rather than assuming ti depends on all previous words and all previous tags, one assumes wi depends only onSecond, rather than assuming the tag depends on the full sequence of previous tags, we can assume that local context is sufficient.",
        "This locality assumed is refered to as a Markov independence assumption.",
        "Using these assumption, we approximate the equation to the following",
        "We can get each probability value front tlw tagged corpus which is prepared for training by using (7) and (8).",
        "where C(ii.",
        "),C(ti, ti) is the frequency obtained front training data.",
        "Niteroi algorithm (Vorney73) is the one generally used to Lind the tag sequence which satisfies (6) and this algorithm guarantees the optimal solution to the problem.",
        "'Ibis model has several problems.",
        "First-, sonic words or tag sequences may not, occur in training data or inay occur with very low Frequency; nevertheless, the words or tag sequences can appear in tagging process.",
        "in tins case, it, usually causes very had result, to compute (6), because the probability has zero value or very low value.",
        "This problem is called data sparseness problem.",
        "To avoid this problem, smoothing of' information must be used.",
        "Smoothing process is almost essential in IIMM.",
        "because I1MM has severe data sparseness problem.",
        "4 combining information sources"
      ]
    },
    {
      "heading": "4.1 linear interpolation",
      "text": [
        "Various kinds of information sources and different knowledge sources must be combined to solve the tagging problem.",
        "The general method used in lIMM is linear interpolation, which is the weighted summation of all probability information SOU rtes.",
        "where 0 < Ai < and ri Ai = This method can be used both as a way of combining knowledge sources and smoothing in formatioit sources.",
        "Il M kl based tagging model uses unigram, grain and trigrain information.",
        "These information sources are linearly combined by weighted summation.",
        "can be estimated by forward-backward algorithm (Deroua86-1-) (Charniak93+) (HUANG90+).",
        "Linear interpolation is so advantageous because it reconciles the different information sources in a straightforward and simple-minded way.",
        "But such simpliticy is also the source of its weaknesses: Linearly interpolated information is generally inconsistent with their information sources because information sources are heterogeneous for each other in general.",
        "Linear interpolation does not make optimal combination of information sources.",
        "Linear interpolation has overestimation problem because it adjusts the model on the training data only and has no policy for untrained data.",
        "This problem occur seriously when the size of the training data is not large",
        "system with distance 2 iterative algorithm, \"Generalized Iterative Scaling\" (GIS), exists, which is guaranteed to converge to the solution (Darroch72+).",
        "(12) is similar to Gibbs distribution, which is the primary probability distribution of MI,' model.",
        "MRF model uses ME principle in combining information sources and parameter estimation.",
        "We will describe MRF model and its parameter estimation method later."
      ]
    },
    {
      "heading": "4.2 ME(maximum entropy) principle",
      "text": [
        "There is very powerful estimation method which combines information sources objectively.",
        "ME(maxirnum entropy) principle (Jaynes57) provides the method to combine information sources consistently and the ability to overcome overestimation problem by maximizing entropy of the domain with which the training data do not provide information.",
        "Let us describe ME principle briefly.",
        "For given x, the quantity x is capable of assuming the discrete values xi, (i = 1, 2, ..., n).",
        "We are not given the corresponding probabilities pi; all we know is the expectation value of the function fr(x), (r =",
        "On the basis of this information, how can we determine the probability value of the function pi(x)?",
        "At first glance, the problem seems insoluble because the given information is insufficient to determine the probabilities pi(x).",
        "We call the function fi.",
        "(xi) a constraint function or feature.",
        "Given consistent constraints, a unique ME soluton is guaranteed to exist and to be of the form: pi(xi)=eAdy(x,)(12) where the A, 's are some unknown constants to be found.",
        "This formula is derived by maximizing the entropy of the probability distribution pi as satisfying all the constraint given.",
        "To search the Ar's that make pi(x) satisfy all the constraints, an 5 MRF-based tagging model"
      ]
    },
    {
      "heading": "5.1 MRF in tagging",
      "text": [
        "Neighborhood of given random variable is defined by the set of random variables that directly affect the given random variable.",
        "Let N(i) denote a set of random variables which are neighbors of ith random variable.",
        "Let's define the neighborhood system with distance L in tagging for words W ton, and tags T",
        "This neighborhood system has one dimensional relation and describes the one dimenstional structure of sentence.",
        "Fig.",
        "1 showes MRF T which is defined for the neighborhood system with distance 2.",
        "The arrows represent that the random variable ti is affected by the neighbors ti_2,tj+2 It also showes thatand ti,ti+i have the neighborhood relation connected by bigram, and thatti_2 andti+2 have the, neighborhood relation connected by trigram.",
        "A clique is defined as the set of random variables that all of the pairs of random variables are neighborhood in it.",
        "Let's define the clique as the tag sequence with size L in tagging problem.",
        "A clique concept is used to define clique function that evaluates current state of random variables in clique.",
        "The definition of MRF is presented as following.",
        "Definition of MBE: Random variable T is Markov random field if T satisfies the following -Iwo properties.",
        "We assume that every probablity yttlite of tag sequence is larger than zero because ungrammatical sentences can appear in human language usage, including meaningless sequence of characters.",
        "So the positivity of Miff is satisfied.",
        "This assumption results in the robustness and adaptability of the model, even though untrained events occur.",
        "The locality of MRE is consistent with the assumption of tagging problem in that the tag of given word can be determined by the local context.",
        "Consequently, the random variable T is MILE for neighborhood system N(i) as 'I' satisfies the positivity and the locality."
      ]
    },
    {
      "heading": "5.2 A Posteriori Probability",
      "text": [
        "A posteriori probability is needed to search for the most likely tag sequence.",
        "kIRF provides the theoretical background about the probability of the system (13esag74) (Geman84+).",
        "Harnmersley-(lifford theorem: The probability distribution 1)(7') is Gibbs distribution if and only if random variable '1' is illarkov random field for ,yiven neigborhood system N(1),",
        "where Tin is temperature, X is normalizing constant, called partition function and U(T) is energy function.",
        "The a priori probability P('1') of tag sequence T is Gibbs distribution because the random variable '1' of tagging is MRE.",
        "It can be proved that a posteriori probability P(TIW) for given word sequence W is also Gibbs distribution (Chun93).",
        "Consequently, a posteriori probability of T for given W is",
        "We use (19) to carry out MAP estimation in the tagging model .",
        "The energy function u vivo is of this form.",
        "where V, is clique function with the property that 14 depends only on those random variable in clique c. This means that energy function can be obtained from each clique funtion which splits the set of random variables to subsets."
      ]
    },
    {
      "heading": "6 Clique function design",
      "text": [
        "The more state of random variables are near to the solution, the more the system becomes stable, and energy function has lower value.",
        "Energy function represents the degree of unstability of current state of random variables in MR,E.",
        "It is similar to the behaviour of molecular particles in the real world.",
        "Clique function is proportional to energy function, and it represents the unstability of current state of random variables in clique or it has high value when the state of Mitt' is bad, low value when the state of MU' is near to solution.",
        "Clique function contributes to reduce the computation of evaluation function of entire Miff by clique concept that separates random variables to the subsets.",
        "Clique function Vi (TO) is described by the featitres that represent the constraint or information sources of given problem domain.",
        "The basic information sources which are used in statistical tagging model are unigram, Ingrain and trigram.",
        "Mitt' model 1 uses unigram, Ingrain and trigram.",
        "We write the feature function of !migrant as flitni fir (1171(22) and the feature function of n-gram, including, bigram, trigram as",
        "The clique function of the model 1 is made as follows.",
        "Morphological level information helps tagger to determine the tag of the word, more especially of the unknown word.",
        "The suffix of a word gives very useful information about the tag of the word in English.",
        "The clique function of model 2 is defined as",
        "We used the statistical distribution of the sixty suffixes that are most frequently used in English.",
        "We can expand the clique function of the model 1 easily by just adding suffix information to the clique function of the model 2.",
        "(T1W)Al.",
        "nigra.+ A2 'fngvam + A3 is 24 f fix (26)"
      ]
    },
    {
      "heading": "6.3 Model 3 (error correction)",
      "text": [
        "There exist error prone words in every tagging system.",
        "We adjust error prone words by collecting the error results and adding more information of the words.",
        "The feature function of Model 3 is for adjusting errors in word level.",
        "We used the probability distribution of five hundred error prone words in Model 2 in order to reduce the number of parameters."
      ]
    },
    {
      "heading": "7 Optimization",
      "text": [
        "The process of selecting the best tag sequence is called as optimization process.",
        "We use MAP (Maximum A Posteriori) estimation method.",
        "The tag sequence 7' is selected to maximize the a posteriori probability of tagging (19) by MAP.",
        "Simulated annealing is used to search the optimal tag sequence as Gibbs distribution provides simulated annealing facility with temperature and energy concept.",
        "We change the tag candidate of one word selected to minimize the energy function in k-th step from 7'(k) to 7(k+1) , and repeat this process until there is no change.",
        "The temperature Tin is started in high value and lower to zero as the above process is doing.",
        "Then the final tag sequence is the solution.",
        "Simulated annealing is useful in the problem which has very huge search space, and it is the approximation of MAP estimation (Gernan814-).",
        "There is another algorithm called Viterbi algorithm to find optimal solution.",
        "Viterbi algorithm guarantees optimal solution but it cannot be used in the problem which has very huge search space.",
        "So it is used in the problem which has small search space and used in IIMM.",
        "MRF model can use both Viterbi algorithm and simulated anealing, but it is not known to use simulated annealing in IIMM."
      ]
    },
    {
      "heading": "8 parameter estimation",
      "text": [
        "The weighting parameter A in the clique function (19) can be estimated front training data by ME principle (Jaynes57).",
        "Let us describe ME principle and ITS algorithm briefly.",
        ".For given x (xi, ..., xr,), the corresponding probabilities pi(xi) is not known.",
        "All we know is the expectation value of the function",
        "Given consistent constraints, we can find the probability distribution pi that makes the entropy In pi value maximum by using Largrangian multipliers in the usual way, and obtain the result: pi(x )exp(Y A ,../;.",
        "(x i))(30) This formula is almost similar to Gibbs distribution (17), also fr corresponds to the feature of clique function in M RE (20) (21).",
        "Using this fact, we can use M P in parameter estimation in M RE.",
        "We can derive (31) to be used in parameter estimation from training data.",
        "Zexp(A,.",
        "fr (xi))(32) To solve the solution of it, a numerical analysis method GIS (Generlaized Iterative Scaling) was suggested (1)arroch72-1-).",
        "Pietra used his own algorithm ITS (Improved Iterative Scaling) based on GIS to induce the features and parameters of random",
        "(2) kk+1, set ilkVI with new Ai (3) If (0) has converged, setq(k)",
        "and terminate.",
        "Otherwise go to step(1) where VA') is the distribution of the model in k-th step, and it corresponds to the posteriori probability of the tagging model ([9).",
        "A, the solution of (33) can be obtained by Newton method (Curtis89+), one of numerical analysis method.",
        "The reference distribution p is the probability distribution winch is obtained directly from training data.",
        "77) corresponds to the posterior distribution",
        "posterior probability of the words sequence of window size n (especially 3 in this model) by counting the entry on training data.",
        "Training data means leafIS tagged corpus here.",
        "Nt/7,1tvi, 1lt2,w)(34)"
      ]
    },
    {
      "heading": "9 Experiments",
      "text": [
        "The main objective of this experiments is to compare the Miff tagging model with the II M M tagging model.",
        "We constructed a Mtif tagger and a 11MM tagger using same information on the smile environment,.",
        "It is necessary to do smoothing process for data sparseness problem which is severe in II M M , while MI' has the facility of smoothing in itself like neural-net .",
        "We used linear interpolation method (Derotia864-) (jelinek89) and assigning frequency I for unknown word (Weisch93+) for smoothing in 11MM.",
        "We used the Brown corpus in PennTree Bank, described in (Marcns93-1-) with 48 different tags.",
        "A set of 800,000 words is collected for each part, of Brown corpus and used as training data, which is used to build the models.",
        "And a set of 30,000 words corpus is used as test, data, which is used to test the quality of the models.",
        "Table 1 shows the accuracy of each tagging model.",
        "'the average accuracy of the 11MM-based tagger is similar to that of MRF(l) tagger because they use the smile infbrination.",
        "Fig.",
        "2 shows that, the error rate as the size of' training data is increased.",
        "WRY( ) has lower error rate than that of 11 MM when the size of training data, is small.",
        "The error rate of MII,F(2) is decreased especially when the size of the training data is small, because morphological information helps the process of unknown words.",
        "Finally, MRF(3) show improvement as the size of training data grows but converges to the limit on some points.",
        "These experiments show that NIKE has better addaptability with small training data than II MM does, and that MIRE tagger has less data sparseness"
      ]
    },
    {
      "heading": "10 Comparison of MRF with HMM",
      "text": [
        "We can derive the simplified equation of IIMM",
        "(30) is considered as the multiplied probabilties of a the local events.",
        "The nearer the probability value of' local event is to zero , the more it affects the probability of' the entire event.",
        "This property strictly reflects on the events which does not occur in training data.",
        "But it prohibits even the event, that does not occur in training data, although the event is legal.",
        "IMRE can be simplified by the summation of clique function as (36).",
        "while 11MM does by multiplication.",
        "Even if a clique function value is very had, other clique function can compensate adequately because the clique functions are connected by summation.",
        "There is no critical point of' posteriori probablity in MEI', while 11MM has critical point in zero value.",
        "This property results in the robustness and the adaptability of the model and makes M model stronger in data sparseness problem."
      ]
    },
    {
      "heading": "11 Conclusion",
      "text": [
        "We proposed a M111?-based tagging model.",
        "Information sources for tagging are combined by ME principle which is used in IMRE as theoretical background.",
        "All parameters used in the model are estimated from training data automatically.",
        "As a result,, our MME-based tagging model has better performance than IIMM.",
        "tagging model, especially when the size of' the training data is small.",
        "We have seen that the performance of the NI I'-based tagging model can be improved by adding information to the model."
      ]
    }
  ]
}
