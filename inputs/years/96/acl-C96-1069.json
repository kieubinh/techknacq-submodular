{
  "info": {
    "authors": [
      "Fumiyo Fukumoto",
      "Yoshimi Suzuki"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C96-1069",
    "title": "An Automatic Clustering of Articles Using Dictionary Definitions",
    "url": "https://aclweb.org/anthology/C96-1069",
    "year": 1996
  },
  "references": [
    "acl-A92-1021",
    "acl-C92-2070",
    "acl-C94-2172",
    "acl-P91-1034"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we propose a statistical approach for clustering of articles using on-line dictionary definitions.",
        "One of the characteristics of our approach is that every sense of word in articles is automatically disambiguated using dictionary definitions.",
        "The other is that in order to cope with the problem of a phrasal lexicon, linking which links words with their semantically similar words in articles is introduced in our method.",
        "The results of experiments demonstrate the effectiveness of the proposed method."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "There has been quite a lot of research concerned with automatic clustering of articles or automatic identification of semantically similar articles(Walker, 1986), (Guthrie, 1994), (Yuasa, 1995).",
        "Most of these works deal with entirely different articles.",
        "In general, the problem that the same word can be used differently in different subject domains is less problematic in entirely different articles, such as 'weather forecasts', 'medical.",
        "reports', and 'computer manuals'.",
        "Because these articles are characterised by a larger number of different words than that of the same words.",
        "However, in texts from a restricted domain such as financial articles, e.g Wall Street Journal (WSJ in short) (Liberman, 1990), one encounters quite a large number of pol-ysemous words.",
        "Therefore, polysemous words often hamper the precise classification of articles, each of which belongs to the restricted subject domain.",
        "In this paper, we report an experimental study for clustering of articles by using on-line dictionary definitions and show how dictionary-definition can use effectively to classify articles, each of which belongs to the restricted subject domain.",
        "We first describe a method for disambiguating word-senses in articles based on dictionary definitions.",
        "Then, we present a method for classifying articles and finally, we report sonic experiments in order to show the effect of the method."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "One of major approaches in automatic clustering of articles is based on statistical information of words in articles.",
        "Every article is characterised by a vector, each dimension of which is associated with a specific word in articles, and every coordinate of the article is represented by term.",
        "weighting.",
        "Term weighting methods have been widely studied in information retrieval research (Salton, 1983), (Jones, 1972) and some of them are used in an automatic clustering of articles.",
        "Guthrie and Yuasa used word frequencies for weighting (Guthrie, 1994), (Yuasa, 1995), arid Tokunaga used weighted inverse document frequency which is a word frequency within the document divided by its frequency throughout the entire document collection (Tokunaga, 1994).",
        "The results of these methods when applied to articles' classification task, seem to show its effectiveness.",
        "However, these works do not seriously deal with the problem of polysemy.",
        "The alternative approach is based on dictionary's information as a thesaurus.",
        "One of major problems using thesaurus categories as sense representation is a statistical sparseness for thesaurus words, since they are mostly rather uncommon words (Niwa, 1995).",
        "Yuasa reported the experimental results when using word frequencies for weighting within large documents were better results in clustering documents as those when EMI, electronic dictionary as a thesaurus (Yuasa, 1995).",
        "The technique developed by Walker also used dictionary's information and seems to cope with the discrimination of polysemy (Walker, 1986).",
        "He used the semantic codes of the Longman Dictionary of Contemporary English in order to determine the subject domain for a set of texts.",
        "For a given text, each word is checked against the dictionary to determine the semantic codes associated with it.",
        "By accumulating the frequencies for these senses and then ordering the list of categories in terms of frequency, the subject matter of",
        "the text can be identified.",
        "However, as lie admits, a phrasal lexicon, such as Atlantic.",
        "Seaboard, New England gives a negative influence for clustering, since it can not be regarded as units, i.e. each word which is the element of a phrasal lexicon is assigned to each semantic code.",
        "The approach proposed in this paper focuses on these problems, i.e. polysemy and a phrasal lexicon.",
        "Like Guthrie and Yuasa's methods, our approach adopts a vector representation, i.e. every article is characterised by a vector.",
        "However, while their approaches assign each coordinate of a vector to each word in articles, we use a word (noun) of which sense is disambiguated.",
        "Our disambiguation method of word-senses is based on Niwa's method which used the similarity between two sentences, i.e. a sentence which contains a polysemous noun and a sentence of dictionary-definition.",
        "In order to cope with Walker's problem, for the results of disambiguation technique, semantic relativeness of words are calculated, and semantically related words are grouped together.",
        "We used WSJ corpus as test articles in the experiments in order to see how our method can effectively classify articles, each of which belongs to the restricted subject domain, i.e. WSJ."
      ]
    },
    {
      "heading": "3 Framework",
      "text": []
    },
    {
      "heading": "3.1 Word-Sense Disambiguation",
      "text": [
        "Every sense of words in articles which should be clustered is automatically disambiguated in advance.",
        "Word-sense disambiguation (WSD in short) is a serious problem for NLP, and a variety of approaches have been proposed for solving it (Brown, 1991), (Yarowsky, 1992).",
        "Our disambiguation method is based on Niwa's method which used the similarity between a sentence containing a polysemous noun and a sentence of dictionary-definition.",
        "Let; X be a polysemous noun and a sentence X be",
        "Here, Mu(x, y) is the value of mutual information proposed by (Church, 1991).",
        "ol,• • •,o,„ (We call them basic words) are selected the 1000th most frequent words in the reference Collins English Dictionary (Liberman, 1990).",
        "Let word x have senses sl,s2,• • •,,sp and the dictionary-definition of si be Ysi: • \" Y – t Yt, • The similarity of X and Ysi is measured by the inner product of their normalised vectors and is defined as follows:",
        "We infer that the sense of word x in X is si if Sint(X,Y,„) is maximum among Vs] ,• • • IT , Given an article, the procedure for WSD is applied to each word (noun) in an article, i.e. the sense of each noun is estimated using formula (1) and the word is replaced by its sense.",
        "Table 1 shows sample of the results of our disambiguation method."
      ]
    },
    {
      "heading": "Input A number of major airlines adopted",
      "text": [
        "In Table 1, underline signifies polysemous 1101111.",
        "`Output' shows that each noun is replaced by a symbol word which corresponds to each sense of a word.",
        "We call 'Input' and 'Output' in Table 1, an original article and a new article, respectively.",
        "Collins English Dictionary.",
        "'numbed.'",
        "'number5' are symbol words and show different senses of 'number'."
      ]
    },
    {
      "heading": "3.2 Linking Nouns with their Semantically Similar Nouns",
      "text": [
        "Our method for classification of articles uses the results of disambiguation method.",
        "The problems here are:",
        "1.",
        "The frequency of every disambiguated noun in new articles is lower than that of every polysemous noun in original articles.",
        "For example, the frequency of `number5' in Table 1 is lower than that of 'number' r. Furthermore, some nouns in articles may be semantically similar with each other.",
        "For example, 'number5' in Table 2 and `sumo' in Table 3 are almost the same sense.",
        "2.",
        "A phrasal lexicon which Walker suggested in his method gives a negative influence for classification.",
        "'If all 'number' are used as `number5' sense, the frequency of 'number' is the same as `number5'.",
        "numberl: number2: number3: number4: number5: Every number occupies a unique position in a sequence.",
        "He was not one of our number.",
        "A telephone number.",
        "She was number seven in the race.",
        "A large number of people.",
        "sums: sums: The result of the addition of num-ers.",
        "One or more columns or rows of numbers to be added.",
        "The limit of the first n terms of a converging infinite series as n tends to infinity.",
        "He borrows enormous sums.",
        "The essence or gist of a matter.",
        "In order to cope with these problems, we linked nouns in new articles with their semantically similar nouns.",
        "The procedures for linking are the following five stages.",
        "Stage One: Calculating Mu The first stage for linking nouns with their semantically similar nouns is to calculate Mu between noun pair s and y in new articles.",
        "In order to get a reliable statistical data, we merged every new article into one and used it to calculate Mu.",
        "The results are used in the following stages.",
        "Stage Two: Representing every noun as a vector The goal of this stage is to represent every noun in a new article as a vector.",
        "Using a term weighting method, nouns in a new article would be represented by vector of the form",
        "where wi is the element of a new article and corresponds to the weight of the noun In our method, the weight of wi is the value of Mu between v and wi which is calculated in Stage One.",
        "Stage Three: Measuring similarity between vectors Given a vector representation of nouns in new articles as in formula (2), a dissimilarity between two words (noun) v1, v2 in an article would be obtained by using formula (3).",
        "A dissimilarity measure is the degree of deviation of the group in an n-dimensional Euclidean space, where n is the number of nouns which co-occur with v1 and",
        "= (9,i, • • • , is the centre of gravity and I j I is the length of it.",
        "A group with a smaller value of (3) is considered semantically less deviant.",
        "Stage Four: Clustering method For a set of nouns w1, w2, • • w„ of a new article, we calculate the semantic deviation value of all possible pairs of nouns.",
        "Table 4 shows sample of the results of nouns with their semantic deviation values.",
        "In Table 4, `BBK' shows the topic of the article which is tagging in the WSJ, i.e. 'Buybacks'.",
        "The value of Table 4 shows the semantic deviation value of two nouns.",
        "The clustering algorithm is applied to the sets shown in Table 4 and produced a set of semantic clusters, which are ordered in the ascending order of their semantic deviation values.",
        "We adopted non-overlapping, group average method in our clustering technique (Jardine, 1991).",
        "The sample results of clustering is shown in Table 5.",
        "The value of Table 5 shows the semantic deviation value of the cluster.",
        "Stage Five: Linking nouns with their semantically similar nouns We selected different 49 articles from 1988, 1989 WSJ, and applied to Stage One ti Four.",
        "From these results, we manually selected clusters which are judged to be semantically similar.",
        "For the selected clusters, if there is a noun which belongs to several clusters, these clusters are grouped together.",
        "As a result, each cluster is added to a sequential number.",
        "The sample of the results are shown in Table 6.",
        "• • • new2 york2 • • •",
        "21n Table 4, there arc some nouns which are not added to the number, '1' N '5', e.g. `giorgio'.",
        "This shows that for these words, there is only one meaning in the dictionary.",
        "`Seq.",
        "num' in Table 6 shows a sequential number, `wordi', • • •,`word„,' which are added to the group of semantically similar nouns\".",
        "Table 6 shows, for example, `new2' and `york2' are semantically similar and form a phrasal lexicon."
      ]
    },
    {
      "heading": "3.3 Clustering of Articles",
      "text": [
        "According to Table 6, frequency of every word in new articles is counted, i.e. if a word in a nein article belongs to the group shown in 'fable 6, the word is replaced by its representative number `wordi' and the frequency of word: is counted.",
        "For example, `bank3' and `banks3' in a new article are replaced by`word', and the frequency of `wordi' equals to the total number of frequency of `bank3' and `banks3'.",
        "Using a term weighting method, articles would be represented by vectors of the form",
        "where corresponds to the weight of the noun i.",
        "The weight is used to the frequency of noun.",
        "Given the vector representations of articles as in formula (4), a similarity between Ai and Aj arc calculated using formula (1).",
        "The greater the value of Sini(Ai, A1) is, the more similar these two articles are.",
        "The clustering algorithm which is described in Stage Four is applied to each pair of articles, and produces a set of clusters which are ordered in the descending order of their semantic similarity values."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "We have conducted four experiments, i.e. 'Frog', `Dis', 'Link', and 'Method' in order to examine how WSD method and linking words with their semantically similar words(linking method in short) affect the clustering results.",
        "`Freq' is frequency-based experiment, i.e. we use word frequency for weighting and do not use WSD and linking methods.",
        "`Dis' is concerned with disambiguation-based experiment, i.e. the clustering algorithm is applied to new articles.",
        "'Link' is concerned with linking-based experiment, i.e. we applied linking method to original articles.",
        "'Method' shows our proposed method."
      ]
    },
    {
      "heading": "4.1 Data",
      "text": [
        "The training corpus we have used is the 1988, 1989 WSJ in ACL/DCI CD-ROM which consists of about 280,000 part-of-speech tagged sentences (Brill, 1992).",
        "From this corpus, we selected at random 49 different articles for test data, each of which consists of 3,500 sentences and has different topic name which is tagging in the 'WSJ.",
        "We classified 49 articles into eight categories, e.g.",
        "Sin our experiments, m equals to 238.",
        "`market news', 'food• restaurant', etc.",
        "The dictionary we have used is Collins English Dictionary in ACL/DCI CD-ROM.",
        "Iu WSD method, the co-occurrence of x and y for calculating Mu is that the two words (s,y) appear in the training corpus in this order in a window of 100 words, i.e. x is followed by y within a 100-word distance.",
        "This is because, the larger window sizes might be considered to be useful for extracting semantic relationships between nouns.",
        "Basic 'words are selected the 1000th most frequent words in the reference Collins English Dictionary.",
        "The length of a sentence X which contains a polysemous noun and the length of a sentence of dictionary-definition are maximum 20 words.",
        "For each polysemous noun, we selected the first top 5 definitions in the dictionary.",
        "In linking method, a window size of the co-occurrence of x and y for calculating Mu is the same as that in WSD method, i.e. a window of 100 words.",
        "We selected 969 – 9128 different (noun, noun) pairs for each article, 377 – 1259 different nouns on condition that frequencies and Mu are not low (.1' (x, y) > 5, 111 u(x, y) > 3) to permit a reliable statistical analysis'.",
        "As a result of Stage Four, we manually selected clusters which are judged to be semantically similar.",
        "As a result, we selected clusters on condition that the threshold value for similarity was 0.475.",
        "For the selected clusters, if there is a noun which belongs to several clusters, these clusters are grouped together.",
        "As a result, we obtained 238 clusters in all."
      ]
    },
    {
      "heading": "4.2 Results of the experiments",
      "text": [
        "The results are shown in Table 7.",
        "In Table 7, 'Article' means the number of articles which are selected from test data.",
        "'Num' means the number for each 'Article', i.e. we selected 10 sets for each 'Article'.",
        "`Freq', 'Link', `Dis', and `Method' show the number of sets which are clustered correctly in each experiment.",
        "'the sample results of 'Article = 20' for each experiment is shown in Figure 1, 2, 3, and 4.",
        "In Figure 1, 2, 3, and 4, the X-axis is the similarity value.",
        "Abbreviation words in each Figure and categories are shown in Table 8.",
        "Here, f (a!, y) is the number of total co-occurrences of words .r and y in this order in a window size of 100 words."
      ]
    },
    {
      "heading": "5 Discussion 1. WSD method",
      "text": [
        "According to Table 7, there are 24 sets which could be clustered correctly in `Dis', while 21 sets in `Freq'.",
        "Examining the results shown in Figure 3, TVG' and `FIRD' are correctly classified into 'food • restaurant' and 'market news', respectively.",
        "However, the results of Treq' (Figure 1) shows that they are classified incorrectly.",
        "Table",
        "9 shows different senses of word in TVG', and `IIRD' which could be discriminated in `Dis'.",
        "In Table 9, for example, 'security' is high frequencies and used in 'being secure' sense in TVG' article, while 'security' is 'certificate of creditorship' sense in `IIRD'.",
        "One possible cause that the results of 'Freq.'",
        "is worse than Tlis' is that these polysemous words which are high-frequencies are not recognised polyseissy in Treq'."
      ]
    },
    {
      "heading": "2. Linking method",
      "text": [
        "As shown in Table 7, there are 23 sets which could be clustered correctly in 'Link', while 21 sets in `Freq'.",
        "For example, 'ERN' and `IIRD' are both concerned with 'market news'.",
        "In Figure 2, they are clustered with high similarity value(0.943), while in Figure 1, they are not(0.260).",
        "Examining the results, there are 811 nouns in 'ERN' article, and 714 nouns in `IIRD', and",
        "We have reported an experimental study for clustering of articles by using on-line dictionary definitions and showed how dictionary-definition can use effectively to classify articles, each of which belongs to the restricted subject domain.",
        "In order to cope with the remaining problems mentioned in section 5 and apply this work to practical use, we will conduct further experiments.",
        "of these, 'shares', 'stock', and `share' which are semantically similar are included.",
        "In linking method, there are 251 nouns in `.ERN' and 492 nouns in `11111)' which are replaced for representative words.",
        "However, in 'Freq', each noun corresponds different coordinate, and regards to different meaning.",
        "As a result, these topics are clustered with low similarity value."
      ]
    },
    {
      "heading": "3. Our method",
      "text": [
        "The results of `Method' show that 31 out of 40 sets are classified correctly, and the percentage attained was 77.5%, while Treq', 'Link', and Pis' experiment, attained 52.5%, 57.5%, 60.0%, respectively.",
        "This shows the effectiveness of our method.",
        "In Figure 4, the articles are judged to classify into eight categories.",
        "Examining 'ERN', 'CEO' and 'CMD' in Figure 1, `CEO' and `CMIY are grouped together, while they have different categories with each other.",
        "On the other hand, in Figure 3, 'ERN' and 'CEO' are grouped together correctly.",
        "Examining the nouns which are belonging to 'ERN' and 'CEO', 'plant'(factory and food senses), 'oil'(petroleum and food), 'order'(command and demand), and 'interest'(debt and curiosity) which arc high frequencies are correctly disambiguated.",
        "Furthermore, in Figure 4, `ERN' and 'CEO' are classified into 'market news', and `CMD' are classified into 'farm', correctly.",
        "For example, 'plant' which is used in 'factory' sense is linked with semantically similar words, 'manufacturing', 'factory', 'production', or 'job' etc..",
        "In a similar way, 'plant' which is used in 'food' sense is linked with 'environment', 'forest'.",
        "As a result, the articles are classified correctly.",
        "As shown in Table 7, there are 9 sets which could not he clustered correctly in our method.",
        "A possible improvement is that we use all definitions of words in the dictionary.",
        "We selected the first top 5 definitions in the dictionary for each noun and used them in the experiment.",
        "However, there are some words of which the meanings are not included these selected definitions.",
        "This causes the fact that it is hard to get a higher percentage of correct clustering.",
        "Another interesting possibility is to use an alternative weighting policy, such as the widf (weighted inverse document frequency) (Tokunaga, 1994).",
        "The widf is reported to have a marked advantage over the idf (inverse document frequency) for the text categorisation task."
      ]
    }
  ]
}
