{
  "info": {
    "authors": [
      "Masahito Takahashi",
      "Tsuyoshi Shinchu",
      "Kenji Yoshimura",
      "Kosho Shudo"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C96-2206",
    "title": "Processing Homonyms in the Kana-To-Kanji Conversion",
    "url": "https://aclweb.org/anthology/C96-2206",
    "year": 1996
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper proposes two new methods to identify the correct meaning of Japanese homonyms in text based on the noun-verb co-occurrence in a sentence which can be obtained easily from corpora.",
        "'The first method uses the near co-occurrence data sets, which are constructed from the above co-occurrence relation, to select the most feasible word among homonyms in the scope of a sentence.",
        "The second uses the far co-occurrence data sets, which are constructed dynamically from the near co-occurrence data sets in the course of processing input sentences, to select the most feasible word among homonyms in the scope of a sequence of sentences.",
        "An experiment of kana-to-kanji(phonogramto-ideograph) conversion has shown that the conversion is carried out at the accuracy rate of 79.6% per word by the first method.",
        "This accuracy rate of our method is 7.4% higher than that of the ordinary method based on the word occurrence frequency."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Processing homonyms, i.e. identifying the correct meaning of homonyms in text, is one of the most important phases of kana-to-kakji conversion, currently the most popular method for inputting Japanese characters into a computer.",
        "Recently, several new methods for processing homonyms, based on neural networks(Kobayashi,1992) or the co-occurrence relation of words(Yamamoto,1992) , have been proposed.",
        "These methods apply to the co-occurrence relation of words not only in a sentence but also in a sequence of sentences.",
        "It appears impracticable to prepare a neural network for co-occurrence data large enough to handle 50,000 to 100,000 Japanese words.",
        "In this paper, we propose two new methods for processing Japanese homonyms based on the co-occurrence relation between a noun and a verb in a sentence.",
        "We have defined two co-occurrence data sets.",
        "One is a set of nouns accompanied by a case marking particle, each element of which has a set of co-occurring verbs in a sentence.",
        "The other is a set of verbs accompanied by a case marking particle, each element of which has a set of co-occurring nouns in a sentence.",
        "We call these t' 'o co-occurrence data sets near co-occurrence data sets.",
        "Thereafter, we apply the data sets to the processing of homonyms.",
        "Two strategies are used to approach the problem.",
        "The first uses the 'near co-occurrence data sets to select the most feasible word among homonyms in the scope of a sentence.",
        "The aim is to evaluate the possible existence of a near co-occurrence relation, or co-occurrence relation between a noun and a verb within a sentence.",
        "The second evaluates the possible existence of a far co-occurrence relation, referring to a co-occurrence relation among words in different sentences.",
        "This is achieved by constructing far co-occurrence data sets from near co-occurrence data sets in the course of processing input sentences."
      ]
    },
    {
      "heading": "2 Co-occurrence data sets",
      "text": [
        "The near co-occurrence data sets are defined.",
        "The first near co-occurrence data set is the set each element of which(n) is a triplet consisting of a noun, a case marking particle, and a set of verbs which co-occur with that noun and particle pair in a sentence, as follows: n = ('noun, partici c,k1), (02, k2), }) In this description, particle is a Japanese case marking particle, such as(nominative case), (accusative case), or (dative case), = 1,2, -) is a verb, and ki(i = 1,2, ) is the frequency of occurrence of the combination noun, particle and which is determined in the course of constructing EN,,,, from corpora.",
        "The following are examples of the elements of EN,,..",
        "The second near co-occurrence data set is the set Ev,,, each element of which(v) is a triplet consisting of a verb, a case marking particle, and a set of nouns which co-occur with that verb and particle pair in a sentence, as follows: v(verb,particle, {(n1,11),(n212),- }) In this description, particle is a Japanese case marking particle, ni(i = 1, 2, ) is a noun, and = 1,2, ) is the frequency of occurrence of the combination verb, particle andThe following are examples of the elements of Ev,,.. can be constructed from EN,,., and vice versa."
      ]
    },
    {
      "heading": "3 Processing homonyms in a",
      "text": [
        "simple sentence Using the near co-occurrence data sets, the most feasible word among possible homonyms can be selected within the scope of a sentence.",
        "Our hypothesis states that the most feasible noun or combination of nouns has the largest number of verbs with which it can co-occur in a sentence.",
        "The structure of an input Japanese sentence written in Lana-characters can be simplified as follows:",
        "where Ni(i = 1, 2, , in) is a noun, Pi(i = 1, 2, , m) is a particle and V is a verb."
      ]
    },
    {
      "heading": "3.1 Procedure",
      "text": [
        "Following is the procedure for finding the most feasible combination of words for an input Lana-string which has the above simplified Japanese sentence structure.",
        "This procedure can also accept an input Lana-string which does not include a final position verb.",
        "Stepl Let nt = 0 and Ti = F(i = 1, 2, ).",
        "Step2 If an input Lana-string is null, go to Step4.",
        "Otherwise read one block of Lana-string, that is N P or V, from the left side of the input Lana-string.",
        "And delete the one block of Lana-string from the left side of the input Lana-string.",
        "Step3 Find all homonymic kanji-variants Wk (k = 1, 2, .)",
        "for the Lana-string N or V which is read in Step2.",
        "Increase rn by 1.",
        "For each W k(k = 1, 2, ):",
        "1.",
        "If Wk is a noun, retrieve (147k, P, 17k) from the near co-occurrence data set 1-,'N,,, and add the doublet (Wk, Vk) to T,.",
        "2.",
        "If Wk is a verb, add the doublet (Wk, {(Wk,O)}) to Tn.",
        "Go to Step2.",
        "which has the largest value of n(vi,v2,-,vni)Where the function",
        "The sequence of words WI, 1472, , is the most feasible combination of homonymic kanji-variants for the input Lana-string."
      ]
    },
    {
      "heading": "3.2 An example of processing homonyms",
      "text": [
        "in a simple sentence Following is an example of homonym processing using the above procedures.",
        "For the input Lana-string",
        "\"7A-) (kawa)\" means a river and \"it L (hashi)\" means a bridge.",
        "\"b4) (Lawn)\" and \"it L (hashi)\" both have homonymic kanii-variants: homonyms of \"N:4-) (Lawn)\" :I I (river) ni.\" (leather) homonyms of \"(IL (hashi)\" :(bridge)",
        "The near co-occurrence data for \")II (river)\" and \")A (leather)\" followed by the particle \"IL- (dative case)\" and the near co-occurrence data for \"44 (bridge)\" and \"* (chopsticks)\" followed by the particle(accusative case)\" are shown below.",
        "A 1101111 file including 323 nouns, which consists of 190 nouns extracted from text concerning current topics and their 133 homonyms, was prepared.",
        "A co-occurrence data file was prepared.",
        "The record format of the file is specified as follows: [noun, case marking particle, verb, the frequency of occurrence] where case marking particle is chosen from 8 kinds of particles, namely, \"ffi\",\"&'\",\"k-\",\"---\",\"L\"",
        "It includes 25,665 records of co-occurrence relation (79 records per noun) for the nouns in the noun file by merging 11,294 records from EDE, Co-occurrence Dictionary(ED1t,1994) with 15,856 records from handmade simple sentences.",
        "4.1.3 an input file and an answer file An input file for an experiment, which includes 1,129 simple sentences written in kana alphabet, and all answer file, which includes the same 1,129 sentences written in kanji characters, were prepared.",
        "Here, every noun of the sentences in the files was chosen from the noun file.",
        "4.1.4 a word dictionary A word dictionary, which consists of 323 nouns in the noun file and 23,912 verbs in a Japanese dictionary for kana-to-kattit conversion I, was prepared.",
        "It is used to find all homonymic kannvariants for each noun or verb of the sentences in the input file."
      ]
    },
    {
      "heading": "4.2 Experiment results",
      "text": [
        "An experiment on processing homonyms in a simple sentence was carried out.",
        "In this experiment, kana-to-kanji conversion was applied to each of the sentences, or the input kana-strings, in the above input file and the near co-occurrence data sets were constructed from the above co-occurrence data file.",
        "Tablel shows the results of kana-to-kanji conversion in the following two cases.",
        "In the first case, an input kana-string does not include a final position verb.",
        "It means that each verb of the kana-strings in the input file is neglected.",
        "In the second case, an input Lana-string includes a final position verb.",
        "The experiment has shown that the conversion is carried out at the accuracy rate of 79.6% per word, where the conversion rate is 93.1% per word, in the first 'This dictionary was made by Al Soft Cu.",
        "case.",
        "Ill the same, way, the accuracy rate is 93.8% per word, where the conversion rate is 14.5% per word, in the second case.",
        "And then, we also conducted the same experiment by using the method based on the word occurrence frequency to compare our method with an ordinary method.",
        "It has shown that the accuracy rate is 72.2% per word in the first case, and 77.8% per word in the second case.",
        "We can find the accuracy rate by our method is 7.4% higher in the first case and 16.9% higher in the second case compared with the ordinary method.",
        "It is clarified that our method is more effective than the ordinary method based on the word occurrence frequency.",
        "5 An approximation of the far co-occurrence relation We can approximate the far co-occurrence relation, which is co-occurrence relation among words in a sequence of sentences, from near co-occurrence data sets.",
        "The far co-occurrence data sets are described as follows: N f= 1(111,11), (112,12), ' , (ni, id} = {(1/1,(v2,112), , where ni(i 1, 2, , 1,,) is a noun, ti is the priority value of = 1,2, ,1) is a verb and ui is the priority value of The procedure for producingfar co-occurrence data sets is:the Step1 Clear the far co-OCCUIMCC data sets.",
        "ENf,.=-c = Step2 After each fixing of noun N among homonyms in the process of kana-to-kanji conversion, renew the far co-occurrence data sets N1.11(.1 Evf,.",
        "by .following these steps:",
        "1.",
        "Change all priority values of ti(i =",
        "1,2, , In) ill the set 27fvf.",
        "to f(ti) (for example, f (ti) = 0.95ti).",
        "This process is intended to decrease priority with the passage of time.",
        "in a, simple sentence For sentencesFor sentences without; a verbwith a verb",
        "which co-occur with the noun N followed by any particle, in the near co-occurrence data set EN.,., Add new elements",
        "to the set Evi..",
        "If an element with the same verb vi already exists in Evi, add the value y(ki) to the priority value of that element instead of the new element.",
        "Here, g(ki) is a function for converting frequency of occurrence to priority value.",
        "For example,",
        "4.",
        "Let vibe the verb described in the previous step.",
        "Find all (ni, li)(j = 1, 2, , q) which co-occur with the verb viand any particle in the near co-occurrence data set Ev... Add new elements (ni,h(ki, IMO = 1,2, ,q) to the setEN1If an element with the same noun nj already exists in ENfar, add the value h(ki,li) to the priority value of that element instead of the new element.",
        "Here, h(ki,li) is a function for converting frequency of occurrence to priority value.",
        "For example, g(ki)(1 (1/1j))"
      ]
    },
    {
      "heading": "6 Processing homonyms in a",
      "text": [
        "sequence of sentences Using the far co-occurrence data sets defined in the previous section, the most feasible word among homonyms can be selected in the scope of a sequence of sentences according to the following two cases.",
        "Casel An input word written in kana-characters is a noun.",
        "where Ni(i = 1,2, ) is a homonymic kanji-variant for the input word written in kana-characters and Ti is the priority value for homonym Ni, which can be retrieved from the far co-occurrence data set EN Step2 The noun Ni which has the greatest Ti priority value in S is the most feasible noun for the input word written in kana-characters."
      ]
    },
    {
      "heading": "6.2 Procedure for case2",
      "text": [
        "Stepl Find set Sv: Sy = {(171,U1), (172,1/2), .1 Here, Vj (j = 1,2, .)",
        "is a homonymic kanji-variant for the input word written in kana-characters and Uj is the priority value for homonym Vj , which can be retrieved from the far co-occurrence data set Evi.. Step2 The verb Vj which has the greatest Uj priority value in Su is the most feasible verb for the input word written in kana-characters."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "We have proposed two new methods for processing Japanese homonyms based on the co-occurrence relation between a noun and a verb in a sentence which can be obtained easily from corpora.",
        "Using these methods, we can evaluate the co-occurrence relation of words in a simple sentence by using the near co-occurrence data sets obtained from corpora.",
        "We can also evaluate the co-occurrence relation of words in different sentences by using the far co-occurrence data sets constructed from the near co-occurrence data sets in the course of processing input sentences.",
        "The far co-occurrence data sets are based on the proposition that it is more practical to maintain a relatively small amount of data on the semantic relations between words, being changed dynamically in the course of processing, than to maintain a huge universal \"thesaurus\" data base, which does not appear to have been built successfully.",
        "An experiment of kana-to-kanji conversion by the first method for 1,129 input simple sentences has shown that the conversion is carried out in.",
        "93.1% per word and the accuracy rate is 79.6% per word.",
        "It is clarified that the first method is more effective than the ordinary method based on the word occurrence frequency.",
        "In the next stage of our study, we intend to evaluate the second method based on the far co-occurrence data sets by conducting experiments."
      ]
    }
  ]
}
