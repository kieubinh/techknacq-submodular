{
  "info": {
    "authors": [
      "Takehito Utsuro",
      "Yuji Matsumoto"
    ],
    "book": "Applied Natural Language Processing Conference",
    "id": "acl-A97-1053",
    "title": "Learning Probabilistic Subcategorization Preference by Identifying Case Dependencies and Optimal Noun Class Generalization Level",
    "url": "https://aclweb.org/anthology/A97-1053",
    "year": 1997
  },
  "references": [
    "acl-C96-1004",
    "acl-H93-1054",
    "acl-J93-1003",
    "acl-J96-1002",
    "acl-P93-1005",
    "acl-P95-1037",
    "acl-P96-1025",
    "acl-W97-0123"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper proposes a novel method of learning probabilistic subcategorization preference.",
        "In the method, for the purpose of coping with the ambiguities of case dependencies and noun class generalization of argument/adjunct nouns, we introduce a data structure which represents a tuple of independent partial subcategorization frames.",
        "Each collocation of a verb and argument/adjunct nouns is assumed to be generated from one of the possible tuples of independent partial subcategorization frames.",
        "Parameters of subcategorization preference are then estimated so as to maximize the subcategorization preference function for each collocation of a verb and argument/adjunct nouns in the training corpus.",
        "We also describe the results of the experiments on learning probabilistic subcategorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In corpus-based NLP, extraction of linguistic knowledge such as lexical/semantic collocation is one of the most important issues and has been intensively studied in recent years.",
        "In those research, extracted lex-ical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP.",
        "For example, in the context of syntactic disambiguation, Black (1993) and Magerman (1995) proposed statistical parsing models based-on decision-tree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.",
        "As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexical forms of words.",
        "Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between headwords in the parse tree.",
        "In those works, lexi-cal/semantic collocation are used for ranking parses in syntactic analysis.",
        "*The authors would like to thank Dr.",
        "Hang Li of NEC C&C Research Laboratories, Dr. Kentaro Inui of Tokyo Institute of Technology, Dr. Koiti Hasida of Electrotech-nical Laboratory, Dr. Takashi Miyata of Nara Institute of Science and Technology, and also anonymous reviewers of ANLP97 for valuable comments on this work.",
        "On the other hand, in the context of automatic lexicon construction, the emphasis is mainly on the extraction of lexical/semantic collocational knowledge of specific words rather than its use in sentence parsing.",
        "For example, Haruno (1995) applied an information-theoretic data compression technique to corpus-based case frame learning, and proposed a method of finding case frames of verbs as compressed representation of verb-noun collocational data in corpus.",
        "The work concentrated on the extraction of declarative representation of case frames and did not consider their performance in sentence parsing.",
        "This paper focuses on extracting lexical/semantic collocational knowledge of verbs for the purpose of applying it to ranking parses in syntactic analysis.",
        "More specifically, we propose a novel method for learning parameters for calculating subcategorization preference functions of verbs.",
        "In general, when learning lex-ical/semantic collocational knowledge of verbs from corpus, it is necessary to cope with the following two types of ambiguities:",
        "1) The ambiguity of case dependencies 2) The ambiguity of noun class generalization 1) is caused by the fact that, only by observing each",
        "verb-noun collocation in corpus, it is not decidable which cases are dependent on each other and which cases are optional and independent of other cases.",
        "2) is caused by the fact that, only by observing each verb-noun collocation in corpus, it is not decidable which superordinate class generates each observed leaf class in the verb-noun collocation.",
        "So far, there exist several researches which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation.",
        "Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus.",
        "Although they evaluated the obtained abstraction level of the argument noun by its performance in syntactic disambiguation, their works are limited to only one argument.",
        "Li and Abe (1996) also studied a method for learning dependencies between case slots and evaluated the discovered dependencies in the syntactic disambiguation task.",
        "They first obtained optimal abstraction levels of the argument nouns by the method in Li and Abe (1995), and then tried to discover dependencies between the class-based case slots.",
        "They reported that dependencies",
        "were discovered only at the slot-level and not at the class-level.",
        "Compared with those previous works, this paper proposes to cope with the above two ambiguities in a uniform way.",
        "First, we introduce a data structure which represents a tuple of independent partial subcategorization frames.",
        "Each collocation of a verb and argument/adjunct nouns is assumed to be generated from one of the possible tuples of independent partial subcategorization frames.",
        "Then, parameters of subcategorization preference are estimated so as to maximize the subcategorization preference function for each collocation of a verb and argument/adjunct nouns in the training corpus.",
        "We describe the results of the experiments on learning probabilistic subcategorization preference from the EDR Japanese bracketed corpus (EDR, 1995), as well as those on evaluating the performance of subcategorization preference."
      ]
    },
    {
      "heading": "2 Data Structure",
      "text": []
    },
    {
      "heading": "2.1 Verb-Noun Collocation",
      "text": [
        "Verb-noun collocation is a data structure for the collocation of a verb and all of its argument/adjunct nouns.",
        "A verb-noun collocation e is represented by a feature structure which consists of the verb v and all the pairs of co-occurring case-markers p and thesaurus classes c of case-marked nouns:1 pred: v",
        "We assume that a thesaurus is a tree-structured type hierarchy in which each node represents a semantic class, and each thesaurus class .",
        ", ck in a verb-noun collocation is a leaf class.",
        "We also introduce as the superordinate-subordinate relation of classes in a thesaurus: c1c2 means that c1 is subordinate to C2."
      ]
    },
    {
      "heading": "2.2 Subcategorization Frame",
      "text": [
        "A subcategorization frame f is represented by a feature structure which consists of a verb v and the pairs of case-markers p and sense restriction c of case-marked argument/adjunct nouns:",
        "red: v Sense restriction c1,... of case-marked argu-ment/adjunct nouns are represented by classes at arbitrary levels of the thesaurus.",
        "A subcategorization frame f can be divided into two parts: one is the verbal part f, containing the verb v while the other is the nominal part fp containing all the pairs of 'Although we ignore sense ambiguities of case-marked nouns in this definition, in section 5.2, we briefly mention how we deal with sense ambiguities of case-marked nouns in the current implementation.",
        "case-markers p and sense restriction c of case-marked nouns.",
        "f= fv A fp =[{ pred : v ] A"
      ]
    },
    {
      "heading": "2.3 Subsumption Relation",
      "text": [
        "We introduce subsumption relation -< f of a verb-noun collocation e and a subcategorization frame f: e f f iff.",
        "for each case-marker pi in f and its noun class Ci f, there exists the same case-marker pi in e and its noun class Cie is subordinate to Ci , i.e. ciecif The subsumption relation --<f is applicable also as a subsumption relation of two subcategorization frames."
      ]
    },
    {
      "heading": "3 A Model of Generating Verb-Noun Collocation",
      "text": [
        "In this section, we introduce a model of generating a verb-noun collocation from subcategorization frame(s).",
        "In order to cope with the ambiguities of case dependencies and noun class generalization in this model, we introduce a data structure which represents a tuple of independent partial subcategorization frames."
      ]
    },
    {
      "heading": "3.1 Generating a Verb-Noun Collocation from Independent Partial Subcategorization Frames",
      "text": [
        "First, we describe the idea of generating a verb-noun collocation from a subcategorization frame, or a tuple of partial subcategorization frames.",
        "Generation from a Subcategorization Frame Suppose a verb-noun collocation e is given as: [pred: v",
        "Then, let us consider a subcategorization frame f which can generate e. We assume that f has exactly the same case-markers as e has,2 and each semantic class cif of a case-marked noun of f is superordinate to the corresponding leaf semantic class Cie of e: pp ired e: fv",
        "Then, we denote the generation of the verb-noun collocation e from the subcategorization frame f as: fe Next, we describe the idea of generating a verb-noun collocation from a tuple of partial subcategorization frames which are independent of each other.",
        "'Since we do not consider ellipsis of argument nouns when generating a verb-noun collocation from a subcategorization frame, the subcategorization frame f is required to have exactly the same case-markers as e."
      ]
    },
    {
      "heading": "Partial Subcategorization Frame",
      "text": [
        "First, we define a partial subcategorization frame fi of f as a subcategorization frame which has the same verb v as f as well as some of the case-markers of f and their semantic classes.",
        "Then, we can find a division of f into a tuplefn) of partial subcategorization frames of f, where any pair fi and fi, (i i') do not have common case-markers and the unification"
      ]
    },
    {
      "heading": "Independence of Partial Subcategorization Frames",
      "text": [
        "We allow the division of f into a tuplefn) of partial subcategorization frames as in the equation (3) only when the partial subcategorization frames , fn can be regarded as events occurring independently of each other.",
        "With some corpus, usually we can estimate the conditional probabilities p(f v) and p(fi v) of the (partial) subcategorization frames f and fi (i = 1, , n) given the verb v. According to the estimated probabilities, we can judge whether fl,...,.fn are independent of each other as follows.",
        "First, we estimate the conditional probability p(f I v) of a (partial) subcategorization frame f by summing up the conditional probabilities p(e v) of all the verb-noun collocations e given the verb v, where e is subsumed by f (e f f)3",
        "The conditional joint probabilityv) is also estimated by summing up p( v) where e is subsumed by all offn (e -L<f f",
        "e\"<..... Then, we give a formal definition of independence of partial subcategorization frames according to the estimated conditional probabilities: partial subcategorization frames fn are independent if, any pair fi and f (i j) do not have common case-markers, and for every subset fi , fii of j of these partial subcategorization frames (j = 2, ... , n), the following equation holds: Afii, , f., I v) = p(f.i I v) p(f., I v)(7) Since it is too strict to judge the independence of partial subcategorization frames by the equation (7), 3 The probability p(e I v) can be estimated as freg(e)1 freg(v) by M.L.E.",
        "(maximum likelihood estimation) directly from the training corpus.",
        "we relax the constraint of independence using a relaxation parameter a (0 < a < 1).",
        "Partial subcategorization framesfn are judged as independent if, for every subset,of j of these partial subcategorization frames (j = 2, ... , n), the following inequalities hold: P(f.i, , f., I v)e 1 a 5 p(f,,I v) p(f., I v) Generation from Independent Partial Subcategorization Frames Now, as in the case of the generation from a subcategorization frame f , we denote the generation of e from a tuple (fl, fn) of independent partial subcategorization frames of f as below: (fi, ,e"
      ]
    },
    {
      "heading": "3.2 The Ambiguity of Case Dependencies",
      "text": [
        "This section describes the problem of the ambiguity of case dependencies when observing verb-noun collocation in corpus.",
        "This problem is caused by the fact that, only by observing each verb-noun collocation in corpus, it is not decidable which cases are dependent on each other and which cases are optional and independent of other cases.",
        "For example, consider the following example: Example 1 Kodomo-ga kouen-de juusu-wo nomu.",
        "child-NOMpark-at juice-ACC drink (A child drinks juice at the park.)",
        "The verb-noun collocation is represented as a feature structure e below:",
        "In this feature structure e, cc, cp and ci represent the leaf classes (in the thesaurus) of the nouns \"kodomo(child)\", \"kouen(park)\", and \"juusu(juice)\".",
        "Next, we assume that the concepts \"human\", \"place \", and \"beverage\" are superordinate to \"kodomo(child) \", \"kouen(park)\", and \"juusu(juice)\", respectively, and introduce the corresponding classes Chum, epic, and cb Then, the following superordinate-subordinate relations hold: Cc -\"<c Chum, Cp'fieCpic, C7-.<c cbet, Allowing these superordinate classes as sense restriction in subcategorization frames, let us consider the several patterns of subcategorization frames which can generate the verb-noun collocation e. Those patterns of subcategorization frames vary according to the dependencies of cases within them.",
        "If the three cases \"ga(NOM)\", \"wo(ACC)\", and \"de(at)\" are dependent on each other and it is not possible to find any division into a tuple of several independent partial subcategorization frames, e can be regarded as generated from a subcategorization frame containing all of the three cases:",
        "Otherwise, if only the two cases \"ga(NOM)\" and \"wo(ACC)\" are dependent on each other and the \"de(at)\" case is independent of those two cases, e can be regarded as generated from the following tuple of independent partial subcategorization frames: ([pred :hnomu wo: Chet, Otherwise, if all the three cases \"ga(NOM)\", \"wo(ACC)\", and \"de(at)\" are independent of each other, e can be regarded as generated from the following tuple of independent partial subcategorization frames, each of which contains only one case:"
      ]
    },
    {
      "heading": "3.3 The Ambiguity of Noun Class Generalization",
      "text": [
        "This section describes the problem of the ambiguity of noun class generalization when observing verb-noun collocation in corpus.",
        "This problem is caused by the fact that, only by observing each verb-noun collocation in corpus, it is not decidable which superordinate class generates each observed leaf class in the verb-noun collocation.",
        "For example, let us again consider Example 1.",
        "We assume that the concepts \"animal\" and \"liquid\" are superordinate to \"human\" and \"beverage\", respectively, and introduce the corresponding classes Cani and cuq.",
        "Then, the following superordinate-subordinate relations hold: Chum -'<c Cani, Cbev -<c Cliq If we additionally allow these superordinate classes as sense restriction in subcategorization frames, we can consider several additional patterns of subcategorization frames which can generate the verb-noun collocation e, along with those patterns described in the previous section.",
        "Suppose that only the two cases \"ga(NOM)\" and \"wo(ACC)\" are dependent on each other and the \"de(at)\" case is independent of those two cases as in the formula (10).",
        "Since the leaf class cc (\"child\") can be generated from either churn or Cani, and also the leaf class ci (\"juice\") can be generated from either Cbev or cuq, e can be regarded as generated according to either of the four formulas (10) and (12)(14):"
      ]
    },
    {
      "heading": "3.4 A Model of Generating Verb-Noun Collocation",
      "text": [
        "When observing each verb-noun collocation e, as we described in the previous two sections, the ambiguities of case dependencies and noun class generalization remain, and it is necessary to consider every possible tuple of independent partial subcategorization frames which can generate the observed verb-noun collocation e. In order to cope with these ambiguities, we introduce two sets: one is a set F of tuples fn) of independent partial subcategorization frames and the other is a set E of verb-noun collocations e. The generation of a verb-noun collocation from a tuple of independent partial subcategorization frames can be regarded as a mapping it from F to E:",
        "Usually, for each given verb-noun collocation in E, there exist several possible tuples of independent partial subcategorization frames in F. Thus, it is a many-to-one mapping.",
        "The mapping from a tuple fn) of independent partial subcategorization frames to a verb-noun collocation e can be denoted also as follows: ,--+ e(16) When observing a verb-noun collocation e, we assume this many-to-one mapping 7r and consider every possible tuple of independent partial subcategorization frames which can generate e, according to the ambiguities of case dependencies and noun class generalization."
      ]
    },
    {
      "heading": "3.5 Parameters of Generating Verb-Noun Collocation",
      "text": [
        "Before we give definitions of subcategorization preference functions in the next section, we introduce the parameter q(fk v) of generating verb-noun collocation, which is used in the calculation of the subcategorization preference.",
        "The parameter q(fk v) can be regarded as the conditional probability of the partial subcategorization frame fk and could be estimated in the similar way as the p(f v) in the formula (5).",
        "However, it is the parameter of generating verb-noun collocation and have to be estimated so as to maximize the subcategorization preference function for the training corpus.",
        "One solution of this parameter estimation process might be to regard the model of generating verb-noun collocation as a probabilistic model and then to apply the maximum likelihood estimation method.",
        "When estimating the parameters from the training sample, we have to note that each verb-noun collocation is ambiguous since it could be interpreted in several different ways according to case dependencies and optimal noun class generalization levels.",
        "As for parameter estimation of probabilistic models from ambiguous training sample, EM algorithm(Baum, 1972) is a well-known solution and has been studied for years.",
        "In EM algorithm, parameters are assigned to events, and it is required that parameters sum up to 1.",
        "However, since two subcategorization frames could have the same case and a subsumption relation could hold",
        "between their sense restrictions, they may have overlap and the requirement that parameters sum up to 1 is not satisfiable.",
        "Therefore, it is not so straightforward to apply EM algorithm to the task of parameter estimation of generating verb-noun collocation.",
        "Instead of introducing a probabilistic model of generating verb-noun collocation', in this paper, we employ more general framework which is applicable to various measures of subcategorization preference including the probability of generating verb-noun collocation.",
        "In the framework, the process of parameter estimation is regarded as a general optimization problem of maximizing the subcategorization preference function for the training corpus.",
        "In order to describe the framework, first we introduce the probability p((fi,... ,ei I ei) of generating a verb-noun collocation e1 in the set E from a tuplefri)j in the set F, given ei, and denote it as a conditional probability p((h, , frz)i I ei).",
        "Then, for each ei in E, we can consider a probability distribution P((ii, ,.fn)i ei) over the set F of tuples of independent partial subcategorization frames: el-el F(.fi, P((.fi , ....f.).7 I et): (, ,,.f..,)1.. , f,o,),,, isfies the following axiom of the probability: Each probability distribution p((fi,..., fn)i I ei) sat-E pc (fi , .",
        ", fn)i I ei) = 1 for all i According to the probability distribution p((fi,..., fn)i ei) of generating ei from (fi, , fn)j, we estimate the frequency of the subcategorization frame fk and then estimate the parameter q(fk v) as below: E1 p((fi,.",
        "fk, , f.)2 I ei)",
        "When learning probabilistic subcategorization preference (section 5), we estimate the probability distribution p((fi, , ei) for each ei so as to maximize the subcategorization preference function for ei."
      ]
    },
    {
      "heading": "4 Subcategorization Preference",
      "text": []
    },
    {
      "heading": "Functions",
      "text": [
        "This section introduces a function 0 which measures the subcategorization preference when generating a verb-noun collocation e from a tuple fn) of independent partial subcategorization frames:",
        "In this paper, we introduce a subcategorization preference function which is based-on the idea of Kullback Leibler distance.5 4Another alternative of solving the problem of learning probabilistic subcategorization preference based-on a probabilistic model is to regard the problem as the construction of probabilistic models from the training sample.",
        "We will discuss this issue in section 7.",
        "5 In Utsuro and Matsumoto (1997), we defined another subcategorization preference function op which is based4.1"
      ]
    },
    {
      "heading": "Nominal Parts of (Partial) Subcategorization Frames",
      "text": [
        "First, let fp, fpi,, fpn be the nominal parts of (partial) subcategorization frames f,,in the equations (2) and (4), respectively:",
        "As in the case of the parameters q(fi v) of fi given the verb v, we estimate the probability p(fpi) of the nominal part fpi in the whole corpus and call it the parameter q(fpi) of fpi in the whole training corpus.",
        "We estimate the frequency of fpi throughout the whole training corpus and then estimate the parameter q(fpi) of fpi as below:"
      ]
    },
    {
      "heading": "4.2 Okl: Kullback Leibler Distance",
      "text": [
        "Rather than the simple conditional probability, this preference function is intended to measure the information-theoretic association of the verb v and the nominal part of the subcategorization frame.",
        "The Kullback Leibler (KL) distance is a measure of the distance between two probability distribution.",
        "p(X) and q(X) is defined as below(Cover and Thomas, tributions p(X) and q(X), the KL distance D(pIq) of Given a random variable X and two probability dis1991), where each term can be regarded as the distance of two probabilities p(x) and q(x) of an event x:",
        "In order to apply the idea of the KL distance to measuring the association of the verb v and the nominal part fp of f, we introduce a random variable Fp which takes fp as its value.",
        "We also introduce the probability distribution p(Fp) of Fp and the conditional probability distribution p(Fp I v) of Fp given the verb v. Then, the KL distance of p(Fp v) and p(Fp) is denoted as D(p(Fp I v)IIp(Fp)) and each term of it can be regarded as the distance of two probabilities p(fp v) and p(fp).",
        "We assume that the larger this distance is, the stronger the association of fp and v is, and measure the association of fp and v with this on the probability of generating the verb-noun collocation and described experimental results of applying Op to the task of learning probabilistic sub categorization.",
        "distance of the two probabilities p( fp I u) and p( fp).",
        "With this idea.",
        "the subcategorization preference function 0k1 is now formally defined as below:6 7",
        "(21) is derived from the independence of the partial subcategorization frames 11, fr, In (22), we use the parameters q(fpi I v) and g(f pi) as an approximation of the probabilities p( f pi v) and p( fpi)."
      ]
    },
    {
      "heading": "5 Learning Probabilistic",
      "text": []
    },
    {
      "heading": "Subcategorization Preference",
      "text": [
        "The problem of learning subcategorization preference can be formalized as an optimization problem of estimating the probability distribution p((fi, , fn):7 ei) (in section 3.5) of generating ei from , fn)i (and then the parameters q(fpk v) and q(fpk)) so as to maximize the value of the subcategorization preference function for the whole training corpus.",
        "In this paper, we give only an approximate solution to this problem: we estimate the probability distribution p((fi, , fr)j ei) for each ei so as to maximize the value of the subcategorization preference function only for ei, not for the whole training corpus."
      ]
    },
    {
      "heading": "5.1 Problem Setting",
      "text": [
        "Let the training corpus be the set of verb-noun collocation e. We define the subcategorization preference q (e) of a verb-noun collocation e as the maximum of the subcategorization preference function 0 (the formula (18)) of generating e from a tuple, fn).",
        "(b(e) =max ci5((f1, e)(23) Now, the problem of learning probabilistic subcategorization preference is stated as: for every verb-noun collocation e in 5, estimating the probability distribution p((fi, 6Resnik (1993) applys the idea of the KL distance to measuring the association of a verb v and its object noun class c. Our definition of OH corresponds to an extension of Resnik's association score, which considers dependencies of more than one case-markers in a subcategorization frame.",
        "7Another related measure is Dunning (1993)'s likelihood ratio tests for binomial and multinomial distributions, which are claimed to be effective even with very much smaller volumes of text than is necessary for other tests based on assumed normal distributions.",
        "fn)i I e) of generating e from (fi.",
        "fr,)j. under the constraint that the value of the subcategorization preference o(e) is maximized."
      ]
    },
    {
      "heading": "5.2 Learning Algorithm",
      "text": [
        "F(e) contains a tuple (f) consisting of only one subcategorization frame f only if f can not be divided into several independent partial subcategorization frames.",
        "Then, we assume that each element of F(e) occurs evenly and estimate the initial conditional probability distribution p((fi,..., fr,)3 e) of generating e from (ul, , in), as an approximation below:"
      ]
    },
    {
      "heading": "5.2.1 Approximate Estimation of Verb-Independent Parameters",
      "text": [
        "Using the initial conditional probability distribution of p((fi, , I e) as in the formula (25), the initial values of the verb-independent parameters q(fpk) are estimated by the formulas (19).",
        "In the current implementation of the learning algorithm, we use these initial values as approximate estimation of those verb-independent parameters and probabilities throughout the learning process."
      ]
    },
    {
      "heading": "Verb-Dependent Parameters",
      "text": [
        "Verb-dependent parameters q(fk v)(= q(fpk v)) are iteratively estimated so as to maximize the subcategorization preference gi(e) for every verb-noun collocation e in the training corpus E. As a learning algorithm, we employ the following stingy algorithm:"
      ]
    },
    {
      "heading": "1. Initialization",
      "text": [
        "As with the case of the verb-independent parameters, for each verb-noun collocatoin e in 5, the set F(e) is initially constructed according to the definition in (24).",
        "Then, the initial conditional probability distribution of p((fi,..., fn), e) and the initial values of the verb-dependent parameters q(fk v) are estimated as (25) and (17), respectively.",
        "8 In the current implementation, we deal with sense ambiguities of case-marked nouns and case ambiguities of Japanese topic-marking post-positional particles such as \"ha(TOPIC)\", \"mo(ALSO)\", and \"dake(ONLY)\".",
        "When constructing the set F(e), we consider all the possible combination of senses of semantically ambiguous nouns and cases of topic-marking post-positional particles.",
        "These ambiguities can be resolved by maximizing the subcategorization preference function (section 5.2.2).",
        "(20)First.",
        "we identify independent partial subcategorization frames according to the condition of (8).",
        "Then, let E(r) be the set of verb-noun collocations containing the verb r in the training corpus E. Let F(e) be the (21)set of tuples (11fn) of independent partial subcategorization frames which can generate e and satisfy the independence condition of (8).8"
      ]
    },
    {
      "heading": "2. Iterative Reestimation",
      "text": [
        "The subcategorization preference .",
        ":/ii(e) are maximized by repeatedly searching the set F(e) for tuples (h,..., fn) which give the maximum subcategorization preference and removing other tuples from F(e).",
        "The following two steps are repeated until the values of the parameters q( fk v) converge.",
        "(2a)For each verb-noun collocatoin e in 1, set F(e) as the set of tuples (fl,..., f) of independent partial subcategorization frames which can generate e and give the maximum subcategorization preference in the equation (23).",
        "F(e){(f1,.",
        ", f.)14((11, ,e)=(7)(e)} (2b) Set the values of the conditional probabilities P((fi, , fn)j e) as below and the parameters q(fk v) as (17), respectively:1"
      ]
    },
    {
      "heading": "6 Experiments and Evaluation",
      "text": []
    },
    {
      "heading": "6.1 Corpus and Thesaurus",
      "text": [
        "As the training and test corpus, we used the EDR Japanese bracketed corpus (EDR, 1995), which contains about 210,000 sentences collected from newspaper and magazine articles.",
        "From the EDR corpus, we extracted 153,014 verb-noun collocations of 835 verbs which appear more than 50 times in the corpus.",
        "These verb-noun collocations contain about 270 case-markers.",
        "We constructed the training set from these 153,014 verb-noun collocations.",
        "We used `Bunrui Goi Hyou'(BGH) (NLRI, 1993) as the Japanese thesaurus.",
        "BGH has a six-layered abstraction hierarchy and more than 60,000 words are assigned at the leaves and its nominal part contains about 45,000 words.",
        "Five classes are allocated at the next level from the root node."
      ]
    },
    {
      "heading": "6.2 Experiments and Results",
      "text": [
        "From the training set S, we first estimated the values of verb-independent parameters as in section 5.2.1, and then iteratively reestimated verb-dependent parameters of the subcategorization preference function Oki for 10 verbs as in section 5.2.2.",
        "For each of the 10 verbs, the numbers of verb-noun collocations are 100 ti 500.",
        "We made experiments with the independence parameter a=0.5/0.7/0.9.",
        "In the iterative reestimation procedure, the values of the verb-dependent parameters converged after 2 5 iterations.",
        "For the 10 verbs, about 75% of the verb-noun collocations have only one case-marked noun.",
        "The rate that tuples of partial subcategorization frames are judged as independent increases as the value of the independence parameter a decreases.",
        "This rate increases from 1.4% (a =0.9) to 12.1% (a =0.5).",
        "As an example, for the verb \"kau(buy,incur)\", Table 1 shows the set F(e) of tuples of independent partial subcategorization frames which give maximum subcategorization preference.",
        "The table lists the sets F(e) with 10 highest preference values of Ok1, along with the numbers (the column 'Egs.')",
        "of verb-noun collocations for each F(e), which are judged as generated from it'.",
        "Since about 75% of the verb-noun collocations have only one case-marked noun, most of the 10 high-scored sets have only one case-marked noun.",
        "However, the 10 high-scored sets cover about 60% of the verb-noun collocations in the training set, and they can be regarded as typical subcategorization frames of the verb \"kau(buy,incur)\"."
      ]
    },
    {
      "heading": "6.3 Evaluation of Subcategorization Preference",
      "text": [
        "We evaluate the performance of the estimated parameters of the subcategorization preference as follows.",
        "Suppose that the following word sequence represents a verb-final Japanese sentence with a subordinate clause, where Nx,N2k are nouns, px,...,p2k are case-marking post-positional particles, v2 are verbs, and the first verb v1 is the head verb of the subordinate clause.",
        "Nx-p.-Ni 1-pi 1 -' -- N21 -P21 - - N2 k-P2k-V2 We consider the subcategorization ambiguity of the post-positional phrase Nx-px: i.e, whether Nx-px is subcategorized by v1 or v2.",
        "We use held-out verb-noun collocations of the verbs v1 and v2 which are not used in the training.",
        "They are like those verb-noun collocations eci and ea in the left side below.",
        "Next, we generate erroneous verb-noun collocations e1 of viand ee2 of v2 as those in the right side below, by choosing a case element px:INT, at random and moving it from v1 to v2.",
        "classes of BGH thesaurus are represented as numerical codes, in which each digit denotes the choice of the branch in the thesaurus.",
        "Then, we compare the sum 0(e c1) 0(ec2) of the maximums (in the definition (23)) of Oki for the correct pair with the sum q; (eel) + (kee2) of those for the erroneous pair, and calculate the rate that the correct pair has the greater value.",
        "For the purpose of evaluating the effectiveness of factors of learning probabilistic subcategorization preference, we perform experiments with different settings and compare their results.",
        "The following two options are examined: Whether the subcategorization preference function uses tuples of partial subcategorization frames judged as independent (\"Independent\"), or any tuples (\"Any\").",
        "The independence parameter a =0.5/0.9.",
        "For three Japanese verbs \"kau (buy,incur)\", \"nomu (drink)\", and \"kasaneru (pile up, repeat)\", we extracted pairs of correct verb-noun collocations and evaluated the performance of subcategorization preference.",
        "Table 2 gives the results averaged over extracted pairs, including the accuracies of subcategorization preference.",
        "The difference of \"Optimal\"/\"Initial\" means that initial values of the parameters are used instead of optimized values (section 5.2.2) when the subcategorization preference function is not applicable to the given verb-noun collocation and returns zero.",
        "The line \"Accuracy\" lists the sums of both \"Optimal\" and \"Initial\" accuracies, while the line \"Applicability\" lists the percentages of positive values of the subcategorization preference function with optimized parameters.",
        "It is natural that the settings with more weak conditions on the independence judgment of partial subcategorization frames result in higher applicabilities.",
        "The setting with independent tuples of partial subcategorization frames achieves higher accuracy than that with any tuples, and this result claims that the result of the independence judgment is effective when applying the estimated parameters to the task of subcategorization preference.",
        "Even in the case of the setting with any tuples, the setting with a =0.5 gives poorer accuracy than that of a = 0.9.",
        "In this case, the difference of the independence parameter a affects only the parameter estimation stage.",
        "This result claims that the independence judgment process is effective also when estimating parameters from the training corpus."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "This paper proposed a novel method of learning probabilistic subcategorization preference of verbs.",
        "We described a part of the results of the experiments on learning probabilistic subcategorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference.",
        "Although the scale of the evaluation experiment was relatively small, we achieved accuracies higher than 96%.",
        "The details of the experimental results are available in Utsuro and Matsumoto (1997).",
        "As we mentioned in section 3.5, probabilistic model construction methods might be also applicable to the task of learning probabilistic subcategorization preference.",
        "We have already applied the maximum entropy methods(Pietra, Pietra, and Lafferty, 1995; Berger, Pietra, and Pietra, 1996) to this task(Utsuro, Miyata, and Matsumoto, 1997) and are also planning to evaluate the effectiveness of the MDL principle(Rissanen, 1989) when combining with the maximum entropy method.",
        "Their results will be compared with those of the method proposed in this paper and reported in the near future."
      ]
    }
  ]
}
