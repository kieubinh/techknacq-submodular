{
  "info": {
    "authors": [
      "Christoph Tillmann",
      "Stephan Vogel",
      "Hermann Ney",
      "Alex Zubiaga"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P97-1037",
    "title": "A DP-Based Search Using Monotone Alignments in Statistical Translation",
    "url": "https://aclweb.org/anthology/P97-1037",
    "year": 1997
  },
  "references": [
    "acl-C94-2178",
    "acl-C96-2141",
    "acl-H94-1028",
    "acl-J93-2003",
    "acl-P96-1021"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we describe a Dynamic Programming (DP) based search algorithm for statistical translation and present experimental results.",
        "The statistical translation uses two sources of information: a translation model and a language model.",
        "The language model used is a standard bigram model.",
        "For the translation model, the alignment probabilities are made dependent on the differences in the alignment positions rather than on the absolute positions.",
        "Thus, the approach amounts to a first-order Hidden Markov model (HMM) as they are used successfully in speech recognition for the time alignment problem.",
        "Under the assumption that the alignment is monotone with respect to the word order in both languages, an efficient search strategy for translation can be formulated.",
        "The details of the search algorithm are described.",
        "Experiments on the EuTrans corpus produced a word error rate of 5.1%."
      ]
    },
    {
      "heading": "1 Overview: The Statistical Approach to Translation",
      "text": [
        "The goal is the translation of a text given in some source language into a target language.",
        "We are given a source (*French') string = which is to be translated into a target ('English') string = Among all possible target strings, we will choose the one with the highest probability which is given by Hayes' decision rule (Brown et al.. 1993):",
        "• arg max {Pr(eldfil )} - 1 e • arg max {Pi'() • Pr( fi/ Id))",
        "Pr(el ) is the language model of the target.",
        "language, whereas Pr( ) is the string translation model.",
        "The argmax operation denotes the search problem.",
        "In this paper, we address",
        "• the problem of introducing structures into the probabilistic dependencies in order to model the string translation probability Pr(ff • the search procedure.",
        "i.e. an algorithm to perform the argmax operation in an efficient way.",
        "• transformation steps for both the source and the target.",
        "languages in order to improve the translation process.",
        "The transformations are very much dependent on the language pair and the specific translation task and are therefore discussed in the context of the task description.",
        "We have to keep in mind that in the search procedure both the language and the translation model are applied after the text transformation steps.",
        "However, to keep the notation simple we will not make this explicit distinction in the subsequent exposition.",
        "The overall architecture of the statistical translation approach is summarized in Figure 1."
      ]
    },
    {
      "heading": "2 Alignment Models",
      "text": [
        "A key issue in modeling the string translation probability Pr(fil is the question of how we define the correspondence between the words of the target.",
        "sentence and the words of the source sentence.",
        "In typical cases, we can assume a sort of pairwise dependence by considering all word pairs (fi, ei) for a given sentence pair [fe; ef].",
        "We further constrain this model by assigning each source word to exactly one target word.",
        "Models describing these types of dependencies are referred to as alignment models (Brown et al., 1993), (Dagan et al.. 1993).",
        "(Kay k ROscheisen, 1993).",
        "(Fung k Church.",
        "1994), (Vogel et al., 1996).",
        "In this section, we introduce a monotone HMAI based alignment and an associated DP based search algorithm for translation.",
        "Another approach to statistical machine translation using DP was presented in (Wu, 1996).",
        "The notational convention will be as follows.",
        "We use the symbol Pr(.)",
        "to denote general",
        "probability distributions with (nearly) no specific assumptions.",
        "In contrast for model-based probability distributions, we use the generic symbol p(.",
        ")."
      ]
    },
    {
      "heading": "2.1 Alignment with HMM",
      "text": [
        "When aligning the words in parallel texts (for Indo-European language pairs like Spanish-English, German-English, Italian-German,...), we typically observe a strong localization effect.",
        "Figure 2 illustrates this effect for the language pair Spanish-boEnglish.",
        "In many cases, although not always, there is an even stronger restriction: the difference in the position index is smaller than 3 and the alignment is essentially monotone.",
        "To be more precise, the sentences can be partitioned into a small number of segments.",
        "within each of which the alignment is monotone with respect to word order in both lan-pages.",
        "To describe these word-by-word alignments, we introduce the mapping j – aj, which assigns a. position j (with source word fj ) to the position i = aj (with target word fi).",
        "The concept of these alignments is similar to the ones introduced by (Brown et al.. 1993), but we will use another type of dependence in the probability distributions.",
        "Looking at such alignments produced by a human expert.",
        "it is evident that the mathematical model should try to capture the strong dependence of ai on the preceding alignment aj _ Therefore the probability of alignment aj for position j should have a dependence on the previous alignment position",
        "A similar approach ha.s been chosen by (Da.gan et al., 1993) and (Vogel et al.. 1996).",
        "Thus the problem formulation is similar to that of the time alignment problem in speech recognition, where the so-called Hidden Markov models have been successfully used for a long time (Jelinek, 1976).",
        "Using the same basic principles, we can rewrite the probability by introducing the 'hidden' alignments a.1 := ...aj for a sentence pair [fi'; ef]:",
        "To avoid any confusion with the term 'hidden in comparison with speech recognition.",
        "we observe that the model states as such (representing words) are not hidden but.",
        "the actual alignments, i.e. the sequence of position index pairs (j• i ai).",
        "So far there has been no basic restriction of the approach.",
        "We now assume a first-order dependence on the alignments aj only:",
        "where, in addition, we have assumed that the lexicon probability p(fle) depends only on ai and not on ai-1.",
        "To reduce the number of alignment parameters, we assume that the HMM alignment.",
        "probabilities Mil/1) depend only on the jump width (i - 1').",
        "The monotony condition can than be formulated as: = 0 for + O.",
        "+ 1.",
        "-1-2.",
        "This monotony requirement limits the applicability of our approach.",
        "However, by performing simple word reorderings, it is possible to approach this requirement (see Section 4.2).",
        "Additional countermeasures will be discussed later.",
        "Figure 3 gives an illustration of the possible alignments for the monotone hidden Markov model.",
        "To draw the analogy with speech recognition.",
        "we have to identify the states (along the vertical axis) with the positions i of the target words ci and the time (along the horizontal axis) with the positions j of the source words fj ."
      ]
    },
    {
      "heading": "2.2 Training",
      "text": [
        "To train the alignment and the lexicon model, we use the maximum likelihood criterion in the so-called maximum approximation.",
        "i.e. the likelihood criterion covers only the most likely alignment rather than the set of all alignments:",
        "the maximum approximation: max II p(6ilei_i) max",
        "Here and in the following, we omit a special treatment of the start and end conditions like j 1 or j = J in order to simplify the presentation and avoid confusing details.",
        "Having the above criterion in mind, we try to associate the language model probabilities with the alignments j ai.",
        "To this purpose, we exploit, the monotony property of our alignment model which allows only transitions from aj...1 to ai if the difference 6 oi - aj-1 is 0,1,2.",
        "We define a modified probability pA(elei) for the language model depending on the alignment difference 6.",
        "We consider each of the three cases 6 = 0,1.2 separately: • 6 = 0 (horizontal transition = alignment repetition): This case corresponds to a target word with two or more aligned source words and therefore requires c = c' so that there is no contribution from the language model:",
        "0 for e e' • 6 = 1 (forward transition = regular alignment): This case is the regular one, and we can use directly the probability of the bigram language model: P6=1 (de') = Meici • 6 = 2 (skip transition = non-aligned word):",
        "This case corresponds to skipping a word.",
        "i.e, there is a word in the target.",
        "string with no aligned word in the source string.",
        "We have to find the highest probability of placing a non-aligned word 7: between a predecessor word c' and a successor word c. Thus we optimize the following product.",
        "over the non-aligned word",
        "This maximization is done beforehand and the result is stored in a table.",
        "Using this modified probability p(e)e'), we can rewrite the overall search criterion: max H [plait(' )1)(hl_a„)] .",
        "The problem now is to find the unknown mapping: j – (ai, c ) which defines a path through a network with a uniform trellis structure.",
        "For this trellis, we can still use Figure 3.",
        "However, in each position i along the To find the optimal alignment we use dynamic programming for which we have the following typical recursion formula: = p(f)rniax[p(iiii ) • (Ai' , - 1)] Here.",
        "Q(i j) is a sort of partial probability as in time alignment for speech recognition (Jelinek, 1976).",
        "As a result the training procedure amounts to a sequence of iterations.",
        "each of which consists of two steps: • position alignment: Given the model parameters, determine the most likely position alignment.",
        "• parameter estimation: Given the position alignment i.e. going along the alignment paths for all sentence pairs, perform maximum likelihood estimation of the model parameters; for model-free distributions, these estimates result in relative frequencies.",
        "The IBM model 1 (Brown et al., 1993) is used to find an initial estimate of the translation probabilities.",
        "3 Search Algorithm for Translation For the translation operation.",
        "we use a bigra.m language model, which is given in terms of the conditional probability of observing word ci given the predecessor word 2Afilei-i) • Using the conditional probability of the bigram language model, we have the overall search criterion in",
        "vertical axis, we have to allow all possible words c of the target vocabulary.",
        "Due to the monotony of our alignment model and the bigram language model.",
        "we have only first-order type dependencies such that the local probabilities (or costs when using the negative logarithms of the probabilities) depend only on the arcs (or transitions) in the lattice.",
        "Each possible index triple (i .",
        "j. c) defines a grid point in the lattice.",
        "and we have the following set of possible transitions from one grid point to another grid point:",
        "Using this formulation of the search task, we can now use the method of dynamic programming (DP) to find the best path through the lattice.",
        "To this purpose.",
        "we introduce the auxiliary quantity: Q(i. j. c): probability of the best partial path which ends in the grid point (i, j, c).",
        "Since we have only first-order dependencies in our model, it is easy to see that the auxiliary quantity must satisfy the following DP recursion equation:",
        "To explicitly construct the unknown word sequence .",
        "it is convenient to make use of so-called back-pointers which store for each grid point.",
        "(i. j, c) the best predecessor grid point (Ney et.",
        "al., 1992).",
        "The DP equation is evaluated recursively to find the best partial path to each grid point.",
        "(i, j, c).",
        "The resulting algorithm is depicted in Table 1.",
        "The complexity of the algorithm is J 'ma., • E2 where E is the size of the target language vocabulary and Ima,,.",
        "is the maximum lengni of the target sentence considered.",
        "It is possible to reduce this computational complexity by using so-called pruning methods (Ney et al.. 1992): due to space limitations, they are not discussed here."
      ]
    },
    {
      "heading": "4 Experimental Results",
      "text": []
    },
    {
      "heading": "4.1 The Task and the Corpus",
      "text": [
        "The search algorithm proposed in this paper was tested on a subtask of the **Traveler Task (Vida, 1997).",
        "The general domain of the task comprises typical situations a visitor to a foreign country is faced with.",
        "The chosen subtask corresponds to a scenario of the human-to-human communication situations at the registration desk in a hotel (see Table 4).",
        "The corpus was generated in a semi-automatic way.",
        "On the basis of examples from traveller booklets, a probabilistic grammar for different language pairs has been constructed from which a large corpus of sentence pairs was generated.",
        "The vocabulary consisted of 692 Spanish and 518 English words (including punctuation marks).",
        "For the experiments, a. training corpus of 80,000 sentence pairs with 628,117 Spanish and 684.777 English words was used.",
        "In addition.",
        "a test corpus with 2.730 sentence pairs different from the training sentence pairs was constructed.",
        "This test corpus contained 28.642 Spanish and 24,927 English words.",
        "For the English sentences, we used a bigram language model whose perplexity on the test corpus varied between 4.7 for the original text and 3.5 when all transformation steps as described below had been applied."
      ]
    },
    {
      "heading": "4.2 Text Transformations",
      "text": [
        "The purpose of the text transformations is to make the two languages resemble each other as closely as possible with respect to sentence length and word order.",
        "In addition, the size of both vocabularies is reduced by exploiting evident regularities; e.g. proper names and numbers are replaced by category markers.",
        "We used different preprocessing steps which were applied consecutively:",
        "• Original Corpus: Punctuation marks are treated like regular words.",
        "• Categorization: Some particular words or word groups are replaced by word categories.",
        "Seven non-overlapping categories are used: three categories for names (surnames, male and female names).",
        "two categories for numbers (regular numbers and room numbers) and two categories for date and time of day.",
        "• Treatment of 'por favor ': The word 'por favor is always moved to the end of the sentence and replaced by the one-word token attor • Word Splitting: In Spanish, the personal pronouns (in subject case and in object case) can be part of the inflected verb form.",
        "To counteract this phenomenon, we split the verb into a verb part and pronoun part, such as 'darnos' – .dar _710.S.",
        "and 'pienso.",
        " – `_yo pienso'.",
        "• Word Joining: Phra.ses in the English language such as 'Would you mind doing ...' and 'I would like you to do are difficult to handle by our alignment model.",
        "Therefore, we apply some word joining, such as 'would you mind' – \"would_you_mind.",
        "and 'would like ' • Word Reordering: This step is applied to the Spanish text to take into account.",
        "cases like the position of the adjective in noun-adjective phrases and the position of object pronouns.",
        "E.g. 'habitaciOn doble' – 'doble habitaciOn'.",
        "By this reordering, our assumption about the monotony of the alignment model is more often satisfied.",
        "The effect of these transformation steps on the sizes of both vocabularies is shown in Table 2.",
        "In addition to all preprocessing steps, we removed the punctuation marks before translation and resubstituted them by rule into the target sentence."
      ]
    },
    {
      "heading": "4.3 Translation Results",
      "text": [
        "For each of the transformation steps described above, all probability models were trained anew, i.e. the lexicon probabilities p(fle), the alignment probabilities p(ili – 6) and the bigra.m language probabilities p(eW).",
        "To produce the translated sentence in normal language, the transformation steps in the target language were inverted.",
        "The translation results are summarized in Table 3.",
        "As an automatic and easy-to-use measure of the translation errors, the Levenshtein distance between the automatic translation and the reference translation was calculated.",
        "Errors are reported at the word level and at the sentence level:",
        "• word level: insertions (INS).",
        "deletions (DEL), and total number of word errors (WER).",
        "• sentence level: a sentence is counted as correct only if it is identical to the reference sentence.",
        "Admittedly, this is not a perfect measure.",
        "In particular, the effect.",
        "of word ordering is not taken into account appropriately.",
        "Actually, the figures for sentence error rate are overly pessimistic.",
        "Many sentences are acceptable and semantically correct translations (see the example translations in Table 4).",
        "As can be seen in Table 3. the translation errors can be reduced systematically by applying all transformation steps.",
        "The word error rate is reduced from 21.27( to 5.1%: the sentence error rate is reduced from 85.5% to 30.1%.",
        "The two most important transformation steps are categorization and word joining.",
        "What is striking, is the large fraction of deletion errors.",
        "These deletion errors are often caused by the omission of word groups like 'for in please' and 'could you'.",
        "Table 4 shows some example translations (for the best translation results).",
        "It can be seen that the semantic meaning of the sentence in the source language may be preserved even if there are three word errors according to our performance criterion.",
        "To study the dependence on the amount of training data, we also performed a training with only 5 000 sentences out of the training corpus.",
        "For this training condition.",
        "the word error rate went up only slightly, namely from 5.1% (for 80,000 training sentences) to 5.3% (for 5 000 training sentences).",
        "To study the effect of the language model.",
        "we tested a zerogram.",
        "a unigram and a bigram language model using the standard set of 80 000 training sentences.",
        "The results are shown in Table 5.",
        "The",
        "WER decreases from 31.1% for the zerogra.m model to 5.1% for the bigram model.",
        "The results presented here can be compared with the results obtained by the finite-state transducer approach described in (Vidal, 1996: Vidal, 1997), where the same training and test.",
        "conditions were used.",
        "However the only preprocessing step was categorization.",
        "In that work.",
        "a WER of 7.1% was obtained as opposed to 5.17 presented in this paper.",
        "For smaller amounts of training data (say 5 000 sentence pairs).",
        "the DP based search seems to be even more superior.",
        "Table 5: Language model perplexity (PP), word error rates (INS/DEL.",
        "WER) and sentence error rates (SER) for different language models."
      ]
    },
    {
      "heading": "4.4 Effect of the Word Reordering",
      "text": [
        "In more general cases and applications, there will always be sentence pairs with word alignments for which the monotony constraint, is not satisfied.",
        "However even then, the monotony constraint, is satisfied locally for the lion's share of all word alignments in such sentences.",
        "Therefore.",
        "we expect to extend the approach presented by the following methods:",
        "• more systematic approaches to local and global word reorderings that try to produce the same word order in both languages.",
        "• a multli-level approach that.",
        "allows a small (say 4) number of large forward and backward tran",
        "sitions.",
        "Within each level, the monotone alignment model can still be applied, and only when moving from one level to the next, we have to handle the problem of different, word orders.",
        "To show the usefulness of global word reordering.",
        "we changed the word order of some sentences by hand.",
        "Table 6 shows the effect.",
        "of the global reordering for two sentences.",
        "In the first example, we changed the order of two groups of consecutive words and placed an additional copy of the Spanish word \"cuesta- into the source sentence.",
        "In the second example, the personal pronoun me was placed at the end of the source sentence.",
        "In both cases, we obtained a correct translation."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we have presented an HMM based approach to handling word alignments and an associated search algorithm for automatic translation.",
        "The characteristic feature of this approach is to make the alignment probabilities explicitly dependent on the alignment, position of the previous word and to assume a monotony constraint for the word order in both languages.",
        "Due to this monotony constraint.",
        "we are able to apply an efficient DP based search algorithm.",
        "We have tested the model successfully On the EuTrans traveller task, a limited domain task with a vocabulary of 200 to 500 words.",
        "The result",
        "ing word error rate was only 5.1(X.",
        "To mitigate the monotony constraint, we plan to reorder the words in the source sentences to produce the same word order in both languages."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": [
        "This work has been supported partly by the German Federal Ministry of Education.",
        "Science, Research and Technology under the contract.",
        "number 01 IV 601 A (Verbmobil) and by the European Community under the ESPRIT project number 20268 (EliTrans)."
      ]
    }
  ]
}
