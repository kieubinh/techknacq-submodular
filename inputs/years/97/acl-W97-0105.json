{
  "info": {
    "authors": [
      "Ezra W. Black",
      "Stephen Eubank",
      "Hideki Kashioka",
      "David M. Magerman"
    ],
    "book": "Workshop on Very Large Corpora",
    "id": "acl-W97-0105",
    "title": "Probabilistic Parsing of Unrestricted English Text With a Highly-Detailed Grammar",
    "url": "https://aclweb.org/anthology/W97-0105",
    "year": 1997
  },
  "references": [
    "acl-C92-2066",
    "acl-C96-1020",
    "acl-C96-1058",
    "acl-C96-2212",
    "acl-H90-1054",
    "acl-H94-1052",
    "acl-J92-4003",
    "acl-J93-2004",
    "acl-P93-1005",
    "acl-P93-1035",
    "acl-P94-1034",
    "acl-P95-1037",
    "acl-P96-1025",
    "acl-W96-0103"
  ],
  "sections": [
    {
      "heading": "Summary",
      "text": [
        "A grammar-based probabilistic parser is described, and experimental results are presented for the parser as trained and tested on a 676,000-word, highly varied treebank of unrestricted English text.",
        "Probabilistic decision trees are utilized as a means of' prediction, and a grammar with about 3000 semantic-and-syntactic tags, and 1100 non-terminal node labels supplies detailed linguistic information.",
        "Further such data is supplied for prediction purposes by thousands of questions about \"raw\" words, expressions, and the sentence as a whole.",
        "The rich information base used for parse prediction allows the system to parse in a domain-general, totally-open-vocabulary setting, and to output highly-detailed semantic as well as syntactic information for sentences processed.",
        "Finally, a statistical procedure is described for converting less-detailed into more-detailed treebank, for use in increasing parser accuracy via much larger training treebanks.",
        "Subject Areas: statistical parsing; automatic treebank conversion; semantic and syntactic analysis of text"
      ]
    },
    {
      "heading": "1. INTRODUCTION",
      "text": [
        "This article describes a grammar-based probabilistic parser, and presents experimental results for the parser as trained and tested on a large, highly varied treebank of unrestricted English text.",
        "Probabilistic decision trees are utilized as a means of prediction, roughly as in (Jelinek et al., 1994; Magerman, 1995), and as in these references, training is supervised, and in particular is treebank-based.",
        "In all other respects, our work departs from previous research on broad-coverage",
        "probabilistic parsing, which either attempts to learn to predict grammatical structure of test data directly from a training treebank (Brill, 1993; Collins, 1996; Eisner, 1996; Jelinek et al., 1994; Magerman, 1995; Sekine and Grishroan, 1995; Sharman et al., 1990), or employs a grammar and sometimes a dictionary to capture linguistic expertise directly (Black et al., 1993a; Grinberg et al., 1995; Schabes, 1992), but arguably at a less detailed and informative level than in the research reported here.",
        "In what follows, Section 2 explains the contribution to the prediction process of the grammar and of the lexical generalizations created by our grammarian.",
        "Section 3 shows, from a formal standpoint, how prediction is carried out, and more generally how the parser operates.",
        "Section 4 presents experimental results.",
        "Finally, Section 5 details our efforts to radically expand the size of our training corpus by employing techniques of treebank conversion."
      ]
    },
    {
      "heading": "2. HOW THE GRAMMAR AND LEXICAL GENERALIZATIONS HELP 2.1. How the Grammar Helps",
      "text": [
        "Figure 1 shows a sampling of parsed sentences from the one-million-word ATRAancaster Treebank of General English (Black et al., 1996), which we employ for training, smoothing and testing our parser.",
        "The Treebank consists of a correct parse for each sentence it contains, with respect to the ATR English Grarnmar.1 Every non-terminal node is labelled with the name of the ATR English Grammar rule2 that generates the node; and each word is labelled with one of the 2843 tags in the Grammar's tagset.3 Together, the bracket locations, rule names, and lexical tags of a Treebank parse specify a unique parse within the Grammar.",
        "In the Grammar parse, rule names and lexical tags are replaced by bundles of feature/value pairs.",
        "Each node contains values for 66 features, and there are 12 values per feature, on average.",
        "Prediction in our parser is conditioned partially on questions about feature values of words and non-terminal nodes.",
        "For instance, when we predict whether a constituent has ended, we ask how many words until the next finite verb; the next comma; the next noun; etc.",
        "In tagging, we ask if the same word has already occurred in the sentence, and if so, what its value is for various features.",
        "By labelling Treebank nodes with Grammar rule names, and not with phrasal and clausal names, as in other (non-grammar-based) treebatilm (Eyes and Leech, 1993; Garside and McEnery, 1993; Marcus et al., 1993), we gain access to all information provided by the Grammar regarding each Treebank node.",
        "It would be difficult to attempt to induce this information from the Treebank alone.",
        "The parent of a rule in the Grammar often contains feature values that are not derived from any of its children.",
        "Further, the parent inherits some feature values from one child, and some from another.",
        "Each rule in the Grammar is associated with a primary and secondary head, and head information is passed up the parse tree.",
        "Filially, extensive Boolean conditions are imposed on the application of each individual rule.",
        "These conditions are intended to permit only useful applications of a given rule, and reflect experience gained by parsing millions of words with the Grammar, and crucially, by generalizing this experience in ways believed appropriate.",
        "Since the ATR English Grammar was created specifically for use in machine parsing, some of its features are designed expressly to facilitate parse prediction.",
        "For example, the feature On the ATR English Grammar, see below; for a detailed description of a precursor to the Grammar, see (Black et al., 1993a).",
        "2 There are 1155 rules in the Grananaar.",
        "aSee (Black et al., 1996).",
        "\"np_modification\" helps to predict attachment events by carrying up to the top node of each noun phrase, data as to how much more modification the noun phrase can probably take.",
        "At one extreme, a noun phrase may not have been modified at all so far, and so, other things being equal, it is a prime target for post-modification.",
        "At the other extreme, it may already have been modified in a way that tends not to permit further modification, such as a noun phrase followed immediately by a postmodifying comparative phrase (\"Such as can understand the topic (may attend)\"; \"More reasons than you can imagine (were adduced)\").",
        "Another feature of this type is \"det_pos\", which reveals, concerning a noun phrase, whether it includes a determiner phrase, and if so, what type.",
        "Deterrainerless noun phrases tend to have different chances of occurring in certain grammatical constructions than noun phrases with determiners, and this feature makes it possible for our models to take account of this tendency.",
        "Note that it is far from trivial to capture and then percolate this information up a treebank parse without a grammar: demarcation of the determiner phrase in each case is involved, along with identification of the type of determiner phrase, and other steps.",
        "The ATR English Grammar is particularly detailed and comprehensive, and this both helps in parse prediction and enhances the value of output that is correctly parsed by our system.",
        "For instance, complete syntactic and semantic analysis is performed on all nominal compounds, e.g. \"the Third Annual Long Branch, New Jersey Rod and Gun Club Picnic and Turkey Shoot\", or \"high fidelity equipment\".",
        "Further, the full range of attachment sites is available within the Grammar for sentential and phrasal modifers, so that differences in meaning can be accurately reflected in parses.",
        "For instance, in 'She didn't attend because she was tired, and didn't call for the same reason,\" the phrases \"because she was tired' and \"for the same reason\" should probably postmodify their entire respective verb phrases, \"didn't attend\" and \"didn't call\", for maximum clarity.",
        "A full range of",
        "attachment sites are available in the Grammar, are used precisely in the Treebank, and are required to be handled correctly by our parser for its output to be considered correct."
      ]
    },
    {
      "heading": "2.2. How Lexical Generalizations Help",
      "text": [
        "Prediction in our parser is conditioned not only on questions about feature values of words and non-terminal nodes, but also on questions about 'raw' words, wordstrings, and whole sentences.",
        "One category of contextual question asks about characteristics of a sentence as a. whole.",
        "For instance, very short \"sentences\" in our training data tend to be free–standing noun phrases or other non–sentential units.",
        "Many of these are titles, speaker–turn indicators, etc.",
        "So we ask about the length of the overall \"sentence' in all models.",
        "In tagging, for instance, there tend not to be any finite verbs in these contexts, and this fact helps with the task of differentiating, say, preterit forms from past participles functioning adjectivally, e.g. \"Said plaintiff and plaintiff's counsel:\".",
        "Similarly, the first and last words of a sentence can be powerful predictors.",
        "If the first word of a sentence is a typical beginning for sentential premodifying phrases (e.g. \"Since\"), and if there is just one comma in the sentence, and that comma occurs in the first quadrant, then there is a. good chance that the overall structure of the sentence is: premodifying phrase, then main clause.",
        "Effective questions about words and expressions, for the purpose of predicting the semantic portion of the lexical tags, are essential to the success of our models.",
        "One strategy we utilize is to identify contexts strongly associated with a given semantic event.",
        "For instance, the context: FirstName \"X\" LastName (e.g. Edward \"Stubby\" Smith) is one of many that are associated with the semantic category NicicName."
      ]
    },
    {
      "heading": "2.3. Formulating Grammar and Lexical Questions For Prediction",
      "text": [
        "We have developed a flexible language for formulating grammar–based and lexically–based questions about Treebank text.",
        "The answers to these questions are made available to the models in our parser.",
        "The language provides facilities for navigating a parse tree, determining feature values of a given node, and making simple boolean or arithmetic computations.",
        "In addition, it allows us to translate answers returned by the question into a more natural format for input to the decision–tree models.",
        "The language provides easy access to word and tag nodes at any offset from the beginning or end of the sentence.",
        "It also provides a reference position – the \"current' node, i.e. the node about which a prediction is being made.",
        "It is easy to navigate from any node to previous nodes, parent/child nodes, and word/tag nodes relative to the node's constituent boundaries.",
        "The navigational commands are recursive, so that, for example, one can arrive at a grandchild of a node by asking about a child's child.",
        "There is nothing in the language itself which restricts the context which can be used in models.",
        "For example, changing a bigram tagger into a trigram tagger requires only adding questions about the additional nodes.",
        "More generally, the ability to ask questions about the entire sentence (and, in the future, document), means that the \"context\" is of variable length.",
        "Every question has access to the current parse state, which contains everything known or predicted about the parse tree up to the time the question is asked.",
        "Any of this information is available for a selected node.",
        "For word nodes, this includes membership on vocabulary lists, whether the word contains various prefixes, suffixes, substrings, etc.",
        "In addition, for tag and nonterminal nodes, the name of the label and the values of all the Grammar's features (including those based on information propagated up the parse tree from lower down) at that node are also available.",
        "Finally, for nonterminal nodes, general information about the number of children, span, constituent boundaries, etc.",
        "is available.",
        "Answers to the questions are of various types: Boolean, categorical, integer, sets of integers.",
        "But we transform all these types of answers into binary strings.",
        "Some transformations are obvious.",
        "Boolean values, for example, are mapped to a single bit.",
        "Other transformations are based on clustering, either expert or automatic.",
        "For example, the sets of tags and rule labels have been clustered by our team grammarian, while a vocabulary of about 60,000 words has been clustered by machiee (Brown et al., 1992; Ushioda, 1996a; Ushioda, 1996b)."
      ]
    },
    {
      "heading": "3. HOW PREDICTION IS CARRIED OUT",
      "text": []
    },
    {
      "heading": "3.1. System Design",
      "text": [
        "The ATR parser is a probabilistic parser which uses decision – tree models.",
        "A parse is built up from a succession of parse states, each of which represents a partial parse tree.",
        "Transition between states is accomplished by one of the following steps: (1) assigning syntax to a word; (2) assigning semantics to a word; (3) deciding whether the current parse tree node is the last node of a constituent; (4) assigning a (rule) label to an internal node of the parse tree.",
        "Note that the first two steps together determine the tag for a word, and the third determines the topology of the tree.",
        "Working from the bottom up, left to right, constrains the parser to produce a unique derivation for each parse state.",
        "Alternatively, we can tag the entire sentence first, then work from tags up, left to right, which also yields a unique derivation for each parse state.",
        "Statistical models corresponding to each type of step provide estimates of the probability of each step's outcome.4 Each model uses as input the answers to a set of questions about context designed specifically for that model by our team grammarian, using the language described in Section 2.3.",
        "Thus the probability of each decision depends on features extracted from the context, including information about any word(s) in the sentence and any tags and parse structure already predicted.",
        "The estimated probability of any parse state is the product of the probabilities of each step taken to reach that state.",
        "Strictly speaking, we estimate relative likelihoods rather than probabilities, since we make no attempt to normalize over all possible parses for a given sentence.",
        "Given a set of models for estimating the probabilities of parse steps, the problem of predicting a. parse reduces to searching the space of possible parses for the most likely one.",
        "We use a chart parser (Kasami, 1965) to build a compact representation of all legal parses for the sentence, which in turn constrains the search to consider only those parse steps guaranteed to lead to a, complete (legal) parse.",
        "Even so, because the Grammar generates a large number of parses for each sentence,5 it is not feasible to rank the parses exhaustively.",
        "Fortunately, incomplete parse states are assigned probabilities, which can be used to guide a search by ruling out unlikely parses without constructing the complete parse.",
        "We have found that a greedy search, which chooses the most likely outcome for each parsing step, usually finds a good candidate parse.",
        "Occasionally, though, choosing a less likely step at one point leads to a parse with higher overall likelihood.",
        "To allow for this possibility, we use the greedy candidate parse to \"seed\" the stack-based decoder described in (Jelirtek, 1969).",
        "There is some freedom in the order in which the parsing steps are taken.",
        "The context in which a model makes its prediction includes any parts of the parse tree which have already been built.",
        "Hence, the order chosen determines what information is available to each model.",
        "We choose to tag the entire sentence first, producing an N-best list of tag sequences.",
        "Specifically, starting from a sequence of words, we first tag the sentence as follows:",
        "• estimate the probability for each part-of-speech of the first word;",
        "• choose one or more most likely parts-of-speech; • estimate the probability for each tag for the first word, given the part-of-speech decision(s) made above; • choose one (or several) likely tag(s); • repeat the steps above for each word in the sentence",
        "Next, starting from the tag of the first word, which is the leftmost leaf node of the parse tree, we take the following steps:",
        "• estimate the probability that the current node of the parse tree is the last child of its parent (e.g. the probability that a constituent ends at this node); • if a constituent is deemed to end at this node, estimate the probability of possible rule labels for that consitutent, i.e. of only those rules which are known to lead to legal parses; make that node the current node; and return to the first step; • otherwise, make the top of the next subtree to the right the current node and return to the first step.",
        "This approach decouples the search over tag sequences from the search over parse trees."
      ]
    },
    {
      "heading": "3.2. Decision – Tree Models",
      "text": [
        "The parser requires models which estimate the probability of membership in a class given an input vector.",
        "We use class probability trees, a slight modification of classification trees, as described in (Breiman et al., 1984; Quinlan, 1986; Bald et al., 1983), with a few enhancements.",
        "We can choose among several different standard splitting criteria for the trees.",
        "The trees are pruned using the minimal cost-complexity algorithm (Breinian et al., 1984).",
        "In addition, estimates for probability distributions are smoothed using the Forward-Backward algorithm (Baum, 1972).",
        "The models are trained using bitstring answers to questions about each state encountered while parsing each sentence in the training set.",
        "We build binary trees, in which each node can split the data based on the value of any bit in the hitstring.",
        "There are situations in which an entire question does not apply – for example, a question about the previous word when the first word of a sentence is under consideration.",
        "These situations are flagged so that the decision tree will split out this data before it asks about any of the bits in the answer to this question."
      ]
    },
    {
      "heading": "4. EXPERIMENTAL RESULTS",
      "text": []
    },
    {
      "heading": "4.1. Evaluation Methodology",
      "text": [
        "In our view, any effective evaluation methodology for automatic grammatical analysis must confront head-on the problem of multiple correct answers in tagging and parsing.",
        "That is, it is often the case that there is more than one \"correct tag\" for a word in context, where that word could be considered to be functioning as: a proper or a common noun; an adjective or a noun; a participle or an adjective; a gerundial noun or a noun;6 an adverbial particle or a locative adverb; and even an adjective or an adverb.",
        "This is true even where there are highly detailed and well-understood 'terminology of (Long, 1961), for e.g. a sleeping pill vs. to make a good living",
        "guidelines for the application of each tag to text.",
        "And obviously the existence of multiple correct taggings for a word is to be expected a fortiori where a highly ramified system of semantic categories is involved.",
        "It follows that multiple correct parses exist for many sentences, since by definition any change in tag means a change in parse.",
        "But other sources of multiple correct passes exist as well, and range from, say, several equally good attachment sites within a parse for a given modifier, even given full document context, to cases where the grammar itself provides several equally good parses for a sentence, through the presence of normally independent rules whose function nonetheless overlaps to some degree.",
        "Barring the recording of the set of correct tags for each word, and of the set of correct parses for each sentence, in a treebank, the next-best solution to the problem of multiple correct answers is to at least provide such a recording in one's test set, i.e. to provide a \"gold standard\" test set with all correct tags and parses for each word in context.",
        "This is the solution that was adopted in creating the ATR/Lancaster English Treebank.",
        "• The way we evaluate our tagger is to compare its performance to the set of correct tags for each word of each sentence of our \"gold standard\" test data.",
        "Thus, in all cases we are able to take into account the full set of \"correct\" answers.7 Since 32% of running words in our test data have 2 or more correct tags, potential differences in performance evaluation are large vis-a-vis traditional metrics.9 Similarly, in the case of the parser, we evaluate performance against a special \"gold standard\" test set which lists every correct parse with respect to the GTO-Tamar for each test sentence.",
        "We utilize two measures.",
        "First is exact match with any correct parse listed for the sentence.",
        "Second is \"exact syntactic match\": exact match with the bracket locations and rule names only.",
        "Notice that in a parse considered correct by our second metric, the syntax 9 of all tags must be correct.",
        "The average number of different correct \"exact syntactic raatches\"10 per sentence in our test data is 3.",
        "Among test-data sentences, 72% have more than one correct exact syntactic matches, and 32% have 5.11 For critiques of other approaches to broad-coverage parser and tagger evaluation, see (Black, 1994).",
        "It is worth inquiring how well expert humans do at the parsing task that we are attempting here by machine.",
        "Accordingly, we present statistics below on the consistency and accuracy of expert hmnAns at parsing using the ATE.",
        "English Grammar.",
        "The ATR/Lancaster treebanking effort features a grammarian, who originated the Grammar, and a treeba.nking team, who apply the Grammar to treebank text.",
        "We can therefore distinguish two different types of evaluation as to how well expert humans do at parsing using the Grammar consistency and accuracy.",
        "Consistency is the degree to which all team members posit the identical parse for the identical sentence in the identical document of test data.",
        "Accuracy is the expected rate of agreemnt between a treebanker and the grammarian on parsing a given sentence in a given document of test data.",
        "In a first experiment to determine consistency, we asked each of the three team members to declare either correct or incorrect a particular parse for a sentence of test data.",
        "The parses had"
      ]
    },
    {
      "heading": "4.2. Experimental Results",
      "text": [
        "As discussed in 3.1, our first step in passing is to tag each sentence.",
        "The tagger currently produces an exact match 74% of the time for the 47,800-word test set, comparing against a single tag sequence for each sentence.16 We present parsing results both for text which starts out correctly tagged (Table 1)17 and for raw text (Table 2).",
        "Results for parsing from raw text are given for both the exact-match and exact-syntactic-match criteria described in 4.1.",
        "The performance of the parser on short sentences of correctly tagged data is extrememly good.",
        "We feel this indicates that the models are performing well in scoring the parses.",
        "The results deteriorate rapidly for longer sentences, but we believe the problem lies in the search procedure rather than the models.",
        "A measure of the performance of a search is whether it 12 In a. parallel experiment to determine consistency on tagging, we asked each of the three team members to choose the first correct tag from a ranked list of tags for each word of each sentence of test data.",
        "These ranked lists were hand-constructed, and an effort was made to make them as difficult as possible to choose from.",
        "About 4,800 words (152 sentences) of test data were utilized.",
        "The result was a 3.1% expected rate of disagreement among the team members on the exact choice of tag.",
        "130f these 248 sentence pairs, 85% were exact matches in terms of the way they were tagged.",
        "\"Actually, the documents were selected from our \"main General-English Treebank\" of 800,000 words.",
        "\"i.e.. the parse was wrong if even one tag was wrong; or, of course, if a rule choice was wrong.",
        "For the tags assigned to the roughly 5000 words in these 308 sentences, expected error rate was 2.9%.",
        "Essentially none of these tagging errors had to do with the use of the syntactic portion of our tags; all of the errors were semantic; the same was true in the two tagging consistency experiments related above.",
        "As noted in 4.1 fn.",
        "8, our experience indicates that we can expect a roughly 10% improvement in this score when we compare performance against \"golden-standard\" test data in which all correct answers are indicated; this would bring our tagging accuracy into the 80-percent area.",
        "\"For the definition of the term 'crossing brackets\" used in Table 1, see (Harrison et al., 1991).",
        "suggests any candidates which are as likely as the correct answer.",
        "If not, the parser has erred by \"ommission\" rather than by \"conrnksion\": it has ommitted the correct parse from consideration, but not because it seemed unlikely.",
        "It is entirely possible that the correct parse is in fact among the highest-scoring parses.",
        "These types of search error are non-existent for exhaustive search, but become important for sentences between 11 and 15 words in length, and dominate the results for longer sentences.",
        "The results in Table 2 reflect tagging accuracy as well as the performance of the parser models per se.",
        "Note that tagging accuracy is quoted on a per-word basis, as is customary.",
        "From previous work, we estimate the accuracy of the tagger on the syntactic portion of tags to be about 94%.",
        "Thus there is typically at least one error in semantic assignment in each sentence, and an error in syntactic assignment in one of every two sentences.",
        "It is not surprising, then, that the per-sentence parsing accuracy suffers when parses are predicted from raw text.",
        "Clearly the present research task is quite considerably harder than the parsing and tagging tasks undertaken in (Jelinek et al., 1994; Magerman, 1995; Black et al., 1993b), which would seem to be the closest work to ours, and any comparison between this work and ours must be approached with extreme caution.",
        "Table 3 shows the differences between the treebanks utilized in (Jelinek et al., 1994) on the one hand, and in the work reported here, on the other.18 Table 4 shows relevant",
        "parsing results by (Jelinek et al., 1994).",
        "Even starker contrasts obtain between the present results and those of e.g. (Magerman, 1995; Black et a).., 1993b), who do not employ an exact-match evaluation criterion, further obscuring possible performance comparisons.",
        "Obviously, no direct comparisons of the results of Tables 1-2 with previous parsing work is possible, as we are the first to parse using the Treebank.",
        "In our current research, we are emphasizing the creation of decision-tree questions for predicting semantic categories in tagging, as well as continuing to develop questions for syntactic tag prediction, and for our rule-name-prediction model."
      ]
    },
    {
      "heading": "5. TOWARDS RADICALLY EXPANDING TRAINING – SET SIZE VIA TREEBANK CONVERSION",
      "text": []
    },
    {
      "heading": "5.1. Introduction",
      "text": [
        "As an additional means of improving the accuracy of our parser, we have been working towards effecting a dramatic increase in the size of our training treebank, via treebank conversion techniques.",
        "We employ a statistical method for converting treeba.nk from a less-detailed format – and we have chosen the IBM/Lancaster Treebank (Eyes and Leech, 1993; Garside and McEnery, 1993) as a first representative of such treebanka – to a more-detailed format, that of the ATR,/Lancaster Treebank.",
        "There has been very little previous work on treebank conversion.",
        "(Hughes et al., 1995) describe an effort to hand-annotate text using the tagging schemes employed in various different treebanks, as a preliminary to attempting to learn, in a way to be determined, how to convert a corpus automatically from one style of tagging markup to another.",
        "(Wang et al., 1994) take on the problem of converting treebank conforming to their English grammar into a format conforming to a later version of the same gamma; and report a conversion accuracy of some 96% on a 141,000-word test set.",
        "They employ a heuristic which scores source-treebank/target-treebank parse pairs based essentially on the percentage of identically-placed brackets in the two parses.",
        "However, their target grammar19 generates only 17 parses on average per sentence of test data.",
        "Although they exhibit no parses with respect to their gramma,rs, it can be assumed that they feature only rudimentary tag and non-terminal vocabularies.",
        "The problem we face in learning to convert IBM/Lancaster Treebank parses into ATR/Lancaster Treebank parses is rather more difficult than this.",
        "For instance, as noted in 3.1, the Parse Base of the ATR English Grammar, which generates the parses of the ATR/Lancaster Treebank, is 1.76, which means that on average, the Grammar generates about 200 parses for 10-word sentence; 2000 parses for a 15-word sentence, and 70,000 parses for a 20-word sentence.",
        "Further, far from featuring a rudimentary set of lexical tags and non-terminal node labels, the ATR/Lancaster Treebank utilizes \"and presumably their source grammar as well",
        "roughly 3,000 lexical tags and about 1,100 different non-terminal node labels,20 as mentioned in 2.1.",
        "Figure 2 shows a parse for a sample sentence, first from the IBM/Lancaster Treebank, and next from the ATR/Lancaster Treebank An impression of the difficulty of the treebank conversion task undertaken here can be gained by closely contrasting the two parses of this Figure.",
        "143,837 words included in the IBM/Lancaster Treebank-35,575 words of Associated Press newswire and 108,262 words of Canadian Hansard legislative proceedings – were treebanked with respect to the ATR English Grammar, in the exact same manner as the data in the ATR/Lancaster Treebank.",
        "We will refer to the IBM/Lancaster Treebank version of this data as the parallel corpus.",
        "As a preliminary step to treebank conversion, we aligned the parallel and ATR corpora.",
        "87.3% of the parallel data-125,530 words – aligned essentially perfectly, and for the work reported here, we decided to operate only on this satisfactorily-aligned data."
      ]
    },
    {
      "heading": "5.2. The Treebank Conversion Problem",
      "text": [
        "Ideally, our treebank-conversion models should take full advantage of data in the full target treebank (i.e. the full ATR/Lancaster Treebank) as well as the parallel corpus.",
        "A direct model of the conditional probability of the ATR parse given the source-treebank parse, p(iliF), uses only data in the parallel corpus.",
        "A more efficient use of data would be to build two models: one to estimate the likelihood of an ATR.",
        "parse, p(A), given raw text; the other to estimate p(F1A).",
        "Then, auy, rules names with respect to the ATR English Grammar; cf. 2.1",
        "using Bayes' rule, one would write p(AfF) as:",
        "The model for p(FIA) uses only the parallel corpus, but the model for p(A) makes full use of the data in the ATR treebank.",
        "In our software environment, this approach would require constructing a feature-based grammar for the source treebank A simpler, but probably adequate approach would combine the two models p(A) and p(AIF) heuristically, using p(AfF) to rescore the N best parses found by the model p(A).",
        "The top-ranked candidate from the rescored parses is selected as the ATR parse.",
        "This way takes advantage of both data sets, though not as efficiently as the Bayesian approach.",
        "We have chosen to explore the problem using an even simpler approach: ignoring the ATR treebank and working only within the model for p(A IF).",
        "This yields lower bounds on potential accuracy at low cost.",
        "We also considered filtering the parses considered by the ATR parser to ensure they satisfied certain constraints implied by the source-treeba.nk parse.",
        "This proved to be impractical because the constraints were not \"hard\", i.e. the exact circumstances in which they should be applied were difficult to determine.",
        "Instead, we relied on the models to learn the constraints and the conditions for their application directly from the data.",
        "However, the issue of applying such constraints is specific to the two treebanks being used; there may well be cases in which such constraints are not hard to develop.",
        "The source-treebank-to-ATR conversion model was built using the same system described in Sections 2 and 3, the sole difference being that the question language was extended to allow for questions about the source treebank.",
        "Since the topology of the parallel tree may be very different from that of the ATR parse tree, it is not obvious what the analog of a node in the ATR tree is.",
        "We chose to use the \"least enclosing\" node: that is, the lowest (non-preterminal) node in the parallel tree which spans (at least) the set of words spanned by the node in the ATR parse."
      ]
    },
    {
      "heading": "5.3. Decision-Tree Questions Asked",
      "text": [
        "We ask all decision-tree questions in our treebank-conversion models that we do normally in parsing with the ATR English Grammar.21 We then add further questions which ask about the source-treebank parse for the sentence being processed.",
        "We use an extremely basic set of question-language functions in querying the structure of the source-treebank parse.",
        "These permit us to ask about the least-enclosing node, and about children and parents of this source-treebank-parse node, or of its children or parents, to any level of structure.",
        "What we can ask about a node in the source-treebank parse is either what its nonterminal label is, or how many children it has.",
        "In addition, we are able to ask whether there is a constituent in the source-treebank parse with the identical span as a given node of an ATR parse; and if so, what its non-terminal label is, or how many children it has.",
        "Similarly, we can ask about constituents that \"cross\" a given node of an ATR parse.",
        "Finally, we can ask about the tag of any word in the source-treebank parse.",
        "There is much farther that we can go in exploiting the information in the source-treebank parse to aid in predicting the ATR parse.",
        "For instance, we can define and query grammatical relations such as clausal subject and main verb.",
        "We can even define and query notions like \"headword\" with respect to the source-treebank parse, although this would involve appreciable work.",
        "Furthermore, carrying over to the source-treebank environment question types that seem helpful when asked about ATR parses will not be difficult."
      ]
    },
    {
      "heading": "2f Section 2",
      "text": []
    },
    {
      "heading": "5.4. Experimental Results",
      "text": [
        "Evaluation Methodology We evaluate treebank conversion to ATR-Treebank format in the same way as we evaluate the parser when it is trained in the normal manner (cf. 4.1), except that test data consists of ATR-Treebank-format documents of which we also possess aligned source treebank (in this case.",
        "IBM/Lancaster-Treebank) versions.",
        "In the performance results cited below, however, we show exact match only with the single correct parse of the test treebank, rather than with any one of the correct parses indicated in the \"golden standard\" version of the test set.",
        "Experimental Results Table 5 displays exact-match parsing results for a normal 6,556-word test set22.",
        "Crucially, the amount of training data here, 118,489 words, is only 17.5% as large as for the models of Tables 1-2.",
        "Considering the simplicity of the approach, we think these results constitute a proof of principle for the idea of treebank conversion.",
        "They indicate that we can build treebank conversion models of accuracy comparable to the current passer using much less data.",
        "Of course, the results here do not include models used in tagging.",
        "The treebank conversion models tag with an accuracy of 62.8%.",
        "A detailed examination of those models shows that the syntactic models are better than the parser's, while the semantic models are worse.",
        "This is to be expected, because the IBM/Lancaster Treebank contains a great deal of relevant information about the syntax, but not so much about the semantics of the sentences they contain.",
        "One idea, therefore, is to utilize large-scale treebank conversion in the tagging domain to overcome the problem noted in 4.2, that even with 94% accuracy at strictly syntactic tagging (i.e. effectively, on tagging with our 440-tag syntax-only tag subset), approximately one word is syntactically mistagged every two sentences, leading to an increased error rate at exact-syntactic-match parsing.",
        "A second direction which suggests itself is to pursue our scaled-down approach to treebank conversion, but with more training data than we have used so far.",
        "Third, we may decide to implement the more laborious two-model approach desribed in 5.2.23 Overall, we expect that conversion models which take full advantage of the existing database as well as of the parallel corpus as outlined above should produce data of high enough quality to use as training data for our parser.",
        "22i.e.",
        "not for a \"golden standard\" test set as described in 4.1, in which all parses are indicated for each test sentence 231t seems worth mentioning that future large-scale treebank-creation efforts would probably benefit from constructing parallel data with respect to other large treebanks, right from the start."
      ]
    },
    {
      "heading": "6. ACKNOWLEDGEMENTS",
      "text": [
        "We wish to thank Joshua Goodman and John Lafferty for their contributions to the treebank conversion work reported here; Akira Ushioda for his implementation of the Brown word-clustering algorithm; and Craig MacDonald and Toyomi Saiga for their contributions to our work overall."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": []
    }
  ]
}
