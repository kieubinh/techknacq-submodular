{
  "info": {
    "authors": [
      "Seungmi Lee",
      "Key-Sun Choi"
    ],
    "book": "Workshop on Very Large Corpora",
    "id": "acl-W97-0107",
    "title": "Reestimation and Best-First Parsing Algorithm for Probabilistic Dependency Grammars",
    "url": "https://aclweb.org/anthology/W97-0107",
    "year": 1997
  },
  "references": [
    "acl-C96-1058",
    "acl-P92-1017",
    "acl-P92-1024",
    "acl-P95-1031",
    "acl-P96-1025",
    "acl-W95-0102"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents a reestimation algorithm and a best-first parsing (BFP) algorithm for probabilistic dependency grammars (PDC).",
        "The proposed reestimation algorithm is a variation of the inside-outside algorithm adapted to probabilistic dependency grammars.",
        "The inside-outside algorithm is a probabilistic parameter reestimation algorithm for phrase structure grammars in Chomslcy Normal Form (CNF).",
        "Dependency grammar represents a sentence structure as a set of dependency links between arbitrary two words in the sentence, and can not be reestimated by the inside-outside algorithm directly.",
        "In this paper, non-constituent objects, complete-link and complete-sequence are defmed as basic units of dependency structure, and the probabilities of them are reestimated.",
        "The reestiotation and BFP algorithms utilize CYK-style chart and the non-constituent objects as chart entries.",
        "Both algorithms have 0(n.3) time complexities."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "There have been many efforts to induce grammars automatically from corpus by utilizing the vast amount of corpora with various degrees of annotations.",
        "Corpus-based, stochastic grammar induction has many profitable advantages such as simple acquisition and extension of linguistic lmowledges, easy treatment of ambiguities by virtue of its innate scoring mechanism, and fail-soft reaction to ill-formed or extra-grammatical sentences.",
        "Most of corpus-based grammar inductions have concentrated on phrase structure grammars (Black, Lafferty, and Roukos, 1992, Lan i and Young, 1990, Magerman, 1994).",
        "The typical works on phrase structure grammar induction are as follows(Laxi and Young, 1990, Carroll, 1992b): (1) generating all the possible rules, (2) reestimating the probabilities of rules using the inside-outside algorithm, and (3) finally finding a stable grammar by eliminating the rules which have probability values close to 0.",
        "Generating all the rules is done by restricting the number of nonterminals and/or the number of the right hand side symbols in the rules and enumerating all the possible combinations.",
        "Chen extracts rules by some heuristics and reestimates the probabilities of rules using the inside-outside algorithm (Chen, 1995).",
        "The inside-outside algorithm learns a grammar by iteratively adjusting the rule probabilities to minimize the training corpus entropy.",
        "It is extensively used as reestimation algorithm for phrase structure grammars.",
        "Most of the works on phrase structure grammar induction, however, have partially succeeded.",
        "Estimating phrase structure grammars by minimizing the training corpus en",
        "tropy does not lead to the desired grammars which is consistent with human intuitions (de Marcken, 1995).",
        "To increase the correctness of the learned grammar, Marcken proposed to include lexical information to the phrase structure grammar.",
        "A recent trend of parsing is also to include lexical information to increase the correctness (Magerman, 1994, COilinR, 1996).",
        "This means that the lack of lexical information in phrase structure grammar is a major weak point for syntactic disambiguation.",
        "Besides the lack of lexical information, the induction of phrase structure grammar may suffer from structural data sparseness with medium sized training corpus.",
        "The structural data.",
        "sparseness means the lack of information on the grammar rules.",
        "An approach to increase the correctness of grammar induction is to learn a grammar from a tree-tagged corpus or bracketed corpus (Pereira and Schabes, 1992, Black, Lafferty, and Roukos, 1992).",
        "But the construction of vast sized tree-corpus or bracketed corpus is very labour-intensive and manual construction of such corpus may produce serious inconsistencies.",
        "And the structural-data sparseness problem still remains The problems of structural-data sparseness and lack of lexical information can be lessened with PDG.",
        "Dependency grammar defines a language as a set of dependency relations between any two words.",
        "The basic units of sentence structure in DC, the dependency relations are much simpler than the rules in phrase structure grammar.",
        "So, the search space of dependency grammar may be smaller and the grammar induction may be less affected by the structural-data sparseness.",
        "Dependency grammar induction has been studied by Carroll (Carroll, 1992b, Carroll, 1992a).",
        "In the works, however, the dependency grammar was rather a restricted form of phrase structure grammars.",
        "Accordingly, they extensively used the inside-outside algorithm to reestimate the grammar and have the same problem of structural-data sparseness.",
        "In this paper, we propose a reestimation algorithm and a best-first parsing algorithm for PDG.",
        "The reestimation algorithm is a variation of the inside-outside algorithm adapted to PDG.",
        "The inside-outside algorithm is a probabilistic parameter reestimation algorithm for phrase structure grammars in CNF and thus can not be directly used for reestimation of probabilistic dependency grammars.",
        "We define non-constituent objects, complete-link and complete-sequence as basic units of dependency structure.",
        "Both of reestimation algorithm and best-first parsing algorithm utilize a CYIC-style chart and the non-constituent objects as chart entries.",
        "Both algorithms have 0(n3) time complexities.",
        "The rest of the paper is organized as follows.",
        "Section 2 defines the basic units and describes best-first parsing algorithm.",
        "Section 3 describes the reestimation algorithm.",
        "Section 4 shows the experimental results of reestimation algorithm on Korean and finally section 5 concludes this paper."
      ]
    },
    {
      "heading": "2 PDG Best First Parsing Algorithm",
      "text": [
        "Dependency grammar describes a language with a set of head-dependent relations between any two words in the language.",
        "Head-dependent relations represent specific relations such as modifiee-modifier, predicate-argument, etc.",
        "In general, a functional role is assigned to a dependency link and specifies the syntactic/semantic relation between the head and the dependent.",
        "However, in this paper, we use the minimal definition of dependency grammar with head-dependent relations only.",
        "In the future we will extend our dependency grammar into one with functions of dependency IirikN",
        "A dependency tree of a n-word sentence is always composed of n-1 dependency links.",
        "Every word in the sentence must have its head, except the word which is the head of the sentence.",
        "In a dependency tree, crossing links axe not allowed.",
        "Salesperson sold the dog buiscuits",
        "Figure 1 shows a dependency tree as a hierarchical representation and a link representation respectively.",
        "In both, the word c'sold\" is the head of the sentence.",
        "Here, we define the non-constituent objects, complete-link and complete-sequence which are used in PDG reestiroation and BFP algorithms.",
        "A set of dependency links constructed for word sequence wij is defined as complete-link if the set satisfies following conditions:",
        "• The set has exclusively (wi to,) or (wi to,).",
        "• There is neither link-crossing nor link-cycle.",
        "• The set is composed of j – i dependency links.",
        "• Every inner word of wid must have its head and thus a link from the head.",
        "Complete-link has directionality.",
        "It is determined by the direction of the outermost dependency relation.",
        "If the complete-link has (wi to,), it is rightward, and if the complete-link has (wi wj), then it is leftward.",
        "Basic complete-link is a dependency link between adjacent two words.",
        "Complete-sequence is defined as a sequence of null or more adjacent complete-links of same direction.",
        "Basic complete-sequence is null sequence of complete-links which is defined on one word, the smallest word sequence.",
        "The direction of complete-sequence is determined by the direction of component complete-links.",
        "If the complete-sequence is composed of leftward complete-links, the complete-sequence is leftward, and vice versa.",
        "Figure 2 shows abstract rightward complete-link for lei j, rightward complete-sequence for won, and leftward complete-sequence for win+11.1.",
        "Double-slashed line means a complete-sequence.",
        "Whatever the direction is, a complete-link for wij is always constructed with a dependency link between wi and wi, a rightward complete-sequence from i to m, and a leftward complete-sequence from j torn + 1, for an m between i and j – 1.",
        "Rightward complete-sequence is always composed with a. combination of a rightward complete-sequence and a rightward complete-link.",
        "On the contrary, leftward complete-sequence is always composed with a combination of a leftward complete-link and a leftward complete-sequence.",
        "These restrictions on composition of complete-sequence is for the ease of description of algorithm.",
        "The basic complete-link and complete-sequence are also shown in the Figure 2.",
        "Following notations are used to represent the four kinds of objects for a word sequence wij and for an m from i to j-1.",
        "• 19.",
        "(i, j): rightward complete-link for w, j, i.e. {(wi wj), Sr(i, m), Sr (rn + 1, n)} • Li(i, j): leftward complete-link for wi 3 .",
        "i.e. {(wi wj), m), Si (m F 1; n)}",
        "• j): rightward complete-sequence for wij, i.e.",
        "{S,.",
        "(i, m), j)1 • Si(i, j): leftward complete-sequence for wij, i.e. {11(i, in) 51(m, j)}",
        "To generalize the structure of a dependency tree, we assume that there are marking tags, BOS (Begin Of Sentence) before tvi and EOS (End Of Sentence) after w„, and that there are always the dependency link, (wBos – >• tvEos) and (wk wsos) when Wk the head word of the sentence.",
        "Then, by definition, any dependency tree of a. sentence, tvi.n can be uniquely represented with either a LABOS, EDS) or a Si(1, EDS) as depicted in Figure 3.",
        "This is because L (BOB, BOB) for any sentence is always composed of null S7.",
        "(BOS,BOS) and S(l, EDS).",
        "The head of a dependency tree Wk can be found in the rightmost Li (k, EDS) of Si (1, BO S).",
        "The probability of each object is defined as follows.",
        "The tn.",
        "varies from i to j – 1 for L1, L, and Sr, and from i + I to j for SI.",
        "The best Li and the best 4. always share the same in.",
        "This is because both are composed of the same sub-Sr and sub-Se with maximum probabilities.",
        "Basis probabilities are as follows:",
        "Thus, the probability of a dependency tree is defined either by p(LT(BOS, E0S)) or by p(Si(1,E0S)."
      ]
    },
    {
      "heading": "Leftward/Rightward Complete Link ISMLeftward Complete Sequence",
      "text": [
        "The PDG best-first parsing algorithm constructs the best dependency tree in bottom-up manner, with dynamic programming method using CYK style chart It is based on complete-link and complete-sequence of non-constituent concept.",
        "The parsing algorithm constructs the complete-links and complete-sequences for substring, and merges incrementally the complete-links into larger complete-sequence and complete-sequences into larger complete-link until the .4(B08,E0S) with maximum probability is constructed.",
        "Eisner (Eisner, 1996) proposed an 0(n3) parsing algorithm for PDG.",
        "In their work, basic unit of chart entry is span which is also of non-constituent concept.",
        "But, the span slightly differs from our complete-sequence and complete-link.",
        "When two adjacent spans are merged into a larger span, some conditional tests must be satisfied.",
        "In our work, best-first parsing is done by inscribing the four entries with maximum probabilities, Lz(i, j), j), j), and Sr(i, j) to each chart positions in bottom-up/left-to-right manner without any extra condition checking_ Figure 4 depicts the possible combinations of chart entries into a larger Lr, LI, Si, and 5,. each.",
        "The sub-entries under the white-headed arrow and the sub-entries under the black-headed arrow are merged into a larger entries.",
        "The larger entries are inscribed into the bold box.",
        "There is an exception for chart entries of n÷ith column In the n+ith column, only the Li(k, BOS) whose sub Se is null can be inscribed.",
        "This is because there can be only one head word for a tree structure.",
        "If L1(k,E0S) whose sub Se is not null is inscribed into the chart, the overall tree structure will have two or more heads.",
        "The best parse is maximum L,-(BOS,E0S) in the chart position (0,n + 1).",
        "The best parse can also be found by the maximum Si(1, BOS) because the .LABOS,E0S) is always composed of ST(BOS,BOS) and Si (1, EOS).",
        "The chart size is n24P+3 for n word sentence.",
        "For four items (1,, LE, ST, and Si) of each chart position, there can be maximally n searches.",
        "Thus, the time complexity of the best-first parsing algorithm is 0(n3)."
      ]
    },
    {
      "heading": "3 PDG Reestimation Algorithm",
      "text": [
        "For reestimation of dependency probabilities of PDG, eight kinds of chart entries are defined based on three factors: inside/outside, complete-link/complete-sequence, and leftward/rightward.",
        "In following definitions, # is for inside probability and a is for outside probability.",
        "Superscripts represent whether the object is complete-link or complete-sequence, I for complete-link and s for complete-sequence.",
        "Subscripts of and a are for the directionality, r for rightward and 1 for leftward.",
        "Complete-link Inside Probabilities: Of Inside probability of a complete-link is the probability that word sequence wij will be generated when there is a. dependency relation between wi and wi.",
        "= 12(tv1iiLf(i,i)) J-1",
        "In Figure 5, #1.",
        "(i, j), the inside probability of j) is depicted.",
        "In the left part of the",
        "figure, the gray partitions indicate all the possible constructions of Sr(i,m) and all the possible constructions of Si (rn + 1, j) respectively.",
        "Double-slashed links depict complete-sequences which compose the L,.",
        "together with the outermost dependency (wi 4 wi).",
        "The right part of the figure represents the chart.",
        "The bold box is the position where the 41. is to be inscribed.",
        "Inside probability of a complete-link is the sum of the probabilities of all the possible constructions of the complete-link.",
        "As explained in the previous section, a",
        "Lr(i, j) is composed of the dependency link between word i and word j (either (wi 4 'WA or (wi tv1)), m) and Si (m+ 1, j) for an m from i to j-1.",
        "Inside probability of Li (i, j) can be computed the same as that of.Lr(i, j) except for the direction of dependency link between tvi and wj.",
        "The outermost dependency (wi wi) must be replaced with (wi and Li are not defined on word sequence of 1 length, wi.",
        "The unit probabilities for 4 and are as follows:",
        "sequence is the probability that word sequence wij is generated",
        "In Figure 6 and 7, the double-slashed link means complete-sequence, a sequence of null or more adjacent complete-links of same direction.",
        "A complete-sequence is composed of sub",
        "complete-sequence and sub-complete-link.",
        "Figure 6 depicts rightward complete-sequence for an m. The value of m varies from i to j-1.",
        "In Figure 7, St is composed of sub-Li and sub-Se.",
        "The inside probability of complete-sequence is the sum of the probabilities that the complete-sequences are constructed with.",
        "The basis for inside probabilities of complete-sequence are as follows.",
        "Because n-Fith word, tvEos can not be a dependent of any other word, /31(k, BO S) or 075.",
        "(k, EDS) for k from 1 to n is not computed.",
        "And because there can be only one head of a tree, WEOS must be head of only one word.",
        "Thus, in computation of $f (z, BO S) and 131 EOS), only the Lis whose sub Si is null are considered.",
        "Complete-link Outside Probability: arr, al This is the probability of producing the words before i and after j of a sentence while complete-link(ij) generates tuij",
        "Si respectively.",
        "In Figure 8, the outside probability of 4. which is inscribed in the bold box is computed by summing the products of the inside probabilities in the boxes under the white-headed arrow and the outside probabilities in the boxes under the black-headed",
        "arrow.",
        "Likewise, in Figure 9, the outside probability of Li in the bold box is computed by summing all the products of the inside probabilities under the white-headed arrow and the outside probabilities under the black-headed arrow each.",
        "This is because, in parsing, the subentries under the white-headed arrows and Lr/Li in the bold boxes are merged into larger entries which are to be inscribed in the boxes under the black-headed arrows.",
        "Basis probability for complete-link outside probability is as follow.",
        "In the above expression, The first term is for the construction of larger Sr(i, h) from the combination of Sr(ti, j) and its adjacent Lr(j, h).",
        "The second term means the construction of larger Lr(i,h) from the combination of Sr(i, j), Si(j + 1, h), and the dependency link from wi to wh.",
        "The third term is for the larger Li(i,h) from the combination of S2-(i, j), Si(j + 1,h) and the dependency link from wh to wi.",
        "The three terms in the expression are depicted in Figure 10.",
        "cris is the sum of all the probabilities that Si is to become a subentry of larger entries: Si, LT, and Li.",
        "The first term in the above expression is for the combination of Sz(v, j) from Li(v,i) and St(i,i).",
        "The second is for the construction of I, (v, j) from Sr(v, – 1), St(i, j),",
        "from S,-(v, i – 1), St(i,i), and dependency relation from tvi to wy.",
        "The three cases are depicted in Figure 11.",
        "The basis probabilities for complete-sequence outside probabilities are as follows.",
        "a(BOS, E0S) = off,.",
        "(BOS, EOS) = (1, BOS) =1 The reestimation algorithm computes the inside probabilities(g., $J,0,.",
        "?, and f3fl inscribing them into the chart in bottom-up and left-to-right.",
        "The outside probabilities(al,., o4, and at) are computed and inscribed into the chart in top-down and right-to-left.",
        "Training The training process is as follow;",
        "1.",
        "Initialize the probabilities of dependency relations between all the possible word pairs.",
        "2.",
        "Compute initial entropy.",
        "3.",
        "Analyze the training corpus using the known probabilities, and recalculate the frequency of each dependency relation based on the analysis result.",
        "4.",
        "Compute the new probabilities based on the newly counted frequencies.",
        "5.",
        "Compute the new entropy.",
        "6.",
        "Continue 3 through 5 until the new entropy previous entropy.",
        "The above iteration is continued until all the probabilities are settled down or the training corpus entropy converges to the minimum_ The new usage count of a dependency relation is calculated as follows.",
        "In the following expression, the 0,„ is 111 the dependency relation is used in the given tree and 0 otherwise.",
        "Similarly, the usage count of (wt Wi), C(Wi 4 – /P) , 421(t13)4(iti)*"
      ]
    },
    {
      "heading": "PktVi,n/",
      "text": [
        "Chart has .24-1.4-3 number of boxes.",
        "The reestimation algorithm computes eight items for each chart box and the computation of each item needs maximally n number of productions and summations respectively.",
        "So the time complexity of the algorithm is 0(n3).",
        "The algorithm can be used for class-based (or tag-based) dependency grammar.",
        "With the concept of word class/tag, the complexity is affected by the class/tag size due to the class/tag ambiguities of each word.",
        "In the worst case, the time needed is 8 x t2 X n3+432+3n, so the complexity will be 0(t2n3) with respect to t, the number of classes and n, the length of input string."
      ]
    },
    {
      "heading": "4 Experimental results on Korean",
      "text": [
        "We performed experiment of the reestimation algorithm on Korean language.",
        "Korean is a partially ordered language in which the head words are always placed to the right of their dependent words.",
        "Under such restriction on word order, an abstract dependency structure for Korean is depicted in Figure 12.",
        "Thus, in Korean, both of ST(i, j) and Lf(i, j) are meaningless and not constructed.",
        "Only St, Li and null Sr (i, i) are considered.",
        "/4(0) is always composed with the combination of null i) and St(i + 1,j).",
        "Korean sentence is spaced by 'word-phrases\" which is a sequence of content word and its functional words.",
        "In this experiment, the final part of speech(POS) of a 'word-phrase\" is selected as the representative POS and the inter-word-phrase dependencies are reestimated.",
        "We used 54 POS set.",
        "The initial probabilities of all the possible dependencies were set equal.",
        "Theexperiment was performed on three kinds of training and test sets extracted from",
        "KAIST corpora'.",
        "The convergence of entropies(bits/word) through training iterations and the test corpus entropy are shown in the table 1.",
        "Set-1 is extracted from information and communication related texts; the train corpus of set-1 is 1,124 sentences (14,427 words) and the test corpus is 162 sentences (2,170 words).",
        "Set-2 is extracted from economy related texts; the train corpus is 3,499 sentences (40,818 words) and the test corpus is 409 sentences (4,662 words).",
        "Set-3 is not restricted to a domain; train corpus is 29,169 sentences (336,824 words) and the test corpus is 1917 sentences (15,903 words).",
        "The experiment result shows that the proposed reestimation algorithm converges to a (local) minimum entropy.",
        "It shows also that the train and test entropies are not affected much by the domain nor by the size of training corpus.",
        "It may be because the reestimation was done on inter-POS dependencies, which is relatively simple.",
        "If the reestimation would be done on the dependencies between POS sequences for \"word-phrase' or on the dependencies between lexical entities, the entropies may be affected much by the domain and the size of corpus.",
        "We plan such experiments.",
        "Below we show the parsing results of two example Korean sentences.",
        "We used the proposed best-first parsing algorithm to find the most probable parse of each sentence.",
        "The inter-word-phrase probabilities used for parsing are the reestimated ones for the training set-3.",
        "To the right of each Korean word-phrase, the meaning of it in English is given in the square brackets.",
        "In the parse representations, each individual inter-word-phrase probability is given to the right of the dependent word-phrase.",
        "The probability of each parse is the product of all the inter-word-phrase probabilities in the parse and is given on the top of each parse."
      ]
    },
    {
      "heading": "Input Sentence:",
      "text": [
        "1KAIST(Korean Advanced Institute of Science and Technology) corpora have been under construction since 1994 under Korean Information Base System construction (KIBS) project.",
        "It is supported by the Ministry of Culture and Sports, Korea.",
        "The KAIST corpora consist of raw text corpus(45,0011000 word-phrases), POS tagged corpus(6,750,1300 word-phrases), and tree tagged corptis(30,000 sentences) at present.",
        "For our experiments, we extracted each train/test set from the POS tagged corpus."
      ]
    },
    {
      "heading": "5 Conclusion and Further Works",
      "text": [
        "In this paper we have proposed a reestimation algorithm and a best-first parsing algorithm for probabilistic dependency grarnmars(PDG).",
        "The reestimation algorithm is a variation of the inside-outside algorithm adapted to PDG.",
        "In both of the reestiraation algorithm and the parsing algorithm, the non-constituent objects, complete-link and complete-sequence, are defined as basic units for dependency structures and a CYK-style chart is utilized.",
        "Both algorithms have 0(n3) time complexities and can be used for corpus-based, stochastic",
        "PDG induction.",
        "By experiment on Korean, we have shown that the reestimation algorithm converges to a local minimum and constitute a stable grammar.",
        "Compared to phrase structure grammars, PDG can be a useful and practical scheme for parsing model and language model.",
        "It is because dependency tree is much simpler and easily understood than the structure constructed by the phrase structure grammars.",
        "Besides the search space of the grammar may be smaller and the effect of structural data sparseness may be less.",
        "This also means that the reestimation algorithm for PDG can converge with smaller training corpus.",
        "We are planning to evaluate the parsing model based on the reestimated PDG and the PDG-based language model."
      ]
    }
  ]
}
