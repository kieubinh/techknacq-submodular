{
  "info": {
    "authors": [
      "Tony G. Rose"
    ],
    "book": "Workshop on Very Large Corpora",
    "id": "acl-W97-0118",
    "title": "The Effects of Corpus Size and Homogeneity on Language Model Quality",
    "url": "https://aclweb.org/anthology/W97-0118",
    "year": 1997
  },
  "references": [
    "acl-J93-1003",
    "acl-J93-2001",
    "acl-W97-0122"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Generic speech recognition systems typically use language models that are trained to cope with a b variety of input.",
        "However, many recognition applications are more constrained, often to a specific I or domain.",
        "In cases such as these, a knowledge of the particular topic can be used to advantage.",
        "report describes the development of a number of techniques for augmenting domain-specific lang models with data from a more general source.",
        "Two investigations are discussed.",
        "The first concerns the problem of acquiring a suitable sample o: domain-specific language data from which to train the models.",
        "The issue here is essentially on quality, since it is shown that not all domain-specific corpora are equal.",
        "Moreover, they can dis significantly different characteristics that affect the quality of any language models built therefrom.",
        "T characteristics are defined using a number of statistical measures, and their significance for lang modelling is discussed.",
        "The second investigation concerns the empirical development and evaluation of a set of language mc for the task of email speech-to-text dictation.",
        "The issue here is essentially one of quantity, since shown that effective language models can be built from very modestly sized corpora, providing training data matches the target application.",
        "Evaluations show that a language model trained on or million words can perform better than one trained on a corpus of over 100 times that size."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The development of robust speech recognition technology offers great potential for the desig: improved interfaces to a wide range of applications.",
        "The current project concerns the development of such application: the speech-to-text dictation of email messages.",
        "The work makes use of the A recogniser, which is a connectionist/HAM continuous speech recognition system developed by Connectionist Speech Group at Cambridge University.",
        "It is designed to recognise British English American English, clearly spoken in a quiet acoustic environment (Hochberg et al., 1994).",
        "The Abbot system is available with a vocabulary of 20,000 words, which means that anything sp■ outside this vocabulary cannot be recognised (and therefore will be recognised as another word or si of words).",
        "The vocabulary and grammar (LM) were optimised for the task of reading from a N American Business newspaper, in this case the Wall Street Journal.",
        "Some 227 million words of trai text were used in building this LM and it is widely used throughout the speech community.",
        "Howe despite the size of the original training corpus, this LM was clearly not designed for the specific tas email dictation, so its performance is likely to be suboptimal.",
        "However, a new vocabulary and LM",
        "easily be created and then substituted for the one supplied.",
        "The LMs described in this paper were `back-off trigram' LMs (Katz, 1987), built using the CMU SLM toolkit (Rosenfeld, 1994)."
      ]
    },
    {
      "heading": "2. Corpus Acquisition",
      "text": []
    },
    {
      "heading": "2.1 The form of email messages",
      "text": [
        "In order to build a LM for the task of email dictation, it is necessary to acquire a corpus of suitable em training data.",
        "However, behind this ostensibly simple objective lie several subtle challenges.",
        "\"Email\", a!",
        "general term, describes a great variety of types of communication.",
        "These types are perhaps best illustrat by considering the range of functions that email messages typically provide.",
        "For example, email can used as a medium for:",
        "• a formal face-to-face meeting; • a casual face-to-face chat; • a broadcast (e.g. \"Tannoy\") message; • requesting information; • replacing an office memo; • replacing a phone call, etc.",
        "Clearly, the purpose of each communication can be very different, and the language used will reflect th Furthermore, apart from the issue of domain (i.e. subject matter), the type of language used will also va according to the role of the participants.",
        "For example, when requesting advice from a mailing list a would tend to be more formal and polite than when requesting the same advice from a friend or colleagt Consequently, it would appear that email messages vary almost as much as spontaneous, spoken dialog% If this is indeed so, then the prospects for building effective language models for email may appe somewhat limited.",
        "Clearly, in order to move forward, it is necessary to define some limits.",
        "The first of these concerns the quantity.",
        "What is a reasonable size for an email corpus?",
        "There are fe precedents for this so the question was answered empirically, by finding a compromise between the nel to acquire sufficient training data and the need to complete the acquisition phase within a reasonat space of time.",
        "However, the time taken to reach a certain quantity depends very much on the source of tl data, which forms the second limit: from where should the email be acquired?",
        "A range of possibiliti exists, e.g. the Internet (i.e. bulletin boards, mailing lists, email archives) or specific individuals (i. previously saved messages, day-by-day output).",
        "Evidently, email acquired from the Internet exhibits a wide range of authorship, function and subje matter.",
        "In addition, downloading large quantities of text from such sources without the authors' conse may involve certain copyright issues.",
        "Clearly, the limits of the source are more easily defined if the em; is restricted to the output of a group of specific individuals.",
        "However, unless the group is very larE acquiring just 1 million words from their day-by-day output would be too slow to enable the acquisiti■ to be completed within a reasonable space of time.",
        "Therefore, individuals with a large collection previously saved messages were identified as more suitable candidates.",
        "Furthermore, a restriction that members of this group must be employees of HPLB (Hewlett-Packard Labs, Bristol) placed a furth constraint on the source.",
        "To ensure controlled authorship, only the outgoing messages of these individw were collected."
      ]
    },
    {
      "heading": "2.2 The content of email messages",
      "text": [
        "The \"content\" of an email message is not an easy concept to define.",
        "Evidently, the body contains mu important data, but what about the other elements, e.g. headers, signatures, quoted sections, etc.",
        "- wl",
        "roles do they play?",
        "In the case of headers, a cursory analysis revealed that they could safely be discat since few contained any useful information.",
        "However, other email components are not quite so eg categorised, e.g.",
        "• quoted (included) messages: these are usually referred to by the message content, but are often product of a different author; • email 'signatures': these are often quite verbose, but rarely contribute anything to the message conte • samples of Postscript/Latex: these were problematic, since people would often quote verbatim L passages to illustrate a point that did indeed contribute to the content of the message.",
        "Howevel build models from data that included such heavily marked-up text is questionable.",
        "Since the above items were all rendered as ASCII strings, their surface form could fairly reliabl] predicted and they were therefore removed from the corpus using suitably designed \"filters\".",
        "Howe there is a further number of email attachments that are not composed of predictable ASCII strings.",
        "11 include items such as sharred or uuencoded files and word processor/DTP output.",
        "Evidently, such it need to be removed, but finding small fragments of such diverse data in a corpus of several million wi is a non-trivial problem.",
        "Alternatively, rather than trying to filter out \"noise\" from the email \"signal\", it is possible to adopt converse approach, and try to identify those lines that constitute genuine English within the overall el data, which may then be retained as the true training data.",
        "It is possible to achieve this using var heuristics, e.g. \"retain those lines that contain at least 90% English words\".",
        "However, this apprx assumes there exists some predefmed vocabulary, which is somewhat tautological since the vocabulal one of the things we seek to define in the first place."
      ]
    },
    {
      "heading": "2.3 The email collection process",
      "text": [
        "A programme of email data collection took place over a period of 2-3 weeks, following the princi described above.",
        "This resulted in the acquisition of some 4 million words of email data.",
        "The \"don were asked to provide both previously saved messages and intermittent day-by-day output.",
        "This necessary since (by their very nature) saved messages tend to possess some sort of significant coo and were therefore often of above average length.",
        "In contrast, much day-by-day email correspond( uses an informal dialogue that is heavily context dependent, and therefore may be no more than a si brief phrase or sentence.",
        "This was then filtered in the manner described above.",
        "The final output w corpus of email data of 1,962,280 words (49% of the original size).",
        "This was then partitioned (95%: into training and test data."
      ]
    },
    {
      "heading": "3. Corpus adaptation",
      "text": [
        "It is theoretically possible to build a LM using the tiniest of corpora.",
        "On balance, however, the 2 mil words of email training data look somewhat inadequate compared to the 227 million words used foi WSJ LM.",
        "The problem is that the coverage of the n-grams is likely to be sparse and any LM so built be degenerate since it does not reliably predict the characteristics of the source.",
        "To illustrate, considel distribution of unigram frequencies: a mere 14,137 word types (19%) in the email corpus frequencies of 6 or greater.",
        "Therefore, to acquire a vocabulary of just 20k words without u frequencies of 5 or less clearly requires a training corpus larger than 2 million words.",
        "It would be highly desirable therefore if a method could be devised whereby information from a I corpus could be combined with a smaller sample of the domain-specific training data to create an opt language model.",
        "One such approach involves augmenting a base model built from a larger, more gei corpus with information from a small sample of the domain-specific language.",
        "There is evidenc suggest that this method can improve recognition performance (e.g. Rudnicky, 1995, Vergyri, 1.995)",
        "alternative approach is to use a suitable similarity metric to acquire further \"email-like\" training data frr the larger corpus (henceforth referred to as the \"background corpus\"), and then build a new langua model from the combined text.",
        "This approach offers interesting possibilities regarding the development a general methodology for corpus adaptation, by attempting to \"grow\" a suitable corpus of training di for any domain using only a small sample as a \"seed\".",
        "A number of ways to implement this techniq have been developed.",
        "Broadly speaking, they fall into two categories: \"top-down\" methods and \"bottoi up\" methods."
      ]
    },
    {
      "heading": "3.1 The top-down approach",
      "text": [
        "At its simplest, this approach involves a combination of manual inspection and regular expressi searching to identify those parts of the background corpus that contain suitable material.",
        "It relies on good classification scheme and reliable organisation of the background corpus.",
        "The British Natior Corpus (BNC) is a suitable example, since it contains 100 million words of modern English, both spok and written, sampled from the widest range of materials.",
        "It is annotated with part-of-speech codes, ai SGML-encoded according to the Text Encoding Initiative's Guidelines (Burnard, 1995).",
        "It is there& possible to use the SGML tags to identify suitable texts.",
        "For example, extracting 10 million words of te for a domain such as World Affairs is trivially easy, since domain information is encoded in the header each individual file (of which there are over 4,000).",
        "Since much BP email concerns the computing business, and the BNC classifies computing as a branch Applied Science, it would appear that the 10 million words from Applied Science section of the BNC m prove sufficiently similar.",
        "Likewise, the 10 million words classified as Commerce and Finance may al prove suitable.",
        "The effect of such an addition would be to increase the size of the training corpus from million words to 22 million, which constitutes an increase of 1100%.",
        "However, methods such as this cannot be justified by subjective judgement and anecdotal evidence.",
        "WI] is required is an objective measure that reliably identifies which of the domains in the BNC is mc similar to HP email.",
        "There may be many standard statistical techniques for measuring the degree similarity of two data sets, but not all are suitable for the task of comparing corpora (Church et al., 199 For example, some assume a normal distribution, which is clearly inappropriate for textual data.",
        "What needed therefore is a test that makes few assumptions about the distributions of the underlying data, b provides a directly usable measure of similarity.",
        "One such test is the rank correlation, using Spearman's The assumptions behind rank correlation are few.",
        "It measures the degree of monotonic associati4 between two rankable variables.",
        "The distribution of r as normal (mean 0, variance 1/(N-1), assumii independence) is asymptotic for large enough samples, and does not make any assumptions abo normality.",
        "This test was therefore applied to the word frequency lists of each of the domains in the BIS and the email corpus, to identify which corpora were most similar.",
        "The correlation with the BNC as whole was also measured.",
        "All calculations were based on Spearman's S, where D2 denotes the sum oft] squares of the differences between the ranks of each pair of word types, and N the number of rank pairs:"
      ]
    },
    {
      "heading": "6D2",
      "text": [
        "r= 1"
      ]
    },
    {
      "heading": "143 - N",
      "text": [
        "It is known that a sublanguage corpus can have very different characteristics to a general corpus (Bib4 1993), yet it is not obvious how the position on this scale of a given corpus can be assesse Consequently, it is necessary to determine the homogeneity of a corpus prior to performing any shrilled measures, since it is not clear what a measure of similarity would mean if a homogeneous corpus w being compared with a heterogeneous one (Kilgarriff, 1996).",
        "A homogeneity test was therefore perform on the corpus of each domain.",
        "This was calculated using the following algorithm:",
        "1.",
        "For each domain corpus, do (times 10) 2.1 Divide the corpus into two halves, by randomly placing 5k-word chunks in one of subcorpora; 2.2 Produce a word frequency list (wfl) for each subcorpus; 2.3 Calculate the rank correlation between the two subcorpora; 3.",
        "Calculate the mean and standard deviation of r.",
        "Email the 2 million word email corpus For large samples such as these the rank correlation coefficient has a normal distribution with mean 0 a] variance 1/(n-1) where n is the number of common words.",
        "Although the significance of the correlation not in doubt, the differences are highly significant too.",
        "The difference between two rank correlatie coefficients will be normally distributed with mean 0.",
        "The maximum possible value for the standa deviation is (1/4(n1-1))+(1/q(n2-1)) where nl,n2 are the two common vocabulary sizes.",
        "Any differem greater than about 0.03 is therefore significant, and there are many pairs for which this is true.",
        "It therefore possible to rank the rank correlations, and hence the BNC domains.",
        "Evidently, the strongest correlation with the email corpus is from As (Applied Science).",
        "Interestingly, th figure is higher than that between email and the whole BNC.",
        "The second highest domain correlation with Cf (Commerce & Finance).",
        "This agrees with intuitions based on a manual inspection of the conten of the email corpus.",
        "The table also shows a polarity of the BNC - the \"arts\" domains at one pol attracting each other (e.g. Ar:Bt = 0.408) but repelling the sciences (e.g. Im:As = 0.159).",
        "Similarly, 11 sciences attract each other (e.g. Np:As = 0.315).",
        "In the middle are domains such as World Affairs, Soci Sciences & Commerce & Finance that correlate with both poles to varying degrees.",
        "Moreover, the Erna corpus really stands out on its own, having a very poor correlation with the others (in many cases it negative).",
        "This suggests that even if the most strongly correlated domains are chosen, it is difficult 1 justify augmenting the email corpus with texts selected from the BNC using this method.",
        "Table 1 al: shows the results of the homogeneity tests.",
        "Email is by far the most heterogeneous, more so even than t1 \"Unclassified\" section of the BNC (!)",
        "This brings into question the results of the similarity calculations which the email corpus was involved, and mitigates further against the strategy of augmenting the ems corpus with texts selected using the top-down method.",
        "These results also provide insight into the relationship between homogeneity and language model qualit A common measure of LM quality is perplexity (PP), which can be thought of as a measure of t1 \"branching factor\" (i.e. the average size of the set of words between which a speech recogniser mu choose), when transcribing a single word of the spoken text.",
        "PP thus measures the recognition difficulty the text relative to the given LM, and is measured by applying the model to a sample of test dat Consequently, a LM derived from a heterogeneous corpus should have a higher perplexity than s equivalent one derived from a more homogenous corpus.",
        "However, homogeneity is defined here as measure of unigram distributions, whereas perplexity is usually calculated using n-grams (where n usually <=3), so it is not clear to what extent the two measures would be related."
      ]
    },
    {
      "heading": "3.2 The bottom-up approach",
      "text": [
        "The top-down approach assumes that the BNC classification system is perfect, in that each text classifie as belonging to a certain domain really belongs in that domain.",
        "However, this is ultimately a subjectil judgement, and frequently more than one classification is possible or even preferable (Lewis, 1992 Moreover, it is often the case that texts from the same medium are more similar to each other than tex from the same domain (e.g. a journal paper on computing may be more similar to a journal paper c geology than an item from a popular computing magazine, because the \"content\" features are lost =or the much more salient \"genre\" features).",
        "Besides, no classification system is 100% reliable, so techniqu4",
        "that are based on them will inherit this uncertainty.",
        "Furthermore, domains such as Applied Scienc very coarse-grained: they contain many more types of material than just those of computing.",
        "Even if corpora are subdivided to a further level of classification they still suffer the same problem, albeii finer level of detail.",
        "An alternative strategy is to work in a \"bottom-up\" direction.",
        "In this approach, a similarity metric is to fmd and extract related material from the background corpus, regardless of the top-down classificr This method may not be as structured as the previous approach, but it is more robust in that it involvi manual intervention and does not rely on correct organisation or SGML tagging of the backgr corpus.",
        "Moreover, it will not \"miss\" material that is classified under an unexpected domain or me but is otherwise suitable.",
        "The success of this approach depends on the use of a reliable similarity it (even more so than the top-down approach, since it is now being applied to each of the 4,000+ files i BNC rather than the 10 domain-based collections).",
        "Using this statistic to find texts that are simil email in the BNC could be achieved using the following algorithm:",
        "1.",
        "Create a wfl for the email corpus.",
        "2.",
        "For each individual text in the BNC, do: 2.1 Create the wfl for the BNC text 2.2 Create a contingency table from the 2 wfls (ignoring function words) 2.3 Calculate the number of common words N and the rank correlation r 3.",
        "Store the filename, title, N and r in RESULTS 4.",
        "Output the RESULTS sorted on the value for r.",
        "Although the rank correlation may be applied to the wfls regardless of their content, it was f empirically that performance improved if function words were excluded from the contingency tab] stop list of 241 function words was therefore applied in Step 2.2 of the above algorithm.",
        "The algoi was run on the entire BNC (i.e. each of its 4,000+ files).",
        "The output was a list of the files si according to the value of r. The top and bottom 10 texts on this list are as follows:",
        "Each line shows the filename, the title of the text, the number of common words and the value for first glance, the results appear to be intuitively satisfying.",
        "Of the top ten texts, six have tides tha clearly related to computing, including all of the top five.",
        "The remaining four could arguably be class as Commerce & Finance (which was identified as the second most similar domain to email).",
        "Howes) suitable title is no guarantee of suitable contents.",
        "As far as can reasonably be expected, the 1 constitute a fair and accurate reflection of the contents of each text.",
        "Of course, the whole point of",
        "approach is to develop techniques that do not rely on ambiguous manual annotations such as title domain, so the presence of suitable titles is merely an initial indication of success.",
        "One way of evaluating this result is to go through the list and calculate the mean rank of the e \"Computergram International\" texts, which are typical of the sort of texts this technique should identify being similar to the email corpus.",
        "If the technique is working perfectly, the mean rank should be 31.",
        "If is completely random, the mean rank would be 2062.",
        "It transpires that the mean rank is 959.85 (std dev 524.44).",
        "Clearly, this result is better than chance, but far from significant.",
        "One of the main reasons f this was a tendency to sometimes give high scores to texts that were actually too short to constitu reliable samples (the BNC attempts to maintain a standard sample size but this is not always possible).",
        "logical modification was therefore to ignore those texts for which the number of common words w below a certain threshold.",
        "A number of thresholds were investigated, and the optimum value (determfile empirically) was around 1,370 words.",
        "However, even with this modification, the mean rank remained high as 818.41 (std dev = 407.81).",
        "It is possible to reduce this value still further, but only t compromising the overall recall value (i.e. genuine texts are eliminated along with the \"noise\").",
        "However, there is a more fundamental limitation to the above methodology.",
        "The rank correlation statist compares differences in rank, ignoring absolute value (which can be significant).",
        "To illustrate, consider case where the word \"of' is ranked 3 in one corpus and 6 is another.",
        "This is a very important differenc Conversely, if \"banana\" is ranked 10,000 in one corpus and 100,000 in another, this is a very insignifical difference.",
        "But the difference of ranks for \"of\" = 3, for \"banana\" = 90,000.",
        "Clearly this technique missing something important.",
        "Consequently, it was decided to investigate an alternative measure: ti Loglikelihood Ratio Statistic.",
        "The Loglikelihood Ratio, 02, is a mathematically well-grounded and accurate method for calculating ha \"surprising\" an event is (Dunning, 1993).",
        "This is true even when the event has only occurred once (as often the case with linguistic phenomena).",
        "It is an effective measure for the determination of domaii specific terms (e.g. Daille, 1995) and can be also used as a measure of corpus similarity.",
        "In the cal where two corpora are being compared, it is possible to calculate the 02 statistic either for single wore (using a 2x2 contingency table) or for a vocabulary of N words (an Nx2 table).",
        "The analysis of the 4,000 BNC files was therefore repeated using the Loglikelihood (instead of rank correlation) as the similarii measure.",
        "This produced the following top and bottom 10 texts:",
        "Each line shows the filename, the title of the text, the length of the contingency table and the value for C These are sorted in ascending order since comparing two identical documents would produce a G2 of zer",
        "A brief inspection of the titles of the documents at the top of the list would indicate that the metric ha produced an improvement.",
        "Moreover, it transpires that the mean rank of the CI texts is now 1171.98, std dev = 178.54.",
        "However, as before, the number of common words is very small for some of the I Therefore, the filter was applied to ignore cases where there were fewer than 1,370 words in corn This produced a mean rank of 125.15 (std.",
        "dev.",
        "= 75.62), which is significantly lower than that prod by the rank correlation (mean rank = 818.41, std.",
        "dev.",
        "= 75.621).",
        "So despite the absence of apparently suitable candidates in the top 10, the overall accuracy 01 technique (measured by the mean rank of the 61 CI texts) is higher.",
        "The G2 statistic appears to be suitable for this type of data since it uses the actual frequency values for the words in the wfls, rathei just their ranks.",
        "Other independent sources indicate that the G2 produces results that appear to corres reasonably well with human judgement (Daille, 1995).",
        "However, both the rank correlation and Loglikelihood Ratio both make use only of unigram inform Clearly, much of the information that humans use to measure textual similarity is found not (solely) i individual word frequencies (unigrams), but rather in the way they combine (n-grams).",
        "The logical step is therefore to compare word bigrams (or trigrams) instead of just unigram data.",
        "A variation oi would be to compare texts using the Loglikelihood applied to bigrams that are not necessarily adja i.e. counting occurrences of wordl and word2 within a limiting distance of each other.",
        "Indeed, methods have been previously used for actually building the LMs themselves, and have been succesE applied to both speech (Rose & Lee, 1994) and handwriting data (Rose & Even, 1995).",
        "Counting v within a limited window would be smoother than using strict bigrams and consequently less affecte the problems caused by sparse data (which are inevitable when small, individual text files are compai Another interesting possibility is to use the LM itself as the similarity metric.",
        "From an inform theoretic point of view, entropy is a measure of a corpus's homogeneity, and the cross-entropy bet two corpora is a measure of their similarity (Charnialc, 1993).",
        "After all, when a LM is applied to text to produce a perplexity score, this value is a measure of the cross-entropy which reflects how we LM predicts the words in the text.",
        "So if a LM is trained on text that is very similar to the test text, ti should predict the test data well and the perplexity should be low.",
        "Conversely, if the test text is different from the training text, then the perplexity will be high.",
        "The perplexity score can therefa used to measure textual similarity.",
        "Moreover, it has the advantage doing so by considering (typic unigram, bigram and trigram data.",
        "Indeed, this method has already been successfully used withii development of a similarity-based Internet search agent, and preliminary findings indicate that perpl is indeed an effective corpus similarity measure (Rose & Wyard, 1997).",
        "However, the use of such an approach is not entirely beyond question.",
        "Firstly, the LM is being used a representation of a training text against which similarity is to be judged, and yet it is, by definition, tt trained and therefore degenerate.",
        "Secondly, the method by which similarity is measured should ideal independent to the method by which success is evaluated.",
        "To use perplexity both as a similarity n and an evaluation metric implies a certain amount of circular reasoning.",
        "However, the use of iterative techniques is not totally without precedent within the LM community.",
        "Several research gi have reported the successful improvement of LMs using techniques that iteratively tune the parameters using new samples of training data (e.g. Jelinek, 1990).",
        "So, this approach may transpire sufficiently well principled to merit further investigation."
      ]
    },
    {
      "heading": "4. Language model quality",
      "text": [
        "A LM is built by collecting trigram, bigram & unigram data from a training corpus.",
        "However, it i always desirable to store all of this data.",
        "Thresholds can be set such that some of the lower frequen grams are discarded.",
        "For example, a trigram cut-off of 5 implies that all the trigrams with frequencie or fewer in the training data are not used in building the model.",
        "Setting lower thresholds allows the",
        "to focus on more frequent events, and produces a proportionately smaller model.",
        "The LMs described this paper were built using the CMU SLM toolkit (Rosenfeld, 1994) which facilitated the construction a variety of LMs representing a range of different settings for each of the pertinent parameters.",
        "The first of these was the Email LM.",
        "This was constructed using a vocabulary of 20,000 words that 'Qv derived directly from the email training data.",
        "The bigram and trigram cut-offs were both set to zero.",
        "T second LM was built from the whole of the BNC, using the same vocabulary as the Email LM (in order ensure consistency).",
        "So although their n-grams had been based on general English rather than Email, thl vocabulary was derived from the Email data.",
        "For comparison therefore, a third BNC LM was built, usi a vocabulary derived directly from the BNC (rather than email).",
        "This allowed the comparative evaluatil of the contribution of vocabulary vs. n-grams to the LM effectiveness (measured using both perplexi and word error rate).",
        "Due to memory constraints it was not possible to build the BNC models with a offs lower than 2-2.",
        "The fourth LM investigated was the 20k WSJ LM that is available from the Abbot f site at Cambridge University.",
        "The standard measure by which LMs are assessed is by calculating their perplexity using a sample of te data.",
        "This process is usually performed off-line, i.e. independently of the speech recogniser for which t1 models are intended.",
        "For the models described above, testing was performed using the CMU toolldt, 1 applying each LM to a sample of 10,000 words from the transcriptions of a database of video in messages, developed by Cambridge University as part of their \"Video Mail Retrieval using Vol° project (Jones et al., 1994).",
        "Evidently, this data is not actually spoken email, but its domain and genre a nevertheless closely related to email.",
        "Unfortunately, it was not possible to calculate the PP of the W LM due to the absence of a readily available version in the correct format.",
        "A second evaluation method is to integrate the LM with the speech recogniser and test the combini system using recorded speech data.",
        "The models can be interchanged between trials, allowing comparad evaluation by measuring the word error rate (WER) produced by each model.",
        "More precisely, the err rates are measured using two standard metrics, percentage correct and accuracy:",
        "1.",
        "%Correct = – Hx100% 2.",
        "Accuracy =(H – I) x100%",
        "where: H is the number of correct transcriptions (words in the utterance that are found in t] transcription), D is the number of deletions (words in the utterance that are missing from t transcription), S is the number of substitutions (words in the utterance that are replaced by an incorre word in the transcription), and I is the number of insertions (extra words in the transcription).",
        "Accuracy more critical than %correct in that it directly penalises insertions.",
        "Deletions & substitutions reduce t value of H, since H = N - (D+S).",
        "As mentioned above, the VMR database is a collection of speech data with transcriptions (of which t latter were used in the above evaluation).",
        "The speech part contains audio files for 15 speakers, of whil 10 were used in the current investigation.",
        "The Abbot recogniser was run using each combination of the speakers' data files (as input) and each of the four LMs: email, BNC with email vocabulary, BNC and t WSJ LM.",
        "The output transcriptions were assessed for %correct and accuracy using the HResults progra which is part of HTK - the Hidden Madcov Model Toolkit (Young & Woodland, 1993).",
        "Table 2 shows the results of this investigation.",
        "The results for %correct and accuracy show the combin effect of the recogniser and LM.",
        "The contribution of the LM depends on its vocabulary and perplexity.",
        "the LM changes, it produces different behaviour in the combined system and therefore different typf errors (e.g. insertions, deletions & substitutions).",
        "The net effect is that the email LM produces the hig %correct and also the highest accuracy.",
        "It is around 5% better (on both measures) than the WSJ LM.",
        "is significant, considering the tiny corpus from which it was derived (2 million vs. 227 million in the of WSJ).",
        "In between these two extremes are the two BNC LMs - the one with the email vocabt performs slightly better (-0.5%) than the one with the BNC vocabulary.",
        "The result for the PP testing is highly revealing.",
        "As described earlier, a corpus of low homogeneity sh4 produce a LM of higher PP than a corpus of high homogeneity.",
        "This is indeed shown to be the case, s the PP for email is 261.58 (homogeneity = 0.362), whereas the PP for the BNC is 227.54 (homogenei 0.687).",
        "These PP values are calculated using the 10K test data sample from the transcriptions of the V project.",
        "The higher PP value for email would tend to indicate that this is the poorer LM.",
        "However, clear that when used on the real spoken data, the email LM provides the lowest error rates.",
        "In explanations for this centred on the vocabulary, since a higher incidence of out-of-vocabulary (01 words can produce a lower PP but a higher WER.",
        "However, the email LM performs better (by 0.1",
        "correct) than the BNC/ernail LM even though both share the same vocabulary.",
        "Two explanations for I are possible.",
        "Firstly, there may be n-grams in the email corpus that are simply not found in the BNC (el though the BNC is 50 times larger).",
        "Secondly, the email LM may be better because it \"wastes\" 1 probability mass on n-grams that never actually occur in the test data.",
        "This implies that quality, quantity, is a major factor in training effective LMs.",
        "Further PP testing, possibly using the compl transcriptions of the VMR data is necessary to clarify this issue.",
        "Evidently, the choice of vocabulary also makes an important contribution.",
        "The BNC LM with the en: vocabulary performs better (by 0.64% correct) than the BNC LM with the BNC vocabulary, so clearly 1 email vocabulary provides better coverage of the test data.",
        "In fact, it is possible to directly compare 00V rates with the performances shown above: the BNC LM with the email vocabulary has an 00V r of 1.16% on the VMR data, and a %correct of 54.04.",
        "By contrast, the BNC LM with the BNC vocabut has an 00V rate of 1.69% and a %correct of 53.40.",
        "These figures suggest that an increase in 00V rate 0.56% leads to a reduction in %correct of 0.64%, or, in other words, a 1% increase in 00V rate prodth a reduction in %correct of around 1.14%.",
        "Interestingly, this figure correlates extremely well with results of a similar experiment performed by Rosenfeld (1995), who found that a 1% increase in the 0( rate can lead to a 1.2% increase in the word error rate."
      ]
    },
    {
      "heading": "5. Conclusions",
      "text": [
        "The analysis of the corpora has provided several revealing insights.",
        "Firstly, it is necessary to determi the homogeneity of a corpus prior to performing any similarity measures, since it is not clear wha measure of similarity would mean if a homogeneous corpus was being compared with a heterogenec one.",
        "A methodology for calculating homogeneity has been described and the accuracy and usefulness this is further described in Kilgarriff (1997).",
        "Clearly, the email corpus is highly heterogeneous.",
        "This means it is particularly prone to \"burstiness\" a unpredictability, which affects all levels of n-grams (including unigrams).",
        "This may be due in part to 1 particular training corpus used, but it is more likely to be inherent to the medium, since email can fulfil many communicative functions.",
        "It therefore exhibits a level of diversity surpassed perhaps only spontaneous speech.",
        "Investigation of the spoken part of the BNC is therefore suggested as an area I further work.",
        "To a certain extent, the apparent heterogeneity of the email undermines the results of any similar measures applied to this corpus.",
        "Nevertheless, the extent to which the email is unlike all the other BI` domains is quite apparent and therefore mitigates any unprincipled approaches to corpus augmentati using crude, top-down techniques that involve complete domains taken from the BNC.",
        "Consequently, 1 best way to acquire more email data appears to be either: (a) to instigate a further collection initiative, 1 to use more sophisticated bottom-up methods, or (c) to use self-organising adaptation techniques (e Clarkson and Robinson, 1997).",
        "The similarity metric used in (b) must be chosen carefully.",
        "Although Loglikelihood and rank correlation metrics both produce results that can look intuitively plausible, ti merely underlines the need for an objective, thorough evaluation method.",
        "Loglikelihood appears to be t more principled of the two measures, and it is suggested that this offers the greater potential.",
        "The results of the language modelling exercise provide clear evidence that it is possible to build effecti LMs from small corpora.",
        "The email LM outperformed the other LMs on real spoken data (albeit tak from a technical, \"email-like\" domain) for eight of the ten speakers.",
        "This is significant, considering other LMs were trained on corpora that were several times larger.",
        "This effect can be mainly attributed the source of the n-grains and the extent .to which the larger LMs \"waste\" probability mass on n-gra] that never actually occur in the test data.",
        "Other researchers also have investigated methods for adapti large, general LMs using data from a small domain corpus and have found merit in simply buildinl",
        "smaller LM directly from the domain corpus.",
        "For example, Ueberla (1997) observed that improvements gained by using adaptation techniques compared to simply \"starting from scratch\" cc domain data become quite small when several tens of thousands of words of domain data are avail (Since the email corpus is almost 2 million words it clearly meets this criterion.)",
        "It is interesting to also that this threshold is seen to vary according to the level of similarity between the adaptation (dor specific) corpus and the background (general) corpus.",
        "It is also possible to adapt LMs dynamically, using cache-based methods (e.g. Kuhn & de Mori 1990: evidence suggests that this may prove the more effective approach (Matsunaga et al., 1992).",
        "It is clear email is highly heterogeneous and therefore inherently unpredictable.",
        "Attempting to model this by 5. means can thus produce only limited success.",
        "By contrast, a dynamic LM would adapt to the current i and update its probabilities accordingly.",
        "However, dynamic LMs still need a set of static, ban probabilities, so the email LM may present the best starting point for this."
      ]
    },
    {
      "heading": "6. References",
      "text": []
    }
  ]
}
