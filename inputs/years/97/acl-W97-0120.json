{
  "info": {
    "authors": [
      "Masaaki Nagata"
    ],
    "book": "Workshop on Very Large Corpora",
    "id": "acl-W97-0120",
    "title": "A Self-Organizing Japanese Word Segmenter Using Heuristic Word Identification and Re-Estimation",
    "url": "https://aclweb.org/anthology/W97-0120",
    "year": 1997
  },
  "references": [
    "acl-A94-1009",
    "acl-C94-1032",
    "acl-C96-2136",
    "acl-J96-3004",
    "acl-P96-1019",
    "acl-W95-0109",
    "acl-W96-0113"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a self-organized method to build a stochastic Japanese word segmenter from a small number of basic words and a large amount of unsegmented training text.",
        "It consists of a word-based statistical language model, an initial estimation procedure, and a re-estimation procedure.",
        "Initial word frequencies are estimated by counting all possible longest match strings between the training text and the word list.",
        "The initial word list is augmented by identifying words in the training text using a heuristic rule based on character type.",
        "The word-based language model is then re-estimated to filter out inappropriate word hypotheses generated by the initial word identification.",
        "When the word segmenter is trained on 3.9M character texts and 1719 initial words, its word segmentation accuracy is 86.3% recall and 82.5% precision.",
        "We find that the combination of heuristic word identification and re-estimation is so effective that the initial word list need not be large."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word segmentation is an important problem for Japanese because word boundaries are not marked in its writing system.",
        "Other Asian languages such as Chinese and Thai have the same problem.",
        "Any Japanese NLP application requires word segmentation as the first stage because there are phonological and semantic units whose pronunciation and meaning is not trivially derivable from that of the individual characters.",
        "Once word segmentation is done, all established techniques can be exploited to build practically important applications such as spelling correction [Nagata, 1996] and text retrieval [Nie and Brisebois, 1996] In a sense, Japanese word segmentation is a solved problem if (and only if) we have plenty of segmented training text.",
        "Around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programing procedure [Nagata, 1994, Takeuchi and Matsumoto, 1995, Yamamoto, 1996].",
        "However, manually segmented corpora are not always available in a particular target domain and manual segmentation is very expensive.",
        "The goal of our research is unsupervised learning of Japanese word segmentation.",
        "That is, to build a Japanese word segmenter from a list of initial words and unsegmented training text.",
        "Today, it is easy to obtain a 10K-100K word list from either commercial or public domain on-line Japanese dictionaries.",
        "Gigabytes of Japanese text are readily available from newspapers, patents, HTML documents, etc.. Few works have examined unsupervised word segmentation in Japanese.",
        "Both [Yamamoto, 1996] and [Takeuchi and Matsumoto, 1995] built a word-based language model from unsegmented text",
        "using a re-estimation procedure whose initial segmentation was obtained by a rule-based word sel menter.",
        "The utility of this approach is limited because it presupposes the existence of a rule-base word segmenter like JUMAN [Matsumoto et al., 1994].",
        "It is impossible to build a word segment for a new domain without human intervention.",
        "For Chinese word segmentation, more self-organized approaches have been tried.",
        "[Sproat et al., built a word unigram model using the Viterbi re-estimation whose initial estimates were derive from the frequencies in the corpus of the strings of each word in the lexicon.",
        "[Chang et al., 199 combined a small seed segmented corpus and a large unsegmented corpus to build a word unigrai model using the Viterbi re-estimation.",
        "[Luo and Roukos, 1996] proposed a re-estimation procedin which alternates word segmentation and word frequency re-estimation on each half of the trainir text divided into halves.",
        "One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plun noun formation, Chinese personal names, and transliterations of foreign words.",
        "[Chang et at, 199 used a statistical method called \"Two-Class Classifier\", which decided whether the string is actual] a word based on the features derived from character N-gram.",
        "In this paper, we present a self-organized method to build a Japanese word segmenter froi a small number of basic words and a large amount of unsegmented training text using a novl re-estimation procedure.",
        "The major contribution of this paper is its treatment of unseen word We devised a statistical word formation model for unseen words which can be re-estimated.",
        "IA show that it is very effective to combine a heuristic initial word identification method with a r4 estimation procedure to filter out inappropriate word hypotheses.",
        "We also devised a new metho to estimate initial word frequencies.",
        "Figure 1 shows the configuration of our Japanese word segmenter.",
        "In the following sections, a first describe the statistical language model and the word segmentation algorithm.",
        "We then describ the initial word frequency estimation method and the initial word identification method.",
        "Final: we describe the experiment results of unsupervised word segmentation under various conditions."
      ]
    },
    {
      "heading": "2 Language Model and Word Segmentation Algorithm",
      "text": []
    },
    {
      "heading": "2.1 Word Segmentation Model",
      "text": [
        "Let the input Japanese character sequence be C = c1c2 c77,.",
        "Our goal is to segment it int word sequence W = w1tv2 ivn.",
        "The word segmentation task can be defined as finding a wor segmentation W that maximizes the joint probability of word sequence given character sequenc P(W IC).",
        "Since the maximization is carried out with fixed character sequence C, the word segment( only has to maximize the probability of the word sequence P(W).",
        "We approximate the joint probability P(W) by the word unigram model, which is the product c word unigram probabilities P(wi).",
        "We used the word unigram model because of its computational efficiency."
      ]
    },
    {
      "heading": "2.2 Unknown Word Model",
      "text": [
        "We defined a statistical word model to assign a reasonable word probability to an arbitrary substring in the input sentence.",
        "It is formally defined as the joint probability of the character sequence ci ...ck if wi is an unknown word.",
        "We decompose it into the product of word length probability and word spelling probability,",
        "where k is the length of the character sequence and <UNK> represents unknown word.",
        "We assume that word length probability .P(*) obeys a Poisson distribution whose parameter is the average word length A in the training corpus.",
        "This means that We regard word length as the interval between hidden word boundary markers, which are randomly placed with an average interval equal to the average word length.",
        "We approximate the spelling probability given word length P(ci cklk) by the product of character unigram probabilities regardless of word length.",
        "P(Ci Ck) = H P(ci) (5) Character unigram probabilities can be estimated from unsegmented texts.",
        "The average word length A can be computed, once the word frequencies in the texts are obtained.",
        "where iwil and C(w2) are the length and the frequency of word wi, respectively.",
        "Therefore, 1 only parameters we have to (re)estimate in the language model are the word frequencies.",
        "Figure 2 shows the actual and estimated word length distributions in the corpus we used the experiment.",
        "It shows two pairs of distributions: word length of all words (A = 1.6) and ti of words appearing only once (A = 4.8).",
        "The latter is expected to be dose to the distribution unknown words.",
        "Although the estimates by Poisson distribution are not so accurate, they enab us to make a robust and computationally efficient word model."
      ]
    },
    {
      "heading": "2.3 Viterbi Re-estimation",
      "text": [
        "We used the Viterbi-like dynamic programing procedure described in [Nagata, 1994] to get t most likely word segmentation.",
        "The generalized Viterbi algorithm starts from the beginning of t input sentence, and proceeds character by character.",
        "At each point in the sentence, it looks the combination of the best partial word segmentation hypothesis ending at the point and all we hypotheses starting at the point.",
        "We used the Viterbi re-estimation procedure to refine the word unigram model because its computational efficiency.",
        "It involves applying the above segmentation algorithm to a traini corpus, using a set of initial estimates of the word frequencies.",
        "The best analysis of the corpus taken to be the true analysis, the frequencies are re-estimated, and the algorithm is repeated um it converges."
      ]
    },
    {
      "heading": "3 Initial Word Frequency Estimation",
      "text": []
    },
    {
      "heading": "3.1 Longest Match",
      "text": [
        "We can get a set of initial estimates of the word frequencies by segmenting the training corpus using a heuristic (non-stochastic) dictionary-based word segmeuter.",
        "In both Japanese and Chinese, one of the most popular non-stochastic dictionary-based approaches is the longest match method 1.",
        "There are many variations of the longest match method, possibly augmented with further heuristics.",
        "We used a simple greedy algorithm described in [Sproat et al., 1996].",
        "It starts at the beginning of the sentence, finds the longest word starting at that point, and then repeats the process starting at the next character until the end of the sentence is reached.",
        "We chose the greedy algorithm because it is easy to implement and guaranteed to produce only one segmentation."
      ]
    },
    {
      "heading": "3.2 String Frequency",
      "text": [
        "[Sproat et al., 1996] also proposed another method to estimate a. set of initial word frequencies without segmenting the corpus.",
        "It derives the initial estimates from the frequencies in the corpus of the strings of character making up each word in the dictionary whether or not each string is actually an instance of the word in question.",
        "The total number of words in the corpus is derived simply by summing the string frequency of each word in the dictionary.",
        "Finding (and counting) all instances of a string W in a large text T can be efficiently accomplished by making a data structure known as a suffix array, which is basically a sorted list of all the suffixes of T Member and Myers, 1993]."
      ]
    },
    {
      "heading": "3.3 Longest Match String Frequency",
      "text": [
        "The estimates of word frequencies by the above string frequency method tend to inflate a lot especially in short words, because of double counts.",
        "We devised a slightly improved version which we term the \"longest match string frequency\" method.",
        "It counts the instances of string W1 in text T, unless the instance is also a substring of another string W2 in dictionary D. This method can be implemented by making two suffix arrays, Sr and SD for text T and dictionary D. By using ST, we first make list Lw of all occurrences of string W in the text.",
        "By using SD, we then look up all strings gr in the dictionary that include W as a substring, and make list kw of all their occurrences in the text by using ST.",
        "The longest match string frequency of word W in text T with respect to dictionary I) is obtained by counting the number of elements in the set difference Lw – Lw.",
        "For example, if the input sentence is \"fit*-A -31gT-A-M-,V,-\"Cra 6„ \" (talk about the Association of English and the Association of Linguistics) and the dictionary has VII* (linguistics), \"1 Ig (language), E.* (language study), 17'2; (association), and E. (talk).",
        "Figure 3 shows the difference of the three methods.",
        "The longest match string frequency (1sf) method considers all possible longest matches in the text, while the greedy longest match (Im) algorithm considers only one possibility.",
        "It is obvious that the longest match string frequency method remedies the problem that the string frequency (sf) method consistently and inappropriately favors short words.",
        "The problem of the longest match string frequency method is that if a word W1 is a substring of other word W2 and if WI always appears as a substring of W2 in the training text, just like Sig",
        "and W4t in the above example, the frequency estimate of W1 becomes 0.",
        "Although this rarely happens for a large training text, we have to smooth the word frequencies."
      ]
    },
    {
      "heading": "4 Initial Word Identification Method",
      "text": [
        "To a first approximation, a point in the text where character type changes is likely to be a word boundary.",
        "This is a. popular heuristics in Japanese word segmentation.",
        "To help readers understand the heuristics, we have to give a brief introduction to the Japanese writing system.",
        "In contemporary Japanese, there are at least five different types of characters other than punctuation marks: kanji, hiragana, katakana, Roman alphabet, and Arabic numeral.",
        "Kanji which means 'Chinese character' is used for both Chinese origin words and Japanese words semantically equivalent to Chinese characters.",
        "There are two syllabaries Mangana and katakana.",
        "The former is used primarily for grammatical function words, such as particles and inflectional endings, while the latter is used primarily to transliterate Western origin words.",
        "Roman alphabet is also used for Western origin words and acronyms.",
        "Arabic numeral is used for numbers.",
        "By using just this character type heuristics, a non-stochastic and non-dictionary word segmenter can be made.",
        "In fact, using the estimated word frequencies obtained by the heuristics results in poor segmentation accuracy 2.",
        "We found, however, that it is very effective to use the character type based word segrnenter as a lexical acquisition tool to augment the initial word list.",
        "The initial word identification procedure is as follows.",
        "First, we segment the training corpus by the character type based word segmenter, and make a list of words with frequencies.",
        "We then filter out hiragana strings because they are likely to be function words.",
        "We add the extracted word",
        "list to the original dictionary with associated frequencies, whether or not each string is actually a word.",
        "Although there are a lot of erroneous words in the augmented word list, most of them are filtered out by the re-estimation.",
        "This method works surprisingly well, as shown in the experiment."
      ]
    },
    {
      "heading": "5 Experiment",
      "text": []
    },
    {
      "heading": "5.1 Language Data",
      "text": [
        "We used the EDR Japanese Corpus Version 1.0 [EDR, 1995] to train and test the word segmenter.",
        "It is a corpus of 5.1 million words (208 thousand sentences).",
        "It contains a, variety of Japanese sentences taken from newspapers, magazines, dictionaries, encyclopedias, textbooks, etc.",
        "It has a variety of annotations including word segmentation, pronunciation, and part of speech tag.",
        "In this experiment, we randomly selected two sets of training sentences, each consisting of 100 thousand sentences.",
        "The first training set (training-0) is used to make initial word lists of various sixes.",
        "The second training set (training-1) is used to train various word segmenters.",
        "From the remaining of 8 thousand sentences, we randomly selected 100 test sentences to evaluate the accuracy of the word segmenters.",
        "Table 1 shows the number of sentences, words, and characters in the training and test sets 3.",
        "Based on the frequency in the manually segmented corpus training-0, we made 7 different initial word lists (D1-D200) whose frequency threshold were 1, 2, 5, 10, 50, 100, 200, respectively.",
        "The size of the resulting word lists and their out-of-vocabulary rate (00V rate) in the test sentences are shown in the second and third columns of Table 2.",
        "For example, D200 consists of words appearing more than 200 times in training-0.",
        "Although D200 consists of only 826 words, it covers 76.6% (00V rate 23.4%) of the test sentences.",
        "This is an example of the Zipf law."
      ]
    },
    {
      "heading": "5.2 Evaluation Measures",
      "text": [
        "Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996].",
        "Let the number of words in the manually segmented corpus be Std, the number of words in the output of the word segmenter be Sys, and the number of matched words be M. Recall is defined as M/Std, and precision is defined as M/Sys.",
        "Since it is inconvenient to use both recall and precision an the we also use the F-measure to indicate the overall performance.",
        "The F-measure was originally developed by the information Training-1 was used as plain texts that are taken from the same information source as training-O.",
        "Its word segmentation information was never used to ensure that training was unsupervised.",
        "retrieval community.",
        "It is calculated by",
        "where P is precision, R is recall, and t3 is the relative importance given to recall over precision.",
        "We set = 1.0 throughout this experiment.",
        "That is, we put equal importance on recall and precision."
      ]
    },
    {
      "heading": "5.3 Comparison of Various Word Frequency Estimation Methods",
      "text": [
        "We first compared the three frequency estimation methods described in the previous section: greedy longest match method (lm), string frequency method (sf), and longest match string frequency method (1sf).",
        "The sixth, seventh, and eighth columns of Table 2 show the word segmentation accuracy (F-measure) of each estimation method using different sets of initial words (D1-D200).",
        "For comparison, the word segmentation accuracy using real word frequency (wf), computed from the manual segmentation of training-1 (not training-0!",
        "), is shown in the fifth column of Table 2.",
        "The results are also diagramed in Figure 4.",
        "First of all, word segmentation accuracy using real word frequencies (wf) significantly (540%) outperformed that of any frequency estimation methods.",
        "Among word frequency estimates, the longest match string frequency method (1sf) consistently outperformed the string frequency method (sf).",
        "The (longest match) string frequency method (sf and lsf) outperformed the greedy longest match method (1m) by about 2-5% when the initial word list size was under 20K (from D5 to D200).",
        "In all estimation methods, word segmentation accuracies of D1 are worse than D2, while D1 is slightly better than D2 in using real word frequencies."
      ]
    },
    {
      "heading": "5.4 Effect of Augmenting Initial Dictionary",
      "text": [
        "We then compared the three frequency estimation methods (Jan, sf, and Isf) with the initial dictionary augmented by the character type based word identification method (ct) described in the previous section.",
        "The word identification method collected a list of 108975 word hypotheses from training-1.",
        "The ninth, tenth, and eleventh columns of Table 2 show the word segmentation accuracies.",
        "Augmenting the dictionary yields a significant improvement in word segmentation accuracy.",
        "Although the difference between the underlying word frequency estimation methods is small, the longest match string frequency method generally performs best.",
        "Surprisingly, the best word segmentation accuracy is achieved when the very small initial word list of 1719 words (D100) is augmented",
        "by the heuristic word identification method, where the recall and precision an 86.3% and 82.5% (F-measure 0.843)."
      ]
    },
    {
      "heading": "5.5 Effect of Re-estimation",
      "text": [
        "To investigate the effect of re-estimation, we tested the combination of three initial word lists: Ell, 1)2, 1)100, and two initial word frequency estimation methods: string frequency method (sf) and longest match string frequency method augmented with the word identification method (1sf+ct).",
        "We applied the Viterbi re-estimation procedure three times.",
        "It seems further re-estimation brings no significant change.",
        "At each stage of re-estimation, we measured the word segmentation accuracy on the test sentences (not the training texts!).",
        "Figure 5 shows the word segmentation accuracy, the number of word tokens in the training texts, and the number of word types in the dictionary at each stage of re-estimation.",
        "In general, re-estimation has little impact on word segmentation accuracy.",
        "It gradually improves the accuracy when the initial word list is relatively large (Dl and 1)2), while it worsen the accuracy a little when the initial word list is relatively small (1)100).",
        "This might correspond with the results on unsupervised learning performed by an English part of speech tagger.",
        "Although tKupiec, 19921 presented a very sophisticated method of unsupervised learning, [Elworthy, 1994] reported that re-estimation is not always helpful.",
        "We think, however, our results are because we used a word unigram model; it is too early to conclude that re-estimation is useless for word segmentation, as discussed in the next section.",
        "It seems the virtue of re-estimation lies in its ability to adjust word frequencies and removing unreliable word hypotheses that are added by heuristic word identification.",
        "The abrupt drop in the number of word tokens at the first re-estimation step indicates that the inflated initial estimates of",
        "estimation stage word frequencies are adjusted to more reasonable values.",
        "The drop in the number of word types indicates the removal of infrequent words and unreliable word hypotheses from the dictionary."
      ]
    },
    {
      "heading": "6 Discussion",
      "text": []
    },
    {
      "heading": "6.1 The Nature of the Word Unigram Model",
      "text": [
        "First, we will clarify the nature of the word unigram model.",
        "ROughly speaking, word unigram based word segmenters maximize the product of the word frequencies under the fewest word principle which subsumes the longest match principle.",
        "Iftwo word segmentation hypotheses differs in the number of words, the one with fewer words is almost always selected.",
        "For example, the input string is c1c2 and the dictionary includes three words 01C2, c, c2.",
        "To prefer segmentation hypothesis ci1c2 over c102, the following relation must",
        "where CO represents the word frequency and N is the number of word tolmns in the training text.",
        "Suppose N is one million.",
        "Even if C(cic2) = 1, c1c2 is preferred unless Cl and c2 are highly frequent, say C(ci) C(c2) > 1000.",
        "It is obvious that the segmentation with fewer words are preferred.",
        "If two word segmentation hypotheses have the same number of words, the one with larger product of word frequencies is selected.",
        "For example, the input string is c1c2c3 and the dictionary includes four words c1c2, ca, c1, c2c3.",
        "To prefer segmentation hypothesis c1c2Ic3 over c11c2c3, the following relation must hold."
      ]
    },
    {
      "heading": "6.2 Classification of Segmentation Errors",
      "text": [
        "There are three major types of segmentation errors.",
        "The first type is not an error but the ambiguity resulting from inconsistent manual segmentation, or the intrinsic indeterminacy of Japanese word segmentation.",
        "For example, in the manually segmented corpus, we found the string PHAA#iitt (foreign laborer) is identified as one word in some places while in others it is divided into two words ADM (foreigner) and %'PNt* (laborer).",
        "However, the word unigram based segmenter consistently identifies it as a single word.",
        "We assume 3-5 % of the segmentation \"errors\" belong to this type.",
        "The second type is breakdown of unknown words.",
        "For example, the word 3D10 (funny) is segmented into two word hypotheses V (rare) and (strange).",
        "This is because #0. is included in the dictionary.",
        "When a substring of an unknown word coincides with other word in the dictionary, it is very likely to be broken down into the dictionary word and the remaining substring.",
        "This is a major flaw of our word model using character unigram.",
        "It assigns too little probability to longer word hypotheses, especially more than three characters.",
        "The third type is erroneous longest match.",
        "This happens frequently at the sequence of grammatical function words written in hiragana.",
        "For example, the phrase (gather) I D (INFL) (and) I (come) I ft (past-AUXV), which means \"came and gathered\", is segmented into Xt D -C. (TOPIC) t It (north), because the number of words is fewer.",
        "The larger the initial word list is, the more often a hiragana word happens to coincide with a sequence of other hiragana words, because the number of character types in hiragana is small (< 100).",
        "This is the major reason why word segmentation accuracy levels off or decreases at a certain point, as the size of the initial word list increases."
      ]
    },
    {
      "heading": "6.3 Classification of the Effects of Re-estimation",
      "text": [
        "There are two types of major changes in segmentation with re-estimation: word boundary adjustment and subdivision.",
        "The former moves a word boundary keeping the number of words unchanged.",
        "The latter break down a word into two or more words.",
        "Re-estimation usually improves a sequence of grammatical function words written in hiragana at the sentence final predicate phrase if the initial segmentation and the correct segmentation have the same number of words.",
        "For example, the incorrect initial segmentation Ztt,* (take away) I (INFL + passive-AUXV) I tzt (ball) I *If-: (not yet) is correctly adjusted to ii (take away) I 641 (INFL + passive-AUXV) Ii (past-AUXV) iE (still) I re (COPULA), which means \"still be taken away\".",
        "Re-estimation subdivides an erroneous longest match if the frequencies of the shorter words are significantly large.",
        "For example, the incorrect initial segmentation I* (restrain) I itleN (sea",
        "Since the denominator N is cancelled, it is obvious that the segmentation with larger product of frequencies is preferred.",
        "bream) is correctly subdivided into (restrain) 1 (want-AUXV) (INFL), which meanA \"want to restrain\".",
        "One of the most frequent undesirable effects of re-estimation is subdividing an infrequent word into highly frequent words, or a frequent word and an unknown word.",
        "For example, the correct infrequent word fitS (ambassador) is subdivided into two frequent words, 4€ (use-ROOT) and fa (node).",
        "As we said before, one of the major virtues of re-estimation is its ability to remove inappropriate word hypotheses generated by the initial word identification procedure.",
        "For example, from the phrase Y3 (Soviet Union) I St (made-SUFFIX) I ra* (tank), which means \"Soviet Union-made tank\", the initial word identifier extracts two word hypotheses .s.1 and Zara*, where the former is written in katakana and the latter is written in kanji.",
        "If YA and Si is in the dictionary, the two erroneous word hypotheses 21 and 4320*aze removed and the correct word fit* is added to the dictionary after re-estimation."
      ]
    },
    {
      "heading": "7 Conclusion and Future Work",
      "text": [
        "We have presented a self-organized method that builds a stochastic Japanese word segmenter from a small word list and a large unsegmented text.",
        "We found that it is very effective to augment the initial word list with automatically extracted words using character type heuristics.",
        "Re-estimation helps in adjusting word frequencies and removing inappropriate word hypotheses, although it has little impact on word segmentation accuracy if the word unigram model is used.",
        "The major drawbacks of the current word segmenter is its breakdown of unknown words whose substrings coincide with other words in the dictionary, and the erroneous longest match at the sequence of functional words written in hiragana.",
        "The first drawback results from the character unigram based word model that prefers short words, while the second drawback results from the nature of the word unigram model which prefers fewest words segmentation.",
        "One may argue that we could use the word bigram model.",
        "However, we don't know how we can estimate the initial word bigram frequencies from scratch.",
        "One may also argue that we could use the character bigram in the word model.",
        "However, the character bigram for the word model must be computed from segmented texts.",
        "Both of these suggest that we need a word segraenter to build a more sophisticated word segmenter.",
        "Therefore, as a next step of our research, we are thinking of using the proposed unigram based word segmenter to obtain the initial estimates of the word bigrams and the word-based character bigrams which will then be refined by a re-estimation procedure."
      ]
    }
  ]
}
