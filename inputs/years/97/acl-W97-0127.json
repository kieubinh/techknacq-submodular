{
  "info": {
    "authors": [
      "Jun Gao",
      "XiXian Chen"
    ],
    "book": "Workshop on Very Large Corpora",
    "id": "acl-W97-0127",
    "title": "Probabilistic Word Classification Based on Context-Sensitive Binary Tree Method",
    "url": "https://aclweb.org/anthology/W97-0127",
    "year": 1997
  },
  "references": [
    "acl-A92-1018",
    "acl-E95-1020",
    "acl-J88-1003",
    "acl-J92-4003",
    "acl-J93-1001",
    "acl-J94-2001",
    "acl-P91-1034",
    "acl-P93-1024",
    "acl-W93-0305"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Corpus-based statistical-oriented Chinese word classification can be regarded as a fundamental step for automatic or non-automatic, monolingual natural processing system.",
        "Word classification can solve the problems of data sparseness and have far fewer parameters.",
        "So far, much rolat:vP work about word classification has been done.",
        "All the work Ls based on some similarity metrics.",
        "We use average mutual informaLion as global similarity metric to do classification.",
        "The clustering process is top-down splitting and the binary tree is growing with splitting.",
        "In natural language, the effect of left neighbors and right neighbors of a word are asymmetric.",
        "To utilize this directional information, we induce the left-right binary and right-left binary tree to represent this property.",
        "The probability is also introduced in our algorithm to merge the resulting classes from left-right and right-left binary tree.",
        "Also, we use the resulting classes to do experiments on word class-based language model.",
        "Some classes results and perplexity of word class-based language model are presented."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Word classification play an important role in computational Many tasks in computational linguistics, whether they use statistical or symbolic methods, reduce the complexity of the problPm by dealing with classes of words rather than individual words.",
        "We know that some words share similar sorts of linguistic properties, thus they should belong to the same class.",
        "Some words have several functions, thus they could belong to more than one class.",
        "The questions are: What attributes distinguish one word from another?",
        "How should we group similar words together so that the partition of word",
        "spaces is most likely to reflect the linguistic properties of language?",
        "What meaningful label or name should be given to each word group?",
        "These questions constitute the problem of finding a word classification.",
        "At present, no method can find the optimal word classification.",
        "However, researchers have been trying hard to find suboptimal strategies which lead to useful classification.",
        "From practical point of view, word classification addresses questions of data sparseness and generalization in statistical language models.",
        "Specially, it can be used as an alternative to grammatical part-of-speech tagging (Bri11,1993; Cutting, Kupiec, Pederson and Sibun, 1992; Chang and Chen 1993a; Chang and Chen 1993b; Lee and Chang Chien, 1992; Kupiec,1992; Lee, 1993; Merialdo,1994; Pop,1996; Peng, 1993; Zhou, 1995; Schütze, 1995;) on statistical language modeling(Huang, Alleva, Hwang, Lee and Rosenfeld 1993; Rosefield,1994;), because Chinese language models using part-of-speech information have had only a very limited success(e.g. Chang, 1992; Lee, Dung, Lai, and Chang Chien, 1993;).",
        "The reason why there are so many of the difficulties in Chine Jd part-of-speech tagging are described by Chang and Chen (1995) and Zhao (1995).",
        "Much relative work on word classification has been done.",
        "The work is based on some similarity metrics.",
        "( Bahl, Brown, DeSouza and Mercer, 1989; Brown, Pietra, deSouza and Mercer,1992; Chang,1995; DeRose,1988; Garside, 1987; Hughes, 1994; Jardino,1993; Jelinek, Mercer, and Roukos, 1990b; Wu, Wang, Yu and Wang, 1995; Magerman, 1994; McMahon, 1994; McMahon, 1995; Pereira, 1992; Resnik, 1992; Zhao, 1995;) Brill (1993) and Pop (1996) present a transformation-based tagging.",
        "Before a part-of-speech tagger can be built, the word classifications are performed to help us choose a set of part-of-speech.",
        "They use the sum of two relative entropies obtained from neighboring words as the similarity metric to compare two words.",
        "Schütze (1995) shows a lOng-distance left and right context of a word as left vector and right vector and the dimensions of each vector are 50.",
        "He uses Cosine as metric to measure the similarity between two words.",
        "To solve the sparseness of the data, he applies a singular value decomposition.",
        "Comparing with Brill,E.",
        "'s method, Schütze,H.",
        "takes 50 neighbors into account for each word.",
        "Chang and Chen (1995) proposed a simulated annealing method, the same as Jardino and Adda 's (1993).",
        "The perplexity, which is the inverse of the probability over the whole text, is measured.",
        "The new value of the perplexity and a control parameter Cp(Metropolis algorithm) will decide whether a new classification ( obtained by moving only one word from its class to another, both word and class being randomly chosen) will replace a previous one.",
        "Compared to the two methods described above, this method attempts to optimize the clustering using perplexity as a global measure.",
        "Pereira, Tishby and Lee (1993) investigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves.",
        "More",
        "specifically, they model senses as probabilistic concepts or clusters C with corresponding cluster membership probabilities P(Ciw) for each word W. That is, while most other class-based modeling techniques for natural language rely on \"hard\" Boolean classes, Pereira, F. et al.",
        "(1993) propose a method for \"soft\" class clustering.",
        "He suggests a deterministic annealing procedure for clustering.",
        "But as stated in their paper, they only considers the special case of classifying nouns according to the distribution as direct objects of verbs.",
        "To address Lhe problems and utilize the advantages of the methods presented above, we put forward a new algorithm to automatically classify the words."
      ]
    },
    {
      "heading": "2. Chinese Word Classification Method",
      "text": []
    },
    {
      "heading": "2.1 Basic Idea",
      "text": [
        "We adopt the top-down binary splitting technique to the all words using average mutual information as similarity metric like McMahon (1995).",
        "This method has its merits: Top-down technique can represent the hierarchy information explicitly; the position of the word in class-space can be obtained without reference to the positions of other words, while the bottom-up technique treats every word in the vocabulary as one class and merges two classes among this vocabulary according to certain similarity metric, then repeats the merging process until the demanded number of classes is obtained."
      ]
    },
    {
      "heading": "2.1.1 Theoretical Basis",
      "text": [
        "Brown g-t al.",
        "(1992) have shown that any classification system whose average class mutual information is maximized will lead to class-based language models of lower perplexities.",
        "The concept of mutual information, taken from information theory, was proposed as a measure of word association (Church 1990; Jelinek et al.",
        "1990,1992; Dagan, 1995;).",
        "It reflects the strength of relationship between words by comparing their actual co-occurrence probability with the probability that would be expected by chance.",
        "The mutual information of two events x and y is defined as follows:",
        "where P(x1)and PUOare the probabilities of the events, and nr1,x0is the probability of the joint event.",
        "If there is a strong association between xiand x, then P(xl,x,)»P(xl)P(x,) as a result /(x1,x2)»0.",
        "If there is a weak association between xiand x, then P(x1,x2)::-4P(x1)P(x2) and/(x, ,x2) 0.",
        "If P(x,,x,)«P(xl)P(x,) then /(x, x2) «0 .",
        "Owing to the unreliability of measuring negative mutual information values between content words in corpora that are not extremely large, we have considered that any negative value to be 0.",
        "We also set /(xi,x,) to 0 if .Axi,x,)= 0.",
        "The average mutual information /a between events xi,x„...,xm is defined similarly.",
        "Rather than estimate the relationship between words, we measure the mutual information between classes.",
        "Let C7„ C71 be the classes, ij=0,1.2....,N; IV denotes the number of classes.",
        "Then average mutual information between classes is"
      ]
    },
    {
      "heading": "2.1.2 The Basic Algorithm",
      "text": [
        "The, completing process is described as follows: We split the vocabulary' into a binary tree.",
        "We only consider one dimension neighbor.",
        "#1:Take the whole word in vocabulary as one class and take this level - in the binary tree as 0.",
        "That is Level=0, Branch=0, Class(Level,Branch)=Vocabulary Set.",
        "Then, Level=L evel+1.",
        "#2:Class(Level,Branch)=Class(Level-1,Branch/2).",
        "Old /0=0.",
        "Class(Level,Branch+1)=empty.Select a word 1,t, E Class(Level,Branch) #3:Move this word to Class(Level,Branch+1).",
        "Calculate the 1000 44:Move this word back to Class(Level,Branch).",
        "If (all words in Class(Level,Branch) have been selected), then goto #5 else select another unselected word 14,,e Class(Level,Branch) to Class(Level,Branch+1), goto #3 #5:Move the word having Maximum(4) from Class(Level,Branch) to Class(Level,Branch+1):",
        "If (Level< predefined classes number) go•o #2; else goto end.",
        "From the algorithm descrbed above, we can conclude that the computation time is to be order C(1g73) for tree height h and vocabulary size V to move a word from one class to another.",
        "If the height of the binary tree is h, the number of all possible classes will be 2h.",
        "During the splitting process, especially at the bottom of the binary tree, some classes may be empty because the classes higher than them can not be splitted further more."
      ]
    },
    {
      "heading": "2.2 Improvement to the Basic Algorithm 2.2.1 Length of Neighbor Dimensions",
      "text": [
        "As mentioned in Introduction, Brill (1993), and McMahon t1995) only consider one dimension neighbor, while Schütze (1995) consider 50 dimensions neighbors.",
        "How long the dimensions neighbors should be indeed?",
        "For long-distance bigrams mentioned in Huang, et al.",
        "(1993) and Rosefield (1994), - training-set perplexity is low for the conventional bigram(d=1), and it increases significantly as they move though d=23,4 and 5.",
        "For•d=6,...10, training-set perplexity remained at about the same level.",
        "Thus, Huang,X.-D.et al.",
        "(1993) conclude that some information indeed exists in the more distant past but it is spread thinly across the entire history.",
        "We do the test on Chinese in the same way.",
        "And similar results are obtained.",
        "So, 50 is too long for dimensions and the search in searching space is computationally prohibitive, and 1 is so small for dimensions that much information will be lost.",
        "In this paper, we let d=2.",
        "where Armud is the total times of words which are in the vocabulary occurring in the corpus.",
        "d is the calculating distance considered in the corpus.",
        "Alw is the total times of word W occurring in the corpus AT,,,„„ is the total times of words couple wiw., occurring in the corpus within the distance d.",
        "In the works of Brill (1993), Brill,E.",
        "et al.",
        "use the sum of two relative entropies as the similarity metric to compare two words.",
        "They treat the word's neighbors equally without considering the possible different influences of left neighbor and right neighbor to the word.",
        "But in natural language, the effect from left neighbor and right neighbor is asymmetric, that is, the effect is directional.",
        "For example, In \"R*LT.W( \"I ate an apple\"), the Chinese word and \"-.\"(\"apple\") has different functions in this sentence.",
        "We can not say that \"T:AIILIt.\"(\"An apple ate I\").",
        "So, it is necessary to induce a similarity metric which reflects this directional property.",
        "Applying this idea in our algorithm, we create two binary trees to represent different directions.",
        "One binary tree is produced to represent the word relation direction from the left to the right, and",
        "the other is to represent the word relation direction from the right to the left.",
        "The former is from the left to the right is the default circumstance mentioned in 2.1.2.",
        "The similar idea about directional property is presented by pagan, et al. (1995) also.",
        "Dagan, et al.",
        "(1995) defines a similarity metric of two words that can reflect the directional property according to mutual information to determine the degree of similarity between two words.",
        "But the metric does not have transitivity.",
        "The intransitivity of the metric determines this metric can not be used in clustering words to equivalence classes.",
        "To reflect the different influence of left neighbor and right neighbor of the word, we introduce the probability for each word W to every class.",
        "That is, for the classes produced by the binary tree which represent the word relation direction from the left to the right, we distribute probability P0.7,1w)for each word w corresponding every class C„ the probability .F“C\",114) reflect the degree the word w belongs to class C. For the classes the binary tree which represent the word relation direction from the right to the left produce, Pr/(.7,1w) is calculated likewise.",
        "Mutual information can be explained as: the ability of dispelling the uncertainty of information source.",
        "And entropy of information is defined as the uncertainty of information source.",
        "So, the probability word w which belongs to class C; can be presented as follows:",
        "where 1(07) is the mutual information between the class C, and the other class C, which is in the same binary branch with C7,.",
        "lir(C,) is the entropy of class (7,.",
        "So, 1-P„ denotes the probability that word w didn't belong to class .",
        "That is, in the binary tree, 1-P, denotes the probability of the other branch class corresponding to C7,.",
        "Because the average mutual information is little, it is possible that F. is less than 1-17c,.",
        "To avoid distributing the less probability to the word assigned to this class than the probability to the word not assigned to this class, we distribute the probability 1 to the word assigned to the class.",
        "Thus, for each class in the certain level of the binary tree, we multiple the probabilities either 1 or 1-1c to its original probabilities, in which (7, is the other branch class opposite to the class the word not belonging to.",
        "The description on above is only word w belong to a certain class in certain level without consider the affection from its upper levels.",
        "To obtain the real probability of word w belonging to certain class, all belonging probabilities of its ancestors should be multiplied together.",
        "The distribution of the probability is not optimal, but it reflects the degree a word belonging to a class.",
        "It should be noted that",
        "EP(C,Iw) must be normalized both for the left-right and the right-left results.",
        "And the normalized results of the left-right and the right-left binary tree also must be normalized together."
      ]
    },
    {
      "heading": "2.2.3 ProbabilisticBottom-upClassesMerging",
      "text": [
        "Since there is directional property between words, the transitivity will not be satisfied between different directions.",
        "That is, if we didn't introduce the probability IVCjw) and Pri(Ci114), we would not merge the classes because there is no transitivity between the class in which word relation is from the left to the right and the class in which word relation is from the right to the left.",
        "For example, \"Itfrin(\"we\") and \"Wr(\"you\") are contained in one class derived by the left-right binary tree, and other two words I`MI\"(\"you\") and '\"(\"apple\") belong to another class derived from right-left binary tree.",
        "This do not mean that the words \"fttI\"(\"we\") and \"4*EIR\"(\"apple\") belong to one class.",
        "But when we put forward the probability, unlike the intransitivity of similarity metric presented by Dagan, et al.",
        "(1995), the classes generated by two binary trees can be merged because the probabilities can make the \"hard\" intransitivity \"soft\".",
        "Although this top-down splitting method has the advantage we mentioned above, it has its obvious shortcomings.",
        "Magerman, (1994) describes these shortcomings in detail.",
        "Since the splitting procedure is restricted to be trees, as opposed to arbitrary directed graphs, there is no mechanism for merging two or more nodes in the tree growing process.",
        "That is to say, if we distribute the words to the wrong classes from global sense, we will not be able to any longer move it back.",
        "So, it is difficult to merge the classes obtained by left-right binary tree and right-left binary tree during the process of growing tree.",
        "To solve this problem, we adopt the bottom-up merging method to the resulting classes.",
        "A number of different similarity measures can be used.",
        "We choose to use relative entropy , also known as the Kullback-Leibler distance(Pereira, et al.1993; Bri11,1993;).",
        "Rather than merge two words, we merge the two classes which belong to the resulting classes generated by left-right' binary tree and right-left binary tree respectively, and select the merged class which can lead to maximum value of similarity metiic.",
        "This procedure can be done recursively until the demanded number of classes is reached.",
        "Let P and Q be the probability distribution.",
        "The Kullback-Leibler distance from P to Q is defined as:",
        "The divergence of P and Q is then defined as:",
        "For two words w and w1, let Pd(wovi) be the probability of word w occurring to the left of wi within the distance d. The probability,",
        "Pd(wolv), Pe,(14!,m%0 and P4044,10, are defined likewise.",
        "And let PVC,114,2), PIACilm)and IVC/114/2) be the probabilities of words 14/1 and Iv, contained in classes G, and C; in the left-right and right-left trees respectively.",
        "Then, the Kullback-Leibler distance between words 14,1 and iv, in the left-right tree is:",
        "The divergence of words 14/1 and 14,2 in the left-right tree is:",
        "Similarly, the Kullback-Leibler distance between words Iv/ and 14/, in the right-left tree is:",
        "where V is the vocabulary.",
        "We can then define the similarity of w, and 14), as:",
        "The computation cost of this simiarity is not high, for the components of equation (10) have been obtained during the early computation.",
        "The number of all possible classes is 2\".",
        "During the splitting process, especially at the bottom of the binary tree, it may be empty for some classes because the classes at higher level than it can not be splitted further more according to the rule of maximum average mutual information.",
        "The-number of the resulting classes can not be controlled accurately.",
        "So, we can define the number of the demanding classes in advance.",
        "As long as the number of the resulting classes is less than the predefined number, the splitting process will be continued.",
        "When the number of the resulting classes is larger than the predefined number, we use the merging technique presented above to reduce the number until it is equal to the predefined number.",
        "The procedure can be described as follows: After we have merged two classes taken from the left-right and the right-left trees respectively, we use this merged class to replace two original classes respectively.",
        "Then we repeat this process until certain step is reached.",
        "In this paper, we define the number of steps as equal to the larger number of the classes between two trees' resulting classes.",
        "Finally, we merge all resulting classes until the predefined number is reached.",
        "This merging process guaranteed the probabililty to be nonzero whenever the word distributions are.",
        "This is a useful advantage compared with agglomerative clustering techniques that need to compare individual objects being considered for grouping."
      ]
    },
    {
      "heading": "3. Experimental Results and Discussion",
      "text": []
    },
    {
      "heading": "3.1 Word Classification Results",
      "text": [
        "We use Pentium 586/133MHz, 32M memory to calculate.",
        "The OS is Windows NT 4.0.",
        "And Visual C++ 4.0- is our programming language.",
        "We use the electric news corpus named \"Chinese One hundred kinds of newspapers---1994\".",
        "The total size of it is 780 million bytes.",
        "It is not feasible to do classification experiments on this original corpus.",
        "So, we extract a part of it which contain the news published in April from the original news texts.",
        "To be convenient, the sentence boundary markers, !",
        ".",
        ".",
        "; ,} are replaced by only two sentence boundary markers: \"!",
        "\" and \".",
        "\" which denote the beginning and end of the sentence or word phrase respectively.",
        "The texts are segmented by Variable-distance algorithm( Gao, J. and Chen, X.X.",
        "(1996)] We select four subcorpora which contains 10323, 17451, 25130 and 44326 Chinese words.",
        "The vocabulary contains 2103, 3577, 4606 and 6472 words correspondingly.",
        "The results of the classification without introducing probabilities can be summarized in Table I.",
        "The computation of merging process is only equal to the splitting calculation in one level in the tree.",
        "From table I, we can find surprisely that the computation time for right-left is much shorter than the time for left-right.",
        "But this is reasonable.",
        "In the process of left-right, the left branch contains more words than the right branch.",
        "To move each word from the left branch to the right branch, we need to match this word throughout the corpus.",
        "But when we do the process of right-left, the left branch has less words than the right.",
        "We only need to match the small number of words in the corpus.",
        "From this, we can know that the preprocessed procedure costs much time.",
        "The number of empty classes is increasing with the tree grows.",
        "Table II shows the number of empty classes in different levels in the left-right tree when we process the subcorpora containing 10323 words.",
        "Although our method is to calculate distributional classification, it still demonstrates that it has powerful part-of-speech functions.",
        "But some of classes present no obvious part-of-speech category.",
        "Most of them contain only very small number of words.",
        "This may caused by the predefined classification number.",
        "Thus, excessive or insufficient classification may be encountered.",
        "And another shortcoming is that a small number of words in almost every resulting class doesn't belong to the part-of-speech categories which most of words in that class belong to."
      ]
    },
    {
      "heading": "3.2UseWordClassificationResultsinStatisticalLanguageModeling",
      "text": [
        "Word class-based language model is more competitive than word-based language model.",
        "It has far fewer parameters, thus making better use of training data to solve the problem of data sparseness.",
        "We compare word class-based N-gram language model with typical N-gram language model using perplexity.",
        "Perplexity (Jelinek, 1990a; McCandless,1994;) is an information-theoretic measure for evaluating how well a statistical language model predicts a particular test set.",
        "It is an excellent metric for comparing two language models because it is entirely independent of how each language model functions internally, and also because it is very simple to compute.",
        "For a given vocabulary size, a language model with lower perplexity is modeling language more accurately, which will generally correlate with lower error rates during speech recognition.",
        "Perplexity is derived from the average log probability that the language model assigns to each word in the test set:",
        "where w1,...041. are all words of the test set constructed by listing the sentences of the test set end to end, separated by a sentence",
        "boundary marker.",
        "The perplexity is then 2'1, k may be interpreted as the average number of bits of information needed to compress each word in the test set given that the language model is providing us with information.",
        "We compare the perplexity result of the N-gram language model with class-based N-gram language model.",
        "The perplexities PP of N-gram for word and class are:",
        "where 14/, denotes the ith word in the corpus and C7040 denotes the class that vv, is assigned to.",
        "Pir is the number of words in the corpus.",
        "P(C7(wi)jqw,..1:0can be estimated by:",
        "The perplexities PP based on different N-gram for word and class are presented in table III.",
        "Note that we present \"hard\" classification and \"soft\" classification results in word class based language model respectively.",
        "For probabilistic classification, we define the word as belonging to certain class in which this word has the largest probability.",
        "The training corpus contains more than 12,000 Chinese words.",
        "And the vocabulary has 1034 Chinese words which are most frequent.",
        "We use four subcorpora mentioned above as test sets.",
        "An arbitrary nonzero probability is given to all Chinese words and",
        "word W which are not in the vocabulary.",
        "Pir is the number of words in the training corpus.",
        "From table III, we can know that perplexity of \"hard\" class-based bigram is 28.7% lower than the word-based bigram, while perplexity of the \"soft\" class-based bigram is much lower than the \"hard\" class-based bigram, perplexity reduction is about 43% compared with \"hard\" class-based bigram.",
        "where"
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "In this paper we show a new method for Chinese words classification.",
        "But it can be applied in multiple language too.",
        "It integrates top-down and bottom-up idea in word classification.",
        "Thus top-down splitting techniques can learn from bottom-up idea's strong points to offset its obvious weakness and keep the advantage of itself.",
        "Especially, unlike other classification methods, this method takes the context-sensitive information which most classification methods do not consider into account and make it reflect the properties of natural language more clearly.",
        "Moreover, the probabilities are assigned to the words to demonstrate how well a word belongs to classes.",
        "This property is very useful in word class-based language modeling used in speech recognition, for it allows the system to have several powerful candidates to be matched during recognition.",
        "It, however, is important to consider the limitations of the method.",
        "The computational cost is very high.",
        "The algorithm's complexity is Cubic when we move one word from one class to another.",
        "Also, the probabilities the word assign to each class is not global optimal.",
        "It reflects the degree of a word belonging to classes approximately.",
        "And excessive or insufficient classification may occur because the class number is fixed artificially."
      ]
    },
    {
      "heading": "Reference",
      "text": []
    }
  ]
}
