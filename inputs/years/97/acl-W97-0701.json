{
  "info": {
    "authors": [
      "Karen Sparck Jones"
    ],
    "book": "Workshop on Intelligent Scalable Text Summarization",
    "id": "acl-W97-0701",
    "title": "Summarising: Where Are We Now? Where Should We Go?",
    "url": "https://aclweb.org/anthology/W97-0701",
    "year": 1997
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Summarising covers a range from text extraction to content condensation Its essential features are picking important concepts from, and reducing, source text or information, to deliver summary information or text General strategies for doing this are clearly preferable to application-specific ones So far, we have found that statistically-based sentence extraction and concatenation does not produce effective summaries But we have not yet found general methods of content analysis and condensation We can only identify key source content and present it in summary with heavy domain and goal guidance The most pressing need is to develop 'sufficient to the day' techniques that do more than surface sentence extraction without depending, MUC-hke, on prior specifications for sought content These needed intermediate techniques include passage extraction and linking, deep phrase selection and ordering, entity identification and relating Such strategies benefit from, or require, shallow text analysis and do or can exploit statistical data They may be enhanced by modern display resources They are apphcable to individual source texts or to data sets as wholes Most importantly, we can tackle this level of summarising because current robust parsing technology may succeed, given source redundancy, in getting enough of value from sources to help users, and because current text production methods can deliver usable summary texts We should push this line hard, seeking to minimise apphcation-specific domain knowledge, to take advantage of discourse structure, and to address summary function for the user"
      ]
    }
  ]
}
