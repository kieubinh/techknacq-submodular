{
  "info": {
    "authors": [
      "Regina Barzilay",
      "Michael Elhadad"
    ],
    "book": "Workshop on Intelligent Scalable Text Summarization",
    "id": "acl-W97-0703",
    "title": "Using Lexical Chains for Text Summarization",
    "url": "https://aclweb.org/anthology/W97-0703",
    "year": 1997
  },
  "references": [
    "acl-C94-1056",
    "acl-J91-1002",
    "acl-P94-1002"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We investigate one technique to produce a summary of an original text without requiring its full semantic interpretation, but instead relying on a model of the topic progression in the text derived from lexical chains We present a new algorithm to compute lexical chains in a text, merging several robust knowledge sources the WordNet thesaurus, a part-of-speech tagger and shallow parser for the identification of nominal groups, and a segmentation algorithm derived from (Hearst, 1994) Summarization proceeds in three steps the original text is first segmented, lexical chains are constructed, strong chains are identified and significant sentences are extracted from the text We present in this paper empirical results on the identification of strong chains and of significant sentences Introduction Summarization is the process of condensing a source text into a. shorter version preserving its information content It can serve several goals – from survey analysis of a scientific field to quick indicative notes on the general topic of a text Producing a quality informative summary of an arbitrary text remains a challenge which requires full understanding of the text Indicative summaries, which can be used to quickly decide whether a text Is worth reading, are naturally easier to produce In this paper we investigate a method for the production of such indicative summaries from arbitrary text , (Jones, 1993) describes summarization as a two-step process (1) Building from the source text a source representation, (2) Summary generation – forming summary representation from the source representation built in the first step and synthesizing the output summary text Within this framework, the relevant question what information has to be included in the source representation in order to create a summary There are three types of source text information linguistic, domain and commumcative Each of these text aspects can be chosen as a basis for source representation Summaries can be built on a deep semantic analysis of the source text For example, (McKeown and Batley, 1996) investigate ways to produce a coherent summary of several texts describing the same event, when a detailed semantic representation of the source texts is available (in their case, they use MUC-style systems to interpret the source texts) Alternatively, early summarization systems (Luhn, 1968) used only linguistic source information The intuition was that the most frequent words represent the important concepts of the text In this approach the source representation was the frequency table of text words This representation abstracts the text into the union of its words without considering any connection among them In contrast to these two extreme positions (using as a source representation a full semantic representation of the text or reducing it to a simple frequency table), we deal in this paper with the issue of producing a summary from an arbitrary text without requiring its full understanding, but using widely available knowledge sources Our main goal is therefore to find a middle ground for source representation, rich enough to build quality indicative summaries, but easy enough to extract from the source text to work on arbitrary text Oversimplification can harm the quality of the source representation As a trivial illustration, consider the following two sequences",
        "so does 'machine\" But sequence 1 is about the ma-clime, and sequence 2 is about the 'doctor\" This example indicates that if the source representation does not supply information about semantically related terms, one cannot capture the \"aboutness\" of the text, and therefore the summary will not capture the main point of the original text The notion of cohesion, introduced in (Halliday and Hasan, 1976) captures part of the intuition Cohesion is a device for \"sticking together\" different parts of the text Cohesion is achieved through the use of semantically related terms, reference, ellipsis and conjunctions Among these different means, the most easily identifiable and the most frequent type is lexical cohesion .",
        "(as discussed in (Hoey, 1991)) Lexical cohesion is created by using semantically related words Halliday and Hasan classified lexical cohesion into reiteration category and collocation category Reiteration can be achieved by repetition, synonyms and hyponyras Collocation relations specify the relation between words that tend to co-occur in the same lexical contexts (e g, \"She works as a teacher in the School\") Collocation relations are more problematic for identification than reiteration, but both of these categories are identifiable on the surface of the text Lexical cohesion occurs not only between two terms, but among sequences of related words – called lexical chains (Morris and Haut, 1991) Lexical chains provide a representation of the lexical cohesive structure of the text Lexical chains have also been used for information retrieval (Stairmand, 1996) and for correction of malapropisms (Hirst and St-Onge, 1997 (to appear)) In this paper, we investigate how lexical chains can be used as a source representation for summarization Another important dimension of the linguistic structure of a source text is captured under the related notion of coherence Coherence defines the macro-level semantic structure of a connected discourse, while cohesion creates connectedness in a non-structural manner.",
        "Coherence is represented in terms of coherence relations between text segments, such as elaboration, cause and explanation Some researchers, e g , (Ono, Kazuo, and Sew, 1994), use discourse structure (encoded using FIST (Mann and Thompson, 1987) as a source representation for summarization) Clearly, this representation is expressive enough, the question is whether it is computable In contrast to lexical cohesion, coherence is difficult to identify without complete understanding of the text and complex inference In addition, there is no precise criteria for classification of differ-exit relations Consider the following example from Hobbs(1978) \"John can open the safe He knows the combination\" (Morns and Hirst, 1991) show that the relation between these two sentences can be interpreted as elaboration or as explanation, depending on \"context, knowledge and beliefs\" There is, however, a close connection between discourse structure and cohesion Related words tend to co-occur within a discourse unit of the text So cohesion is one of the surface signs of discourse structure and lexical chains can be used to identify it Other signs can be used to identify discourse structure as well (connectives, paragraph markers, tense shifts) In this paper, we investigate the use of lexical chains as a model of the source text for the purpose of producing a summary Obviously, other aspects of the source text need to be integrated in the text representation to produce quality summaries, but we want to empirically investigate how far one can go exploiting mainly lexical chains In the rest of the paper we first present our algorithm for lexical chain construction We then present empirical results on the identification of strong chains among the possible candidates produced by our algorithm Fmally, we describe how lexical chains are used to identify significant sentences within the source text and eventually produce a summary"
      ]
    },
    {
      "heading": "Algorithm for Chain Computing",
      "text": [
        "One of the chief advantages of lexical cohesion is that it is an easily recognizable relation, enabling lexical chains computation The first computational model for lexical chains was presented in (Morns and Hirst, 1991) They define lexical cohesion relations in terms of categories, index entries and pointers in Roget's Thesaurus Morris and Hirst evaluated that their relatedness criterion covered over 90% of the intuitive lexical relations Chains are created by taking a. new text word and finding a related chain for it according to relatedness criteria Morns and Hirst introduce the notion of \"activated chain\" and \"chain returns\", to take into account the distance between occurrences of related words They also analyze factors contributing to the strength of a chain – repetition, density and length Morris and Hirst did not implement their algorithm, because there was no machine-readable version of ftoget's Thesaurus at the time One of the drawbacks of their approach was that they did not require the same word to appear with the same sense in its different occurrences for it to belong to a chain For semantically ambiguous",
        "words, this can lead to confusions (e g, mixing two senses of table as a piece of furniture or an array) Note that choosing the appropriate chain for a word is equivalent to desambiguatmg this word in context, which is a well-known difficult problem m text understanding More recently, two algorithms for the calculation of lexical chains have been presented in Hirst and St-Onge (1995) and Staumand (1996) Both of these algorithms use the WordNet lexical database for determining relatedness of the words (Miller et al. 1990) Senses in the WordNet database are represented relationally by synonym sets ('synsets') – which are the sets of all the words sharing a common sense For example two senses of \"computer\" are represented as {calculator, reckoner, figurer, estimator, computer} (3 5, a person who computes) and {computer, data processor, electromc computer, information processing system} WordNet contains more than 118,000 different word forms Words of the same category are linked through semantic relations like synonymy and hyponymy Polysemous words appear in more than one syn-sets (for example, computer occurs m two synsets) Approximately 17% of the words in WordNet are polysemous But, as noted by Staumand, this figure is very misleading \"a significant proportion of WordNet nouns are Latin labels for biological entities, which by their nature are monosemous and our experience with the news-report texts we have processed is that approximately half of the nouns encountered are polysemous\" (Staarmand, 1996) Generally, a procedure for constructing lexical chains follows three steps (1) Select a set of candidate words, (2) For each candidate word, find an appropriate chain relying on a relatedness criterion among members of the chains, (3) If it is found, insert the word in the chain and update it accordingly An example of such a procedure was represented by Hirst and St-Onge (H&S) In the preprocessor step, all words that appear as a noun entry in WordNet are chosen Relatedness of words is determined in terms of the distance between their occurrences and the shape of the path connecting them in the WordNet thesaurus Three kinds of relation are defined extra-strong (between a word and its repetition), strong (between two words connected by a Wordnet relation) and medium-strong when the link between the synsets of the words is longer than one (only paths satisfying certain restrictions are accepted as valid connections) The maximum distance between related words depends on the kind of relation for extra-strong relations, there is not limit in distance, for strong relations, it is limited to a window of seven sentences, and for medium-strong relations, it is within three sentences back To find a chain in which to insert a given candidate word, extra-strong relations are preferred to strong-relations and both of them are preferred to medium-strong relations If a chain is found, then the candidate word is inserted with the appropriate sense, and the senses of the other words in the receiving chain are updated, so that every word connected to the new word in the chain relates to its selected senses only If no chain is found, then a new chain is created and the candidate word is inserted with all its possible senses in WordNet The greedy disambiguation strategy implemented in this algorithm has some hmitation,s illustrated by the following example Mr. Kenny is the person that invented an anaesthetic machine which uses microcomputers to control the rate at which an anaesthetic es pumped into the blood Such machines are nothing new But his device uses two microcomputers to achieve much closer monitoring of the pump feeding the anaesthetic into the patient According to H&S's algorithm, the chain for the word \"Mr\" is first created [lox \"Hr.\", sense {mist er , ?Ir.)]",
        "\"Mr\" belongs only to one synset, so it is disambiguated from the beginning The word \"person\" is related to this chain in the sense \"a human being\" by a medium-strong relation, so the chain now contains two entries [lex \"Mr.\", sense {mutter, ?Ir.}]",
        "[lox \"person\", sense {person, andtvidual, someone, man, mortal, Inman, son1}1 When the algorithm processes the word \"machine\", it relates it to this chain, because \"machine\" in the first WordNet sense (\"an efficient person\") is a holonym of \"person\" in the chosen sense In other words, \"machine' and \"person\" are related by a strong relation In this case, \"machine' is disambiguated in the wrong way, even though after this first occurrence of \"machine\", there is strong evidence supporting the selection of its more common sense \"micro-computer\", \"device\" and \"pump\" all point to its correct sense in this context – \"any mechanical or electrical device that performs or assists in the performance\" This example indicates that disambiguation cannot be a greedy decision In order to choose the right sense of the word the 'whole picture' of chain distribution in the text must be considered We propose to develop a chaining model according to all possible alternatives of word senses and then choose the best one among them Let us illustrate this method on the above exam",
        "pie First, a node for the word \"Mr\" is created [lax sense {mister, Mr }] The next candidate word is \"person\" It has two senses \"human being\" (person – 1) and 'grammatical category of pronouns and verb forme (person-2) The choice of sense for \"person\" splits the chain world to two different interpretations as shown in Figure",
        "We define a component as a list of interpretations that are exclusive of each other Component words influence each other in the selection of their respective senses The next candidate word \"anaesthetic\" is not related to any word in the first component, so we create a new component for it with a single interpretation The word 'machine\" has 5 senses maelimel to maclitnea In its first sense, \"an efficient person\", it is related to the senses \"person\" and \"Mr\" It therefore influences the selection of their senses, thus \"machme\" has to be inserted in the first component After its insertion the picture of the first component becomes the one shown in Figure 2 But if we continue the process and insert the words \"micro-computer\", \"device\" and \"pump\", the number of alternative greatly increases The strongest interpretations are given in Figures 3 and 4 Under the assumption that the text is cohesive, we define the best interpretation as the interpretation with the most connections (edges in the graph) In this case, the second interpretation at the end of Step 3 is selected, which predicts the right sense for \"machine\" We define the score of an interpretation.",
        "as the sum of its chain scores Chain sdoze is determined by the number and weight of the relations between chain members Experimentally, we fixed the weight of reiteration and synonym to 10, of antonym to 7, and of hyperonym and holonym to 4.",
        "Our algorithm develops all possible interpretations, maintaining each one without self contradiction When the number of possible Interpretations is larger than a certain threshold, we prune the weak interpretations according to this criteria In the end, we select from each component the strongest interpretation",
        "In summary, our algorithm differs from 118rS's algorithm in that it introduces, in addition to the relatedness criterion for membership to a chain, anon-greedy disambiguation heuristic to select the appropriate senses of chain members The two algorithms differ in two other major aspects the criterion for the selection of candidate words and the operative definition of a text unit We choose as candidate words simple nouns and noun compounds As mentioned above, nouns are the main contributors to the \"aboutness\" of a text, and noun synsets dominate in WordNet Both (Stairmand, 1996) and H&S rely only on name as candidate words In our algorithm, we rely on the results of Brill's part-of-speech tagging algorithm to identify nouns, while USES do not go through this step and only select tokens that happen to occur as nouns in WordNet In addition, we extend the set of candidate words to include noun compound We first empirically evaluated the importance of noun compounds by taking into account the noun compounds explicitly present in WordNet (some 50,000 entries in WordNet are noun compounds such as \"sea level\" or collocations",
        "such as \"digital computer') However, English includes a productive system of noun compobnds, and in each domain, new noun-compounds and collocations not present in WordNet play a major role We addressed the issue, by using a shallow parser (developed by Ido Dagan's team at Bar Ilan University) to identify noun-compounds using a simple characterization of noun sequences This has two major benefits (1) it identifies important concepts in the domain (for example, in a text on \"quanttnn COMPIthIll\" the main token was the noun compound \"quantum computing\" which was not present in WordNet), (2) it ehminates words that occur as modifiers as possible candidates for chain membership For example, when \"quantum computing\" is selected as a single unit, the word \"quantum\" is not selected This is beneficial because in tins example, the text was not about -“quantunz\", but more about computers When a noun compound is selected, the relatedness criterion in WordNet is used by considering its head noun only Thus, \"quantum computer\" is related to \"machine' as a \"computer\" The second difference in our algorithm lies in the operative definition we give to the notion of text unit We use as text units the segments obtained from Hearst's algorithm of text segmentation (Hearst, 1994) We build chains in every segment according to relatedness criteria, and in a second stage, we merge chains from the different segments using much stronger criteria for connectedness only two chains are merged across a segment boundary only if they contain a common word with the same sense Our mtra-segment relatedness criterion is less strict members of the same synsets are related, a node and its offspnng in the hyperonym graph axe related, siblings in the hyperonym graph axe related only if the length of the path is less than a threshold The relation between text segmentation and lexical chain is delicate, since they are both derived.",
        "from partially common source of knowledge lexical distribution and repetitions In fact, lexical chains could serve as a basis for an algorithm for segmentation We have found empirically, however, that Hearst's algorithm behaves well on the type of texts we checked and that it provides effectively a solid basis for lexical chains construction"
      ]
    },
    {
      "heading": "Building Summaries Using Lexical Chains",
      "text": [
        "We now investigate how lexical chains can serve as a source representation of the original text to build a summary The next question is how to build summary representation from this source representation The most prevalent discourse topic will play an important role in the summary We first present the intuition why lexical chains are a good indicator of the central topic of a text Gwen an appropriate measure of strength, we show that picking the concepts represented by strong lexical chains gives a better indication of the central topic of a text than simply picking the most frequent words in the text (which forms the zero-hypothesis) For example, we show in Appendix a sample text about Hayman Network technology There, the concept of network was represented by the words \"network\" with 6 occurrences, \"net\" with 2, and \"system* with 4 But the summary representation has to reflect that all these words represent the same concept Otherwise, the summary generation stage would extract information separately for each term The chain representation approach avoids completely this problem, because all these terms occur in the same them, which reflects that they represent the same concept An additional argument for the chain represents-ben as opposed to a simple word frequency model is the case when a single concept is represented by a number of words, each with relatively low frequency In the same Bayesian Network sample text, the concept of \"Information\" was represented by the words \"information\" (3), \"datum\" (2), \"knowledge\" (3), \"concept\" (1) and \"model\" 1 In this text, \"information\" is a more important concept than \"computer\"",
        "which occurs 4 times Because the 'information\" chain combines the number of occurrences of all its members, it can overcome the weight of the single word \"computer\""
      ]
    },
    {
      "heading": "Scoring Chains",
      "text": [
        "In order to use lexical chains as outlined above, one must first identify the strongest chains among all those that are produced by our algorithm As is frequent in summarization, there is no formal way to evaluate chain strength (as there is no formal method to evaluate a summary quality) We therefore rely on an empirical methodology We have developed an environment to compute and graphically visualize lexical chains to evaluate experimentally how they capture the main topics of the texts Figure 5 shows how lexical chains are visualized to help human testers evaluate their importance Figure 5 Visual representation of lexical chains We have collected data for a set of 30 texts extracted from popular magazines (from \"The Economist\" and \"Scientific American\"), all of them are popular science genre For each text, we manually ranked chains in terms of relevance to the main topics We then computed different formal measures on the chains, including chain length, distribution in the text, text span covered by the chain, density, graph topology (diameter of the graph of the words) and number of repetitions The results on our data set indicate that only the following parameters are good predictors of the strength of a chain Length: The number of occurrences of members of the chain Homogeneity index: 1 - the number of distinct occurrences divided by the length We designed a score function for chains as",
        "When ranking chains according to their score, we evaluated that strong chains are those which satisfy our \"Strength Criterion\"",
        "These are prehnimary results but they are confirmed by our experience on 30 texts analyzed extensively We have experimented with different nor-mahzation methods for the score function, but they do not seem to improve the results We plan on extendmg the empirical analysis in the future and to use formal learning methods to determine a good scoring function The average number of strong chains selected by this selection method was 5 for texts of 1055 words on average (474 words minimum, 3198 words maximum), when 32 chains were originally generated on average The strongest chain of the sample text are represented in Appendix Extracting Significant Sentences Once strong chains have been selected, the next step of the summarization algorithm is to extract full sentences from the original text based on chain distribution We investigated three alternatives for this step Heuristic I For each chain in the summary representation choose the sentence that contains the first appearance of a chain member in the text This heuristic produced the following summary for the text shown in Appendix"
      ]
    },
    {
      "heading": "When Microsoft Senior Vice President Steve Ballmer",
      "text": [
        "first heard his company was planning to make a huge investment in an Internet service offering moose reviews and local entertainment information in major cities across the nation, he went to Chairman NI Gates with his concerns Microsoft's competitive advantage, he responded, was its expertise in Dayesiats networks Dagestan networks are complex dtagnams that organ= the body of knowledge in any given area by mapping out came and effect relationships among key variables and encoding them with numbers that represent the extent to which one minable is likely to affect another Programmed into computers, these systems can automatically generate optimal predictions or decisions even when key pieces of information are missing When Microsoft in 1993 hired Eric Beretta, David Beck-erman and Jack Breese, pioneers in the development of Bayesian systems, colleagues in the field were surprised The problem with this approach is that all words in a chain reflect the same concept, but to a different extent For example, in the Al chain, (Appendix, Chain 3) the token 'science\" is related to the concept \"Ar, but the words \"Al\" and \"field\" are more suitable to represent the main topic \"Al\" in the context of the text That is, not all chain members are good representatives of the topic (even though they all contribute to its meaning)",
        "We therefore defined a criterion to evaluate the appropriateness of a chain member to represent its chain based on its frequency of occurrence in the chain We found experimentally that such words, • call them representative words, have a frequency in the chain no less than the average word frequency in the chain For example, in the third chain the representative words are \"field\" and \"Al\" Heuristic 2 We therefore defined a second heuristic based on the notion of representative words For each chain in the summary representation, choose the sentence that contains the first appearance of a representative chain member in the text in this special case this heuristic gives the same result as the first one Heuristic 3 Often, the same topic is discussed in a number of places in the text, so its chain is distributed across the whole text Still, in some text unit, this global topic is the central topic (focus) of the segment We try to identify this unit and extract sentences related to the topic from this segment (or successive segments) only We characterize this text unit as a cluster of successive segments with high density of chain members Our third heuristic is based on this approach For each chain, find the text unit where the chain is highly concentrated Extract the sentence with the first chain appearance in this central unit Concentration is computed as the number of chain members occurrences in a segment divided by the number of nouns in the segment A chum has high concentration if its concentration is the maximum of all chains Cluster IS group of successive segments such that every segment contains chain members Note that in all these three techniques only one sentence is extracted for each chain (regardless of its strength) - For most texts we tested, the first and second techniques produce the same results, but when they are different, the output of the second technique is better Generally, the second technique produces the best summary We checked these methods on our 30 texts data set Surprisingly, the third heuristic, which intuition predicts as the most sophisticated, gives the least indicative results This may be due to several factors our criteria for 'centrality' or 'clustering' may be insufficient or, more likely, the problem seems to be related to the interaction with text structure The third heuristics tends to extract sentences from the middle of the text and to extract several sentences from distant places in the text for a single chain The complete results of our experiments are available on-line at http / Iwyni cs bgn.",
        "ac 32/sumniar2zat ion-t est",
        "• Limitations and Future Work",
        "We have identified the following main problems with our method • Sentence granularity all our methods extract whole sentences as single units This has several drawbacks long sentences have significantly higher likelihood to be selected, they also include many constituents which would not have been selected on their own merit The alternative is extremely costly it involves some parsing of the sentences, the extraction of only the central constituents from the source text and the regeneration of a summary text using text generation techniques • Extracted sentences contain anaphora links to the rest of the text This has been investigated and observed by (Black, 1994) Several heuristics have been proposed in the literature to address this problem (Psice, 1990), (Peace and Husk) 1991) and (Black, 1994) The strongest seems to be to include together with the extracted sentence the one immediately preceding it Unfortunately, when we select the first sentence in a segment, the preceding sentence does not belong to the paragraph and its insertion has a detrimental effect on the overall coherence of the summary A preferable solution would be to replace anaphora with their referent, but again this is an extremely costly solution • Our method does not provide any way to control the length and level of detail of the summary In all of the methods, we extract one sentence for each chain The number of strong chains remains small (around 5 or 6 for the texts we have tested, regardless of their length), and the re-mauling chains would introduce too much noise to be of interest in adding details The best solution seems to be to extract more material for the strongest chains The method presented in this paper is obviously partial in that it only considers lexical chains as a source representation, and ignores any other clues that could be gathered from the text Still, our first informal evaluation indicates our results are of a quality superior to that of summarizers usually employed in commercial systems such as search systems on the World Wide Web on the texts we investigated A large-scale evaluation of the method and how sensitive it is to the quality of the thesaurus and to its parameters is under way"
      ]
    },
    {
      "heading": "Bayesian Networks Text",
      "text": []
    },
    {
      "heading": "When Microsoft Senior Vice President Steve Ulmer first heard his company was",
      "text": [
        "planning to make a huge wiveatment in an intemet service offering movie reviews and local entertainment mfonnabenm maperaues across the nation he want to Chairman"
      ]
    },
    {
      "heading": "Bin Gates with his COSICS1111",
      "text": [
        "After all Bagmer has billions of dollars of tin own money in Microsoft stock, and entertainment ISO C exactly the company's strong point But Gates damaged such reservations Microsoft's competitive advantage, he responded, was its expanse in Baytown nebteeirs Asked recently when computers would finally begin to understand human speech Gates began discussmg the astical role of Barman systems Ask any other software executive obout anything Baytown and you're liable to gat a blank stare"
      ]
    },
    {
      "heading": "Is Gatos onto sornethmer Is this then-sounding technology Microsoft a new secret weapon?",
      "text": [
        "Bowmen networks are complex diagrams that organic.",
        "the body of knowledge la any given area by mapping out cause-and-affect relationships among lay variables and encoding them with numbers that represent the extent to which one variable Is likely to affect another ( Programmed into computers, these systems can automatically generate optimal predictions or decisions even when key pieces of inhumation are missing When Microsoft in 1993 hired Eric Horvitz, Dawd Beckerman end Jack Breese pm-nem in the development of Bayesian systems, colleagues in the field were surprised The field was still an obscure, largely academic enterprise"
      ]
    },
    {
      "heading": "Soya:non nets provide an overarching graphical framework' that lamp together & verse elements of Al and increases the range of its likely application to the real world mys Michael 'Jordon professor of brain' end cognitive science at the Maleachmetra Institute of Technology",
      "text": [
        "Microsoft ts unquestionably the most aggressive in mploitmg the new approach The company offers a free Web service that helps customers diagnose printing probkrns with thew computers and recommends the quickest way to resolve them Another Web service helps parents diagnose their children's health problems ( Horvitz who with two colleagues founded Knowledge lndustnes to develop tools for developrng Bayesan systems says he and the othersleft the companytopun Microsoft in part because they wanted to use their theoretical work more broadly applied Although the company did impatient work for the National Aeronauts:sand Space Ad,"
      ]
    }
  ]
}
