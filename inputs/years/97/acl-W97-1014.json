{
  "info": {
    "authors": [
      "Christoph Tillmann",
      "Hermann Ney"
    ],
    "book": "Workshop on Computational Natural Language Learning CoNLL",
    "id": "acl-W97-1014",
    "title": "Word Triggers and the EM Algorithm",
    "url": "https://aclweb.org/anthology/W97-1014",
    "year": 1997
  },
  "references": [
    "acl-H92-1073",
    "acl-J93-2003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we study the use of so-called word trigger pairs to improve an existing language model, which is typically a trigram model in combination with a cache component.",
        "A word trigger pair is defined as a long-distance word pair.",
        "We present two methods to select the most significant single word trigger pairs.",
        "The selected trigger pairs are used in a combined model where the interpolation parameters and trigger interaction parameters are trained by the EM algorithm."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In this paper, we study the use of so-called word trigger pairs (for short: word triggers) (Bahl et al., 1984, Lau and Rosenfeld, 1993, Tillmann and Ney, 1996) to improve an existing language model, which is typically a trigram model in combination with a cache component (Ney and Essen, 1994).",
        "We use a reference model p(w1h), i.e. the conditional probability of observing the word w for a given history h. For a trigram model, this history h includes the two predecessor words of the word under consideration, but in general it can be the whole sequence of the last M predecessor words.",
        "The criterion for measuring the quality of a language model p(will) is the so-called log-likelihood criterion (Ney and Essen, 1994), which for a corpus wi , ...wN is defined by:",
        "According to this definition, the log-likelihood criterion measures for each position n how well the language model can predict the next word given",
        "the knowledge about the preceeding words and computes an average over all word positions n. In the context of language modeling, the log-likelihood criterion F is converted to perplexity PP, defined by PP := – FIN.",
        "For applications where the topic-dependence of the language model is important, e.g. text dictation, the history h may reach back several sentences so that the history length M covers several hundred words, say, M = 400 as it is for the cache model.",
        "To illustrate what is meant by word triggers, we give a few examples:",
        "Thus word trigger pairs can be viewed as long-distance word bigrams.",
        "In this view, we are faced the problem of finding suitable word trigger pairs.",
        "This will be achieved by analysing a large text corpus (i.e. several millions of running words) and learning those trigger pairs that are able to improve the baseline language model.",
        "A related approach to capturing long-distance dependencies is based on stochastic variants of link grammars (Pietra and Pietra, 1994).",
        "In several papers (Bahl et al., 1984, Lau and Rosenfeld, 1993, Tillmann and Ney, 1996), selection criteria for single word trigger pairs were studied.",
        "In this paper, this work is extended as follows:",
        "• Single-Trigger Model: We consider the def",
        "inition of a single word trigger pair.",
        "There are two models we consider, namely a backing-off model and a linear interpolation model.",
        "For the case of the backing-off model, there is a closed-form solution for estimating the trigger parameter by maximum likelihood.",
        "For the linear interpolation model, there is no explicit solution",
        "anymore, but this model is better suited for the extension towards a large number of simultaneous trigger pairs.",
        "• Multi-Trigger Model: In practice, we have to take into account the interaction of many trigger pairs.",
        "Here, we introduce .a model for this purpose.",
        "To really use the word triggers for a language model, they must be combined with an existing language model.",
        "This is achieved by using linear interpolation between the existing language model and a model for the multi-trigger effects.",
        "The parameters of the resulting model, namely the trigger parameters and one interpolation parameter, are trained by the EM algorithm.",
        "• We present experimental results on the Wall Street Journal corpus.",
        "Both the single-trigger approach and the multi-trigger approach are used to improve the perplexity of a baseline language model.",
        "We give examples of selected trigger pairs with and without using the EM algorithm."
      ]
    },
    {
      "heading": "2 Single-Trigger Model",
      "text": [
        "In this section, we review the basic model definition for single word trigger pairs as introduced in (Tillmann and Ney, 1996).",
        "We fix one trigger word pair (a 4 b) and define an extended model pab(w1h) with an trigger interaction parameter g(bla) .",
        "To pave the way for the following extensions, we consider the asymmetric model rather than the symmetric model as originally described in (Tillmann and Ney, 1996)."
      ]
    },
    {
      "heading": "Backing-Off",
      "text": [
        "As indicated by the results of several groups (Lau and Rosenfeld, 1993, Rosenfeld, 1994, Tillmann and Ney, 1996), the word trigger pairs do not help much to predict the next word if there is already a good model based on specific contexts like trigram, bi-gram or cache.",
        "Therefore, we allow the trigger interaction a b only if the probability p(blh ) of the reference model is not sufficiently high, i.e. if p(b1h) < p0 for a certain threshold p0 (note that, by setting po := 1.0, the trigger effect is used in all cases).",
        "Thus, we use the trigger effect only for the following subset of histories:",
        "For a training corpus wi ...wN, we consider the log-likelihood functions of both the extended model and the reference model p(wr,Ihn), where we define the history hn:",
        "For the difference Fab – F0 in the log-likelihoods of the extended language model Pab(w1h) and the reference model p(w1h), we obtain:",
        "where we have used the usual counts N (h, w):",
        "and two additional counts (a; b) and-N (a;b) defined particularly for word trigger modeling:",
        "if h E flab, w b if h Hat.",
        "+ N (h , b) log[l – p(b1h)]] h: w Tillmann & Ney 118 Word Triggers and EM Note that, for the counts g(a; b) and g(a; b), it does not matter how often the triggering word a actually occurred in the history h E Hab• The unknown trigger parameter g (bla) is estimated using maximum likelihood estimation.",
        "By taking the derivative and setting it to zero, we obtain the estimate:",
        "which can be interpreted as the relative frequency of the occurrence of the word trigger (a b).",
        "Linear Interpolation Although the backing-off method presented results in a closed-form solution for the trigger parameter q(bla), explicit probability threshold po to decide whether or not the trigger effect applies.",
        "Furthermore, the ultimate goal is to combine several word trigger pairs into a single model, and it is not clear how this could be done with the backing-off model.",
        "Therefore, we replace the backing-off model by the corresponding model for linear interpolation: Pab (wlh ) = where 6(w, v) = 1 if and only if v = w. Note that this interpolation model allows a smooth transition from no trigger effect (q(b la) – * 0) to a strong trigger effect (q (bla) – > 1).",
        "For a corpus wi...w,...wN, we have the log-likelihood difference:",
        "n.Ehn Thus M (a) counts how many of all positions n (n = 1, • • • , N) with history hn contain word a and is therefore different from the unigram count N (a).",
        "To apply maximum likelihood estimation, we take the derivative with respect to q(bla) and obtain the following implicit equation for q(bla) after some elementary manipulations:",
        "No explicit solution is possible.",
        "However, we can give bounds for the exact solution (proof omitted):",
        "n:aEhn,b=„ and an additional count N (a; b):",
        "An improved estimate can be obtained by the EM algorithm (Dempster and Laird, 1977):",
        "An example of the full derivation of the iteration formula for the EM algorithm will be given in the next section for the more general case of a multi-trigger language model."
      ]
    },
    {
      "heading": "3 Multi-Trigger Model",
      "text": [
        "The trigger pairs are used in combination with a conventional baseline model p(wa I hn) (e.g. m-gram) to define a trigger model pT (wnihn):",
        "with the trigger parameters cx(w Iv) that must be normalized for each v:",
        "Tillmann Ney 119 Word Triggers and EM To simplify the notation, we have used the convention:",
        "• Mr,: the set of triggering words for position n • Mn = IMn I: the number of triggering words for position n",
        "Unfortunately, no method is known that produces closed-form solutions for the maximum-likelihood estimates.",
        "Therefore, we resort to the EM algorithm in order to obtain the maximum-likelihood estimates.",
        "The framework of the EM algorithm is based on the so-called Q(p;r2) function, where 71 is the new estimate obtained from the previous estimate p (Baum, 1972), (Dempster and Laird, 1977).",
        "The symbol p stands for the whole set of parameters to be estimated.",
        "The Q(p;Ti) function is an extension of the usual log-likelihood function and is for our model:",
        "Taking the partial derivatives and solving for -A-, we obtain: A ,",
        "When taking the partial derivatives with respect to Ti(wlv), we use the method of Lagrangian multipliers for the normalization constraints and obtain:",
        "position n with wn = w, the contribution of a particular observed distant word pair (v...w) to (w Iv) depends on the interaction parameters of all other word pairs (v'...w) with v' E {wnn:1 } and the baseline probability p(w th).",
        "Note that the local convergence property still holds when the length Mn of the history is dependent on the word position n, e.g. if the history reaches back only to the beginning of the current paragraph.",
        "A remark about the functional form of the multi-trigger model is in order.",
        "The form chosen in this paper is a sort of linear combination of the trigger pairs.",
        "A different approach is to combine the various trigger pairs in multiplicative way, which results from a Maximum-Entropy approach (Lau and Rosenfeld, 1993)."
      ]
    },
    {
      "heading": "4 Experimental results",
      "text": []
    },
    {
      "heading": "Language Model Training and Corpus",
      "text": [
        "We first describe the details of the language model used and of its training.",
        "The trigger pairs were selected as described in subsection 2 and were used to extend a baseline language model.",
        "As in many other systems, the baseline language model used here consists of two parts, an m-gram model (here: trigram/bigram/unigram) and a cache part(Ney and Essen, 1994).",
        "Since the cache effect is equivalent to self-trigger pairs (a a), we can expect that there is some trade-off between the word triggers and the cache, which was confirmed in some initial informal experiments.",
        "For this reason, it is suitable to consider the simultaneous interpolation of these three language model parts to define the refined language model.",
        "Thus we have the following equation for the refined language model p(wn I hn):",
        "where pm(wnIhn) is the rn-gram model, pc(wn1hn) is the cache model and pT(wn Ihn) is the trigger model.",
        "The three interpolation parameters must be normalized:",
        "The details of the m-gram model are similar to those given in (Ney and Generet, 1995).",
        "The cache model",
        "Note how the interaction of word triggers is taken n-1 \\ _ E6.",
        "(Wn, Wn-m) into account by a local weighting effect: For a fixed PC(wnlwmn- m=1 Tillmann Ney 120 Word Triggers and EM",
        "where .5(w, v) = 1 if and only if v = w. The trigger model pT(tunihn) is defined as: There were two methods used to compute the trigger parameters:",
        "• method 'no EM': The trigger parameters a(w1v) are obtained by renormalization from the single trigger parameters OwIv):",
        "The backing-off method described in Section 2.1 was used to select the top-K most significant single trigger pairs.",
        "In the experiments, we used K = 1.5 million trigger pairs.",
        "• method 'with EM': The trigger parameters a(w1v) are initialized by the 'no EM' values and re-estimated using the EM algorithm as described in Section 3.",
        "The typical number of iterations is 10.",
        "The experimental tests were performed on the Wall Street Journal (WSJ) task (Paul and Baker, 1992) for a vocabulary size of 20000 words.",
        "To train the m-gram language model and the interpolation parameters, we used three training corpora with sizes of 1, 5 and 39 million running words.",
        "However, the word trigger pairs were always selected and trained from the 39-million word training corpus.",
        "In the experiments, the history h was defined to start with the most recent article delimiter.",
        "The interpolation parameters are trained by using the EM algorithm.",
        "In the case of the 'EM triggers', this is done jointly with the reestimation of the trigger parameters a(w1v).",
        "To avoid the overfitting of the interpolation parameters on the training corpus, which was used to train both the m-gram language model and the interpolation parameters, we applied the leaving-one-out technique."
      ]
    },
    {
      "heading": "Examples of Trigger Pairs",
      "text": [
        "In Table 2 and Table 3 we present examples of selected trigger pairs for the two methods no EM and EM.",
        "For a fixed triggering word v, we show the most significant triggered words w along with the trigger interaction parameter a(w1v) for both methods.",
        "There are 8 triggering words v for each of which we show the 15 triggered words w with the highest trigger parameter a(w1v).",
        "The triggered words w are sorted by the a(w Iv) parameter.",
        "/From the table it can be seen that for the no EM trigger pairs the trigger parameter a(w1v) varies only slightly over the triggered words w. This is different for the EM triggers, where the trigger parameters a(w1v) have a much larger variation.",
        "In addition the probability mass of the EM-trained trigger pairs is much more concentrated on the first 15 triggered words."
      ]
    },
    {
      "heading": "Perplexity Results",
      "text": [
        "The perplexity was computed on a test corpus of 325 000 words from the WSJ task.",
        "The results are shown in Table 1 for each of the three training corpora (1,5 and 39 million words).",
        "For comparison purposes, the perplexities of the trigram model with and without cache are included.",
        "As can be seen from this table, the trigger model is able to improve the perplexities in all conditions, and the EM triggers are consistently (although sometimes only slightly) better than the no EM triggers.",
        "There is an effect of the training corpus size: if the trigram model is already well trained, the trigger model does not help as much as for a less well trained trigram model.",
        "This observation is confirmed by the part b of Table 1, which shows the EM trained interpolation parameters.",
        "As the size of the training corpus decreases the relative weight of the cache and trigger component increases.",
        "Furthermore in the last row of Table 1 it can be seen that the relative weight of the trigger component increases after the EM training which indicates that the parameters of our trigger modell are successfully trained by this EM approach."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "We have presented a model and an algorithm for training a multi-word trigger model along with some experimental evaluations.",
        "The results can be sum-merized as follows: • The trigger parameters for all word triggers are jointly trained using the EM algorithm.",
        "This leads to a systematic (although small) improvement over the condition that each trigger parameter is trained separately.",
        "• The word-trigger model is used in combination with a full language model (m-gram /cache) .",
        "Thus the perplexity is reduced from 138.9 to 127.2 for the 5-million training corpus and from 92.2 to 87.4 for the 39-million corpus."
      ]
    }
  ]
}
