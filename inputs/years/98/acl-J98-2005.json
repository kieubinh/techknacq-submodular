{
  "info": {
    "authors": [
      "Zhiyi Chi",
      "Stuart Geman"
    ],
    "book": "Computational Linguistics",
    "id": "acl-J98-2005",
    "title": "Estimation of Probabilistic Context-Free Grammars",
    "url": "https://aclweb.org/anthology/J98-2005",
    "year": 1998
  },
  "references": [
    "acl-J95-2002"
  ],
  "sections": [
    {
      "text": [
        "The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one.",
        "The condition for proper assignment is rather subtle.",
        "Production probabilities can be estimated from parsed or unparsed sentences, and the question arises as to whether or not an estimated system is automatically proper.",
        "We show here that estimated production probabilities always yield proper distributions."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Context-free grammars (CFG's) are useful because of their relatively broad coverage and because of the availability of efficient parsing algorithms.",
        "Furthermore, CFG's are readily fit with a probability distribution (to make probabilistic CFG's – or PCFG's), rendering them suitable for ambiguous languages through the maximum a posteriori rule of choosing the most probable parse.",
        "For each nonterminal symbol, a (normalized) probability is placed on the set of all productions from that symbol.",
        "Unfortunately, this simple procedure runs into an unexpected complication: the language generated by the grammar may have probability less than one.",
        "The reason is that the derivation tree may have probability greater than zero of never terminating – some mass can be lost to infinity.",
        "This phenomenon is well known and well understood, and there are tests for \"tightness\" (by which we mean total probability mass equal to one) involving a matrix derived from the expected growth in numbers of symbols generated by the probabilistic rules (see for example Booth and Thompson [1973], Grenander [1976], and Harris [1963]).",
        "What if the production probabilities are estimated from data?",
        "Suppose, for example, that we have a parsed corpus that we treat as a collection of (independent) samples from a grammar.",
        "It is reasonable to hope that if the trees in the sample are finite, then an estimate of production probabilities based upon the sample will produce a system that assigns probability zero to the set of infinite trees.",
        "For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG.",
        "Is it tight?",
        "If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm – again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees?",
        "We will show that in both cases the estimated probability is tight.'",
        "Wetherell (1980) has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight.",
        "(Wetherell and others use the designation \"consistent\" instead of \"tight,\" but in statistics, consistency refers to the asymptotic correctness of an estimator.)",
        "A trivial example is the CFG with one nonterminal and one terminal symbol, in Chomsky normal form:",
        "where a is the only terminal symbol.",
        "Assign probability p to the first production (A AA) and q = 1 – p to the second (A a).",
        "Let Sh be the total probability of all trees with depth less than or equal to h. For example, S2 = q corresponding to A – > a, and S3 = q + pq2 corresponding to {A – > al U {A – > AA, A a, A a}.",
        "In general, Sh+1 = q + pS.",
        "(Condition on the first production: with probability q the tree terminates and with probability p it produces two nonterminal symbols, each of which must now terminate with depth less than or equal to h.) It is not hard to show that Sh is nondecreasirtg and converges to min(1, pi), meaning that a proper probability is obtained if and only if p < What if p is estimated from data?",
        "Given a set of finite parse trees wi, (.02, ,w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the \"relative frequency\" estimator",
        "where f(.",
        "; co) is the number of occurrences of the production '1.\"",
        "in the tree Le.",
        "The sentence am, although ambiguous (there are multiple parses when m > 2), always involves m – 1 of the A – > AA productions and m of the A a productions.",
        "Hence f (A AA; co,) < f (A a; w) for each wi.",
        "Consequently:",
        "If only the yields (left-to-right sequence of terminals) Y(wi), Y(w2), , Y(con) are available, the EM algorithm can be used to iteratively \"climb\" the likelihood surface (see Section 2).",
        "In the simple example here, the estimator converges in one step and is the same as if we had observed the entire parse tree for each w,.",
        "Thus, fr is again less than and the distribution is again tight."
      ]
    },
    {
      "heading": "2. Maximum-Likelihood Estimation",
      "text": [
        "More generally, let G = (V, T , R, S) denote a context-free grammar with finite variable set V, start symbol S E V, finite terminal set T, and finite production (or rule) set R. (We use R in place of the more typical P to avoid confusion with probabilities.)",
        "Each production in R has the form A – > a, where A E V and a E (Vu T)*.",
        "In the usual way, probabilities are introduced through the productions: P : R --+ [0,1] such that VA E V:",
        "Chi and Geman Probabilistic Context-Free Grammars Given a set of finite parse trees wi, (.02, ••• ,w, drawn independently according to the distribution imposed by p, we wish to estimate p. In terms of the frequency function f , introduced in Section 1, the likelihood of the data is",
        "The maximum-likelihood estimator is the natural, \"relative frequency,\" estimator.",
        "Suppose B E V is unobserved among the parse trees cvl, w2, , wn.",
        "Then we can assign p(B 0) arbitrarily, requiring only that (1) be respected.",
        "Evidently the likelihood is unaffected by the particular assignment of p (B ,(3).",
        "Furthermore, it is not hard to see that any such B has probability zero of arising in any derivation that is based upon the maximum-likelihood probabilities3 – hence the issue of tightness is independent of this assignment.",
        "We will show that if C2 is the set of all (finite) parse trees generated by G, and if p(w) is the probability of (.4., E St under the maximum-likelihood production probabilities, then P(12) 1."
      ]
    },
    {
      "heading": "2.1 The EM Algorithm",
      "text": [
        "Usually the derivation trees are unobserved – the sample, or corpus, contains only the yields Y(wi), Y(w2), , Y(w) (Y(w,) E T* for each 1 < i < n).",
        "The likelihood is substantially more complex, since p(Y (o.)))",
        "is now a marginal probability; we need to sum over the set of w E St that yield Y (w):",
        "In the case where only yields are observed, the treatment is complicated considerably by the possibility of null productions (A – > 0) and unit productions (A B E V).",
        "If, however, the language of the grammar does not include the null string, then there is an equivalent grammar (one with the same language) that has no null productions and no unit productions (cf. Hoperoft & Ullman [1979], Theorem 4.4).",
        "It is, then, perhaps best to simplify the treatment by assuming that there are no null or unit productions.",
        "Therefore, when the corpus consists of yields only, we shall assume a priori a model free of null and unit productions, and study tightness for probabilities estimated under such a model.",
        "Based upon the results of Stokke [1995] it is likely that this restriction can be relaxed, but we have not pursued this.",
        "where Efr is expectation under fr and where \" lw E Cly(,)\" means \"conditioned on E There is no hope for a closed form solution, but (4) does suggest an iteration scheme, which, as it turns out, \"climbs\" the likelihood surface (though there are no guarantees about approaching a global maximum): Let 1)0 be an arbitrary assignment respecting (1).",
        "Define a sequence of probabilities, pn, by the iteration",
        "The right-hand side is manageable, as long as we can manageably compute all possible parses of a sentence (yield) Y(w).",
        "(More efficient approaches exist; see Baker [1979].)",
        "This iteration procedure is an instance of the EM Algorithm.",
        "Baum [1972] first introduced it for hidden Markov models (regular grammars) and Baker [1979] extended it to the problem addressed here (estimation for context-free grammars).",
        "Dempster, Laird, and Rubin [1977] put the idea into a much more general setting and coined the",
        "term EM for Expectation-Maximization.",
        "The right-hand side of (5) is computed using the expected frequencies under Pn; 15n1-1 is then the maximum-likelihood estimator, treating the expected frequencies as though they were observed frequencies.",
        "The issue of tightness comes up again.",
        "We will show that frn(Q) -= 1 for each n > 0."
      ]
    },
    {
      "heading": "3. Tightness of the Maximum-Likelihood Estimator",
      "text": [
        "Given a context-free grammar G = (V ,T, R, S), let ft be the set of finite parse trees, let p: R --> [0,1] be a system of production probabilities satisfying (1), and let wi,w2, • • • , con be a set (sample) of finite parse trees wk E ft For now, null and unit productions are permitted.",
        "Finally, let fr be the maximum-likelihood estimator of p, as defined by (3).",
        "(See also the remarks following [3] concerning variables unobserved M WI, w2, wn.)",
        "More generally, fr will refer to the probability distribution on (possibly infinite) parse trees induced by the maximum-likelihood estimator.",
        "Let qA = P (derivation tree rooted with A fails to terminate).",
        "We will show that qs = 0 (i.e., derivation trees rooted with S always terminate).",
        "For each A E V, let F (A; w) be the number of instances of A in w and let PA; w) be the number of nonroot instances of A in w. Given a E (V U T)* , let nA(a) be the number of instances of A in the string a, and, finally, let ai be the ith component of the string a.",
        "For any A E V:",
        "{ai fails to terminate} IA – > a)",
        "In the absence of unit productions and null productions, F (A; w) < 2lco I (twice the length of the string co).",
        "Hence the expectations in (6) are finite.",
        "Furthermore, F (A; w) and F (A; co) satisfy the same conditions as before: F (A; w) = F(A; w) except when A = S. in which case F. (A; w) < F (A; co).",
        "Again, we conclude that qs = 0."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We are indebted to Mark Johnson for encouraging us to look at this problem in the first place, and for much good advice along the way.",
        "This work was supported by the Army Research Office (DAAL03-92-G-0115), the National Science Foundation (DMS-9217655), and the Office of Naval Research (N00014-96-1-0647)."
      ]
    }
  ]
}
