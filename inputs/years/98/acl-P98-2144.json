{
  "info": {
    "authors": [
      "Yutaka Mitsuishi",
      "Kentaro Torisawa",
      "Jun'ichi Tsujii"
    ],
    "book": "COLING-ACL",
    "id": "acl-P98-2144",
    "title": "HPSG-Style Underspecified Japanese Grammar with Wide Coverage",
    "url": "https://aclweb.org/anthology/P98-2144",
    "year": 1998
  },
  "references": [
    "acl-C96-2160",
    "acl-J94-4001",
    "acl-P98-2132",
    "acl-P98-2133"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes a wide-coverage Japanese grammar based on HPSG.",
        "The aim of this work is to see the coverage and accuracy attainable using an underspecified grammar.",
        "Underspecification, allowed in a typed feature structure formalism, enables us to write down a wide-coverage grammar concisely.",
        "The grammar we have implemented consists of only 6 ID schemata, 68 lexical entries (assigned to functional words).",
        "and 63 lexical entry templates (assigned to parts of speech (POSs) ).",
        "Furthermore, word-specific constraints such as subcategorization of verbs are not fixed in the grammar.",
        "However.",
        "this grammar can generate parse trees for 87% of the 10000 sentences in the Japanese EDR corpus.",
        "The dependency accuracy is 78% when a parser uses the heuristic that every bunsetsul is attached to the nearest possible one."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Our purpose is to design a practical Japanese grammar based on HPSG (Head-driven Phrase Structure Grammar) (Pollard and Sag, 1994), with wide coverage and reasonable accuracy for syntactic structures of real-world texts.",
        "In this paper, \"coverage\" refers to the percentage of input sentences for which the grammar returns at least one parse tree, and \"accuracy\" refers to the percentage of bunsetsus which are attached correctly.",
        "To realize wide coverage and reasonable accuracy, the following steps had been taken: A) At first we prepared a linguistically valid but coarse grammar with wide coverage.",
        "B) We then refined the grammar in regard to accuracy, using practical heuristics which are not linguistically motivated.",
        "As for A), the first grammar we have constructed actually consists of only 68 lexical en",
        "tries (LEs) for some functional words2, 63 lexical entry templates (LETs) for POSO, and 6 ID schemata.",
        "Nevertheless, the coverage of our grammar was 92% for the Japanese corpus in the EDR Electronic Dictionary (EDR, 1996), mainly due to underspecification, which is allowed in HPSG and does not always require detailed grammar descriptions.",
        "As for B), in order to improve accuracy, the grammar should restrict ambiguity as much as possible.",
        "For this purpose, the grammar needs more constraints in itself.",
        "To reduce ambiguity.",
        "we added additional feature structures which may not be linguistically valid but be empirically correct.",
        "as constraints to i) the original LEs and LETs.",
        "and ii) the ID schemata.",
        "The rest of this paper describes the architecture of our Japanese grammar (Section 2).",
        "refinement of our grammar (Section 3).",
        "experimental results (Section 4).",
        "and discussion regarding errors (Section 5)."
      ]
    },
    {
      "heading": "2 Architecture of Japanese Grammar",
      "text": [
        "In this section we describe the architecture of the HPSG-style Japanese grammar we have developed.",
        "In the HPSG framework, a grammar consists of (i) immediate dominance schemata (ID schemata), (ii) principles, and (iii) lexical entries (LEs).",
        "All of them are represented by typed feature structures (TFSs) (Carpenter, 1992), the fundamental data structures of HPSG.",
        "ID schemata, corresponding to rewriting rules in CFG, are significant for constructing syntactic structures.",
        "The details of our ID schemata are discussed in Section 2.1.",
        "Principles are constraints between mother and daughter feature structures:1 LEs, which compose the lexicon, are detailed constraints on each word.",
        "In our grammar, we do not always assign LEs to each word.",
        "Instead, we assign lexical entry"
      ]
    },
    {
      "heading": "2.1 ID Schemata",
      "text": [
        "Our grammar includes the 6 ID schemata shown in Table 1.",
        "Although they are similar to the ones used for English in standard HPSG, there is a fundamental difference in the treatment of relative clauses.",
        "Our grammar adopts the head-relative schema to treat relative clauses instead of the head-filler schema.",
        "More specifically, our grammar does not have SLASH features and does not use traces.",
        "Informally speaking, this is because SLASH features and traces are really necessary only when there are more than one verb between the head and the filler (e.g., Sentence (1) ).",
        "But such sentences are rare in real-world corpora in Japanese.",
        "Just using a Head-relative schema makes our grammar simpler and thus less ambiguous.",
        "(1) Taro ga aisuru to iu onna.",
        "-SUBJ love QUOTE say woman",
        "The woman who Taro says that he loves.'"
      ]
    },
    {
      "heading": "2.2 Lexical Entries (LEs) and Lexical Entry Templates (LETs)",
      "text": [
        "Basically, we assign LETs to POSs.",
        "For example, common nouns are assigned one LET, which has general constraints that they can be complements of predicates, that they can be a compound noun with other common nouns, and so on.",
        "However, we assign LEs to some single functional words which behave in a special way.",
        "For example, the verb `suru' can be adjacent to some nouns unlike other ordinary verbs.",
        "The solution we have adopted is that we assign a special LE to the verb `suru'.",
        "Our lexicon consists of 68 LEs for some functional words, and 63 LETs for POSs.",
        "A functional word is assigned one or more LEs, and a POS is also assigned one or more LETs."
      ]
    },
    {
      "heading": "3 Refinement of our Grammar",
      "text": [
        "Our goal in this section is to improve accuracy without losing coverage.",
        "Constraints to improve accuracy can also be represented by TFSs and be added to the original grammar components such as ID schemata, LEs, and LETs.",
        "The basic idea to improve accuracy is that including descriptions for rare linguistic phenomena might make it more difficult for our system to choose the right analyses.",
        "Thus, we abandon some rare linguistic phenomena.",
        "This approach is not always linguistically valid but at least is practical for real-world corpora.",
        "In this section, we consider some frequent linguistic phenomena, and explain how we discarded the treatment of rare linguistic phenomena in favor of frequent ones, regarding three components: (i) the postposition 'tun', (ii) relative clauses and commas and (iii) nominal suffixes representing time.",
        "The way how we abandon the treatment of rare linguistic phenomena is by introducing additional constraints in feature structures.",
        "Regarding (i) and (ii), we introduce `pseudo-principles', which are unified with ID schemata in the same way principles are unified.",
        "Regarding (iii), we add some feature structures to LEs/LETs."
      ]
    },
    {
      "heading": "3.1 Postposition 'Wa'",
      "text": [
        "The main usage of the postposition `tva' is divided into the following two patterns':",
        "• If two PPs with the postposition 'Ara' appear consecutively, we treat the first PP as",
        "a complement of a predicate just before the second ,PP.",
        "• Otherwise, PP with the postposition 'we' is treated as the complement of the last predicate in the sentence.",
        "(4) Ude wa nai ga, konjo ga aru.",
        "ability TOPIC missing but guts -SUBJ exist 'Though he does not have ability, he has guts.' To deal with the characteristic of `tva', we introduced the WA feature and the P_WA feature.",
        "Both of them are binary features as follows:",
        "We then introduced a 'pseudo-principle' for `wa' in a disjunctive form as below6:",
        "wa_tunCI, where wa_hm( – , – ).",
        "wa_hm( – , +).",
        "wa_hra(+, +).",
        "and so on.",
        "This treatment prunes the parse trees like those in Figure 1(b, d) as follows:",
        "• Figure 1(b) 1) At ( 4), the head-complement schema should be applied, and (A) of the 'pseudoprinciple' should also be applied.",
        "2) Since the phrase `iku kedo ashita we ika nai' contains a.",
        "'we', Ei is +.",
        "3) Since the PP `Kyou wa' is marked by 'we', is +.",
        "4) wa_hcg, n a fails.",
        "• Figure 1(d) 1) At (#), the head-modifier schema should be applied, and (B) of the 'pseudoprinciple' should also be applied.",
        "2) Since the phrase Tokai wa hito ga ookute' contains a `wa', El is +.",
        "3) Since the phrase `sawagashii' contains no `wa', 13 is – .",
        "4) wa_lunG, fails."
      ]
    },
    {
      "heading": "3.2 Relative Clauses and Commas",
      "text": [
        "Relative clauses have a tendency to contain no commas.",
        "In Sentence (5), the PP 'Nippon de,' is a complement of the main verb `atta', not a complement of `urnareta' in the relative clause (Figure 3(a) ), though 'Nippon de' is preferred to `umareta' if the comma after 'de' does not exist (Figure 3(b) ).",
        "We, therefore, abandon the treatment of relative clauses containing a",
        "To treat such a tendency of relative clauses.",
        "we first introduced the TOUTEN feature'''.",
        "The TOUTEN feature is a binary feature which takes +/ – if the phrase contains a/no comma.",
        "We then introduced a 'pseudo-principle' for relative clauses as follows:",
        "(A) When applying head-relative schema, also apply: [ DTRSINH_DTRITOUTEN - (B) When applying other ID schemata, this pseudo-principle has no effect.",
        "This is to make sure that parse trees for relative clauses with a comma cannot be produced."
      ]
    },
    {
      "heading": "3.3 Nominal Suffixes Representing Time and Commas",
      "text": [
        "Noun phrases (NPs) with nominal suffixes such as nen (year), gatsu (month), and ji (hour) represent information about time.",
        "Such NPs are sometimes used adverbially, rather than nominally.",
        "Especially NPs with such a nominal suffix and comma are often used adverbially (Sentence",
        "(6) & Figure 4(a) ), while general NPs with a comma are used in coordinate structures (Sentence (7) & Figure 4(h) ).",
        "(6) 1995 nen, jishin go okita.",
        "year earthquake -SUBJ occur-PAST An earthquake occurred in 1995.",
        "TA touten stands for a comma in Japanese.",
        "(6) and (7) respectively (7) Kyoto, Nara ni itta."
      ]
    },
    {
      "heading": "-GOAL &0-PAST",
      "text": [
        "I went to Kyoto and Nara.",
        "In order to restrict the behavior of NPs with nominal time suffixes and commas to adverbial usage only, we added the following constraint to the LE of a comma, constructing a coordinate structure:",
        "This prohibits an NP with a nominal suffix from being marked by a comma for coordination."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "We implemented our parser and grammar in LiLFeS (Makin° et al., 1998)8, a feature-structure description language developed by our group.",
        "We tested randomly selected 10000 sentences from the Japanese EDR corpus (EDR, 1996).",
        "The EDR Corpus is a Japanese version of treebank with morphological, structural, and semantic information.",
        "In our experiments, we used only the structural information, that is, parse trees.",
        "Both the parse trees in our parser and the parse trees in the EDR Corpus are first converted into bunsetsu dependencies, and they are compared when calculating accuracy.",
        "Note that the internal structures of bunsetsus, e.g. structures of compound nouns, are not considered in our evaluations.",
        "We evaluated the following grammars: (a) the original underspecified grammar, (b) (a) + constraint for wa-marked PPs, (c) (a) + constraint for relative clauses with a comma, (d) (a) + constraint for nominal time suffixes with a comma, and (e) (a) + all the three constraints.",
        "We evaluated those grammars by the following three measurements: Coverage The percentage of the sentences that generate at least one parse tree.",
        "Partial Accuracy The percentage of the correct dependencies between bunsetsus (excepting the last obvious dependency) for the parsable sentences.",
        "Total Accuracy The percentage of the correct dependencies between bunsetsus (excepting the last dependency) over all sentences.",
        "from the Japanese EDR Corpus: (a – e) are grammars respectively corresponding to Section 2 (a), Section 2 + Subsection 3.1 (b), Section 2 + Subsection 3.2 (c), Section 2 + Subsection 3.3 (d), and Section 2 + Section 3 (e).",
        "When calculating total accuracy, the dependencies for unparsable sentences are predicted so that every bunsetsu is attached to the nearest bunsetsu.",
        "In other words, total accuracy can be regarded as a weighted average of partial accuracy and baseline accuracy.",
        "Table 2 lists the results of our experiments.",
        "Comparison of the results between (a) and (b – d) shows that all the three constraints improve partial accuracy and total accuracy with little coverage loss.",
        "And grammar (e) using the combination of the three constraints still works with no side effect.",
        "We also measured average parsing time per sentence for the original grammar (a) and the fully augmented grammar (e).",
        "The parser we adopted is a naive CKY-style parser.",
        "Table 3 gives the average parsing time per sentence for those 2 grammars.",
        "Pseudo-principles and further constraints on LEs/LETs also make parsing more time-efficient.",
        "Even though they are sometimes considered to be slow in practical application because of their heavy feature structures, actually we found them to improve speed.",
        "In (Torisawa and Tsujii, 1996), an efficient HPSG parser is proposed, and our preliminary experiments show that the parsing time of the efficient parser is about three times shorter than that of the naive one.",
        "Thus, the average parsing time per sentence will be about 300 msec., and we believe our grammar will achive a practical speed.",
        "Other techniques to speed-up the parser are proposed in (Makin° et al., 1998)."
      ]
    },
    {
      "heading": "5 Discussion",
      "text": [
        "This section focuses on the behavior of commas.",
        "Out of randomly selected 119 errors in experiment (e), 34 errors are considered to have been caused by the insufficient treatment of commas.",
        "Especially the fatal errors (28 errors) occurred due to the nature of commas.",
        "To put it",
        "in another way, a phrase with a comma, sometimes, is attached to a phrase farther than the nearest possible phrase.",
        "In (Kurohashi and Na-ga°, 1994), the parser always attaches a phrase with a comma to the second nearest possible phrase.",
        "We need to introduce such a constraint into our grammar.",
        "Though the grammar (e) had the pseudo-principle prohibiting relative clauses containing commas, there were still 6 relative clauses containing commas.",
        "This can be fixed by investigating the nature of relative clauses."
      ]
    },
    {
      "heading": "6 Conclusion and Future Work",
      "text": [
        "We have introduced an underspecified Japanese grammar using the HPSG framework.",
        "The techniques for improving accuracy were easy to include into our grammar due to the HPSG framework.",
        "Experimental results have shown that our grammar has wide coverage with reasonable accuracy.",
        "Though the pseudo-principles and further constraints on LEs/LETs that we have introduced contribute to accuracy, they are too strong and therefore cause some coverage loss.",
        "One way we could prevent coverage loss is by introducing preferences for feature structures."
      ]
    }
  ]
}
