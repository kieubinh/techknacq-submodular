{
  "info": {
    "authors": [
      "Anoop Sarkar"
    ],
    "book": "COLING-ACL",
    "id": "acl-P98-2190",
    "title": "Conditions on Consistency of Probabilistic Tree Adjoining Grammars",
    "url": "https://aclweb.org/anthology/P98-2190",
    "year": 1998
  },
  "references": [
    "acl-C92-2066"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Much of the power of probabilistic methods in modelling language comes from their ability to compare several derivations for the same string in the language.",
        "An important starting point for the study of such cross-derivational properties is the notion of consistency.",
        "The probability model defined by a probabilistic grammar is said to be consistent if the probabilities assigned to all the strings in the language sum to one.",
        "From the literature on probabilistic context-free grammars (CFGs), we know precisely the conditions which ensure that consistency is true for a given CFG.",
        "This paper derives the conditions under which a given probabilistic Tree Adjoining Grammar (TAG) can be shown to be consistent.",
        "It gives a simple algorithm for checking consistency and gives the formal justification for its correctness.",
        "The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deficiency (i.e. whether any probability mass is assigned to strings that cannot be generated)."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Much of the power of probabilistic methods in modelling language comes from their ability to compare several derivations for the same string in the language.",
        "This cross-derivational power arises naturally from comparison of various derivational paths, each of which is a product of the probabilities associated with each step in each derivation.",
        "A common approach used to assign structure to language is to use a probabilistic grammar where each elementary rule • This research was partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94-G-0426.",
        "The author would like to thank Aravind Joshi, Jeff Rey-nal., Giorgio Satta, B. Srinivas, Fei Xia and the two anonymous reviewers for their valuable comments.",
        "or production is associated with a probability.",
        "Using such a grammar, a probability for each string in the language is computed.",
        "Assuming that the probability of each derivation of a sentence is well-defined, the probability of each string in the language is simply the sum of the probabilities of all derivations of the string.",
        "In general, for a probabilistic grammar G the language of G is denoted by L(G).",
        "Then if a string v is in the language L(G) the probabilistic grammar assigns v some non-zero probability.",
        "There are several cross-derivational properties that can be studied for a given probabilistic grammar formalism.",
        "An important starting point for such studies is the notion of consistency.",
        "The probability model defined by a probabilistic grammar is said to be consistent if the probabilities assigned to all the strings in the language sum to 1.",
        "That is, if Pr defined by a probabilistic grammar, assigns a probability to each string v E E*, where Pr(v) = 0 if v L(G), then",
        "From the literature on probabilistic context-free grammars (CFGs) we know precisely the conditions which ensure that (1) is true for a given CFG.",
        "This paper derives the conditions under which a given probabilistic TAG can be shown to be consistent.",
        "TAGs are important in the modelling of natural language since they can be easily lexical-ized; moreover the trees associated with words can be used to encode argument and adjunct relations in various syntactic environments.",
        "This paper assumes some familiarity with the TAG formalism.",
        "(Joshi, 1988) and (Joshi and Sch-abes, 1992) are good introductions to the formalism and its linguistic relevance.",
        "TAGs have",
        "been shown to have relations with both phrase-structure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies.",
        "Consistency of probabilistic TAGs has practical significance for the following reasons:",
        "• The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deficiency.",
        "• Existing EM based estimation algorithms for probabilistic TAGs assume that the property of consistency holds (Schabes, 1992).",
        "EM based algorithms begin with an initial (usually random) value for each parameter.",
        "If the initial assignment causes the grammar to be inconsistent, then iterative re-estimation might converge to an inconsistent grammar'.",
        "• Techniques used in this paper can be used to determine consistency for other probability models based on TAGs (Carroll and Weir, 1997)."
      ]
    },
    {
      "heading": "2 Notation",
      "text": [
        "In this section we establish some notational conventions and definitions that we use in this paper.",
        "Those familiar with the TAG formalism only need to give a cursory glance through this section.",
        "A probabilistic TAG is represented by (N, E, I, A, S,0) where N, E are, respectively, non-terminal and terminal symbols.",
        "I U A is a set of trees termed as elementary trees.",
        "We take V to be the set of all nodes in all the elementary trees.",
        "For each leaf A E V, label(A) is an element from E U {€}, and for each other node A, label(A) is an element from N. S is an element from N which is a distinguished start symbol.",
        "The root node A of every initial tree which can start a derivation must have label (A) = S. I are termed initial trees and A are auxiliary trees which can rewrite a tree node A E V. This rewrite step is called adjunction.",
        "(/) is a function which assigns each adjunction with a probability and denotes the set of parameters 'Note that for CFGs it has been shown in (Chaud-hari et al., 1983; Sanchez and Benedf, 1997) that inside-outside reestimation can be used to avoid inconsistency.",
        "We will show later in the paper that the method used to show consistency in this paper precludes a straightforward extension of that result for TAGs.",
        "in the model.",
        "In practice, TAGs also allow a leaf nodes A such that label(A) is an element from N. Such nodes A are rewritten with initial trees from I using the rewrite step called substitution.",
        "Except in one special case, we will not need to treat substitution as being distinct from adjunction.",
        "For t e I U A, A(t) are the nodes in tree t that can be modified by adjunction.",
        "For label(A) E N we denote Adj(label(A)) as the set of trees that can adjoin at node A E V. The adjunction of t into N E V is denoted by N t. No adjunction at N E V is denoted",
        "by N nil.",
        "We assume the following properties hold for every probabilistic TAG G that we consider: 1.",
        "G is lexicalized.",
        "There is at least one leaf node a that lexicalizes each elementary tree, i.e. a E E. 2.",
        "G is proper.",
        "For each N E V, cb(N nil) + E 1 3.",
        "Adjunction is prohibited on the foot node of every auxiliary tree.",
        "This condition is imposed to avoid unnecessary ambiguity and can be easily relaxed.",
        "4.",
        "There is a distinguished non-lexicalized initial tree T such that each initial tree rooted by a node A with label (A) = S substitutes into 7 to complete the derivation.",
        "This ensures that probabilities assigned to the input string at the start of the derivation are well-formed.",
        "We use symbols S, A, B,... to range over V, symbols a, b, c, .",
        ".",
        ".",
        "to range over E. We use t1, t2, ... to range over I U A and c to denote the empty string.",
        "We use Xi to range over all i nodes in the grammar."
      ]
    },
    {
      "heading": "3 Applying probability measures to",
      "text": []
    },
    {
      "heading": "Tree Adjoining Languages",
      "text": [
        "To gain some intuition about probability assignments to languages, let us take for example, a language well known to be a tree adjoining language:",
        "It seems that we should be able to use a function '0 to assign any probability distribution to the strings in L(G) and then expect that we can assign appropriate probabilites to the adjunc-tions in G such that the language generated by G has the same distribution as that given by .",
        "However a function 0 that grows smaller by repeated multiplication as the inverse of an exponential function cannot be matched by any TAG because of the constant growth property of TAGs (see (Vijay-Shanker, 1987), P. 104).",
        "An example of such a function '0 is a simple Pois-son distribution (2), which in fact was also used as the counterexample in (Booth and Thompson, 1973) for CFGs, since CFGs also have the constant growth property.",
        "This shows that probabilistic TAGs, like CFGs, are constrained in the probabilistic languages that they can recognize or learn.",
        "As shown above, a probabilistic language can fail to have a generating probabilistic TAG.",
        "The reverse is also true: some probabilistic TAGs, like some CFGs, fail to have a corresponding probabilistic language, i.e. they are not consistent.",
        "There are two reasons why a probabilistic TAG could be inconsistent: \"dirty\" grammars, and destructive or incorrect probability assignments.",
        "\"Dirty\" grammars.",
        "Usually, when applied to language, TAGs are lexicalized and so probabilities assigned to trees are used only when the words anchoring the trees are used in a derivation.",
        "However, if the TAG allows non-lexicalized trees, or more precisely, auxiliary trees with no yield, then looping adjunctions which never generate a string are possible.",
        "However, this can be detected and corrected by a simple search over the grammar.",
        "Even in lexicalized grammars, there could be some auxiliary trees that are assigned some probability mass but which can never adjoin into another tree.",
        "Such auxiliary trees are termed unreachable and techniques similar to the ones used in detecting unreachable productions in CFGs can be used here to detect and eliminate such trees.",
        "Destructive probability assignments.",
        "This problem is a more serious one, and is the main subject of this paper.",
        "Consider the probabilistic TAG shown in (3)2.",
        "Consider a derivation in this TAG as a generative process.",
        "It proceeds as follows: node S1 in t1 is rewritten as t2 with probability 1.0.",
        "Node S2 in t2 is 99 times more likely than not to be rewritten as t2 itself, and similarly node S3 is 49 times more likely than not to be rewritten as t2.",
        "This however, creates two more instances of 52 and S3 with same probabilities.",
        "This continues, creating multiple instances of t2 at each level of the derivation process with each instance of t2 creating two more instances of itself.",
        "The grammar itself is not malicious; the probability assignments are to blame.",
        "It is important to note that inconsistency is a problem even though for any given string there are only a finite number of derivations, all halting.",
        "Consider the probability mass function (pmf) over the set of all derivations for this grammar.",
        "An inconsistent grammar would have a pmf which assigns a large portion of probability mass to derivations that are non-terminating.",
        "This means there is a finite probability the generative process can enter a generation sequence which has a finite probability of non-termination."
      ]
    },
    {
      "heading": "4 Conditions for Consistency",
      "text": [
        "A probabilistic TAG G is consistent if and only if:",
        "where Pr(v) is the probability assigned to a string in the language.",
        "If a grammar G does not satisfy this condition, G is said to be inconsistent.",
        "To explain the conditions under which a probabilistic TAG is consistent we will use the TAG",
        "From this grammar, we compute a square matrix M which of size 1V1, where V is the set of nodes in the grammar that can be rewritten by adjunction.",
        "Each Mij contains the expected value of obtaining node Xi when node X, is rewritten by adjunction at each level of a TAG derivation.",
        "We call M the stochastic expectation matrix associated with a probabilistic TAG.",
        "To get M for a grammar we first write a matrix P which has 1V1 rows and 1/ U Al columns.",
        "An element Pij corresponds to the probability of adjoining tree tj at node X„ i.e. 0(X 4 ti)3.",
        "We then write a matrix N which has 1/ U Al rows and 1V1 columns.",
        "An element Nij is 1.0 if node Xi is a node in tree ti.",
        "Then the stochastic expectation matrix M is simply the product of these two matrices.",
        "By inspecting the values of M in terms of the grammar probabilities indicates that Mij contains the values we wanted, i.e. expectation of obtaining node Ai when node Ai is rewritten by adjunction at each level of the TAG derivation process.",
        "By construction we have ensured that the following theorem from (Booth and Thompson, 1973) applies to probabilistic TAGs.",
        "A formal justification for this claim is given in the next section by showing a reduction of the TAG derivation process to a multitype Galton-Watson branching process (Harris, 1963).",
        "This theorem provides a way to determine whether a grammar is consistent.",
        "All we need to do is compute the spectral radius of the square matrix M which is equal to the modulus of the largest eigenvalue of M. If this value is less than one then the grammar is consistent4.",
        "Computing consistency can bypass the computation of the eigenvalues for M by using the following theorem by GerSgorin (see (Horn and Johnson, 1985; Wetherell, 1980)).",
        "This makes for a very simple algorithm to check consistency of a grammar We sum the values of the elements of each row of the stochastic expectation matrix M computed from the grammar If any of the row sums are greater than one then we compute A42, repeat the test and compute M22 if the test fails, and so on until the test succeeds5.",
        "The algorithm does not halt if p(M) > 1.",
        "In practice, such an algorithm works better in the average case since computation of eigenvalues is more expensive for very large matrices.",
        "An upper bound can be set on the number of iterations in this algorithm.",
        "Once the bound is passed, the exact eigenvalues can be computed.",
        "For the grammar in (5) we computed the following stochastic expectation matrix:",
        "The first row sum is 2.4.",
        "Since the sum of each row must be less than one, we compute the power matrix M2.",
        "However, the sum of one of the rows is still greater than 1.",
        "Continuing we compute M22.",
        "- 0 0.1728 0.1728 0.1728 0.0688 - 0 0.0432 0.0432 0.0432 0.0172 m22 0 0 0 0 0.0002 0 0.0864 0.0864 0.0864 0.0344 0 0 0 0 0.0001 This time all the row sums are less than one, hence p(M) <1.",
        "So we can say that the grammar defined in (5) is consistent.",
        "We can confirm this by computing the eigenvalues for M which are 0,0,0.6,0 and 0.1, all less than 1.",
        "Now consider the grammar (3) we had considered in Section 3.",
        "The value of M for that grammar is computed to be: M(3) = Si 0 Si 52 S3 S2 0 1.0 1.0 53 0 0.99 0.99 0.98 0.98 5We compute M22 and subsequently only successive powers of 2 because Theorem 4.2 holds for any n' > n. This permits us to use a single matrix at each step in the algorithm.",
        "The eigenvalues for the expectation matrix M computed for the grammar (3) are 0, 1.97 and 0.",
        "The largest eigenvalue is greater than 1 and this confirms (3) to be an inconsistent grammar."
      ]
    },
    {
      "heading": "5 TAG Derivations and Branching",
      "text": []
    },
    {
      "heading": "Processes",
      "text": [
        "To show that Theorem 4.1 in Section 4 holds for any probabilistic TAG, it is sufficient to show that the derivation process in TAGs is a Galton-Watson branching process.",
        "A Galton-Watson branching process (Harris, 1963) is simply a model of processes that have objects that can produce additional objects of the same kind, i.e. recursive processes, with certain properties.",
        "There is an initial set of objects in the 0-th generation which produces with some probability a first generation which in turn with some probability generates a second, and so on.",
        "We will denote by vectors Zo, Zi, Z2, the 0-th, first, second, ... generations.",
        "There are two assumptions made about Zo, Z1, Z2,",
        "1.",
        "The size of the nth generation does not influence the probability with which any of the objects in the (n + 1)-th generation is produced.",
        "In other words, Zo, Zi, Z2, ... form a Markov chain.",
        "2.",
        "The number of objects born to a parent object does not depend on how many other objects are present at the same level.",
        "We can associate a generating function for each level Zi.",
        "The value for the vector Zn, is the value assigned by the nth iterate of this generating function.",
        "The expectation matrix M is defined using this generating function.",
        "The theorem attributed to Galton and Watson specifies the conditions for the probability of extinction of a family starting from its 0-th generation, assuming the branching process represents a family tree (i.e, respecting the conditions outlined above).",
        "The theorem states that p(M) < 1 when the probability of extinction is",
        "The assumptions made about the generating process intuitively holds for probabilistic TAGs.",
        "(6), for example, depicts a derivation of the string a2a2a2a2a3a3a1 by a sequence of adjunctions in the grammar given in (5)6.",
        "The parse tree derived from such a sequence is shown in Fig. 7.",
        "In the derivation tree (6), nodes in the trees at each level i are rewritten by adjunction to produce a level i + 1.",
        "There is a final level 4 in (6) since we also consider the probability that a node is not rewritten further, i.e. Pr(A nil) for each node A.",
        "We give a precise statement of a TAG derivation process by defining a generating function for the levels in a derivation tree.",
        "Each level i in the TAG derivation tree then corresponds to Zi in the Markov chain of branching pro",
        "cesses.",
        "This is sufficient to justify the use of Theorem 4.1 in Section 4.",
        "The conditions on the probability of extinction then relates to the probability that TAG derivations for a probabilistic TAG will not recurse infinitely.",
        "Hence the probability of extinction is the same as the probability that a probabilistic TAG is consistent.",
        "For each Xj E V, where V is the set of nodes in the grammar where adjunction can occur, we define the k-argument adjunction generating function over variables Si,.sk corresponding to the k nodes in V.",
        "where, r3 (t) = 1 if node Xj is in tree t, rj(t) = 0 otherwise.",
        "For example, for the grammar in (5) we get the following adjunction generating functions taking the variable Si, s2, s3, 84, s5 to represent the nodes A1, A2, B1, A3, B2 respectively.",
        "The nth level generating function Gn(si, ,․) is defined recursively as follows.",
        "For the grammar in (5) we get the following level generating functions.",
        ", s5) = Si",
        "Examining this example, we can express Gi(si,...,sk) as a sum Di(si,... , sk) + where Ci is a constant and Di(.)",
        "is a polynomial with no constant terms.",
        "A probabilistic TAG will be consistent if these recursive equations terminate, i.e. if",
        "We can rewrite the level generation functions in terms of the stochastic expectation matrix M, where each element m2,3 of M is computed as follows (cf. (Booth and Thompson, 1973)).",
        "The limit condition above translates to the condition that the spectral radius of M must be less than 1 for the grammar to be consistent.",
        "This shows that Theorem 4.1 used in Section 4 to give an algorithm to detect inconsistency in a probabilistic holds for any given TAG, hence demonstrating the correctness of the algorithm.",
        "Note that the formulation of the adjunction generating function means that the values for O(X 1-4 nil) for all X E V do not appear in the expectation matrix.",
        "This is a crucial difference between the test for consistency in TAGs as compared to CFGs.",
        "For CFGs, the expectation matrix for a grammar G can be interpreted as the contribution of each non-terminal to the derivations for a sample set of strings drawn from L(G).",
        "Using this it was shown in (Chaud-hari et al., 1983) and (Sanchez and Benedi, 1997) that a single step of the inside-outside algorithm implies consistency for a probabilistic CFG.",
        "However, in the TAG case, the inclusion of values for .0(X 1--+ nil) (which is essential if we are to interpret the expectation matrix in terms of derivations over a sample set of strings) means that we cannot use the method used in (8) to compute the expectation matrix and furthermore the limit condition will not be convergent."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We have shown in this paper the conditions under which a given probabilistic TAG can be shown to be consistent.",
        "We gave a simple algorithm for checking consistency and gave the formal justification for its correctness.",
        "The result is practically significant for its applications in checking for deficiency in probabilistic TAGs."
      ]
    }
  ]
}
