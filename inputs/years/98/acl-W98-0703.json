{
  "info": {
    "authors": [
      "Rada Mihalcea",
      "Dan Moldovan"
    ],
    "book": "Workshop on Usage of WordNet in Natural Language Processing Systems",
    "id": "acl-W98-0703",
    "title": "Word Sense Disambiguation Based on Semantic Density",
    "url": "https://aclweb.org/anthology/W98-0703",
    "year": 1998
  },
  "references": [
    "acl-C92-1056",
    "acl-H92-1045",
    "acl-H93-1061",
    "acl-H94-1046",
    "acl-J92-1001",
    "acl-P94-1020",
    "acl-P95-1026",
    "acl-P96-1006",
    "acl-P97-1007",
    "acl-W97-0209",
    "acl-W97-0213"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents a Word Sense Disambiguation method based on the idea of semantic density between words.",
        "The disambiguation is done in the context of WordNet.",
        "The Internet is used as a raw corpora to provide statistical information for word associations.",
        "A metric is introduced and used to measure the semantic density and to rank all possible combinations of the senses of two words.",
        "This method provides a precision of 58% in indicating the correct sense for both words at the same time.",
        "The precision increases as we consider more choices: 70% for top two ranked and 73% for top three ranked."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word Sense Disambiguation (WSD) is an open problem in Natural Language Processing.",
        "Its solution impacts other tasks such as discourse, reference resolution, coherence, inference and others.",
        "WSD methods can be broadly classified into three types:",
        "1.",
        "WSD that make use of the information pro",
        "vided by machine readable dictionaries (Cowie et al.1992), (Miller et al.1994), (Agirre and Rigau, 1995), (Li et al.1995), (NIcRoy, 1992);",
        "2.",
        "WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale, Church et al., 1992), (Ng and Lee, 1996); 3.",
        "WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997).",
        "There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics.",
        "collocations and others (NIcRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau, Asterias et al., 1997).",
        "Statistical methods produce high accuracy results for small number of preselected words.",
        "A lack of widely available semantically tagged corpora almost excludes supervised learning methods.",
        "On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined.",
        "To our knowledge, none of the statistical methods disambiguate adjectives or adverbs so far.",
        "One approach to WSD is to determine the conceptual distance between words, that is to measure the semantic closeness of the words within a semantic network.",
        "Essentially, it is the length of the shortest path connecting the concepts (Rada et al.1989), (Rigau.",
        "Asterias et al., 1997).",
        "By measuring the conceptual distance between words, it is possible to determine the likelihood of word sense associations.",
        "For example, the method proposed in (Li et al.1995) tries to determine the possible sense of a noun associated with a verb using WordNet and a large text.",
        "Based on other occurrences of the verb or semantically related verbs in the text, the possible object is determined by measuring the semantic similarity between the noun objects.",
        "Methods that do not need large corpora are usually based exclusively on MRD.",
        "A proposal in this sense has been made in (Agirre and Rigau.",
        "1995): they measure the conceptual density between nouns, by using WordNet, but the method proposed in their paper cannot be applied to measuring a conceptual distance between a verb and a noun, as no direct links are provided in NIRDs between the nouns and verbs hierarchies.",
        "A WordNet-based method for measuring the semantic similarity between nouns was also proposed in (Richardson et al.. 1994).",
        "Their method consists of using hierarchical concept graphs constructed from WordNet data files, and a semantic similarity formula.",
        "Still, the method does not provide a link between different part-of-speech words."
      ]
    },
    {
      "heading": "2 Our approach",
      "text": [
        "The approach described in this paper is based on the idea of semantic density.",
        "This can be measured by the number of common words that are within a semantic distance of two or more words.",
        "The closer the semantic relationship between two words the higher the semantic density between them.",
        "The way it is defined here, the semantic density works well in the case of uniform MRD.",
        "In reality there are gaps in the knowledge representations and the semantic density can provide only an estimation of the actual semantic relatedness between words.",
        "We introduce the semantic density because it is",
        "relatively easy to measure it on a MRD like WordNet.",
        "This is done by counting the number of concepts two words have in common.",
        "A metric is introduced in this sense which when applied to all possible combinations of the senses of two or more words it ranks them.",
        "Another idea of this paper is to use the Internet as a raw corpora.",
        "Thus we have two sources of information: (1) the Internet for gathering statistics and (2) WordNet for measuring semantic density.",
        "As will be shown below, a ranking of words senses results from each of these two sources.",
        "The issue now is how to combine these two rankings in order to provide an overall ranking.",
        "One possibility is to use them in parallel and the other one is to use them serially.",
        "We have tried both and the serial approach provided better results.",
        "Thus, for a verb - noun pair, the WSD method consists of two Algorithms, the first one ranks the noun senses, of which we retain only the best two senses; and a second Algorithm takes the output produced by the first Algorithm and ranks the pairs of verb - noun senses.",
        "Extensions of this method to other pairs than verb - noun are discussed, and larger windows of more than two words are considered.",
        "An essential aspect of the WSD method presented here is that we provide a raking of possible associations between words instead of a binary yes/no decision for each possible sense combination.",
        "This allows for a controllable precision as other modules may be able to distinguish later the correct sense association from such a small pool.",
        "WordNet is a fine grain MRD and this makes it more difficult to pinpoint the correct sense combination since there are many to choose from and many are semantically close.",
        "For applications such as machine translation, fine grain disambiguation works well but for information extraction and some other applications this is an overkill, and some senses may be lumped together.",
        "A simple sentence or question can usually be briefly described by an action and an object; for example, the main idea from the sentence He has to investigate all the reports can be described by the action-object pair investigate-report.",
        "Even the phrase may be ambiguous by having a poor context, still the results of a search or interface based on such a sentence can be improved if the possible associations between the senses of the verb and the noun are determined.",
        "In WordNet (Miller 1990), the gloss of a verb synset provides a noun-context for that verb, i.e. the possible nouns occurring in the context of that particular verb.",
        "The glosses are used here in the same way a corpus is used.",
        "In order to improve the precision of determining the conceptual density between a verb and a noun, the senses of the noun should be ranked, such as to indicate with a reasonable accuracy the first possible senses that it might have.",
        "The approach we considered for this task is the use of unsupervised statistical methods on large texts.",
        "The larger the collection of texts, the bigger is the probability to provide an accurate ranking of senses.",
        "As the biggest number of texts electronically stored - and thus favoring an automatic processing - is contained on the Web, we thought of using the Internet as a source of corpora for ranking the senses of the words.",
        "This first step of our method takes into consideration verb-noun pairs V â€“ N, and it creates pairs in which the verb remains constant, i.e. V, and the noun is replaced by the words in its similarity lists.",
        "Using WordNet, a similarity list is created for each sense of the noun, and it contains: the words from the noun synset and the words from the noun hy-pernym synset."
      ]
    },
    {
      "heading": "Algorithm 1",
      "text": [
        "Input: untagged verb - noun pair Output: ranking of noun senses Procedure:",
        "1.",
        "Form a similarity list for each noun sense.",
        "Consider, for example, that the noun N has m senses.",
        "This means that N appears in m similarity lists,",
        "where NI, N2, ..., :Vrn represent the different senses of N, and N'(') represents the synonym number s of the sense N' of the noun N as defined in WordNet.",
        "2.",
        "Form verb - noun pairs.",
        "The pairs that may be formed are:",
        "3.",
        "Search the Internet and rank senses.",
        "A search",
        "performed on the Internet for each of these groups will indicate a ranking over the possible senses of the noun N. In our experiments we used (AltaVista) since it is one of the most powerful search engines currently available.",
        "Using the operators provided by AltaVista, the verb-noun groups derived above can be expressed in two query-forms:",
        "(a) (\"V* N's\" OR \"V* /VII)\" OR \"V* N(2)*\" OR ... OR \"V* Art(*.",
        ")*\") (b) ((V* NEAR NI*) OR (V* NEAR NI(1)*) OR (V* NEAR 02)*) OR ... OR (V* NEAR Aroo*))",
        "where the asterisk (*) is used as a wildcard indicating that we want to find all words containing a match for the specified pattern of letters.",
        "Using one of these queries, we can get the number of hits for each sense i of the noun and this provides a ranking of the m senses of the noun as they relate with the verb V. We tested this method for 80 verb-noun pairs extracted from SemCor 1.5 of the Brown corpus.",
        "I Using query form (a) as an input to the search engine, we obtained an accuracy of 83% in providing a ranking over the noun senses, such as the sense indicated in SemCor was one of the first two senses in this classification.",
        "In Table 1, we present a sample of the results we obtained.",
        "The column Result in this table presents the ranking over the noun senses: a 1 in this column means that the sense indicated in SemCor was also indicated by our method: 2 means that the sense indicated in SemCor was in top two of the sense ranking provided by our method; similarly, 3 or 4 indicates that the sense of the noun, as specified in SemCor, was in the top three, respectively four, of this sense ranking.",
        "We wed also the query form (b), but the results we obtained have been proved to be similar; using the operator NEAR, a bigger number of hits is reported, but the sense ranking remains the same.",
        "It is iateresting to observe that even we are creating queries starting with a verb-noun pair, it is 'These verb-noun pairs have been extracted from the file br-a0 .",
        "in ranking the noun senses using the Internet not guaranteed that the search on the web will identify only words linked by such a lexical relation.",
        "We based our idea on the fact that: (1) the noun directly following a verb is highly probable to be an object of the verb (as in the expression \"Verb* Noun*\") and (2) for our method, we are actually interested in determining possible senses of a verb and a noun that can share a common context."
      ]
    },
    {
      "heading": "4 Determining the conceptual",
      "text": []
    },
    {
      "heading": "density between verbs and nouns",
      "text": [
        "A measure of the relatedness between words can be a knowledge source for several decisions in the NLP applications.",
        "The conceptual density between verbs and nouns seems difficult to determine, without large corpora or a without a machine-readable dictionary having semantic links between verbs and nouns.",
        "Such semantic links can be trwed however if we consider the glosses for the verr,-.",
        "which are providing a possible context of a verb."
      ]
    },
    {
      "heading": "Algorithm 2",
      "text": [
        "Input: untagged verb - noun pair and a ranking of noun senses (as determined by Algorithm 1) Output: sense tagged verb - noun pair",
        "Procedure: 1.",
        "Given a verb-noun pair V â€“ N, determine all the possible senses for the verb and the noun, by using WordNet.",
        "Let us denote them by < u1, v2, > and < nt, n2, ..., ni > respectively.",
        "2.",
        "Using the method described in section 3, the senses of the noun are ranked.",
        "Only the first two possible senses indicated by this step will be considered.",
        "3.",
        "For each possible pair ri â€“ nj, the conceptual density is computed as follows:",
        "(a) extract all the glosses from the sub-hierarchy including vi (the rationale of the method used to determine these sub-hierarchies is explained below) (b) Determine the nouns from these glosses.",
        "These constitute the noun-context of the verb.",
        "All these nouns are stored together with the level of the associated verb within the sub-hierarchy of vi.",
        "(c) Determine the nouns from the sub-hierarchy including ni.",
        "(d) Determine the number Cij of common concepts between the nouns obtained at (b) and the nouns obtained at (c).",
        "4.",
        "The most suitable combinations between the senses of the verb and the noun vi â€“ nj are the ones that provide the biggest values for Cif.",
        "In order to determine the sub-hierarchies that should be used for viand ni, we used statistics provided by SemCor, a sense tagged version of the Brown corpus (Francis and Kucera, 1967) (Miller, Leacock et al., 1993), containing 250,000 words.",
        "Each word (noun, verb, adjective, adverb) is included in a synset within a hierarchy.",
        "The tops of these hierarchies denominate the class of the word.",
        "The sense in SemCor for a word W is indicated by the class C of the word W, and the sense of the word within the class C. For example, the SemCor entry:"
      ]
    },
    {
      "heading": "<wf cmd=done pos=NN lemma=investigation wnsn=1 1exsn=1:09:00::>investigation</v0",
      "text": [
        "indicates: word: investigation part of speech: common noun sense in WordNet: I A statistic measure performed on SemCor, indicates the following probabilities for the sense of a word within a class:",
        "As shown in Table 2, the class of the noun indicates with a probability of 85% a correct sense 1 within that class.",
        "Thus, for this algorithm, we consider for a noun the hierarchy including the noun (if the class of the noun ni is C, then the method considers all the nouns from the class C).",
        "This does not work for the verbs, as the probability to indicate a correct sense knowing the class is much smaller (only 60%).",
        "For this reason, and based on the experiments we computed, the sub-hierarchy including a verb vi is determined as follows: (i) consider the hypernym hi of the verb viand (ii) consider the hierarchy having hi as top.",
        "It is necessary to consider a bigger hierarchy then just the one provided by synonyms and direct hy-ponyms, since providing accuracy in a metric computation needs large corpora.",
        "As we replaced the corpora with the glosses, better results are achieved if more glosses are considered.",
        "Still, we do not have to enlarge too much the context, in order not to miss the correct answers."
      ]
    },
    {
      "heading": "Conceptual Density Metric",
      "text": [
        "For determining the conceptual density between a noun ni and a verb vj, the algorithm considers:",
        "â€¢ the list of nouns svk associated with the glosses of the verbs within the hierarchy determined by hi: (svk,wk), where: â€“ hj is the hypernym of vj â€“ wk is the level in this hierarchy â€¢ the list of nouns silt within the class of nâ€¢ (sni)",
        "The common words between these two lists (svk,wk) and (sni) will produce a list of common concepts with the associated weights cd,i < wk >.",
        "The conceptual density between ni and vi is given by the formula:",
        "where:",
        "â€¢ lcdijj is the number of common concepts between the hierarchies of ni and vi â€¢ wk are the weights associated with the nouns from the noun-context of the verb vi â€¢ desci is the total number of words within the hierarchy of noun ni",
        "As the nouns with a big hierarchy tend to indicate a big value for jcdiji, the weighted sum of common concepts has to be normalized in respect with the dimension of the noun hierarchy.",
        "This is estimated as the logarithm of the total number of descendants in the hierarchy (i.e. log(desci)).",
        "We also took into consideration other metrics, like:",
        "(2) The number of common concepts between the noun and verb hierarchies, without considering the weights.",
        "(3) A weighted summation of the common concepts between the noun and verb hierarchies, as indicated in (1), but without a normalization in rapport with the noun hierarchy.",
        "We considered also the metrics indicated in (Agirre and Rigau, 1995).",
        "But after running the program on several examples, the formula indicated in (1) provided the best results.",
        "A possible improvement to the metric (1) is to consider the weights for the levels in the noun hierarchy, in addition to the levels in the verb hierarchy."
      ]
    },
    {
      "heading": "5 An example",
      "text": [
        "Consider as example of a verb-noun pair the phrase revise law.",
        "The verb revise has two possible senses in WordNet 1.5: Sense 1 revise, make revisions in gloss: (revise a thesis, for example) * rewrite, write differently, alter by writing gloss: (\"The student rewrote his thesis\") Sense 2 re tool, revise * reorganize, shake up, organize an The noun law has 7 possible senses Sense 1 law, jurisprudence gloss: (the collection of rules imposed by authority; \"civilization presupposes respect for the law\") * collection, aggregation, accumulation, assemblage gloss: (several things grouped together) Sense 2 law gloss: (one of a set of rules governing a particular activity or a legal document setting forth such a rule; \"there is a law against kidnapping\") * rule, prescript gloss: (prescribed guide for conduct or action) * legal document, legal instrument, official document, instrument Sense 3 law,.",
        "natural law gloss: (a rule or body of rules of conduct inherent in human nature and essential to or binding upon human society) * concept, conception gloss: (an abstract or general idea inferred or derived from specific instances) Sense 4 law, law of nature gloss: (a generalization based on recurring facts or events (in science or mathematics etc): \"the laws of thermodynamics) * concept, conception gloss: (an abstract or general idea inferred or derived from specific instances) Sense 5 jurisprudence, law, legal philosophy gloss: (the branch of philosophy concerned with the law) * philosophy gloss: (the rational investigation of questions about existence and knowledge and ethics) Sense 6 police, police force, constabulary, law gloss: (the force of policemen and officers; \"the law came looking for him\")",
        "law, practice of law gloss: (the learned profession that is mastered by graduate study in a law school and that is responsible for the judicial system; \"he studied law at Yale\") * learned profession gloss: (one of the three professions traditionally believed to require advanced learning and high principles) We searched on Internet, using AltaVista, for all possible pairs V-N that may be created using revise and the words from the similarity lists of law.",
        "Over the seven possible senses for this noun, the first step of our method indicated the following ranking (we indicate the number of hits between parenthe-sis):law#2(2829), latv#3(648), law#4(640), law#6(397), law#1(224), law#5(37), law#7(0).",
        "Thus, only the sense ge2 and #3 of the noun law are eligible to be used for the next algorithm.",
        "For each of the two senses of the verb, we determined the noun-context, including the nouns from the glosses in the sub-hierarchy of the verb, and the associated weights.",
        "For each of the two possible senses of the noun, we determined the nouns from the class of each sense.",
        "In Table 3, we present: (1) the values obtained for the combinations of different senses, i.e. the number of common concepts between the verb and noun hierarchies - fcclijf (columns 2-3); (2) the summations of the weights associated with each noun within the noun-context of the verb vj (columns 4-5); (3) the total number of nouns within the hierarchy of each sense n, i.e. desci (columns 6-7); (4) the conceptual density Cij for each pair ni â€“ vj, derived using the formula presented above (columns 8-9).",
        "In this table: - vi indicates the sense number i of verb revise - ni indicates the sense number i of noun law The biggest value for conceptual density is given by v1 â€“ n,:",
        "This combination of verb-noun senses2 appears in SemCor, file br-a01."
      ]
    },
    {
      "heading": "6 Tests against SemCor",
      "text": [
        "We tested this method by using verb-noun pairs from SemCor.",
        "A randomly selected sample from the entire table with 80 pairs is presented in Table 4.",
        "For each pair verb-noun, we indicate the sense of the verb (column B), the sense of the noun (column C), as they result from SemCor; the total number of possible senses for both the verb (column D) 2 The notation #As/ ri means sense i out of n possible.",
        ":;",
        "these cases, the NEAR operator should be used for the first step of this algorithm).",
        "2.",
        "The number of words considered at a time can be increased, from two to three, four or even more words."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "In this paper, we have presented a method for WSD that is based on measuring the conceptual density between words using WordNet.",
        "The metric proposed may be further improved by considering the weights for verbs as well as for nouns.",
        "The senses of the words are ranked, and an user may select the first choice or the first few choices, depending upon the application.",
        "We have also proposed to use the Internet as a source of statistics on a raw corpora.",
        "The method extends well to considering more than two words at a time,- and also for all parts of speech covered by WordNet.",
        "It is difficult to compare the precision obtained by this method with other methods, since we consider here the collective meaning of two or more words, while most of other methods consider one word at a time.",
        "However, an estimation can be done by extracting the square root of the accuracy for a pair of verb-noun words; and that is 76.15% for the first choice, 83.66% for the first two choices and 85.44% for the first three choices.",
        "Since the disambiguation precision for nouns is usually higher than for verbs, those numbers provide only an average."
      ]
    },
    {
      "heading": "References",
      "text": [
        "Digital Equipment Corporation.",
        "AltaVista Home Page.",
        "URL:hitp://www.altavista.digital.corn.",
        "E. Agirre and G. Rigau, A Proposal for Word Sense Disambiguation using Conceptual Distance, Proceedings of the 1st International Conference on Recent Advances in Natural Language Processing, Velingrad, 1995 R. Bruce and J. Wiebe, Word Sense Disambiguation using Decomposable Models, Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL-94), 139-146, LasCruces, NM, June 1994.",
        "J. Cowie, L. Guthrie and J. Guthrie, Lexical disambiguation using simulated annealing.",
        "Proceedings of the Fifth International Conference on Computational Linguistics COLING-92, 157-161, 1992.",
        "S. Francis and H. Kucera, Computational Analisys of present-day American English, Providence, RI: Brown University Press, 1967 W. Gale, K. Church and D. Yarowsky, One Sense per Discourse, Proceedings of the DARPA Speech and Natural Language Workshop, Harriman, New York.",
        "1992.",
        "X. Li, S. Szpakowicz and S. Matwin.",
        "A WordNet-based algorithm for word semantic sense disambiguation.",
        "Proceedings of the 14th International Joint Conference on Artificial Intelligence IJCAI95, Montreal, Canada, 1995.",
        "and R.G.",
        "Thomas, Using a semantic concordance for sense identification.",
        "Proceedings of the ARPA Human Language Technology Workshop, 240-243, 1994.",
        "H.T.",
        "Ng and H.B.",
        "Lee, Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Examplar-Based Approach, Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL-96), Santa Cruz, 1996.",
        "R. Rada.",
        "H. Mill, E. Bickell and M. Blettner.",
        "Development and Application of a Metric on Semantic Nets.",
        "IEEE Transactions on Systems.",
        "Man and Cybernetics, vol.",
        "19, pp 17-30, Jan/Feb 1989.",
        "P. Resnik, Selectional Preference and Sense Disambiguation, Proceedings of ACL Siglex Workshop on Tagging Text with Lexical Semantics, Why, What and How?, Washington, April 4-5, 1997.",
        "P. Resnik and D. Yarowsky, A Perspective on Word Sense Disambiguation Methods and Their Evaluation.",
        "Proceedings of ACL Siglex Workshop on Tagging Text with Lexical Semantics, Why, What and How?, Washington, April 4-5.",
        "1997.",
        "R. Richardson, A.F.",
        "Smeaton and J. Murphy, Using WordNet as a Knowledge Base for Measuring Semantic Similarity between Words, Technical Report, Working paper CA-1294, School of Computer Applications, Dublin City University.",
        "Dublin, Ireland, 1994.",
        "G. Rigau, J. Atserias and E. Agirre.",
        "Combining Unsupervised Lexical Knowledge Methods for Word Sense Disambiguation.",
        "Computational Linguistics /9704007, 1997.",
        "D. Yarowsky.",
        "Unsupervised Word Sense Disambiguation rivaling Supervised Methods.",
        "Proceedings of the 33rd Association of Computational Linguistics, 1995."
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "This paper describes a probabilistic model that is formed from the integration of an analytical and empirical component.",
        "The analytical component is a Bayesian network derived from WordNet, and the empirical component is composed of compatible probabilistic models formulated from tagged training data.",
        "The components are integrated in a formal, uniform framework based on the semantics of causal dependence.",
        "The paper explores various representational issues that must be addressed when formulating a Bayesian network representation of lexical information such as that expressed in WordNet.",
        "These issues are essential to the design of such a network and they have not been previously explored.",
        "We describe two choices for the representation of lexical items and two choices for the representation lexical relations.",
        "The effect of each combination of choices on evidence propagation in the network is discussed."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "There is a long tradition in Al of resolving interdependent lexical ambiguities through spreading activation, from Quillian's (1968) seminal work on semantic networks, through Hirst's work (1988) on Polaroid words, to more recent work by Voorhees (1993) and Veronis and Ide (1990) on large-scale disambiguation.",
        "This research investigates a probabilistic realization of spreading activation to resolve interdependent word-sense ambiguities.",
        "The core idea is to exploit belief propagation in Bayesian networks: Words are mapped to nodes, lexical relations are mapped to edges, and evidence is propagated from word senses to other related word senses.",
        "The lexical relations are derived from an existing knowledge source, because this information cannot be automatically extracted from training data with existing techniques.",
        "The knowledge source we use is the WordNet is-a hierarchy, i.e., the hypernym/hyponym taxonomy (Miller, 1990).",
        "Although this hierarchy was developed for other purposes, it",
        "has been frequently applied to word-sense disambiguation (Resnik, 1995; Sussna, 1993).",
        "In this work, we investigate various approaches to constructing a Bayesian network representation of the is-a hierarchy for use in word-sense disambiguation.",
        "As this work continues, other relations such as part/whole and entailment relations will also be included in the network.",
        "Another contribution of our work is a novel proposal for integrating symbolic and statistical information for the purpose of performing NLP tasks.",
        "Statistical approaches to word-sense disambiguation have had the most success to date, when evaluated on unseen test data.",
        "The \"analytical\" Bayesian network component of our method is actually built on top of \"empirical\" probabilistic classifiers induced statistically from training data.",
        "In particular, an empirical classifier is induced for each word in the current sentence to be disambiguated (i.e., for each target word).",
        "Each empirical classifier is developed independently of the empirical classifiers for other target words.",
        "A Bayesian network is constructed from the segment of the WordNet is-a hierarchy that is connected to the target words.",
        "The results of the empirical classifiers are fed as evidence into the Bayesian network, thus initiating belief propagation.",
        "All of the information is represented in a formal, uniform framework: a probabilistic model embodying conditional independence relationships among the variables that form the joint distribution.",
        "Conditional independence relationships simplify the formulation of the joint distribution making it possible to work with a large number of variables.",
        "Further, models that characterize conditional independence relationships have desirable computational properties (e.g., see the discussion on decomposable models in (Pearl, 1988)).",
        "These properties form the basis of the evidence propagation scheme used for Bayesian networks discussed in Section 7.",
        "We also make use of these properties in formulating the empirical classifiers as described in (Bruce and Wiebe, 1994).",
        "Bayesian networks are a very rich and complex representational framework.",
        "They support easy integration of diverse information sources and form",
        "the basis for much of the current work on reasoning under uncertainty (Pearl, 1988).",
        "This paper explores the representational issues that must be addressed when mapping the lexical information in WordNet to a Bayesian network.",
        "The implications of the various choices are analyzed in depth.",
        "In section 2, we introduce the basic concepts and illustrate them with an example in section 3, which also includes a brief description of the empirical component.",
        "The Bayesian network representations of lexical items and lexical relations are discussed in sections 4 and 5, respectively.",
        "In section 6, we describe the integration of the empirical component into the Bayesian network The process of sense disambiguation is described in section 7.",
        "Section 8 discusses related work followed by conclusions in section 9."
      ]
    },
    {
      "heading": "2 Bayesian Networks: Background",
      "text": [
        "Bayesian networks model dependencies among nodes through the use of conditional probabilities.",
        "Specifically, if a node (C ause2) is considered as a cause for another node (Symptom 1), then the second node is defined relative to the first (i.e., _ P (SymptomlIC ause2)) .",
        "Some nodes don't have associated causes, so they are just defined via unconditional probabilities (e.g., P(C ause2)).",
        "Taken together, the set of all the conditional and unconditional probabilities determine a joint distribution for all the nodes being modeled (e.g., P (Symptoml, SymptomN, C ousel , ...0 ause M)).",
        "Such global distributions are usually difficult to assess directly; hence, the Bayesian network provides a convenient formalism for specifying the same distribution via local distributions, under conditional independence assumptions.",
        "Furthermore.",
        "without the conditional independence relations, the full joint distribution for cases with hundreds of senses would be infeasible to process â€“ the independence assumptions are key.",
        "Pearl (1988) presents an in-depth coverage of the theory of Bayesian networks and provides an efficient algorithm for evaluating them.",
        "In a Bayesian approach to statistical inference, we distinguish between prior and posterior probabilities.",
        "Prior probabilities express the beliefs that we hold about the likelihood of events prior to being given any evidence, posterior probabilities express our beliefs in the likelihood of events given all the evidence that is currently known.",
        "Thus, the posterior probability of an event changes as new evidence is learned.",
        "The conditional and unconditional probabilities mentioned above are the prior probabilities.",
        "The posterior probabilities are calculated using the Bayesian network propagation algorithm each time new evidence is added.",
        "We discuss propagation in greater detail in Section 7.",
        "Intuitively, the posterior probability of a node, say the node GATHERING# 1 (switching to a word-sense disambiguation example), is a combination of the beliefs received from its children and the beliefs received from its parents.",
        "Once a node has calculated its own belief, it calculates outgoing messages to send to its parents and to its children, which enable them, in turn, to calculate their posterior probabilities.",
        "In this way information is propagated throughout the network."
      ]
    },
    {
      "heading": "3 An Example",
      "text": [
        "In this section, we illustrate how a simple Bayesian network can be constructed to model the interdependencies among words.",
        "This identifies the basic steps in the overall process and helps to motivate the representational issues discussed later.",
        "Suppose that the words \"community\" and \"town\" appear in a single sentence, and that their correct senses in that context are COMMUNITY#1 and TowN#2, respectively.",
        "Our task is to assign the correct word senses to both of them, considering information automatically derived from the corpus and gathered individually for each word, as well as information derived from the WordNet is-a hierarchy and represented in a Bayesian network.",
        "The basic strategy is to add the corpus-derived information to the Bayesian network representations of \"community\" and \"town,\" in such a way that it initiates propagation.",
        "Let us consider this process in more detail.",
        "The words \"community\" and \"town\" have the following senses in WordNet:",
        "community: 1. people living in a particular local area 2. an association of people with similar interests 3. common ownership 4. the body of people in a learned occupation town: 1. an urban area with a fixed boundary that is smaller than a city 2. the people living in a municipality smaller than a city 3. an administrative division of a county",
        "These senses are represented as sets of synonyms, or synsets.",
        "In the is-a hierarchy, each synset is linked to its hypernym, i.e., the synset representing its conceptual parent.",
        "For example, the synset corresponding to (occupation, vocation, occupational group} is the hypernym of the synset corresponding to (profession, community].",
        "A new Bayesian network is created for each sentence.",
        "It includes all of the synsets for the target words in the sentence, together with all of the",
        "synsets reachable from them in the WordNet is-a hierarchy.",
        "Extracting this information from WordNet is straightforward.",
        "Figure 1 illustrates one way that the Bayesian network for the example sentence containing \"town\" and \"community\" can be constructed.",
        "In this representation, each word sense is mapped to a node in the network, and there is an edge from X to Y if word sense X is a hypernym (i.e., a superordinate) of word sense Y (please ignore the octagonal nodes at the bottom for now).",
        "Notice that the relation between o0mmurtrrv#1 and Towx#2 is mediated by GATHERING#1, a type of GROUP#1.",
        "Our goal is for the contextual evidence provided by the empirical classifiers to propagate along this path in such a way that the correct senses of the target words reinforce one another.",
        "After the topology of the network has been established, the conditional probability tables required for each node must be defined.",
        "As will be discussed later in section 5, we can make independence assumptions that make estimating the necessary probabilities more easier.",
        "Next, an empirical classifier is developed for each ambiguous word, in this case, \"town\" and \"community\".",
        "Each classifier defines a probability distribution describing the likelihood of each sense of the targeted word given the automatically derived features of the context.",
        "An example of the type of feature used is the part-of-speech of the word to the right; see (Bruce and Wiebe, 1994) for the other ones we use.",
        "The distributions determined by the empirical classifiers are added as evidence to the Bayesian network, initiating belief propagation.",
        "Once the network reaches equilibrium, the posterior probabilities of the nodes for \"town\" and \"community\" determine the senses assigned to each ambiguous word."
      ]
    },
    {
      "heading": "4 Representing Lexical Items: What",
      "text": []
    },
    {
      "heading": "does a Node mean?",
      "text": [
        "There are two basic approaches to representing WordNet synsets in a Bayesian network.",
        "Since the lexical relations are among synsets and not words, a natural approach is to represent the synsets as nodes.",
        "Alternatively, one node could be used to represent all senses of a word."
      ]
    },
    {
      "heading": "4.1 The One Node Per Word Approach",
      "text": [
        "When nodes correspond to words, the possible values for each node are senses) through senseN, where N is the number of WordNet synsets representing senses of the target word.",
        "Sense() represents the composite of all other meanings, i.e., of all meanings that are not represented by WordNet synsets.",
        "Figure 2 shows the graph for the Bayesian network when word nodes are used for the relations.",
        "It also illustrates the use of logical links, which are described in the next section.",
        "This involves more than just a change in link direction."
      ]
    },
    {
      "heading": "4.2 The One Node Per Sense Approach",
      "text": [
        "Figure 1 illustrates the approach in which each synset (each sense) is mapped to a node.",
        "An important advantage of using the node per sense approach is that it facilitates handling dependencies among the senses of a word.",
        "In the node per word approach, single node cycles are produced when modeling the dependencies of words that have a meaning that is defined in terms of other meanings for that same word.",
        "A disadvantage of this approach is that modeling mutual exclusion among the senses of a single word becomes more difficult.",
        "The most straightforward approach modeling mutual exclusion is to create a dependency from each sense node to a separate node with a CPT enforcing mutual exclusion.",
        "But since the table must have 2N entries, this approach becomes impractical for words with a large number of senses.",
        "To get around this problem, two levels of mutual-exclusion dependencies could be introduced: one at which mutual exclusion among small groups of senses is enforced, and another enforcing mutual exclusion of the groups."
      ]
    },
    {
      "heading": "5 Representing Lexical Relations:",
      "text": []
    },
    {
      "heading": "What does an edge mean?",
      "text": [
        "Here, we address issues concerning the representation of WorciNet is-a relationships as causal dependencies.",
        "The two primary issues to be addressed are: (1) expressing the Hypemym/Hyponym relationship as a causal dependency, and (2) quantifying the causal dependencies with conditional probability distributions."
      ]
    },
    {
      "heading": "5.1 Hypernym-+Hyponym Representations",
      "text": [
        "The Hypernym Hyponym Representation was illustrated above in section 3: there is an edge from node X to node Y if X represents a hypernym of node Y in the WordNet is-a hierarchy.",
        "Consider the node per sense representation (see figure 1).",
        "Suppose Hyper is a synset that is a hypernym of synsets Hypoi Hypok.",
        "Then, the relevant part of the Bayesian network expresses the following: Hyper Hypo' v â€¢ â€¢ V Hypok As such, we are making a closed world assumption.",
        "If, for example, there is a synset ANIMAL#1 with three hyponyms DoG#1, oAT#1, and mousE#1, we are assuming that these three are the only kinds of ANIMAL# 1 's there are.",
        "When using this link representation with either of the node per sense or the node per word representations, the roots of the network are the most superordinate synsets reachable from the target words, and",
        "the target words are typically (but not necessarily) the leafs of the network.",
        "We now turn to defining the CPT.",
        "We discuss this with respect to the node per sense representation (in figure 1) because it is easier to discuss and similar conditional probabilities must be defined under the node per word representation.",
        "To define the CPT for each child node in the Bayesian network, where each child node corresponds to a hyponym node in WordNet, we assign the conditional probability P(hyponymihypernym) to be inversely proportional to the number of children that the hypernym has.",
        "For instance, mUNICIPALITY#1 has two children in WordNet, so we assign the following conditional probability for TowN#1 given this hypemym.",
        "In so doing we are: (1) considering each hyponym of a given hypemym to be equally likely, and (2) maintaining the closed world assumption by requiring that these conditional probabilities sum to one.",
        "In all CPTs, we add a small positive probability e to all zero probability values in order to allow the realization of all possible configurations of node values (e.g., to handle inconsistent evidence).",
        "In future work, we will consider using frequency of occurrence information in tagged training data to define these CPTs.",
        "For the root nodes, which represent the most superordinate concepts, prior probabilities must be specified.",
        "With no evidence to the contrary, uniform prior distributions are assigned to the root nodes; the empirical classifiers are relied upon to provide contextual support (through the leafs of the network)."
      ]
    },
    {
      "heading": "5.2 Hyponym--+Hypernym Representations",
      "text": [
        "Under the Hyponym Hypernym Representation, there is an edge from node X to node Y if X represents a hyponym of node Y in the WordNet is-a hierarchy.",
        "Consider the node per sense representation (see figure 2).",
        "The Bayesian network represents the following:",
        "Under the semantics of the WordNet is-a hierarchy, all instances of a hyponym are instances of its hypernym.",
        "So, a typical CPT for this representation is as follows:",
        "Note that this case is not illustrated in the graphs shown: these only cover two of the four main possibilities.",
        "Interestingly, in this representation, the root nodes represent the target words.",
        "Thus, the root nodes are the sites where evidence from the empirical classifiers is added to then network.",
        "In the absence of this evidence, these nodes take on their prior probabilities.",
        "As above, we assign uniform distributions as the priors.",
        "Recall that, in the case of multiple parents, CPTs must specify the conditional distribution of the child node given the values of all of its parent nodes.",
        "The issues involved in working with multiple parent nodes are discussed below."
      ]
    },
    {
      "heading": "5.3 CPT Entries when Multiple Parents: Causal Independence",
      "text": [
        "If a node has multiple parents, say n parents, then specifying all of the entries in the CPT for that node can be prohibitive.",
        "If no additional independence assumptions are made regarding the interactions among the parent nodes, then the number of probabilities that must be specified is exponential in n, and probabilistic inference is made correspondingly more complex (Heckerman and Breese, 1994).",
        "To overcome this problem, the noisy-OR model (Pearl, 1988) is often adopted.",
        "Under this model, certain independence assumptions are made regarding the interactions among the parent nodes, with the effect that the number of probabilities that must be specified is linear in n. Basically, one need only specify the conditional probabilities of the child and each parent individually.",
        "As presented in (Pearl, 1988), the noisy-OR model assumes that all of the variables are binary.",
        "Heck-erman and Breese (1994) present a generalization of the noisy-OR model, causal independence.",
        "In this model, the parents are assumed to be independent causes for the child.",
        "This allows us to formulate a CPT from the specification of only the following conditional probabilities: P(clpij), where c ranges over the values of the child, and pii ranges over the values of parent Pi.",
        "These values are combined via the constraints of the model to produce the CPT for the child node.",
        "We assign the probabilities using a causal independence model which specializes to the noisy-OR model when applied to binary nodes.",
        "First consider that the inclusive-or connective can be viewed as outputting a true value if none of the inputs is false:",
        "where each 1,i is a logical-valued input variable.",
        "The extension to the case where probabilities are associated with each input is relatively straightforward:",
        "When extending to the general case, the relationship between the value of the child node and the values of its parent nodes is not necessarily defined by a",
        "truth function.",
        "But, the probabilities are assigned analogously:",
        "In their work on plan recognition, Charniak and Goldman (1993) use the noisy-OR model, specifically for representing the dependencies of observed actions on the potential plans that could explain them."
      ]
    },
    {
      "heading": "6 Integrating Empirical and Analytical Information: Virtual Evidence Nodes",
      "text": [
        "Due to space limitations, we consider just one method for integrating the empirical and analytical components.",
        "In this technique, support from the empirical classifiers is added to the Bayesian network using virtual evidence nodes (Pearl, 1988).",
        "The usual way to add evidence to a Bayesian network is to instantiate a node to a particular value (called \"clamping\"); the influence of this evidence is then propagated through the network.",
        "However, that method is not appropriate for our task, because we do not know the sense of any word (so there is no node in the Bayesian network that can be initially instantiated).",
        "Virtual evidence nodes provide a way to specify uncertain evidence, in the form of a distribution over node values (i.e., the probability of each node value).",
        "They are represented by the octagonal nodes in figures 1 and 2.",
        "There is one for each of the target words to be disambiguated.",
        "These nodes represent the support for each sense that was derived from the corpus by the empirical component.",
        "Each virtual evidence node is implemented as a binary-valued node whose parent is the node for which evidence is being provided.",
        "The evidence distribution determines the conditional probability table."
      ]
    },
    {
      "heading": "7 Edge Direction and Belief Propagation",
      "text": [
        "There is a very important implication of the choice between the hypernym hyponym and the hyponym hypernym representations.",
        "In a Bayesian network, suppose that evidence is added to a node (either by clamping or by virtual evidence nodes).",
        "This evidence will propagate to its ancestors in the Bayesian network, and also to the children of its ancestors.",
        "For example, in figure 1, evidence introduced at node SUPPOFtT_COMMUNITY will propagate, among other places, back to COMMUNITY#1, back to CATHERING#1, and then down to MUNICIPALITY#2, and so on.",
        "Thus, this representation, hypernym hyponym, supports the kind of propagation described in this paper.",
        "On the other hand, consider the hyponym hy",
        "representations, the targeted words are the roots of the Bayesian network, so the evidence is added to the roots of the network.",
        "This evidence will not propagate from, say, COMMUNITY#1 to TOWN#2 in figure 1.",
        "Information propagates between such nodes only if evidence were added to their mutual descendents.",
        "As Pearl says, \"evidence gathered at a particular node does not influence any of its spouses until their common child gathers diagnostic support\" ((Pearl, 1988), p. 182).",
        "Thus, if evidence is only added at the virtual evidence nodes in figure 2, evidence will not propagate from COMMUNITY=1 to MUNICIPALITY=2 (SO it will not propagate further to TowN=2).",
        "The corresponding nodes are spouses, but their child (GATHERING) has not received diagnostic support, by which Pearl means evidence prom agated from below.",
        "However, there are many other possibilities for adding evidence to the network, under which desired propagation would occur.",
        "Thus, our discussion of the hyponym hypernym representations is not just a cautionary tale.",
        "For example, one might use Yarowsky's (1992) unsupervised method for assigning words to thesaural categories to add evidence to a node representing a superordinate concept in the WordNet is-a hierarchy.",
        "(Virtual evidence nodes could be used for this purpose too.)",
        "In the hyponym hypemym representations, this superordinate concept (say GATHERING# 1. or SOCIAL-GROUP#1) is a descendent of the nodes representing the targeted words.",
        "It would thus provide the needed diagnostic support to enable propagation from one target word to another.",
        "Note that the hyponym hypernym representation is conceptually appealing, since its semantics is based directly on the semantics of the WordNet is-a hierarchy.",
        "As an illustration, consider applying sample evidence of (.70, .10, .10, .10) for the senses of \"community\" (with no evidence for town).",
        "Table 1 shows the posterior probabilities before and after applying this evidence.",
        "As can be seen, the high evidence for COMMU",
        "NiTY#1 increases the support for the hypemym GATHERING#1 (as well as for the other ancestors in the same path not shown).",
        "However, no support is reaching MUNICIPALITY#2.",
        "If the hypemym -+ hyponym representation is used instead (as in figure 1), an appropriate propagation does take place.",
        "The propagation occurs in two phases.",
        "First, the high evidence for COMMUNITY#1 is propagated \"upstream\" to the hypernym node.",
        "Then, the increased support for this synset is propagated \"downstream\" to increase the likelihood of the value for the appropriate sense of \"town\".",
        "Table 2 shows the posterior probabilities in this case."
      ]
    },
    {
      "heading": "7.1 Attenuation of Spreading Activation",
      "text": [
        "An important aspect of spreading activation approaches is that the strength of the evidence being propagated is attenuated the further the evidence spreads from the original source.",
        "Traditional spreading activation schemes have used various heuristics to model this attenuation, often incorporating a distance factor in terms of number of links.",
        "By using probabilistic propagation, we can account for both length of path and fanout at the nodes along the path (i.e., how many children they have).",
        "The length of the path is taken into account by the propagation algorithm.",
        "Intuitively, when a node calculates its posterior distribution, it calculates a distribution taking into account all possibilities (e.g., gathering# 1 =1, municipality# 2 =1 ; gathering# 1=1, municipality#2=0; and so on).",
        "As the evidence is dispersed among the various possibilities at subsequent nodes, the evidence for any single possibility tends to decrease.",
        "This is so for either edge direction."
      ]
    },
    {
      "heading": "8 Comparison to Related Work",
      "text": [
        "Spreading activation schemes have been common in various forms, starting with Quillian's (Quillian, 1968) work on semantic memory.",
        "Quithan used spreading activation to identify paths between concepts for the purpose of comparison and contrast.",
        "To construct the semantic networks, dictionary definitions were manually encoded in the form a graph.",
        "Hirst (1988) also used spreading activation to perform word-sense disambiguation.",
        "The approach relies on the identification of paths between interdependent word meanings.",
        "To avoid extraneous connections, constraints were introduced; for instance, a limit on path length was introduced, and is-a links were normally not traversed in reverse direction.",
        "Furthermore, heuristics were used to give preference to shorter paths and to avoid connections through nodes with many outgoing arcs.",
        "There have been several approaches that have relied upon word-overlap in dictionary definitions to resolve word-sense ambiguities in context, starting with (Lesk, 1986).",
        "Cowie et al.",
        "(1992) extend the idea by using simulated annealing to optimize a configuration of word senses simultaneously in terms of degree of word overlap.",
        "Veronis and Ide (1990) developed a neural network model to overcome the limitation of addressing only pairwise dependencies in word-overlap approaches.",
        "Using dictionary definitions, they constructed a network containing links from each word node to the nodes for each of its senses and links from each of the sense nodes to the nodes of the words used in the definition.",
        "Sussna (1993) produces a semantic network based on several different WordNet relations.",
        "His disambiguation method minimizes the pairwise distance among senses via a weighting scheme that accounts for both fanout and depth in the hierarchy.",
        "Of the approaches we have surveyed, his is most similar to our analytical component.",
        "Voorhees (1993) describes an unsupervised approach that exploits the WordNet hypernym taxonomy.",
        "In particular, the hierarchy for a given word is automatically partitioned so that the words occurring in the synsets of a partition (or hood) only occur with one of the senses for the word.",
        "Disambiguation is based on the selecting the hood which has the highest estimated relative frequency for the context relative to training text.",
        "Resnik (1995) also describes an unsupervised approach that is based on estimating synset frequencies.",
        "As with Voorhees, the estimated frequency of a synset is based on the frequency of the word plus the frequencies of all its descendant synsets in a large corpus.",
        "Therefore, the top-level synsets have the highest frequencies and thus the highest estimated frequency of occurrence.",
        "For each pair of nouns from the text to be disambiguated, the most-informative-subsumer is determined by finding the common ancestor with the highest information content, where information content is inversely related to frequency.",
        "Then each noun is disambiguated by selecting the synset that receives the most support (i.e., information content) from the all of the most-informative",
        "subsumers.",
        "Eizirik et al.",
        "(1993) also describe a Bayesian network model for word-sense disambiguation, which includes syntactic disambiguation as well as lexical information.",
        "However, their networks are not automatically constructed."
      ]
    },
    {
      "heading": "9 Conclusion",
      "text": [
        "This paper explores various representational issues that must be addressed when formulating a Bayesian network representation of lexical information such as is expressed in WordNet.",
        "We describe two choices for the representation of lexical items and two choices for the representation lexical relations.",
        "The effects on evidence propagation in the network is also discussed."
      ]
    }
  ]
}
