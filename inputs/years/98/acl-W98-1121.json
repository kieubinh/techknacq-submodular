{
  "info": {
    "authors": [
      "Peter A. Heeman"
    ],
    "book": "Workshop on Very Large Corpora",
    "id": "acl-W98-1121",
    "title": "POS Tagging Versus Classes in Language Modeling",
    "url": "https://aclweb.org/anthology/W98-1121",
    "year": 1998
  },
  "references": [
    "acl-H89-2027",
    "acl-H92-1026",
    "acl-J92-4003",
    "acl-J93-2004",
    "acl-P92-1008",
    "acl-P97-1033"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Language models for speech recognition concentrate solely on recognizing the words that were spoken.",
        "In this paper, we advocate redefining the speech recognition problem so that its goal is to find both the best sequence of words and their POS tags, and thus incorporate POS tagging.",
        "The use of POS tags allows more sophisticated generalizations than are afforded by using a class-based approach.",
        "Furthermore, if we want to incorporate speech repair and intonational phrase modeling into the language model, using POS tags rather than classes gives better performance in this task."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "For recognizing spontaneous speech, the acoustic signal is to weak to narrow down the number of word candidates.",
        "Hence, speech recognizers employ a language model that prunes out acoustic alternatives by taking into account the previous words that were recognized.",
        "In doing this, the speech recognition problem is viewed as finding the most likely word sequence W given the acoustic signal (Jelinek, 1985).",
        "Since Pr(A) is independent of the choice of W, we simplify the above as follows.",
        "= arg max Pr(A1W) Pr(W) (3) The first term, Pr(AjW), is the acoustic model and the second term, Pr(W), is the langiiage model, which assigns a probability to the sequence of words W. We can rewrite W explicitly as a sequence of words WI W2W3 WN, where N is the number of words in the sequence.",
        "For expository ease, we use the notation Wia to refer to the sequence of words Wi to W3.",
        "We now use the definition of conditional probabilities to rewrite Pr(WI,N) as follows.",
        "To estimate the probability distribution, a training corpus is typically used from which the probabilities can be estimated using relative frequencies.",
        "Due to sparseness of data one must define equivalence classes amongst the contexts Wâ€œ..1, which can be done by limiting the context to an n-gram language model (Jelinek, 1985).",
        "One can also mix in smaller size language models when there is not enough data to support the larger context by using either interpolated estimation (Jelinek and Mercer, 1980) or a backoff approach (Katz, 1987).",
        "A way of measuring the effectiveness of the estimated probability distribution is to measure the perplexity that it assigns to a test corpus (Bahl et al., 1977).",
        "Perplexity is an estimate of how well the language model is able to predict the next word of a test corpus in terms of the number of alternatives that need to be considered at each point.",
        "The perplexity of a test set wi,N is calculated as 211, where H is the entropy, which is defined as follows."
      ]
    },
    {
      "heading": "1.1 Class-based Language Models",
      "text": [
        "The choice of equivalence classes for a language model need not be the previous words.",
        "Words can be grouped into classes, and these classes can be used as the basis of the equivalence classes of the context rather than the word identities (Jelinek, 1985).",
        "Below we give the equation usually used for a class-based trigram model, where the function g maps each word to its unambiguous class.",
        "Pr(W,Ig(IK)) Pr(g(Wi)Ig(VV,..0g(IV,_2)) Using classes has the potential of reducing the problem of sparseness of data by allowing generalizations",
        "over similar words, as well as reducing the size of the language model.",
        "To determine the word classes, one can use the algorithm of Brown et al.",
        "(1992), which finds the classes that give high mutual information between the classes of adjacent words.",
        "In other words, for each bigram wi4w2 in a training corpus, choose the classes such that the classes for adjacent words g(wi.4) and g(w,) lose as little information about each other as possible.",
        "Brown et al.",
        "give a greedy algorithm for finding the classes.",
        "They start with each word in a separate class and iteratively combine classes that lead to the smallest decrease in mutual information between adjacent words.",
        "Kneser and Ney (1993) found that a class-based language model results in a perplexity improvement for the LOB corpus from 541 for a word-based bigram model to 478 for a class-based bigram model.",
        "Interpolating the word-based and class-based models resulted in an improvement to 439."
      ]
    },
    {
      "heading": "1.2 POS-Based Models",
      "text": [
        "One can also use POS tags, which capture the syntactic role of each word, as the basis of the equivalence classes (Jelinek, 1985).",
        "Consider the sequence of words \"hello can I help you\".",
        "Here, \"hello\" is being used as an acknowledgment, \"can\" as a modal verb, \"I\" as a pronoun, \"help\" as an un-tensed verb, and \"you\" as a pronoun.",
        "To use POS tags in language modeling, the typical approach is to sum over all of the POS possibilities.",
        "Below, we give the derivation based on using trigrams.",
        "The above approach for incorporating POS information into a language model has not been of much success in improving speech recognition performance.",
        "Srinivas (1996) reports that sucli a model results in a 24.5% increase in perplexity over a word-based model on the Wall Street Journal; Niesler and Woodland (1996) report an 11.3% increase (but a 22-fold decrease in the number of parameters of such a model) for the LOB corpus; and ICneser and Ney (1993) report a 3% increase on the LOB corpus.",
        "The POS tags remove too much of the lexical information that is necessary for predicting the next word.",
        "Only by interpolating it with a word-based model is an improvement seen (Jelinek, 1985).",
        "In the rest of the paper, we first describe the annotations of the Trains corpus.",
        "We next present our POS-based language model and contrast its performance with a class-based model.",
        "We then augment these models to account for speech repairs and intonational phrase, and show that the POS-based one performs better than the class-based one for modeling speech repairs and intonational phrases."
      ]
    },
    {
      "heading": "2 The Trains Corpus",
      "text": [
        "As part of the TRAINS project (Allen et al., 1995), a long term research project to build a conversationally proficient planning assistant, we collected a corpus of problem solving dialogs (Heeman and Allen, 1995).",
        "The dialogs involve two human participants, one who is playing the role of a user and has a certain task to accomplish, and another who is playing the role of a planning assistant.",
        "The collection methodology was designed to make the setting as close to human-computer interaction as possible, but was not a wizard scenario, where one person pretends to be a computer.",
        "Table 1 gives information about the corpus."
      ]
    },
    {
      "heading": "2.1 POS Annotations",
      "text": [
        "Our POS tagset is based on the Penn Treebank tagset (Marcus et al., 1993), but modified to include tags for discourse markers and end-of-turns, and to provide richer syntactic information (Heeman, 1997).",
        "Table 2 lists our tagset with differences from the Penn tagset marked in bold.",
        "Contractions are annotated using 'A' to conjoin the tag for each part; for instance, \"can't\" is annotated as '111DARB'."
      ]
    },
    {
      "heading": "2.2 Speech Repair Annotations",
      "text": [
        "Speech repairs occur where the speaker goes back and changes or repeats what was just said (Heeman, 1997), as illustrated by the following.",
        "Speech repairs have three parts (some of which are optional): the reparandum, which are the words.",
        "the speaker wants to replace, an editing term, which helps mark the repair, and the alteration, which is the replacement of the reparandum.",
        "The end of the reparandum is referred to as the interruption point.",
        "For annotating speech repairs, we have extended the scheme proposed by Bear et aL (1992) so that it better deals with overlapping and ambiguous repairs.",
        "Like their scheme, ours allows the annotator to capture the word correspondences that exist between the reparandum and the alteration.",
        "Below, we illustrate how a speech repair is annotated.",
        "In this example, the reparandum is \"engine two from Elmi(ra)-\", the editing term is \"or\", and the alteration is \"engine three from Elmira\".",
        "The word matches on \"engine\" and \"from\" are annotated with 'm' and the word replacement of \"two\" by \"three\" is annotated with 'r'.",
        "Example 2 (d93-15.2 utt42) engine two from Elmi(ra)- or engine three from Elmira ml r2 m3 m4 t et ml r2 m3 m4 ip:mod+"
      ]
    },
    {
      "heading": "2.3 Intonation Annotations",
      "text": [
        "Speakers break up their speech into intonational phrases.",
        "This segmentation serves a similar purpose as punctuation does in written speech.",
        "The ToBI annotation scheme (Silverman et al., 1992) involves labeling the accented words, intermediate phrases and intonational phrases with high and low accents.",
        "Since we are currently only interested in the intonational phrase segmentation, we only label the intonational phrase endings."
      ]
    },
    {
      "heading": "3 POS-Based Language Model",
      "text": [
        "In this section, we present an alternative formulation for using POS tags in a statistical language model.",
        "Here, POS tags are viewed as part of the output of the speech recognizer, rather than intermediate objects (Heeman and Allen, 1997a; Heeman, 1997)."
      ]
    },
    {
      "heading": "3.1 Redefining the Recognition Problem",
      "text": [
        "To add PUS tags into the language model, we refrain from simply summing over all PUS sequences as illustrated in Section 1.2.",
        "Instead, we redefine the speech recognition problem so that it finds the best word and PUS sequence.",
        "Let P be a PUS sequence for the word sequence W. The goal of the speech recognizer is to now solve the following.",
        "The first term Pr(AIWP) is the acoustic model, which traditionally excludes the category assignment.",
        "In fact, the acoustic model can probably be reasonably approximated by Pr(AIW).",
        "The second term Pr(WP) is the POS-based language model and this accounts for both the sequence of words and the POS assignment for those words.",
        "We rewrite the sequence WP explicitly in terms of the N words and their corresponding POS tags, thus giving us the sequence WI,NPI,N.",
        "The probability Pr(Wi,NPLN) forms the basis for POS taggers, with the exception that PUS taggers work from a sequence of given words.",
        "As in Equation 4, we rewrite the probabillity Pr(1471,NPI,N) as follows using the definition of conditional probability.",
        "Equation 8 involves two probability distributions that need to be estimated.",
        "Previous attempts at using POS tags in a language model as well as POS taggers (i.e. (Chamiak et al., 1993)) simplify these probability distributions, as given in Equations 9 and 10.",
        "However, to successfully incorporate POS information, we need to account for the full richness of the probability distributions.",
        "Hence, we cannot use these two assumptions when learning the probability distributions."
      ]
    },
    {
      "heading": "3.2 Estimating the Probabilities",
      "text": [
        "To estimate the probability distributions, we follow the approach of Bahl et al.",
        "(1989) and use a decision tree learning algorithm (Breiman et al., 1984) to partition the context into equivalence classes.",
        "The algorithm starts with a single node.",
        "It then finds a question to ask about the node in order to partition the node into two leaves, each being more informative as to which event occurred than the parent node.",
        "Information theoretic metrics, such as minimizing entropy, are used to decide which question to propose.",
        "The proposed question is then verified using heldout data: if the split does not lead to a decrease in entropy according to the heldout data, the split is rejected and the node is not further explored (Bahl et al., 1989).",
        "This process continues with the new leaves and results in a hierarchical partitioning of the context.",
        "After growing a tree, the next step is to use the partitioning of the context induced by the decision tree to determine the probability estimates.",
        "Using the relative frequencies in each node will be biased towards the training data that was used in choosing the questions.",
        "Hence, Bahl et al.",
        "smooth these probabilities with the probabilities of the parent node using interpolated estimation with a second heldout dataset.",
        "Using the decision tree algorithm to estimate probabilities is attractive since the algorithm can choose which parts of the context are relevant, and in what order.",
        "Hence, this approach lends itself more readily to allowing extra contextual information to be included, such as both the word identifies and POS tags, and even hierarchical clusterings of them.",
        "If the extra information is not relevant, it will not be used.",
        "The approach of using decision trees will become even more critical in the next two sections where the probability distributions will be conditioned on even richer context.",
        "One important aspects of using a decision tree algorithm is the form of the questions that it is allowed to ask.",
        "We allow two basic types of information to be used as part of the context: numeric and categorical.",
        "For a numeric variable N, the decision tree searches for questions of the form 'is N >= n', where it is a numeric constant.",
        "For a categorical variable C, it searches over questions of the form 'is C E S' where S is a subset of the possible values of C. We also allow restricted boolean combinations of elementary questions (Bahl et al., 1989)."
      ]
    },
    {
      "heading": "3.2.2 Questions about POS Tags",
      "text": [
        "The context that we use for estimating the probabilities includes both word identities and POS tags.",
        "To make effective use of this information, we need to allow the decision tree algorithm to generalize between words and POS tags that behave similarly.",
        "To learn which words behave similarly, Black et al.",
        "(1989) and Magerrnan (1994) used the clustering algorithm of Brown et al.",
        "(1992) to build a hierarchical classification tree.",
        "Figure 1 gives the classification tree that we built for the POS tags.",
        "The algorithm starts with each token in a separate class and iteratively finds two classes to merge that results in the smallest lost of information about POS adjacency.",
        "Rather than stopping at a certain number of classes, one continues until only a single class remains.",
        "However, the order in which classes were merged gives a hierarchical binary tree with the root corresponding to the entire tagset, each leaf to a single POS tag, and intermediate nodes to groupings of tags that are statistically similar.",
        "The path from the root to a tag gives the binary encoding for the tag.",
        "The decision tree algorithm can ask which partition a word belongs to by asking questions about the binary encoding."
      ]
    },
    {
      "heading": "3.2.3 Questions about Word Identities",
      "text": [
        "For handling word identities, one could follow the approach used for handling the POS tags (e.g. (Black et al., 1992; Magerman, 1994)) and view the POS tags and word identities as two separate sources of information.",
        "Instead, we view the word identities as a further refinement of the POS tags.",
        "We start the clustering algorithm with a separate class for each word and each POS tag that it takes on and only allow it to merge classes if the POS tags are the same.",
        "This results in a word classification tree for each POS tag.",
        "Building a word classification tree for each POS tag means that the tree will not be polluted by words that are ambiguous as to their POS tag, as exemplified by the word \"loads\", which is used in the Trains corpus as both a third-person present tense verb 'VIM and as a plural noun NNS.",
        "Furthermore, building a-tree for each POS tag simplifies the task because the hand annotations of the POS tags resolve a lot of the difficulty that the algorithm would otherwise have to handle.",
        "This allows effective trees to be built even when only a small amount of data is available.",
        "sonal pronouns (PRP).",
        "For reference, we list the number of occurrences of each word.",
        "Notice that the algorithm distinguished between the subjective pronouns T, 'we', and 'they', and the objective pronouns 'me', `us' and `them'.",
        "The pronouns `you' and 'it' take both cases and were probably clustered according to their most common usage in the corpus.",
        "Although we could have added extra POS tags to distinguish between these two types of pronouns, it seems that the clustering algorithm can make up for some of the shortcomings of the POS tagset.",
        "The class low is used to group singleton words."
      ]
    },
    {
      "heading": "3.3 Results",
      "text": [
        "Before giving a comparison between our POS-based model and a class-based model, we first describe the experimental setup and define the perplexity measures that we use to measure the performance.",
        "To make the best use of our limited data, we used a sixfold cross-validation procedure: each sixth of the data was tested using a model built from the remaining data.",
        "Changes in speaker are marked in the word transcription with the special token <turn>.",
        "We treat contractions, such as \"that'll\" and \"gonna\", as separate words, treating them as \"that\" and -1r for the first example, and \"going\" and \"ta\" for the second.I We also changed all word fragments into the token <fragment>.",
        "Since current speech recognition rates for spontaneous speech are quite low, we have run the experiments on the hand-collected transcripts.",
        "In searching for the best sequence of POS tags for the transcribed words, we follow the technique proposed by Chow and Schwartz (1989) and only keep a small number of alternative paths by pruning the low probability paths after processing each word."
      ]
    },
    {
      "heading": "3.3.2 Branching Perplexity",
      "text": [
        "Our POS-based model is not only predicting the next word, but its POS tag as well.",
        "To estimate ISee Heeman and Damnati (1997) for how to treat contractions as separate words in a speech recognizer.",
        ".",
        "the branching factor, and thus the size of the search space, we use the following formula for the entropy, where di is the POS tag for word w1.",
        "In order to compare a POS-based model against a traditional language model, we should not penalize the POS-based model for incorrect POS tags, and hence we should ignore them when defining the perplexity.",
        "Just as with a traditional model, we base the Perplexity measure on Pr(wilwi,i-i).",
        "However, for our model, this probability is not estimated.",
        "Hence, we must rewrite it in terms of the probabilities that we do estimate.",
        "To do this, our only recourse is to sum over all possible POS sequences."
      ]
    },
    {
      "heading": "3.3.4 Using Richer Context",
      "text": [
        "Table 3 shows the effect of varying the richness of the information that the decision tree algorithm is allowed to use in estimating the POS and word probabilities.",
        "The second column uses the approximations given in Equation 9 and 10.",
        "The third column gives the results using the full context.",
        "The results show that adding the extra context has the biggest effect on the perplexity measures, decreasing the word perplexity from 43.22 to 24.04, a reduction of 44.4%.",
        "The effect on POS tagging is less pronounced, but still gives an error rate reduction of 3.8%.",
        "Hence, to use POS tags during speech recognition, one must use a richer context for estimating the probabilities than what is typically used."
      ]
    },
    {
      "heading": "3.3.5 Class-Based Decision-Tree Models",
      "text": [
        "In this section, we compare the POS-based model against a class-based model.",
        "To make the comparison as focused as possible, we use the same methodology for estimating the probability distributions as we used for the POS-based model.",
        "The classes were obtained from the word clustering algorithm, but stopping once a certain number of classes has been reached.",
        "Unfortunately, the clustering algorithm of Brown et al.",
        "does not have a mechanism to decide an optimal number of word classes (cf. (Kneser and Ney, 1993)).",
        "Hence, to give an optimal evaluation of the class-based approach, we choose the number of classes that gives the best perplexity results, which was 100 classes.",
        "We then built word classification trees, just as we did for the POS-based approach, where words from different classes are not allowed to be merged.",
        "The resulting class-based model achieved a perplexity of 25.24 in comparison to 24.04 for the POS-based model.",
        "This improvement is due to two factors.",
        "First, tracking the syntactic role of each word gives valuable information for predicting the subsequent words.",
        "Second, the classification trees for the POS-based approach, which the decision tree algorithm uses to determine the equivalence classes, are of higher quality.",
        "This is due to the POS-based classification trees using the hand-annotated POS information, since they take advantage of the hand-coded knowledge present in the POS tags and are not polluted by words that take on more than one syntactic role."
      ]
    },
    {
      "heading": "3.3.6 Preliminary Wall Street Journal Results",
      "text": [
        "For building a system that partakes in dialogue, read-speech corpora, such as the Wall Street Journal, are not appropriate.",
        "However, to make our results more comparable to the literature, we have done preliminary tests on the Wall Street Journal corpus in the Penn Treebank, which has POS annotations.",
        "This corpus has a significantly larger vocabulary size (55800 words) than the Trains corpus.",
        "Our current algorithm for clustering the words takes space in proportion to the square of the number of unique word/POS combinations (minus any that get grouped into the low occurring class).",
        "More work is needed to handle larger vocabulary sizes.",
        "Using 78,800 words of data, with a vocabulary size of 9711, we achieved a perplexity of 250.75 on the known words in comparison to a trigram word-based backoff model (Katz, 1987) built with the CMU toolkit (Rosenfeld, 1995), which achieved a perplexity of 296.43.",
        "More work is needed to see if these results scale up to larger vocabulary and training data sizes."
      ]
    },
    {
      "heading": "4 Adding Repairs and Phrasing",
      "text": [
        "Just as we redefined the speech recognition problem so as to account for POS tagging, we do the same for modeling intonational phrases and speech",
        "repairs.",
        "We introduce null tokens between each pair of words and wi (Heeman and Allen, 1997b), which will be tagged as to the occurrence of these events.",
        "The variable Ti indicates if word w1..1 ends an intonational phrase (7;=%), or not (Tr=null).",
        "For detecting speech repairs, we have the problem that repairs are often accompanied by an editing term, such as -um\", \"uh\", \"okay\", or \"well\", and these must be identified as such.",
        "Furthermore, an editing term might be composed of a number of words, such as \"let's see\" or uh well\".",
        "Hence we use two tags: an editing term tag Ei and a repair tag Ri.",
        "The editing term tag indicates if wi starts an editing term (Ei=Push), if wi continues an editing term (E,=ET), if wi-1 ends an editing term (Ei=Pop), or otherwise (E2=null).",
        "The repair tag R, indicates whether word wi is the onset of the alteration of a fresh start (Ri=C), a modification repair (Ri=M), or an abridged repair (R:=A), or there is no repair (Rt=null).",
        "Note that for repairs with an editing term, the repair is tagged after the extent of the editing term has been determined.",
        "Below, we give an example showing all non-null tone, editing term and repair tags.",
        "Example 3 (d93-18.1 utt47) it takes one Push you ET know Pop M two hours % If a modification repair or fresh start occurs, we need to determine the extent (or the onset) of the reparandum, which we refer to as correcting the speech repair.",
        "Often, speech repairs have strong word correspondences between the reparandum and alteration, involving word matches and word replacements.",
        "Hence, knowing the extent of the reparandum means that we can use the reparandum to predict the words (and their POS tags) that make up the alteration.",
        "In our full model, we add three variables to account for the correction of speech repairs (Heeman and Allen, 1997b; Heeman, 1997).",
        "We also add an extra variable to account for silences between words.",
        "After a silence has occurred, we can use the silence to better predict whether an intonational boundary or speech repair has just occurred.",
        "Below we give the redefinition of the speech recognition problem (without speech repair correction and silence information).",
        "The speech recognition problem is redefined so that its goal is to find the maximal assignment for the words as well as the POS, intonational, and repair tags.",
        "Just as we did in Equation 8, we rewrite the above in terms of five probability distributions, each of which need to be estimated.",
        "The context for each of the probability distributions includes all of the previous context.",
        "In principal, we could give all of this context to the decision tree algorithm and let it decide what information is relevant in constructing equivalence classes of the contexts.",
        "However, the amount of training data is limited (as are the learning techniques) and so we need to encode the context in order to simplify the task of constructing meaningful equivalence classes.",
        "Hence we restructure the context to take into account the speech repairs and boundary tones (Heeman, 1997)."
      ]
    },
    {
      "heading": "4.1 Results",
      "text": [
        "We now contrast the performance of augmenting the POS-based model with speech repair and intonational modeling versus augmenting the class-based model.",
        "Just as in Section 3, all results were obtained using a sixfold cross-validation procedure from the the hand-collected transcripts.",
        "We ran these transcripts through a word-aligner (Ent, 1994), a speech recognizer constrained to recognize what was transcribed, in order to automatically obtain silence durations.",
        "In predicting the end of turn marker <turn>, we do not use any silence information."
      ]
    },
    {
      "heading": "4.1.1 Recall and Precision",
      "text": [
        "We report results on identifying speech repairs and intonational phrases in terms of recall, precision and error rate.",
        "The recall rate is the number of times that the algorithm correctly identifies an event over the total number of times that it actually occurred.",
        "The precision rate is the number of times the algorithm correctly identifies it over the total number of times it identifies it.",
        "The error rate is the number of errors in identifying an event over the number of times that the event occurred."
      ]
    },
    {
      "heading": "4.1.2 POS Tagging and Perplexity",
      "text": [
        "The first set of experiments, whose results are given in Table 4, explore how POS tagging and word perplexity benefit from modeling boundary tones and speech repairs.",
        "The second column gives the results of the POS-based language model, introduced in Section 3.",
        "The third column adds in speech repair detection and correction, boundary tone identification, and makes use of silence information in detecting speech repairs and boundary tones.",
        "We see that this results in a perplexity reduction of 7.0%, and a POS error reduction of 8.1%.",
        "As we further improve the modeling of the user's utterance, we",
        "expect to see further improvements in the language model.",
        "Of course, there is a penalty to pay in terms of increased search space size, as the increase in the branching perplexity shows.",
        "In Table 5, we demonstrate that modeling intonational phrases benefits from modeling POS tags.",
        "Column two gives the results of augmenting the class-based model of Section 3.3.5 with intonational phrase modeling and column three gives the results of augmenting the POS-based model.",
        "Contrasting the results in column two with those in column three, we see that using the POS-based model results in a reduction in the error rate of 17.2% over the class-based model.",
        "Hence, we see that modeling the POS tags allows much better modeling of intonational phrases than can be achieved with a class-based model.",
        "The fourth column reports the results using the full model, which accounts for interactions with speech repairs and the benefit of using silence information (Heeman and Allen, 1997b)."
      ]
    },
    {
      "heading": "4.1.4 Detecting Speech Repairs",
      "text": [
        "In Table 6, we demonstrate that modeling the detection of speech repairs (and editing terms) benefits from modeling POS tags.",
        "In the results below, we ignore errors that are the result of improperly identifying the type of repair, and hence score a repair as correctly detected as long as it-was identified as either an abridged repair, modification repair or fresh start.",
        "Column two gives the results of augmenting the class-based model of Section 3.3.5 with speech repair modeling and column three gives the results of augmenting the POS-based model.",
        "In terms of overall detection, the POS-based model reduces the error rate from 52.0% to 46.2%, a reduction of 11.2%.",
        "This shows that speech repair detection profits from being able to make use of syntactic generalizations, which are not available from a class-based approach.",
        "The final column gives the results of the full model, which accounts for interactions with speech repair correction and intonational phrasing, and uses silence information."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we presented a POS-based language model.",
        "Unlike previous approaches that use POS tags in language modeling, we redefine the speech recognition problem so that it includes finding the best word sequence and best POS tag interpretation for those words.",
        "Thus this work can be seen as a first-step towards tightening the integration between speech recognition and natural language processing.",
        "In order to make use of the POS tags, we use a decision tree algorithm to learn the probability distributions, and a clustering algorithm to build hierarchical partitionings of the POS tags and the word identities.",
        "Furthermore, we take advantage of the POS tags in building the word classification trees and in estimating the word probabilities, which both results in better performance and significantly speeds up the training procedure.",
        "We find that using the rich context afforded by decision tree results in a perplexity reduction of 44.4%.",
        "We also find that the POS-based model gives a 4.2% reduction in perplexity over a class-based model, also built with the decision tree and clustering algorithms.",
        "Preliminary results on the Wall Street Journal corpus are also encouraging.",
        "Hence, using a POS-based model results in an improved language model as well as accomplishes the first part of the task in linguistic understanding.",
        "We also see that using POS tags in the language model aids in the identification of boundary tones and speech repairs, which we have also incorporated into the model by further redefining the speech recognition problem.",
        "The POS tags allow these two",
        "processes to generalize about the syntactic role that words are playing in the utterance rather than using crude class-based approaches which does not distinguish this information.",
        "We also see that modeling these phenomena improves the POS tagging results as well as the word perplexity."
      ]
    },
    {
      "heading": "6 Acknowledgments",
      "text": [
        "We wish to thank Geraldine Damnati.",
        "The research involved in this paper was done while the first author was visiting at CNET, France Telecom."
      ]
    }
  ]
}
