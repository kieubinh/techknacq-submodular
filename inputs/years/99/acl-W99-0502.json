{
  "info": {
    "authors": [
      "Hwee Tou Ng",
      "Daniel Chung Yong Lim",
      "Shou King Foo"
    ],
    "book": "SIGLEX Workshop on Standardizing Lexical Resources",
    "id": "acl-W99-0502",
    "title": "A Case Study on Inter-Annotator Agreement for Word Sense Disambiguation",
    "url": "https://aclweb.org/anthology/W99-0502",
    "year": 1999
  },
  "references": [
    "acl-H93-1061",
    "acl-J93-2004",
    "acl-J96-2004",
    "acl-P96-1006",
    "acl-W97-0323"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "There is a general concern within the field of word sense disambiguation about the inter-annotator agreement between human annotators In this paper, we examine this issue by comparing the agreement rate on a large corpus of more than 30,000 sense-tagged instances This corpus is the intersection of the WORDNET Semcor corpus and the DSO corpus, which has been independently tagged by two separate groups of human annotators The contribution of this paper is twofold First, it presents a greedy search algorithm that can automatically derive coarser sense classes based on the sense tags assigned by two human annotators The resulting derived coarse sense classes achieve a higher agreement rate but we still maintain as many of the original sense classes as possible Second, the coarse sense grouping derived by the algorithm, upon verification by human, can potentially serve as a better sense inventory for evaluating automated word sense disambiguation algorithms Moreover, we examined the derived coarse sense classes and found some interesting groupings of word senses that correspond to human intuitive judgment of sense granularity"
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "It is widely acknowledged that word sense disambiguation (WSD) is a central problem in natural language processing In order for computers to be able to understand and process natural language beyond simple keyword matching, the problem of disambiguating word sense, or discerning the meaning of a word in context, must be effectively dealt with Advances in WSD will have significant impact on applications like information retrieval and machine translation For natural language subtasks like part-of-speech tagging or syntactic parsing, there are relatively well defined and agreed-upon criteria of what it means to have the \"correct\" part of speech or syntactic structure assigned to a word or sentence For instance, the Penn Treebank corpus (Marcus et al., 1993) pro-'ides a large repository of texts annotated with part-of-speech and syntactic structure information Two independent human annotators can achieve a high rate of agreement on assigning part-of-speech tags to words in a given sentence Unfortunately, this is not the case for word sense assignment Firstly, it is rarely the case that any two dictionaries will have the same set of sense definitions for a given word Different dictionaries tend to carve up the \"semantic space\" in a different way, so to speak Secondly, the list of senses for a word in a typical dictionary tend to be rather refined and comprehensive This is especially so for the commonly used words which have a large number of senses The sense distinction between the different senses for a commonly used word in a dictionary like WORDNET (Miller, 1990) tend to be rather fine Hence, two human annotators may genuinely disagree in their sense assignment to a word in context The agreement rate between human annotators on word sense assignment is an important concern for the evaluation of WSD algorithms One would prefer to define a disambiguation task for which there is reasonably high agreement between human annotators The agreement rate between human annotators will then form the upper ceiling against which to compare the performance of WSD algorithms For instance, the SENSEVAL exercise has performed a detailed study to find out the inter-annotator agreement among its lexicographers tagging the word senses (Kilgarriff, 1998c, Kilgarriff, 1998a, Kilgarriff, 1998b)"
      ]
    },
    {
      "heading": "2 A Case Study",
      "text": [
        "In this paper, we examine the issue of inter-annotator agreement by comparing the agreement rate of human annotators on a large sense-tagged corpus of more than 30,000 instances of the most frequently occurring nouns and verbs of English This corpus is the intersection of the WORDNET Semcor corpus (Miller et al., 1993) and the DSO corpus (Ng and Lee, 1996, Ng, 1997), which has been independently tagged with the refined senses of WORDNET by two separate groups of human annotators The Semcur corpus is a subset of the Brown corpus tagged with WoRDNET senses, and consists of more than 670,000 words from 352 text files Sense tagging was done on the content words (nouns, verbs, adjectives and adverbs) in this subset The DSO corpus consists of sentences drawn from the Brown corpus and the Wall Street Journal For each word w from a list of 191 frequently occurring words of English (121 nouns and 70 verbs), sentences containing w (in singular or plural form, and in its various inflectional verb form) are selected and each word occurrence w is tagged with a sense from WORDNET There is a total of about 192,800 sentences in the DSO corpus in which one word occurrence has been sense-tagged in each sentence The intersection of the Semcor corpus and the DSO corpus thus consists of Brown corpus sentences in which a word occurrence w is sense-tagged in each sentence, where w is one of the 191 frequently occurring English nouns or verbs Since this common portion has been sense-tagged by two independent groups of human annotators, it serves as our data set for investigating inter-annotator agreement in this paper"
      ]
    },
    {
      "heading": "3 Sentence Matching",
      "text": [
        "To determine the extent of inter-annotator agreement, the first step is to match each sentence in Semcor to its corresponding counterpart in the DSO corpus This step is complicated by the following factors I Although the intersected portion of both corpora came from Brown corpus, they adopted different tokenization convention, and segmentation into sentences differed sometimes 2 The latest version of Semcor makes use of the senses from WORDNET 1 6, whereas the senses used in the DSO corpus were from WoRDNET 15' To match the sentences, we first converted the senses in the DSO corpus to those of WoRDNET 1 6 We ignored all sentences in the DSO corpus in which a word is tagged with sense 0 or 1 (A word is tagged with sense 0 or 1 if none of the given senses in WORDNFT applies ) k sentence from Semcor is considered to match one from the DSO corpus if both sentences are exactly identical or if they differ only in the presence or absence of the characters \" (period) or -' (hyphen) For each remaining Semcor sentence, taking into account word ordering, if 75% or more of the words in the sentence match those in a DSO corpus sentence, then a potential match is recorded These I -kctually, the WoRDNET senses used in the DSO corpus were from a slight variant of the official WortuNea 1 5 release This was brought to our attention after the public release of the DSO corpus potential matches are then manually verified to ensure that they are true matches and to weed out any false matches Using this method of matching, a total of 13,188 sentence-pairs containing nouns and 17,127 sentence-pairs containing verbs are found to match from both corpora, yielding 30,315 sentences which form the intersected corpus used in our present study"
      ]
    },
    {
      "heading": "4 The Kappa Statistic",
      "text": [
        "Suppose there are N sentences in our corpus where each sentence contains the word w Assume that w has M senses Let 4 be the number of sentences which are assigned identical sense by two human annotators Then a simple measure to quantify the agreement rate between two human annotators is Pa, where Pa = AIN The drawback of this simple measure is that it does not take into account chance agreement between two annotators The Kappa statistic it (Cohen, 1960) is a better measure of inter-annotator agreement which takes into account the effect of chance agreement It has been used recently within computational linguistics to measure inter-annotator agreement (Bruce and Wiebe, 1998, Car-letta, 1996, Verorus, 1998) Let C.7 be the sum of the number of sentences which have been assigned sense 3 by annotator 1 and the number of sentences which have been assigned sense ,j by annotator 2 Then",
        "where EC /2 ( IN )2 i=1 and P, measures the chance agreement between two annotators A Kappa value of 0 indicates that the agreement is purely due to chance agreement, whereas a Kappa value of 1 indicates perfect agreement A Kappa value of 0 8 and above is considered as indicating good agreement (Carletta, 1996) Table 1 summarizes the inter-annotator agreement on the intersected corpus The first (second) row denotes agreement on the nouns (verbs), while the last row denotes agreement on all words combined The al erage it reported in the table is a simple average of the individual it value of each word The agreement rate on the 30,315 sentences as measured by Pa is 57% This tallies with the figure reported in our earlier paper (Ng and Lee, 1996) where we performed a quick test on a subset of 5,317 sentences Ln the intersection of both the Semcor corpus and the DSO corpus"
      ]
    },
    {
      "heading": "5 Algorithm",
      "text": [
        "Since the inter-annotator agreement on the intersected corpus is not high, we would like to find out how the agreement rate would be affected if different sense classes were in use In this section, we present a greedy search algorithm that can automatically derive coarser sense classes based on the sense tags assigned by two human annotators The resulting derived coarse sense classes achieve a higher agreement rate but we still maintain as many of the original sense classes as possible The algorithm is given in Figure 1 The algorithm operates on a set of sentences where each sentence contains an occurrence of the word w which has been sense-tagged by two human annotators At each iteration of the algorithm, it finds the pair of sense classes C, and C3 such that merging these two sense classes results in the highest it value for the resulting merged group of sense classes It then proceeds to merge C, and C3 This process is repeated until the it value reaches a satisfactory value ltrn, which we set as 0 8 Note that this algorithm is also applicable to deriving any coarser set of classes from a refined set for any NLP tasks in which prior human agreement rate may not be high enough Such NLP tasks could be discourse tagging, speech-act categorization, etc"
      ]
    },
    {
      "heading": "6 Results",
      "text": [
        "For each word w from the list of 121 nouns and 70 verbs, we applied the greedy search algorithm to each set of sentences in the intersected corpus containing w For a subset of 95 words (53 nouns and 42 verbs), the algorithm was able to derive a coarser set of 2 or more senses for each of these 95 words such that the resulting Kappa value reaches 0 8 or higher For the other 96 words, in order for the Kappa value to reach 0 8 or higher, the algorithm collapses all senses of the word to a single (trivial) class Table 2 and 3 summarises the results for the set of 53 nouns and 42 verbs, respectively Table 2 indicates that before the collapse of sense classes, these 53 nouns have an average of 7 6 senses per noun There is a total of 5,339 sentences in the intersected corpus containing these nouns, of which 3,387 sentences were assigned the same sense by the two groups of human annotators The average Kappa statistic (computed as a simple average of the Kappa statistic of the individual nouns) is 0 463 After the collapse of sense classes by the greedy search algorithm, the average number of senses per noun for these 53 nouns drops to 40 However, the number of sentences which have been assigned the same coarse sense by the annotators increases to 5,033 That is, about 94 3% of the sentences have been assigned the same coarse sense, and that the average Kappa statistic has improved to 0 862, signifying high inter-annotator agreement on the derived coarse senses Table 3 gives the analogous figures for the 42 verbs, again indicating that high agreement is achieved on the coarse sense classes derived for verbs"
      ]
    },
    {
      "heading": "7 Discussion",
      "text": [
        "Our findings on inter-annotator agreement for word sense tagging indicate that for average language users, it is quite difficult to achieve high agreement when they are asked to assign refined sense tags (such as those found in WoRDNET) given only the scanty definition entries in the WORDNET dictionary and a few or no example sentences for the usage of each word sense This observation agrees with that obtained in a recent study done by (Vero-ms, 1998), where the agreement on sense-tagging by naive users was also not high Thus it appears that an average language user is able to process language without needing to perform the task of disambiguating word sense to a very fine-grained resolution as formulated in a traditional dictionary In contrast, expert lexicographers tagged the word sense in the sentences used in the SENSEVAL exercise, where high inter-annotator agreement was reported There are also fuller dictionary entries in the HECTOR dictionary used and more examples showing the usage of each word sense in HECTOR These factors are likely to have contributed to the difference in inter-annotator agreement observed in the three studies conducted We also examined the coarse sense classes derived by the greedy search algorithm We found some interesting groupings of coarse senses for nouns which we list in Table 4 From Table 4, it is apparent that the greedy search algorithm can derive interesting groupings of word senses that correspond to human intuitive judgment of sense granularity It is clear that some of the disagreement between the two groups of human annotators can be attributed solely to the overly refined senses of WoRDNET As an example, there is a total",
        "Sense 1 change, alteration, modification – (an event that occurs when something passes from one state or phase to another \"the change was intended to increase sales\", this storm is certainly a change for the worse\") Sense 2 change – (a relational difference between states, esp between states before and after some event \"he attributed the change to their marriage\") Sense 3 change – (the act of changing something, 'the change of government had no impact on the economy\", \"his change on abortion cost him the elec-tion\") Sense 4 change – (the result of alteration or modification, there were marked changes in the lining of the lungs\", \"there had been no change in the moun-tains\") Sense 5 change – (the balance of money received when the amount you tender is greater than the amount due, 'I paid with a twenty and pocketed the change\") Sense 6 change – (a thing that is different, 'he inspected several changes before selecting one\") Sense 8 change – (coins of small denomination regarded collectively, 'he had a pocketful of change\") Figure 2 Seven senses of the noun \"change\" used by the human annotators of 111 sentences in the intersected corpus containing the noun with root word form 'change\" They are assigned one of the seven senses listed in Figure 2 by the two groups of human annotators Based on the initial word senses assigned, Pa = 0 38 and K = – 0 09 (n is negative when there is systematic disagreement ) However, the greedy search algorithm collapses sense 1, 2, 3, 4 and 6 into one coarse sense and sense 5 and 8 into another coarse sense As a result, Pa = = 1, indicating perfect agreement when the senses are collapsed in the manner found This corresponds to our intuitive judgment of the relative closeness of the various senses here Similarly, some of the 96 words for which the greedy search algorithm collapses into one single sense are such that the various senses are too close to be reliably distinguished In short, we believe that the coarse sense classes derived by the greedy search algorithm, upon verification by human, can potentially serve as a better sense inventory for evaluating automated word sense disambiguation algorithms"
      ]
    },
    {
      "heading": "8 Related Work",
      "text": [
        "Recently, both Bruce and Wiebe (1998) and Veronis (1998) have looked into algorithms to automatically generate better sense classes in a corpus-based, data-driven manner However, the algorithms they used differ from ours Bruce and Wiebe (1998) made use of an EM algorithm v la a latent class model to derive better sense classes Veronis (1998) performed a Multiple Correspondence Analysis on the table of annotations (a triple composed of a context, a judge and a sense) to reduce dimensionality followed by tree-clustering In contrast, our greedy search algorithm is a simple but effective method that makes use of the Kappa statistic to search the space of possible sense groupings directly"
      ]
    },
    {
      "heading": "9 Conclusion",
      "text": [
        "In this paper, we examined the issue of inter-annotator agreement on word sense tagging and presented a greedy search algorithm capable of generating coarse sense classes based on the sense tags",
        "assigned by two human annotators We found interesting groupings of word senses that correspond to human intuitive judgment of sense granularity"
      ]
    }
  ]
}
