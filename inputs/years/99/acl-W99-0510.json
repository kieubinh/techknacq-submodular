{
  "info": {
    "authors": [
      "Satoshi Sekine",
      "Kiyoshi Sudo",
      "Takano Ogino"
    ],
    "book": "SIGLEX Workshop on Standardizing Lexical Resources",
    "id": "acl-W99-0510",
    "title": "Statistical Matching of Two Ontologies",
    "url": "https://aclweb.org/anthology/W99-0510",
    "year": 1999
  },
  "references": [
    "acl-C96-2195"
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": [
        "Standardizing ontologies is a challenging task Ontolopes have been created based on different backgrounds, different purposes and different people However, standardizing them is useful not only for applications, such as Machine Translation and Information Retrieval, but also to improve the ontologies themselves During the process of standardization, people can find bugs or gaps in ontologies So standardization brings benefits compared to just using them separately There is a committee for standardizing ontologies at ANSI, the \"ANSI Ad-Hoc Group for Ontology Stan-dards\" (Hovy 1996) Although there have been a few attempts to merge and compare ontologies, this work is still at a preliminary stage of research (Ogino et al. 1997) attempts manual merging of EDR (EDR 1996) (Miyoshi et al. 1996) and WordNet (Word-net) (Miller 1995), (Utiyama and Hashida 1997) used statistical methods to merge EDR and WordNet (Pangloss) is also working on standardizing ontologies It is certain that manual methods have great difficulty in matching the entire ontologies It would require three thousand years for a person to check all possible node pairings, if the two ontologies have 40 000 nodes each and each judgement takes a minute So automatic methods are needed to find matches automatically or at least to narrow down the candidates for matching In this paper, we investigate a simple statistical method for matching two ontologies The method can apply to any ontologies which are formulated from is-a relationships In our experiments, we used EDR and WoidNet This work is similar to the work in (Utiyama and Hashida 1997) They defined the task as the MWM (Maximum V, eignt Match) of bipartite graphs, an approach which is basically common to most ontology matching schemes The information they used is partially fuzzy, i e for calculating the distance between two nodes, they used the information from each node and its neighborhood, not distinguishing between information from parent and child nodes However, since the structure of the ontologies (the relation between parent and children) is significant, it might be better to utilize such structural information In our experiments, we will focus on this issue, rather than trying to achieve a higher performance The importance of parent, child and grandchild information will be examined We will conduct several experiments with or without some of the information It is also important to discol,er what weighting balance gives good matches"
      ]
    },
    {
      "heading": "2 Ontologies",
      "text": [
        "First we will briefly explain the ontologies we used in our experiments"
      ]
    },
    {
      "heading": "2.1 EDR",
      "text": [
        "The EDR Concept Dictionary contains 400,000 concepts listed in the Japanese and English Word Dictionaries of 200,000 words each The EDR Concept Dictionary is one of the five types of EDR dictionaries, the others are the Word Dictionaries for English and Japanese the Bilingual Dictionary, the Coocurrence Dictionary, and the Technical Teiminology Dictionary The EDR Concept Dictionary consists of three sub-dictionaries the Headconcept Dictionaiy contains concept explanations in natural language (both in English and Japanese), • the Concept Classification Dictionary contains a set of is-a relationships, and the Concept Description Dictionary contains pairs of concepts that have certain semantic relationships other than is-a relationship i e object, agent goal, implement a-object (object of a particular attribute), place, scene and cause The Concept Classification Dictionary classifies all the 400 000 concepts based on their meaning A polysemous word is put into several word classifications (concepts) As multiple inheritance is allowed, the entire structure is not a tree but a DAG (directed acyclic graph) There are 6,000 intermediate nodes and the maximum depth is 16"
      ]
    },
    {
      "heading": "2 2 WordNet",
      "text": [
        "WordNet (Wordnet) is an English ontology The nodes are represented by a set of synonym words (called sy nsets ') WordNet contains 60,557 noun",
        "synsets, 11,363 adjective synsets, and 3,243 adverb synsets Between synsets, there are relations which include (but are not limited to) hypernymy/hyponymy, antonymy, entailment and meronymy/holonymy A word or collocation may appear in more than one synset, and in more than one part of speech The words in a synset are logically grouped such that they are interchangeable in some context"
      ]
    },
    {
      "heading": "3 Experiments",
      "text": [
        "The basic idea of the matching is to find the distance (similarity) between a node in EDR and a node in WordNet There could be several strategies for defining a distance between two nodes, we will use the words attached to each node and its parent, child and grandchild in the computation We did not use the descriptions of concepts As a preliminary experiment, we restricted the number of nodes to be considered, because both ontologies are big We used the nodes at the top 5 levels (distance from the top is at most 5) and deleted nodes which have no English words and no descendents in EDR (some EDR nodes have only Japanese words) This left 14,712 nodes in EDR and 5,185 in WordNet Even with these restriction, the number of possible pairings is 76,281,720 Our target is to find good matches among them"
      ]
    },
    {
      "heading": "3 1 Definition of Distance",
      "text": [
        "The distance between nodes is defined based on the notion which is commonly used, the dice coefficient Assume the node N1 in ontologyl has n1 words and N2 in ontology2 has n2 words, and there are m words in common The dice coefficient (DC) is defined as follows",
        "Now we define the basic distance as 1 minus the %alue The smaller the distance, the closer the two nodes",
        "We now define the distance of two nodes (N1,N2) based on the basic distance definition The words in parents, children and giandchildren are also used Such nodes are taken as a bag of nodes, i e only one set of words is created for each category regardless of the number of nodeq Such a bag of nodes is represented as NParent and so on The distance of each category is calculated just like the basic distance In the following equation, cat should be replaced by parent, itself, child and gchtld (for grandchild)",
        "Then interpolation is used to merge the four basic distances in order to keep the range from 0 to 1 We introduce four coefficients cparent,citsei f ,cchtld ,cgchtid to define the node distance , D(NI, N2)",
        "The coefficients (Caat'S) vill be the important factor in the experiments As will be described in the next section, we use several combinations of the coefficients to observe which information is import ant"
      ]
    },
    {
      "heading": "3 2 Experiments",
      "text": [
        "We conducted eight experiments using different combinations of the coefficients The first experiment uses only the information in the nodes themselves, while other experiments use the node and parent, the node and children, or all four sets Table 1 shows the coefficient combinations used in the experiments",
        "3 2 1 Analysis of the statistical results Before describing the e‘aluation results, some interesting anal3.ses are presented in this section These analyses do not concern directly the evaluation of the experiment, but indicate the natine of the experiments oi the nature of the °neologies"
      ]
    },
    {
      "heading": "Number of outputs",
      "text": [
        "We used a threshold to restuct the number of outputs If the distance is greater than 0 9, the result is not generated Table 2 shows the number of outputs in each experiment Recall that there are 76,281,720- possible pairings of nodes It is interesting to see that the numbers are almost the same The number of outputs in Experiment-4 is slightly smaller, we believe this is because the weight assigned to the nodes themsehes, which ghes the greatest contribution, is low",
        "The numbers are around 10,000, which represents 0 013% of the possible matches This suggests that there is a possibility of narrowing down the matches to be examined by a human, as the distance 0 9 is very large and the number of outputs is so small To prove this assumption, we have to conduct an evaluation to see if there are good matches which were not generated This is beyond the evaluation in this paper, because it requires manual matching from scratch We will discuss this later"
      ]
    },
    {
      "heading": "Complete Match",
      "text": [
        "We can find the number of complete matches (which have exactly the same word(s)) by counting the pairs with distance 0 0 in Experiment-1 The number of complete matches is 1778, which is quite large compared to the number of nodes under consideration in WordNet (about 5,000) Also, by counting up the number of pairs with distance 0 0 in Experiment-5, we can find parent-complete matches which are complete matches where the parents also have the same words The number of parent-complete matches is 1 This is surprisingly small, even considering that we used only subsets of the ontologies The only parent-match is the following parent invertebrate child arthropod Naturally people might guess that there would be more parent-complete matches For example, the name of a mammal might be a plausible candidate (where the parent is \"mammal\" and child is, for example, \"elephant\") However, this is not the case \"Elephant\" and \"mammal\" appear as follows (unrelated nodes are not shown)",
        "This is one of the typical problems of ontology design, how detail concepts should be introcuced Also, there is a translation problem in EDR, i e sometimes there is words or a description in only one language - There are some other *reasons why the number of parent-matches is so small",
        "• Some nodes in EDR have no words associated with them This is how the EDR Classification Dictionary was designed It is based on the classification of words into some predefined boxes, and not creating hierarchy of words It would be better to use the concept descriptions of the dictionary, although it is not clear how to compare a sy nset (set of words) and a description Also, we might be able to use information written in Japanese when there is no English word but there are Japanese words • WordNet uses a synset to represent a node, whereas EDR's node is primarily represented by a description, there could be differences caused by this The average numbers of words in a node are also different",
        "There were no children-matches, which are complete matches where the words in the child nodes are also the same The closest matches in Experiment-2 and 3 are the following",
        "(There are actually 4 child nodes )"
      ]
    },
    {
      "heading": "3 2 2 Evaluation",
      "text": [
        "As it is impossible to evaluate all the results, we selected four ranges (rank 1 to 20, 501 to 520, 2001 to 2020, and 9001 to 9020) and the data in these ranges was evaluated manually Ei,aluation w as done by putting the matches into three categories",
        "• A Two nodes are completely the same concept • B Other than A and C • C Two nodes aie completely different concepts",
        "Category B includes several different things, including partial matches and ambiguous cases by the manual evaluation However, the number of results in this category was not so large, so it should not affect the overall evaluation Table 3 shows the evaluation result The columns represent the four ranges and the each row represents one of the eight experiments An element has",
        "three numbers, corresponding to the categories A, B and C, separated by \"/\" We can't make a direct comparison to other methods For example, while (Utiyama and Hashida 1997) also used EDR and WordNet, they used only connected components and we imposed the level restriction However, relative comparisons among our 8 experiments are meaningful and important We will discuss them in the next section 3 3 Discussion Using only the nodes themselves (Exp-1) In Experiment-1, only the words in the nodes being compared are used The evaluation result was not very good For example, there are only 3 matches of category A in the highest range Based on an examination of the results, we observed that this is due to word polysemy Even if two nodes have a word in common, the word could have several meanings, and hence the corresponding nodes could have different meanings For example, the word \"love\" can mean \"emotion\" or \"no point in tennis\" To see how the results we obtained might arise, suppose a word has 4 senses in ontology1 and 5 in ontology2, and there are 3 senses which are the same in the' two oritologies Then there are 20 pairings of the senses and out of them only • 3 can be judged as category A Although this is just an assumption, the reality might not be that far from this explanation based on the observation of the result Adding child nodes (Exp-2,3,4) In Experiment-2,3 and 4, we used the information of the nodes themselves and their child nodes The evaluation results for Experiment-2 and 3 are the same, both of them have 6 A's in the highest range The number is twice that in Experiment1 This improvement is due to disambiguation of polysemous words For example, the same sense of a polysemous word might have similar words in the child nodes, whereas it might be rare that different senses have the same words in the two ontologies In Experiment-4, we put more weight on child nodes lather than the nodes themselves This experiment was conducted based on the assumption that the number of words in child nodes may be much larger than the number of words in the nodes themselves However, this turns out to give a degradation at the higher range Observing the result, the matches at the higher range have 'very few words in the child nodes If the number of child nodes are small in both ontologies and they have many in common, the distance between the nodes becomes extremely small This could be both beneficial and harmful It can pick up some matches which could not be found in Experiment1, but the matches could be good or bad ones The following example is a good one which is actually found at the ninth rank in Experiment-4",
        "Adding parent nodes (Exp-5) In Experiment-5, the words in the nodes themselves and their parent nodes are used It can be naturally thought that the words in the parent nodes are useful to disambiguate polysemous words The result confirmed this In the highest range, category A has 10 matches out of 90 which is three times as much as in Experiment-1, and twice that in Experiment-2 and 3 Using parents, self and children (Exp-6,7) In Experiment-6 and 7 w olds in parent, self and child nodes are used with different weightings All ei,aluation results are identical except the lowest range, and these have the largest number of A's at the highest range among all of the experiments This indicates that three sources together is better than any two or any single source of information Adding grandchild nodes (Exp-8) Finally, in Experiment-8, words in all four kinds of nodes, parent, self, child and grandchild, are used The evaluation result is the same as that in prey, quarry game",
        "Experiment-6, and we could not see improvement by adding grandchild information Actually, by observing the result, we can see that the information at the grandchild level is not so useful Observing the evaluation process From the evaluation process, we understand that a human uses not only the four kinds of information, but also information in grandparent or the successor's nodes Some improvement might be obtained if we used such information Also, we might be able to achieve more improvement by using sibling nodes, and the result of distance calculation of other nodes As we presented by the example of \"mammal\" and \"elephant\", there are the cases where in one ontology a relationship is parent-child, but in the other ontology it is a grandparent-grandchild relationship or a sibling-relationship It would be better if we took the characteristics of each ontology and differences of the ontologies into account in the calculation In particular, the information in ancestors might be very useful Other distance definitions In our method, we simply used the dice coefficient However, we can use more complicated or sophisticated measures For example, (Resnik 1995) proposed a measure of semantic similarity based on the notion of information content Although this proposal defines similarity between two nodes in a single taxonomy or ontology, we may be able to apply it m our situation (Agirre et al. 1995) proposed conceptual distance between nodes on ontologies captured by a Conceptual Density formula It is also a definition in a single ontology Recently, (O'Hara and et al. 1998) conducted an experiment of matching two ontologies, WordNet and the Mikrokosmos Ontology They used the definition proposed in (Resnik 1995) among other heuristics It is not so clear hoW to compare the method to our method, as they used several heuristics which is not directly comparable to our method However we noticed that it is a very important to investigate their methods"
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "We proposed a statistical method of matching two ontologies Since it is impossible to exhaustively consider all matches by hand, automatic methods to make matches or to narrow down the candidate matches are needed Although the experiments are preliminary, they show what kinds of information is useful in statistical matching We found that parent nodes, besides the nodes themselves, are the most useful for matching by disambiguating the synonyms of words The best performance was achieved by using words in parent, itself and child nodes We observed that it is important to consider the characteristics of the ontologies One goal of our future work is to understand how to incorporate such characteristics into these statistical methods"
      ]
    },
    {
      "heading": "5 Acknowledgements",
      "text": [
        "We would like to thank Prof Ralph Grishman at New York University for his suggestions and anonymous reviewers who gave us some severe comments"
      ]
    }
  ]
}
