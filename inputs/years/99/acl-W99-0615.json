{
  "info": {
    "authors": [
      "Jin-Dong Kim",
      "Sang-Zoo Lee",
      "Hae-Chang Rim"
    ],
    "book": "Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora",
    "id": "acl-W99-0615",
    "title": "HMM Specialization With Selective Lexicalization",
    "url": "https://aclweb.org/anthology/W99-0615",
    "year": 1999
  },
  "references": [
    "acl-A88-1019",
    "acl-J88-1003",
    "acl-J94-2001"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words.",
        "'Our approach examines the distribution of transitions, selects the uncommon words, and makes lexicalized states for the words.",
        "We performed a part-of-speech tagging experiment on the Brown corpus to evaluate the resultant language model and discovered that this technique improved the tagging accuracy by 0.21% at the 95% level of confidence."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Hidden Markov 'Models are widely used for statistical language modelling in various fields, e.g., part-of-speech tagging or speech recognition (Rabiner and Juang, 1986).",
        "The models are based on Markov assumptions, which make it possible to view the language prediction as a Markov process.",
        "In general, we make the first-order Markov assumptions that the current tag is only dependant on the previous tag and that the current word is only dependant on the current tag.",
        "These are very 'strong' assumptions, so that the first-order Hidden Markov Models have the advantage of drastically reducing the number of its parameters.",
        "On the other hand, the assumptions restrict the model from utilizing enough constraints provided by the local context and the resultant model consults only a single category as the contex.",
        "A lot of effort has been devoted in the past to make up for the insufficient contextual information of the first-order probabilistic model.",
        "The second order Hidden Markov Models with The research underlying this paper was supported by research grants from Korea Science and Engineering Foundation.",
        "appropriate smoothing techniques show better performance than the first order models and is considered a state-of-the-art technique (Meri-aldo, 1994; Brants, 1996).",
        "The complexity of the model is however relatively very high considering the small improvement of the performance.",
        "Garside describes IDIOMTAG (Garside et al., 1987) which is a component of a part-of-speech tagging system named CLAWS.",
        "ID-IOMTAG serves as a front-end to the tagger and modifies some initially assigned tags in order to reduce the amount of ambiguity to be dealt with by the tagger.",
        "IDIOMTAG can look at any combination of words and tags, with or without intervening words.",
        "By using the IDIOMTAG, CLAWS system improved tagging accuracy from 94% to 96-97%.",
        "However, the manual-intensive process of producing idiom tags is very expensive although IDIOMTAG proved fruitful.",
        "Kupiec (Kupiec, 1992) describes a technique of augmenting the Hidden Markov Models for part-of-speech tagging by the use of networks.",
        "Besides the original states representing each part-of-speech, the network contains additional states to reduce the noun/adjective confusion, and to extend the context for predicting past participles from preceding auxiliary verbs when they are separated by adverbs.",
        "By using these additional states, the tagging system improved the accuracy from 95.7% to 96.0%.",
        "However, the additional context is chosen by analyzing the tagging errors manually.",
        "An automatic refining technique for Hidden Markov Models has been proposed by Brants (Brants, 1996).",
        "It starts with some initial first order Markov Model.",
        "Some states of the model are selected to be split or merged to take into account their predecessors.",
        "As a result, each of",
        "new states represents a extended context.",
        "With this technique, Brants reported a performance equivalent to the second order Hidden Markov Models.",
        "In this paper, we present an automatic refining technique for statistical language models.",
        "First, we examine the distribution of transitions of lexicalized categories.",
        "Next, we break out the uncommon ones from their categories and make new states for them.",
        "All processes are automated and the user has only to determine the extent of the breaking-out."
      ]
    },
    {
      "heading": "2 \"Standard\" Part-of-Speech",
      "text": []
    },
    {
      "heading": "Tagging Model based on HMM",
      "text": [
        "From the statistical point of view, the tagging problem can be defined as the problem of finding the proper sequence of categories co, -= c1, c, , Cn (n > 1) given the sequence of words wi.n w1, w2, wn (We denote the i'th word by wi, and the category assigned to the wi by ci), which is formally defined by the following equation:",
        "Charniak (Charniak et al., 1993) describes the \"standard\" HMM-based tagging model as Equation 2, which is the simplified version of Equation 1.",
        "With Equation 4, we also assume that the correct word is independent of everything except the knowledge of its category.",
        "Through these assumptions, the Hidden Markov Models have the advantage of drastically reducing the number of parameters, thereby alleviating the sparse data problem.",
        "However, as mentioned above, this model consults only a single category as context and does not utilize enough constraints provided by the local context."
      ]
    },
    {
      "heading": "3 Some Refining Techniques for HMM",
      "text": [
        "The first-order Hidden Markov Models described in the previous section provides only a single category as context.",
        "Sometimes, this first-order context is sufficient to predict the following parts-of-speech, but at other times (probably much more often) it is insufficient.",
        "The goal of the work reported here is to develop a method that can automatically refine the Hidden Markov Models to produce a more accurate language model.",
        "We start with the careful observation on the assumptions which are made for the \"standard\" Hidden Markov Models.",
        "With the Equation 3, we assume that the current category is only dependent on the preceding category.",
        "As we know, it is not always true and this first-order Markov assumption restricts the disambiguation information within the first-order context.",
        "The immediate ways of enriching the context are as follows:",
        "• to lexicalize the context.",
        "With this model, we select the proper category for each word by making use of the contextual probabilities, P(cdci_i), and the lexical probabilities, P (wilcz) • This model has the advantages of a provided theoretical framework, automatic learning facility and relatively high performance.",
        "It is thereby at the basis of most tagging programs created over the last few years.",
        "For this model, the first-order Markov assum-tions are made as follows:",
        "With Equation 3, we assume that the current category is independent of the previous words and only dependent on the previous category.",
        "• to extend the context to higher-order.",
        "To lexicalize the context, we include the preceding word into the context.",
        "Contextual probabilities are then defined by P(c, Ici_i , wi-i)• Figure 1 illustrates the change of dependency when each method is applied respectively.",
        "Figure 1(a) represents that each first-order contextual probability and lexical probability are independent of each other in the \"standard\" Hidden Markov Models, where Figure 1(b) represents that the lexical probability of the preceding word and the contextual probability of the current category are tied into a lexicalized contextual probability.",
        "To extend the context to higher-order, we extend the contextual probability to the second",
        "order.",
        "Contextual probabilities are then defined by P(czici_i, cz_2).",
        "Figure 1(c) represents that the two adjacent contextual probabilities are tied into the second-order contextual probability.",
        "The simple way of enriching the context is to extend or lexicalize it uniformly.",
        "The uniform extension of context to the second order is feasible with an appropriate smoothing technique and is considered a state-of-the-art technique, though its complexity is very high: In the case of the Brown corpus, we need trigrams up to the number of 0.6 million.",
        "An alternative to the uniform extension of context is the selective extension of context.",
        "Brants(Brants, 1996) takes this approach and reports a performance equivalent to the uniform extension with relatively much low complexity of the model.",
        "The uniform lexicalization of context is computationally prohibitively expensive: In the case of the Brown corpus, we need lexicalized bigrams up to the number of almost 3 billion.",
        "Moreover, many, of these bigrams neither contribute to the performance of the model, nor occur frequently enough to be estimated properly.",
        "An alternative to the uniform lexicalization is the selective lexicalization of context, which is the main topic of this paper."
      ]
    },
    {
      "heading": "4 Selective texicalization of HMM",
      "text": [
        "This section describes a new technique for refining the Hidden Markov Model, which we call selective lexicalization.",
        "Our approach automatically finds out ylitactically uncommon words and makes a new state (we call it a lexicalized state)for each of the words.",
        "Given a fixed set of categories, {cl, c2, c9, e.g., {adjective, ..., verb} , we assume the discrete random variable X0 with domain the set of categories and range a set of conditional probabilities.",
        "The random variable X0 then represents a process of assigning a conditional probability P(cz Ic3) to every category c' (c' ranges over cl ...cc )",
        "We convert the process of X0 into the state transition vector, V, which consists of the corresponding conditional probabilities, e.g.,",
        "The (squared) distance between two arbitrary vectors is then computed as follows: R(Vi, V2) = (V1 – V2)T(Vi – V2) (5) Similarly, we define the lexicalized state transition vector', Vc; ,wk e.g.,",
        "In this situation, it is possible to regard each lexicalized state transition vector, V0 ,k, of the same category ci as members of a cluster whose centroid is the state transition vector, V0 .",
        "We can then compute the deviation of each lexicalized state transition vector, V0,„k, from its corresponding centroid.",
        "D(V, ) = – V, ) – Vs) (6) Figure 2 represents the distribution of lexicalized state transition vectors according to their deviations.",
        "As you can see in the figure, the majority of the vectors are near their centroids and only a small number of vectors are very far from their centroids.",
        "In the first-order context model (without considering lexicalized context).",
        "'To alleviate the sparse data problem.",
        "we smoothed the lexicalized state transition probabilities by MacKay and Peto(MacKay and Peto, 1995)'s smoothing technique.",
        "the centroids represent all the members belonging to it.",
        "In fact, the deviation of a vector is a kind of (squared) error for the vector.",
        "The error for a cluster is",
        "and the error for the overall model is simply the sum of the individual cluster errors:",
        "Now, we could break out a few lexicalized state vectors which have large deviation (D > 0) and make them individual clusters to reduce the error of the given model.",
        "As an example, let's consider the preposition cluster.",
        "The value of each component of the centroid, Vp„p , is illustrated in Figure 3(a) and that of the lexicalized vectors, Vprep,zn, Vp„p,with and Vprep,out are in Figure 3(b), (c) and (d) respectively.",
        "As you can see in these figures, most of the prepositions including in and with are immediately followed by article(AT), noun(NN) or pronoun(NP), but the word out as preposition shows a completely different distribution.",
        "Therefore, it would be a good choice to break out the lexicalized vector.",
        "Vprep,out, frOM its centroid, V prep.",
        "From the viewpoint of a network, the state representing preposition is split into two states; the one is the state representing ordinary prepositions except out, and the other is the state representing the special preposition out, which we call a lexicalized state.",
        "This process of splitting is illustrated in Figure 4.",
        "Splitting a state results in some changes of the parameters.",
        "The changes of the parameters resulting from lexicalizing a word, wk, in a category, c-1, are indicated in Table 1 (cl ranges over c1 ...cc).",
        "This full splitting will increase the complexity of the model rapidly, so that estimating the parameters may suffer from the sparseness of the data.",
        "To alleviate it, we use the pseudo splitting which leads to relatively small increment of the",
        "parameters.",
        "The changes of the parameters in pseudo splitting are indicated in Table 2."
      ]
    },
    {
      "heading": "5 Experimental Result",
      "text": [
        "We have tested our technique through part-of-speech tagging experiments with the Hidden Markov Models which are variously lexicalized.",
        "In order to conduct the tagging experiments, we divided the whole Brown (tagged) corpus containing 53,887 sentences (1,113,191 words) into two parts.",
        "For the training set, 90% of the",
        "sentences were chosen at random, from which we collected all of the statistical data.",
        "We reserved the other 10% for testing.",
        "Table 3 lists the basic statistics of our corpus.",
        "training set",
        "test set",
        "We used a tag set containing 85 categories.",
        "The amount of ambiguity of the test set is summarized in Table 4.",
        "The second column shows that words to the ratio of 52% (the number of 57,808) are not ambiguous.",
        "The tagger attempts to resolve the ambiguity of the remaining words.",
        "Figure 5 and Figure 6 show the results of our part-of-speech tagging experiments with the \"standard\" Hidden Markov Model and variously lexicalized Hidden Markov Models using full splitting method and pseudo splitting method respectively.",
        "We got 95.7858% of the tags correct when we applied the standard Hidden Markov Model without any lexicalized states.",
        "As the number of lexicalized states increases, the tagging accuracy increases until the number of lexicalized states becomes 160 (using full splitting) and 210 (using pseudo splitting).",
        "As you can see in these figures, the full splitting improves the performance of the model more rapidly but suffer more sevelery from the sparseness of the training data.",
        "In this experiment, we employed Mackay and Peto's smoothing techniques for estimating the parameters required for the models.",
        "The best precision has been found to be 95.9966% through the model with the 210 lexcalized states using the pseudo splitting method."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "In this paper, we present a method for complementing the Hidden Markov Models.",
        "With this method, we lexicalize the Hidden Markov Model seletively and automatically by examining the transition distribution of each state relating to certain words.",
        "Experimental results showed that the selective lexicalization improved the tagging accurary from about 95.79% to about 96.00%.",
        "Using normal tests for statistical significance we found that the improvement is significant at the 95% level of confidence.",
        "The cost for this improvement is minimal.",
        "The resulting network contains 210 additional lexicalized states which are found automatically.",
        "Moreover, the lexicalization will not decrease the tagging speed2, because the lexicalized states and their corresponding original states are exclusive in our lexicalized network, and thus the rate of ambiguity is not increased even if the lexicalized states are included.",
        "Our approach leaves much room for improvement.",
        "We have so far considered only the outgoing transitions from the target states.",
        "As a result, we have discriminated only the words with right-associativity.",
        "We could also discriminate the words with left-associativity by examining the incoming transitions to the state.",
        "Furthermore, we could extend the context by using the second-order context as represented in Figure 1(c).",
        "We believe that the same technique presented in this paper could be applied to the proposed extensions."
      ]
    },
    {
      "heading": "References",
      "text": [
        "L. Rabiner and B. Juang.",
        "1986.",
        "An introduction to hidden markov models.",
        "IEEE ASSP Magazine, pages 4-16, January."
      ]
    },
    {
      "heading": "Appendix :",
      "text": [
        "Top 100 words with high deviation"
      ]
    }
  ]
}
