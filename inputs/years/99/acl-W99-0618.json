{
  "info": {
    "authors": [
      "Dekai Wu",
      "Zhao Jun",
      "Zhifang Sui"
    ],
    "book": "Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora",
    "id": "acl-W99-0618",
    "title": "An Information-Theoretic Empirical Analysis of Dependency-Based Feature Types for Word Prediction Models",
    "url": "https://aclweb.org/anthology/W99-0618",
    "year": 1999
  },
  "references": [
    "acl-P96-1025",
    "acl-P97-1003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Over the years, many proposals have been made to incorporate assorted types of feature in language models.",
        "However, discrepancies between training sets, evaluation criteria, algorithms, and hardware environments make it difficult to compare the models objectively.",
        "In this paper, we take an information theoretic approach to select feature types in a systematic manner.",
        "We describe a quantitative analysis of the information gain and the information redundancy for various combinations of feature types inspired by both dependency structure and bigram structure, using a Chinese treebank and taking word prediction as the object.",
        "The experiments yield several conclusions on the predictive value of several feature types and feature types combinations for word prediction, which are expected to provide guidelines for feature type selection in language modeling."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "There are many types of features that a language model can use to predict a word in a sentence.",
        "Standard n-gram models use the immediately preceding words.",
        "Other fixed physical distance feature types may inspect word classes or parts of speech.",
        "Grammatically-based feature types may also be used, such as the incident syntactic and semantic relations or the other words involved in those relations.",
        "Our ultimate aim is to determine which combination of feature types is optimal for language modeling.",
        "Unfortunately, the state of knowledge in this regard is very limited.",
        "Many language models have been published inspired by one or more of these feature types111121131141151, but discrepancies between training sets, evaluation criteria, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively.",
        "The paper uses an information theoretic approach to select feature types for language modeling in a systematic manner.",
        "We are concerned with quantitative analysis of the information quantity, information gain and the information redundancy for various feature type combinations in both dependency grammar structure and adjacent bigram structure.",
        "The experiments yield a number of conclusions on the predictive value of various feature types and the combinations thereof, which can provide useful information on what level of performance gain can be expected in principle from a bigram model augmented with long distance dependency features.",
        "The results are expected to provide a reliable reference for feature type selection in language modeling.",
        "We have used Chinese data for the experiments in this paper.",
        "Strictly speaking, our",
        "conclusions apply only to Chinese.",
        "However, we actually expect very similar results on English, and all our preliminary experiments on English data do bear this out16).",
        "We believe the general methodology as well as many of the specific conclusions apply to a wide range of languages.",
        "We will begin by introducing an information theoretic framework for feature type selection and analysis.",
        "We then describe the experimental setup.",
        "Finally, we discuss a number of claims deriving from the experimental evidence."
      ]
    },
    {
      "heading": "2 Framework 2.1 Features for Language Models",
      "text": [
        "A language model predicts a given word based on its history.",
        "By the laws of conditional probabilities, a language model can be represented in left-to-right fashion as",
        "where S denotes a sequence of words wo, w1, w„, and hi denotes the history of w, (0 < i n) .",
        "In order to construct a language model, the individual probabilities P(wii hi) should be estimated from the training set.",
        "Since there are too many possible histories but not enough evidence in the training set, several feature types must be used to divide the space of possible histories into equivalence classes via the map (1:0 :h, f2 fic >[k] to make the model feasible in the implementation.",
        "In speech recognition, these feature types are most often fixed physical position based features, as in N-gram models.",
        "The feature types can be the words before the predicted word or the part-of-speech of the words before the predicted word.",
        "In order to remedy the linguistic implausibility and inefficient usage of the training set of N-gram models, we would like to incorporate grammatically-based feature types into the language model, which could incorporate the predictive power of words that lie outside of N-gram range171181.",
        "However, we would like to do so without sacrificing the known performance advantages of N-gram models191.",
        "We follow the general approach of the aforementioned authors in taking dependency grammar as a framework, since it extends N-gram models more naturally than stochastic context-free grammars.",
        "The feature types studied in this paper are combinations of the fixed physical distance features and grammatically based features listed in Table 1 and graphically depicted in Figure 1.",
        "To understand the feature types, consider the task of predicting \"ileAk (zuo4 ye4, assignment)\" in the example sentence shown in Figure 2.",
        "We denote this word by 0, which stands for \"observed\".",
        "The word bigram feature B is the nearest preceding word of 0, in this case \"75I-E3C (yingl wen2, English)\".",
        "The nearest word modifying 0 is denoted by M, and is also \"X3C (yingl wen2, English)\" in this case.",
        "Conversely, the nearest preceding word modified by 0 is denoted by R, (zuo4, do)\" here.",
        "BP is the part of speech of \"X3c (yingl wen2, English)\", in this case \"n(noun)\".",
        "Similarly, MP is the POS of \"“ying1 wen2, English)\", and RP is the POS \"v(verb)\" for \" itt( (zuo4, do)\".",
        "The modifying type or dependency relation between (yingl wen2, English)\" and \" (zuo4 ye4, assignment)\" is denoted by MT, in this case \"np(noun phrase)\".",
        "RT is the modifying type between \"'fttt (zuo4, do)\" and \"IIF (zuo4 ye4, assignment)\", here \"vp(verb phrase) \" .",
        "Faced with so many feature types, one of the dilemmas for language modeling is which feature types, or feature type combinations, should be used.",
        "The experience has shown that the feature types should not be selected by intuition.",
        "In order to obtain a more reliable reference to guide the addition of structural features to a stochastic language model, our objective is to establish in principle the amount of information available from various long distance dependency features and feature combinations.",
        "This can be regarded as an upper bound on the improvement that could be obtained by augmenting a language model with the corresponding features.",
        "We evaluate the informativeness of several feature types in bigram and dependency grammatical structure from the viewpoint of information theory.",
        "The experiments draw some conclusions on which feature types should be selected or should not be selected given specific baseline assumptions, and provide a ranking of the",
        "feature types according to their importance from this viewpoint."
      ]
    },
    {
      "heading": "2.2 Information-based Model for Feature Type Analysis",
      "text": [
        "We now introduce some relevant concepts from information theory that we adopt as a foundation for analyzing feature types.",
        "Information quantity (IQ).",
        "The information quantity of a feature type F to the predicted word 0 is defined using the standard definition of average mutual informationimi; we define IQ as the average mutual information between F and 0.",
        "Information gain (IG).",
        "The information gain of adding F2 on top of a baseline model that already employs F, for predicting word 0 is defined as the average mutual information between the predicted word 0 and feature type F2, given that feature type F, is known.",
        "Information redundancy (IR).",
        "The above two definitions lead naturally to a complementary concept of information redundancy.",
        "IR(F1,F2;0) denotes the redundant information between Fl and F2 in predicting 0, which is defined as the difference between IQ(F2;0) and IG(F2;01F1), or the difference between IQ(F1;0) and IG(F1;01F2).",
        "We shall use IG to select the feature type series, and use IR to analyze the overlapped degree between the variant and the baseline."
      ]
    },
    {
      "heading": "3 The Corpus Used in the Experiments",
      "text": [
        "The training corpus used in our experiments is a treebank consisting of Chinese primary school textstn11121.",
        "The basic statistics characterizing the training set are summarized in Table 2.",
        "In the experiments, we use 80% of the above corpus as a training set for estimating the various co-occurrence probabilities, while 10% of the corpus is used as a testing set to compute the information gain, information quantity, and information redundancy.",
        "The feature types we used in the experiments are those shown in Table 1."
      ]
    },
    {
      "heading": "4 Experimental Results and Analysis",
      "text": [
        "Our experiments aim to quantitatively establish the amount of information intrinsically present in each feature type, and the information gain of each feature type on the top of various baselines.",
        "We were led to a number of conclusions on the predictive power of various feature types and feature types combinations, some in support of traditional linguistic intuition and some more surprising.",
        "These observations provide guidelines for language modeling.",
        "Below, we warm up with a well-known observation, and then move on to more focussed analysis.",
        "4.1 Grammatically motivated feature types do not easily yield as much predictive information as simple bigrams.",
        "From a traditional linguistics viewpoint, R (the nearest preceding word modified by the",
        "predicted word 0) should be more significant for word prediction than the bigram predictor B (the nearest preceding word of the predicted word 0).",
        "Consider the sentence showed in Figure 3, where 0 is \" /di4tu2/map\", B is the aspectual marker \" lzhe0\", and R is \" /gua4/hang\".",
        "It seems somehow obvious that R (\"g/gua4/hang\") should be more predictive for 0 (\" ith I1 /di4tu2/map\") than B (the aspectual marker \"Vzhe0\").",
        "However, as is well known in speech recognition and statistical NLP research, the opposite turns out to be true.",
        "This is corroborated by the empirical information quantities shown in Table 3, which shows that B has the largest information quantity in all of the feature types.",
        "That bigram features outperform the grammatically-based features is commonly attributed to the predictive power of lexical association.",
        "Similarly, M (the nearest preceding word modifying the predicted word 0) should be more significant for word prediction than B (the nearest preceding word of the predicted word 0).",
        "For example, consider the sentence showed in Figure 4, where 0 is \"/zou3/walk\", then B is /A-II lgonglyuan21garden\" and M is \" /cong2/from\".",
        "Again, it seems that M (\" )& /cong2/from\") ought to be more predictive to 0 (\" /zou3/walk\") than B (\" lgonglyuan2/garden\"), but from Table 3 we see that the opposite is true.",
        "From a linguistic viewpoint, the explanation for the fact that R (IQ(R;0)=1.581) is less predictive than B (IQ(B;0)=3.826) may be as follows.",
        "Within a sentence, every word has",
        "exactly one B and one R feature.",
        "But on one hand, the B feature always lies to the left of 0 since it is by definition the preceding word, while on the other hand, R generally lies to the right of 0 in Chinese sentences (with a few notable exceptions such as prepositional phrases).",
        "When R is not in the history preceding 0, it cannot be used to predict 0.",
        "Similarly, a possible factor in the fact that M (IQ(M;0)=2.237) is less predictive than B is that M sometimes lies to the right of 0.",
        "Another factor in the case of M is that none of the leaf nodes in a dependency tree have an M. 4.2 Although it (the word modified by the predicted word) is less effective than M (the word modifying the predicted word) when they are used individually for word prediction, R is more effective than M if they are used On top of a standard bigram model (the feature B).",
        "Consider the following measurements from our experiments: IQ(R;0)=1.581 bits which is less than IQ(M;0)=2.237 bits, whereas IG(R;0113)=0.683 bits which is greater than IG(M;0113)=0.541 bits.",
        "That is, given a baseline bigram model employing only B features, augmenting the model with R features brings more information than augmenting it with M features.",
        "Therefore, in principle, the language model which incorporates bigram and feature type R can achieve higher performance than the model which incorporates bigram and M. We believe this because there is more information redundancy between M and B than between R and B.",
        "From the above data, we see that there exists large information redundancy both between B and R (IR(B,R;0)=0.898) and between B and M (IR(B,M;0)=1.696).",
        "One explanation is that often B and M are in fact the same word, where the nearest preceding word modifies the predicted word.",
        "For example, consider the sentence in Figure 5, where \"1/FR /zuo4ye4/assignment\" is the predicted 0, and B and M are the same word /ying4wen2/English\".",
        "It is also possible that B and R are the same word, where the nearest preceding word is modified by the predicted word.",
        "For example, the dependency grammatical structure of the phrase \"/zai4/in tk/jiao4shi4/c1assroom\" is showed in Figure 6.",
        "Here, \" /jiao4shi4/classroom\" is the predicted 0, and B and R are the same word \"/zai4/in\".",
        "In Chinese (as well as in English), the head word typically lies at the end of the phrase.",
        "This makes B more likely to be M than R, so the information redundancy between B and M is larger than that between B and R. 4.3 If M (the nearest preceding word modifying the predicted word 0) is one of the feature types of the baseline, MT (the modifying type between M and 0) will bring less information gain for word prediction.",
        "We are interested in knowing how much non-redundant information is present in MT if M is included in the baseline.",
        "To assess this, we conducted the following experiment, which focuses directly on the relationship between MT and the two words involved.",
        "We measured the information gain of MT over M to be only IG(MT;0IM)=0.110 bits, while the information redundancy of MT and M is a much larger IR(MT,M;0)=0.861 bits.",
        "This means that the prediction information for 0 in M (which at IQ(M;0)=2.237 bits is much larger, incidentally, than that in MT at IQ(MT;0)=0.971 bits) contains almost all the prediction information for 0 in MT.",
        "The corresponding",
        "linguistic explanation may be as follows.",
        "The lexical identities of the predicted word 0 and its modifying word M involved in a dependency relation determine to a large extent the type of modification relation MT that holds between 0 and its modifying Word M. Consider the sentence in Figure 7.",
        "In the phrase \"U /zuo2tian1/yesterday T /xia4wu3/afternoon\", just knowing the identity of the two words \"Ut X /zuo2tianl/yesterday\" and \" T /xia4NV,u3/afternoon\" is enough to predict with near certainty that the relation between them is time phrase (tp), thus giving the following dependency structure as Figure 7.",
        "4.4 If R (the nearest preceding word modified by the predicted word 0) is one of the feature types of the baseline, RT (the modifying type between R and 0) will bring less information gain for word prediction.",
        "This simply mirrors the immediately preceding point, except that R is the modified word (parent) instead of the modifying word (child).",
        "In this case, we measured the information gain of RT over R to be only IG(RT;0IR)=0.271' bits, while the information redundancy of RT and R is a much larger IR(RT,R;0)=0.683 bits.",
        "This means that the information in R (IQ(R;0)=1.581 bits) contains almost all the information in MT (IQ(RT;0)=0.954 bits).",
        "The corresponding linguistic explanation is as follows.",
        "The lexical identities of the words (R, 0) involved in a dependency relation determine to a large extent the type of modification relation RT that holds between 0 and the word it modifies, R. Consider the sentence in Figure 8, the identity of the words \"\"q/xie3/write\" and \"it)/lun4wen2/paper\" determine with near certainty that their relationship is verb phrase (vp): 4.5 Among the feature types in {B, BP, M, MP, MT, R, RP, RT}, the preference order for selecting feature types is B, R, M, RT, MT, BP, RP, MP.",
        "We used the metric IG to obtain a ranking for feature types according to their preklictiveness.",
        "This ranking only considers information gain; it ignores complexity (for a practical application, we would also consider the complexity of the model at the same time.).",
        "To obtain this order, we performed a greedy search where at each step we selected the next most informative feature type (i.e., the feature type that has the largest information gain).",
        "The empirical information gain measurements in each search step is shown in Table 4, where the feature which has the boldface IG in each column is the feature type selected in that step, and IG(F;01Null)=IQ(F;0).",
        "This preference ordering can serve as a guideline for selecting feature type combinations in a language model.",
        "That is to say, given the",
        "feature type set {B, BP, M, MP, MT, R, RP, RT }, if a language model uses only one feature type, feature type B should be used; if a language model uses two feature types, the feature type combination (B, R} should be used, and so on.",
        "However, we can see from Figure 9 that the additional information gain falls off rapidly when more than three feature types are selected."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "We have described a series of corpus-based analyses that take a Chinese treebank and quantify the information gain and the information redundancy for various feature types combinations involving both dependency and bigram feature types.",
        "The analysis yields several interesting conclusions that explain linguistic observations from an information theoretic point of view, and in addition will find practical use in the design of language models.",
        "Although perhaps we have been aware of some of the observations to varying extents, here we introduce a methodology that uses concrete evidence drawn from real contexts in order to give more reliable and objective results.",
        "We have already begun conducting similar experiments on an English training corpusi61, which so far yield the same types of behavior described in this paper.",
        "We aim to discover which, if any, claims about the information present in dependency based features are peculiar to Chinese language, which are peculiar to English, and which are common across multiple languages.",
        "Based on the analysis, we will design, construct, and incrementally refine new language models for written and spoken English and Chinese that incorporate varying levels of linguistic structure.",
        "These models will aim to capture regularities that arise from long-distance dependencies, which n-gram models cannot represent.",
        "At the same time, we will retain as many of the n-gram parameters as needed to capture important lexical dependencies."
      ]
    }
  ]
}
