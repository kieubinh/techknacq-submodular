{
  "info": {
    "authors": [
      "Andre Kempe"
    ],
    "book": "Workshop on Computational Natural Language Learning CoNLL",
    "id": "acl-W99-0702",
    "title": "Experiments in Unsupervised Entropy-Based Corpus Segmentation",
    "url": "https://aclweb.org/anthology/W99-0702",
    "year": 1999
  },
  "references": [
    "acl-W98-1210"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The paper presents an entropy-based approach to segment a corpus into words, when no additional information about the corpus or the language, and no other resources such as a lexicon or grammar are available.",
        "To segment the corpus, the algorithm searches for separators, without knowing a priory by which symbols they are constituted.",
        "Good results can be obtained with corpora containing \"clearly perceptible\" separators such as blank or newline."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The paper presents an approach to segment a corpus into words, based on entropy.",
        "We assume that the corpus is not annotated with additional information, and that we have no information whatsoever about the corpus or the language, and no linguistic resources such as a lexicon or grammar.",
        "Such a situation may occur e.g. if there is a (sufficiently large) corpus of an unknown or unidentified language and alphabet.'",
        "Based on entropy, we search for separators, without knowing a priory by which symbols or sequences of symbols they are constituted.",
        "Over the last decades, entropy has frequently been used to segment corpora [Wolff, 1977, Alder, 1988, Hutchens and Alder, 1998, among many others].",
        "and it is commonly used with compression techniques.",
        "Harris [1955] proposed an approach for segmenting words into morphemes that, although it did not use entropy, was based on an intuitively similar concept: Every symbol of a word is annotated with the count of all possible successor symbols given the substring that ends with the current symbol, and with the count of all possible predecessor symbols 'Such a corpus can be electronically encoded with arbi-given the tail of the word that starts with the current symbol.",
        "Maxima in these counts are used to segment the word into morphemes.",
        "All steps of the present approach will be described on the example of a German corpus.",
        "In addition, we will give results obtained on modified versions of this corpus, and on an English corpus."
      ]
    },
    {
      "heading": "2 The Approach",
      "text": []
    },
    {
      "heading": "2.1 The Corpus",
      "text": [
        "We assume that any corpus C can be described by the expression: C = S* T[S+ T ]* S* (1) There must be at least one token T (\"word\") which is a string of one or more symbols s:",
        "Different tokens T must be separated form each other by one or more separators S which are strings of zero or more symbols s",
        "Separators can consist of blanks, new-line, or -real\" symbols.",
        "They can also be empty strings."
      ]
    },
    {
      "heading": "2.2 Recoding the Corpus",
      "text": [
        "We will describe the approach on the example of a German corpus.",
        "First, all symbols s (actually all character codes) of the corpus are recoded by strings of \"visible\" ASCII characters.",
        "For example:2",
        "If the language and the alphabet are unknown or unidentified, the symbols of the corpus can be encoded by arbitrarily defined ASCII strings."
      ]
    },
    {
      "heading": "2.3 Information and Entropy",
      "text": [
        "We estimate probabilities of symbols of the corpus using a 3rd order Markov model based on maximum likelihood.",
        "The probability of a symbol s with respect to this model M and to a context c can be estimated by:",
        "The information of a symbol s with respect to the model M and to a context c is defined by:",
        "Intuitively, information can be considered as the surprise of the model about the symbol s after having seen the context c. The more the symbol is unexpected from the model's experience, the higher is the value of information [Shannon and Weaver, 1949].",
        "The entropy of a context c with respect to this model M expresses the expected value of information, and is defined by:",
        "Monitoring entropy and information across a corpus shows that maxima often correspond with word",
        "boundaries [Alder, 1988, Hutchens and Alder, 1998, among many others].",
        "More exactly, maxima in left-to-right entropy HLR and information ILR often mark the end of a separator string S, and maxima in right-to-left entropy HRL and information IRL often mark the beginning of a separator string, as can be seen in Figure 1.",
        "Here, an information value is assigned to every symbol.",
        "This value expresses the information of the symbol in a given left or right context.",
        "An entropy value is assigned between every two symbols.",
        "It expresses the model's uncertainty after having seen the left or right context, but not yet the symbol.",
        "When going from left to right, an end of a separator, is often marked by a maximum in entropy because the next word to the right can start with almost any symbol, and the model has no \"idea\" what it will be.",
        "There is also a maximum in information because the first symbol of the word is (more or less) unexpected; the model has no particular expectation.",
        "Similarly, when going from right to left, a beginning of a separator is often marked by a maximum in entropy because the word next to the left can end with almost any symbol.",
        "There is also a maximum in information because the last symbol of the word is (more or less) unexpected; the model has no particular expectation.",
        "Usually, there is no maximum at a beginning of a separator, when going from left to right, and no maximum at a separator ending, when going from right to left, because words often have \"typical\" beginnings or endings, e.g. prefixes or suffixes.",
        "This means, when we come from inside a word to the beginning or end of this word then the model will anticipate a separator, and since the number of alternative separators is usually small, the model will not be \"surprised\" to see a particular one.",
        "On the other side, when we come from inside a separator to the beginning or end of this separator, although the model will expect a word, it will be \"surprised\" about any particular word because the number of alternative beginnings or endings of words is large.",
        "It also may be observed that the maxima in one direction are bigger then the maxima in the other direction due to the fact that a particular language may have e.g. stronger constraints on endings than on beginnings of words: A language may employ suffixes with most words in a corpus, which limits the number of endings, but rarely use prefixes, which allows a word to start with almost any symbol."
      ]
    },
    {
      "heading": "2.4 Thresholds",
      "text": [
        "Not all maxima correspond with word boundaries.",
        "Hutchens and Alder [1998] apply a threshold of 0.5 1092 11E11 to select among all maxima, those that represent boundaries.",
        "The present approach uses two thresholds that are based on the corpus data and contain no other factors: The first threshold Tall is the average of all values of the particular function, H --LR, HRL, ILR, or IRL, across the corpus.",
        "The second threshold rma, is the average of all maxima of the particular function.",
        "All graphs of Figure 1 contain both thresholds (as dotted lines).",
        "To decide whether a value v of HLR, HRL, or IRL should be considered as a boundary, we use the four functions:"
      ]
    },
    {
      "heading": "2.5 Detection of Separators",
      "text": [
        "To find a separator, we are looking for a strong boundary to serve as a beginning or end of the separator.",
        "In the current example, we have chosen as a criterion for strong boundaries:",
        "Here H and I mean either HLR and ILR if we are looking for the end of a separator, or HRL and IRL if we are looking for the beginning of a separator.",
        "The variables h and i denote values of these functions at the considered point.",
        "Once a strong boundary is found, we search for a weak boundary to serve as an ending that matches the previously found beginning, or to serve as a beginning that matches the previously found ending For weak boundaries, we use the criterion:",
        "If a matching pair of boundaries, i.e. a beginning and an end of a separator, are found, the separator",
        "is marked.",
        "In Figure 1 this is visualized by I for empty and { } for non-empty separators.",
        "The search for a weak boundary that matches a strong one is stopped (without success) either after a certain distance.'",
        "or at a breakpoint.",
        "For example, if we have the beginning of a separator and search for a matching end then the occurrence of another beginning will stop the search.",
        "As a criterion for a breakpoint we have chosen:",
        "If the search for a matching point has been stopped for either reason, we need to decide whether the initially found strong boundary should be marked despite the missing match.",
        "It will only be marked if it is an obligatory boundary.",
        "Here we apply the criterion:",
        "In Figure 1 these unmatched obligatory boundaries are visualized by {u or }u.",
        "Each of the four criteria, for strong boundaries, weak boundaries, break points, and obligatory boundaries, can be built of any of the four functions boo to b30 (eq.s 7 to 10)."
      ]
    },
    {
      "heading": "2.6 Validation of Separators",
      "text": [
        "All separator strings that have a matching beginning and end marker are collected and counted.",
        "4 In the example, the maximal separator length is set to 6.",
        "This seems sufficient because we found no separators longer than 3 so far (Tables 1 to 5).",
        "Table 1 shows such separators collected from the German example corpus.",
        "Column 5 contains the strings that constitute the separators, column 2 shows the count of these strings as separators, col-unm 3 says in how may different contexts' the separators occurred, colunm 4 shows the total count of the strings in the corpus, and column 1 contains aliases furtheron used to denote the separators.",
        "In Table 1 all separators are sorted with respect to column 3.",
        "From these separators we retain those that are above a defined threshold relative to the number of different contexts of the topmost separator.",
        "In all examples throughout this article, we are using a relative threshold of 0.5, which means in this case (Table 1) that the topmost two separators, \"BL\" and \"NL\" that occur in 1484 and 850 different contexts respectively, are retained.6 In the corpus, all separators that have been retained (Table 1) and that have at least one detected boundary (Fig.",
        "1), are validated and marked.",
        "For the above corpus section this leads to:"
      ]
    },
    {
      "heading": "2.7 Recall of Separators",
      "text": [
        "For the above corpus we measured a recall of 86.0 % for both blank (BL) and newline (NL) together (Table 2).",
        "Due to the approach, the precision for BL and NL is 100 %.",
        "A string which is different from BL and NL cannot be marked as a separator in the above example.",
        "If empty string separators were admitted, the precision would decrease.",
        "We applied the approach to modified versions of the above mentioned German corpus and to an English corpus."
      ]
    },
    {
      "heading": "3.1 German with Empty String Separators",
      "text": [
        "For this experiment, we remove all original separators, \"BL\" and \"NL\", from the above German corpus:",
        "and obtained the result: bst bandsetz lou bng buud loNen 6bau bder 6Kanalis loation dfirf loten 6 nden bnich bsten loze haJahren loBeträg be bin loMillia rd 6en lohah loeaus bge logeben 6 w erden lo.",
        "All be bin bind ben balte n 10Bun 6 d lo es bland loer lonmiisse n bbiszur 10Jahrh bund ber lotwen de bdie bKommunen bkmde los loin s bgesam 6 t bkm blangen 6Kanal-und bLei bt bungs bnetzessan tole ren 10.",
        "For the next experiment, we changed all original separators, \"BL\" and \"NL\", in the above German corpus into a string from the list { \" \", \"-\" , \".# _ _If #1), tg... _it } :7 Fiir---Instandsetzung##und--N eubauder-Kanalisation--diirfte n#in##den---nachsten##zehn- -JahrenBetrage-in--Milliardenh Ohe#ausgegeben##werden.",
        "--- A llein##in--denalten-Bundeslan dern--miissen#bis##zur---Jahr hundertwende##die--Kommune nkm-des--insgesamt#km##lang en---Kanal-##und--Leitungsne tzessanieren."
      ]
    },
    {
      "heading": "3.3 English Corpus",
      "text": [
        "On an English corpus where all original separators have been preserved:8 In the days when the spinning-wheels \\ hummed busily in the farmhouses and even great ladies clothed \\ in silk and thread lace had their toy spinning-wheels \\ of polished oak there might be seen in \\ districts far away among the lanes or deep in the bosom of the hills certain \\ pallid undersized men who by the side of the brawny \\ country-folk looked like the remnants of a disinherited race.",
        "and obtained the result: In Jo the Jo days Jo when Jo the 6spinn ing-wheels Jo hummed Jo busily Jo in Jo the Ii farmhouses band loeven Jog reat loladies 6clothed Join 6silk 6 andNLthread Jo lace 6had Jo their toy lospinning-wheelsBLof 6polis Ii ed looak 6there Jo might Jo be Jo see n Jo ii Jo districts lofar 6awayBLam ong 6the 61anesNLor 6deep 6in 6 t Ii e lohosom loot' 1othe 10 hills locerta in lopallid loundersized limen 6wh o Jo by 6the 6sideBLof 6the 6braw ny Jo country-folk Jo looked lilike 6 the loremnantsBLof 6a 6disinheri ted Jo race."
      ]
    },
    {
      "heading": "4 Conclusion and Future Investigations",
      "text": [
        "The paper attempted to show that entropy and information can be used to segment a corpus into words, when no additional knowledge about the corpus or the language, and no other resources such as a lexicon or grammar are available.",
        "To segment the corpus, the algorithm searches for separators, without knowing a priory by which symbols or sequences of symbols they are constituted.",
        "Good results were obtained with a German and an English corpus with \"clearly perceptible\" separators (blank and new-line).",
        "Precision and recall decrease if the original separators of these corpora are removed or changed into a set of different co-occurring separators.",
        "So far, only separators and their frequencies have been taken into account.",
        "Future investigations may include:",
        "• the use of frequencies of tokens and their different alternative contexts, to validate these tokens and the adjacent separators.",
        "and • a search for criteria (based on the corpus itself and on the obtained result) to evaluate the \"quality\" of segmentation, thus enabling a self-optimizing approach."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "Many thanks to the anonymous reviewers of my article and to my colleagues."
      ]
    }
  ]
}
